<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>reinforcement-learning on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/tags/reinforcement-learning/</link>
        <description>Recent content in reinforcement-learning on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 14 Mar 2023 16:21:23 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Actor-Critic</title>
        <link>https://roykesydon.github.io/Blog/p/actor-critic/</link>
        <pubDate>Tue, 14 Mar 2023 16:21:23 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/actor-critic/</guid>
        <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;結合 policy-based 和 value-based&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A3C
&lt;ul&gt;
&lt;li&gt;Actor-Critic 最知名的方法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantage Actor-Critic 是 A2C&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;advantage-actor-critic&#34;&gt;Advantage Actor-Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Review: Policy gradient&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(\displaystyle\sum_{t^{&amp;rsquo;}=t}^{T_n}\gamma^{t^{&amp;rsquo;}-t}r_{t^{&amp;rsquo;}}^n-b)\triangledown log p_{\theta}(a_t^n|s_t^n)$
&lt;ul&gt;
&lt;li&gt;$G_t^n=\displaystyle\sum_{t^{&amp;rsquo;}=t}^{T_n}\gamma^{t^{&amp;rsquo;}-t}r_{t^{&amp;rsquo;}}^n-b$
&lt;ul&gt;
&lt;li&gt;G very unstable，因為給同樣的 state 作同樣的 action 不一定會得到同樣的結果，G 是個 random variable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;想要改獲得期望值，取代掉 sample 的值(G 的部分)，可以用 Q-Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E[G_t^n]=Q^{\pi_\theta}(s_t^n,a_t^n)$
&lt;ul&gt;
&lt;li&gt;Q function 這樣定義&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;所以我們可以把 G 的部分改用 Q 替換掉，就可以把 Actor 和 Critic 結合起來&lt;/li&gt;
&lt;li&gt;baseline 的部分也可以用 value function 替換掉&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;但用 $Q^{\pi}(s_t^n,a_t^n)-V^{\pi}(s_t^n)$ 要一次 estimate 兩個 network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以把 Q 以 V 來表示，那只需要估測 V
&lt;ul&gt;
&lt;li&gt;$Q^{\pi}(s_t^n,a_t^n)=E[r_t^n+V^{\pi}(s_{t+1}^n)]$&lt;/li&gt;
&lt;li&gt;雖然有隨機性(獲得的 reward 和跳到什麼 state 不一定)，但先不管期望值 $Q^{\pi}(s_t^n,a_t^n)=r_t^n+V^{\pi}(s_{t+1}^n)$&lt;/li&gt;
&lt;li&gt;現在雖然多個一個 r，有一些 variance，但也比 G 好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(r_t^n+V^{\pi}(s_{t+1}^n)-V^{\pi}(s_t^n))\triangledown log p_{\theta}(a_t^n|s_t^n)$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/actor-critic/A2C.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;tips&#34;&gt;Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;actor $\pi(s)$ 和 critic $V^{\pi}(s)$ 的權重可以共享&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;前面幾個 layer 可以 share&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對 $\pi$ 的 output 下 constrain，讓他的 entropy 不要太小，達到 exploration 的效果&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;asynchronous-advantage-actor-critic&#34;&gt;Asynchronous Advantage Actor-Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;一開始有個 global network，開一堆 worker，每次工作前，把 global network 的參數 copy 過去&lt;/li&gt;
&lt;li&gt;個別去和環境作互動，更新的梯度施加在 global network 上&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pathwise-derivative-policy-gradient&#34;&gt;Pathwise Derivative Policy Gradient&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可以當作是 Q-Learning 解 continuous action 的一種方法&lt;/li&gt;
&lt;li&gt;訓練一個 actor，目標是生出的 a 餵給 Q 後，可以讓 Q function 的輸出越大越好
&lt;ul&gt;
&lt;li&gt;只會調 actor 的參數，會 fix Q 的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;就是個 GAN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/actor-critic/pathwise.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在每個 episode&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對於每個 time step t
&lt;ul&gt;
&lt;li&gt;⚠️給 state $s_t$，根據 $\pi$ 執行 action $a_t$ (epsilon greedy)⚠️&lt;/li&gt;
&lt;li&gt;獲得 reward $r_t$，到達 $s_{t+1}$&lt;/li&gt;
&lt;li&gt;把 {$s_t,a_t,r_t,s_{t+1}$} 存到 buffer&lt;/li&gt;
&lt;li&gt;從 buffer sample {$s_t,a_t,r_t,s_{t+1}$}(通常是一個 batch)&lt;/li&gt;
&lt;li&gt;⚠️Target $y=r_i+\hat{Q}(s_{i+1},\hat{\pi}(s_{i+1}))$⚠️&lt;/li&gt;
&lt;li&gt;Update Q 的參數，好讓 $Q(s_i,a_i)$ 更接近 y(regression)&lt;/li&gt;
&lt;li&gt;⚠️Update $\pi$ 的參數，讓 $Q(s_i,\pi(s_i))$ 最大化⚠️&lt;/li&gt;
&lt;li&gt;每 C 步 reset $\hat{Q}=Q$&lt;/li&gt;
&lt;li&gt;⚠️每 C 步 reset $\hat{\pi}=\pi$⚠️&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;⚠️ 是和 Q-Learning 不一樣的地方&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Q-learning</title>
        <link>https://roykesydon.github.io/Blog/p/q-learning/</link>
        <pubDate>Mon, 20 Feb 2023 16:21:23 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/q-learning/</guid>
        <description>&lt;h2 id=&#34;rl-方法&#34;&gt;RL 方法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Policy-based
&lt;ul&gt;
&lt;li&gt;learn 做事的 actor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Value-based
&lt;ul&gt;
&lt;li&gt;不直接 learn policy，而是 Learn critic，負責批評&lt;/li&gt;
&lt;li&gt;Q-learning 屬於這種&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;critic&#34;&gt;Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;不直接決定 action&lt;/li&gt;
&lt;li&gt;給予 actor $\pi$，評估 actor $\pi$ 有多好&lt;/li&gt;
&lt;li&gt;critic 的 output 依賴於 actor 的表現&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state-value-function&#34;&gt;State Value Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;State value function $V^{\pi}(s)$
&lt;ul&gt;
&lt;li&gt;用 actor $\pi$，看到 s 後玩到結束，cumulated reward expectation 是多少&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;評估方法&#34;&gt;評估方法&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Monte-Carlo(MC) based approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;critic 看 $\pi$ 玩遊戲&lt;/li&gt;
&lt;li&gt;訓練一個 network，看到不同的 state ，輸出 cumulated reward(直到遊戲結束，以下稱為 $G_a$)，解 regression 問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal-difference(TD) approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC 的方法至少要玩到遊戲結束才可以 update network，但有些遊戲超長
&lt;ul&gt;
&lt;li&gt;TD 只需要 {$s_t,a_t,r_t,s_{t+1}$}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$V^{\pi}(s_t)=V^{\pi}(s_{t+1})+r_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MS v.s. TD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC
&lt;ul&gt;
&lt;li&gt;Larger variance
&lt;ul&gt;
&lt;li&gt;每次的輸出差異很大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD
&lt;ul&gt;
&lt;li&gt;smaller variance
&lt;ul&gt;
&lt;li&gt;相較 $G_a$ 較小，因為這邊的 random variable 是 r，但 $G_a$ 是由很多 r 組合而成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V 可能估得不準確
&lt;ul&gt;
&lt;li&gt;那 learn 出來的結果自然也不准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;較常見&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;another-critic&#34;&gt;Another Critic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;State-action value function $Q^\pi(s,a)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;又叫 Q function&lt;/li&gt;
&lt;li&gt;當用 actor $\pi$ 時，在 state s 採取 a 這個 action 後的 cumulated reward expectation
&lt;ul&gt;
&lt;li&gt;有一個要注意的地方是，actor 看到 s 不一定會採取 a&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/q-function.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/how-to-use-q.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只要有 Q function，就可以找到&amp;quot;更好的&amp;quot; policy，再替換掉原本的 policy
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;更好的&amp;quot;定義
&lt;ul&gt;
&lt;li&gt;$V^{\pi^{&amp;rsquo;}} \ge V^{\pi}(s), \text{for all state s}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\pi^{&amp;rsquo;}(s)=arg \underset{a}{max}Q^{\pi}(s,a)$
&lt;ul&gt;
&lt;li&gt;$\pi^{&amp;rsquo;}$ 沒有多餘的參數，就單純靠 Q function 推出來&lt;/li&gt;
&lt;li&gt;這邊如果 a 是 continuous 的會有問題，等等解決&lt;/li&gt;
&lt;li&gt;這樣就可以達到&amp;quot;更好的&amp;quot;policy，不過就不列證明了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basic-tip&#34;&gt;Basic Tip&lt;/h2&gt;
&lt;h3 id=&#34;target-network&#34;&gt;Target network&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在 training 的時候，把其中一個 Q 固定住，不然要學的 target 是不固定的，會不好 train&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/target-network.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;exploration&#34;&gt;Exploration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;policy 完全 depend on Q function&lt;/li&gt;
&lt;li&gt;如果 action 總是固定，這不是好的 data collection 方法，要在 s 採取 a 過，才比較好估計 Q(s, a)，如果 Q function 是 table 就根本不可能估出來，network 也會有一樣的問題，只是沒那麼嚴重。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;解法&#34;&gt;解法&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Epsilon Greedy
&lt;ul&gt;
&lt;li&gt;$a=\begin{cases}
arg \underset{a}{max}Q(s,a), &amp;amp; \text{with probability } 1-\varepsilon \\
random, &amp;amp; otherwise
\end{cases}$&lt;/li&gt;
&lt;li&gt;通常 $\varepsilon$ 會隨時間遞減，因為你一開始 train 的時候不知道怎麼比較好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Boltzmann Exploration
&lt;ul&gt;
&lt;li&gt;$P(a|s)=\frac{exp(Q(s,a))}{\sum_a exp(Q(s,a))}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;replay-buffer&#34;&gt;Replay Buffer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;把一堆的 {$s_t,a_t,r_t,s_{t+1}$} 存放在一個 buffer&lt;/li&gt;
&lt;li&gt;{$s_t,a_t,r_t,s_{t+1}$} 簡稱為 exp&lt;/li&gt;
&lt;li&gt;裡面的 exp 可能來自於不同的 policy&lt;/li&gt;
&lt;li&gt;在 buffer 裝滿的時候才把舊的資料丟掉&lt;/li&gt;
&lt;li&gt;每次從 buffer 隨機挑一個 batch 出來，update Q function&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;好處&#34;&gt;好處&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;跟環境作互動很花時間，這樣可以減少跟環境作互動的次數&lt;/li&gt;
&lt;li&gt;本來就希望 batch 裡的 data 越 diverse 越好，不會希望 batch 裡的 data 都是同性質的&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;issue&#34;&gt;issue&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;我們要觀察 $\pi$ 的 value，混雜了一些不是 $\pi$ 的 exp 到底有沒有關係?
&lt;ul&gt;
&lt;li&gt;理論上沒問題，但李老師沒解釋&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;typical-q-learning-演算法&#34;&gt;Typical Q-learning 演算法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;初始化 Q-fucntion Q，target Q-function $\hat{Q}=Q$&lt;/li&gt;
&lt;li&gt;在每個 episode
&lt;ul&gt;
&lt;li&gt;對於每個 time step t
&lt;ul&gt;
&lt;li&gt;給 state $s_t$，根據 Q 執行 action $a_t$ (epsilon greedy)&lt;/li&gt;
&lt;li&gt;獲得 reward $r_t$，到達 $s_{t+1}$&lt;/li&gt;
&lt;li&gt;把 {$s_t,a_t,r_t,s_{t+1}$} 存到 buffer&lt;/li&gt;
&lt;li&gt;從 buffer sample {$s_t,a_t,r_t,s_{t+1}$}(通常是一個 batch)&lt;/li&gt;
&lt;li&gt;Target $y=r_i+\underset{a}{max}\hat{Q}(s_{i+1},a)$&lt;/li&gt;
&lt;li&gt;Update Q 的參數，好讓 $Q(s_i,a_i)$ 更接近 y(regression)&lt;/li&gt;
&lt;li&gt;每 C 步 reset $\hat{Q}=Q$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;adveanced-tip&#34;&gt;Adveanced Tip&lt;/h2&gt;
&lt;h3 id=&#34;double-dqn&#34;&gt;Double DQN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q Value 往往被高估
&lt;ul&gt;
&lt;li&gt;我們的目的是要讓 $Q(s_t, a_t)$ 和 $r_t+\underset{a}{max}Q(s_{t+1},a)$ 越接近越好(後者就是 target)&lt;/li&gt;
&lt;li&gt;target 常常不小心設太高，因為如果有 action 被高估了，就會選那個當 target&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Double DQN: 兩個函式 $Q$ 和 $Q^{&amp;rsquo;}$
&lt;ul&gt;
&lt;li&gt;把 target 換成  $r_t+Q^{&amp;rsquo;}(s_{t+1},arg \underset{a}{max}Q(s_{t+1},a))$&lt;/li&gt;
&lt;li&gt;選 action 交給 $Q$，實際算交給 $Q^{&amp;rsquo;}$
&lt;ul&gt;
&lt;li&gt;如果 $Q$ 選了高估的 action，$Q^{&amp;rsquo;}$ 有可能修正回來&lt;/li&gt;
&lt;li&gt;如果 $Q^{&amp;rsquo;}$ 高估，$Q$ 不一定會選到&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$Q^{&amp;rsquo;}$ 是 target network(固定不動)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dueling-dqn&#34;&gt;Dueling DQN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;改變 network 架構&lt;/li&gt;
&lt;li&gt;分成兩條 path
&lt;ul&gt;
&lt;li&gt;第一條算 scalar&lt;/li&gt;
&lt;li&gt;第二條算 vector，每個 action 都有個 value&lt;/li&gt;
&lt;li&gt;把 scalar 加到每一個維度&lt;/li&gt;
&lt;li&gt;只更改到 V(s) 的時候，會全部的 action 都改到，可能會是一個比較有效率的方式，不用 sample 所有的 action
&lt;ul&gt;
&lt;li&gt;但有可能模型不管 V(s)，直接設 0，只改 A&lt;/li&gt;
&lt;li&gt;所以會對 A 下 constrain，讓 network 傾向於改 V
&lt;ul&gt;
&lt;li&gt;比如同個 state 下的所有 action 要生出 A(s,a) 總和為 0
&lt;ul&gt;
&lt;li&gt;在 A 的輸出加個 normalization 即可辦到，這個 normalization 就是把每個維度都減掉平均&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/dueling-dqn.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;prioritized-replay&#34;&gt;Prioritized Replay&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;原本是 uniform 的從 buffer sample data&lt;/li&gt;
&lt;li&gt;改讓 「有更大的 TD error」的 data 有更高的機率被 sample
&lt;ul&gt;
&lt;li&gt;TD error 就是 $Q(s_t, a_t)$ 和 target 的差距&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;實際在做的時候有額外的細節，不會只改 sampling 的 process，還要改 update 參數的方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-step&#34;&gt;Multi-step&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Balance between MC 和 TD&lt;/li&gt;
&lt;li&gt;TD 只需要存 {$s_t,a_t,r_t,s_{t+1}$}&lt;/li&gt;
&lt;li&gt;改存 {$s_t,a_t,r_t,&amp;hellip;,s_{t+N},a_{t+N},r_{t+N}, s_{t+N+1}$}&lt;/li&gt;
&lt;li&gt;我們的目的是要讓 $Q(s_t, a_t)$ 和 $\displaystyle\sum_{t^{&amp;rsquo;}=t}^{t+N} r_{t^{&amp;rsquo;}}+\hat{Q}(s_{t+N+1},a_{t+N+1})$ 越接近越好(後者就是 target)
&lt;ul&gt;
&lt;li&gt;$a_{t+N+1}=arg\underset{a}{max}\hat{Q}(s_{t+N+1},a)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;同時有 MC 和 TD 的好處和壞處
&lt;ul&gt;
&lt;li&gt;估測的影響比較輕微&lt;/li&gt;
&lt;li&gt;r 比較多項，variance 比較大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;noisy-net&#34;&gt;Noisy Net&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;improve exploration&lt;/li&gt;
&lt;li&gt;Noise on Action
&lt;ul&gt;
&lt;li&gt;Epsilon Greedy(之前的回顧)
&lt;ul&gt;
&lt;li&gt;$f_X(x) = \begin{cases}
arg \underset{a}{max}Q(s,a), &amp;amp; \text{with probability }1-\varepsilon \\
random, &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;給同樣的 state，採取的 action 不一定一樣&lt;/li&gt;
&lt;li&gt;沒有真實的 policy 會這樣運作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Noise on Parameters
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$a = arg \underset{a}{max}\tilde{Q}(s,a)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在每個 episode 剛開始的時候，在 Q-function 的參數上面加上 gaussian noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;給同樣的 state，採取同樣的 action&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;叫做 state-dependent exploration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;explore in a consistent way&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distributional-q-function&#34;&gt;Distributional Q-function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q-function 生出的東西是 cumulated reward 的期望值
&lt;ul&gt;
&lt;li&gt;所以我們是在對 distribution 取 mean，但不同的 distribution 也可能有同樣的 mean&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;想做的事情是 model distribution&lt;/li&gt;
&lt;li&gt;如果有做這個，就比較不會有 over estimate reward 的結果，反而容易 under estimate，使 double 比較沒用
&lt;ul&gt;
&lt;li&gt;output 的 range 不可能無限寬，超過邊界的 reward 會被丟掉&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rainbow&#34;&gt;Rainbow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;綜合一堆方法&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;continuous-actions&#34;&gt;Continuous actions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Q learning 不容易處理 continuous action&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;sample n 個可能的 a，都丟 Q function 看誰最大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;gradient descent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把 a 當作 parameter，要找一組 a 去 maximize Q function
&lt;ul&gt;
&lt;li&gt;運算量大，要 iterative 的 update a&lt;/li&gt;
&lt;li&gt;不一定可以找到 global 的最佳解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;特別設計 Q network，讓解 optimization 的問題變容易&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;範例
&lt;ul&gt;
&lt;li&gt;Q network 輸出 $\mu(s)$、$\Sigma(s)$、$V(s)$，個別是 vector、matrix、scalar&lt;/li&gt;
&lt;li&gt;a 是 continuous 的 Action，是一個 vector，每個維度都是實數&lt;/li&gt;
&lt;li&gt;$\Sigma(s)$ 是 positive definite 的，實作的時候會把 $\Sigma$ 和它的 transpose 相乘&lt;/li&gt;
&lt;li&gt;$Q(s,a)=-(a-\mu(s))^T\Sigma(s)(a-\mu(s))+V(s)$&lt;/li&gt;
&lt;li&gt;$(a-\mu(s))^T\Sigma(s)(a-\mu(s))$ 這項必為正，所以 $a=\mu(s)$ 的時候就是最佳解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不要用 Q-learning&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Proximal Policy Optimization(PPO)</title>
        <link>https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/</link>
        <pubDate>Mon, 20 Feb 2023 12:35:56 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/</guid>
        <description>&lt;h1 id=&#34;onoff-policy&#34;&gt;On/Off-policy&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;On-policy
&lt;ul&gt;
&lt;li&gt;學習的 agent 和與環境互動的 agent 是同一個&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Off-policy
&lt;ul&gt;
&lt;li&gt;學習的 agent 和與環境互動的 agent 是不同個&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;想從-on-policy-轉-off-policy&#34;&gt;想從 On-policy 轉 Off-policy&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;On-policy 每次都要重新蒐集資料，很花時間&lt;/li&gt;
&lt;li&gt;由另一個 $\pi_{\theta^{&amp;rsquo;}}$ 去 train $\theta$，$\theta^{&amp;rsquo;}$是固定的，所以我們可以 re-use sample data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;是一個 general 的想法，不限於 RL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$E_{x \text{\textasciitilde} p}[f(x)]\approx \frac{1}{N}\displaystyle\sum_{i=1}^N f(x^i)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x^i$ is sampled from p(x)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我們遇到的問題是沒辦法從 p 來 sample data，只能透過 q(x) 去 sample $x^i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以把上式改寫成 $E_{x \text{\textasciitilde} p}[f(x)]=E_{x \text{\textasciitilde} q}[f(x)\frac{p(x)}{q(x)}]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;雖然理論上 q 可以任意選，只要不要 q(x) 是 0 的時候 p(x) 不是 0，實作上 p 和 q 不能差太多，不然會有問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這兩項的 Variance 不一樣，如果 p 除以 q 差距很大，右邊的 Variance 會很大，如果 sample 不夠多次就會有問題&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/ppo/importance-sample-issue.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;轉換&#34;&gt;轉換&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}=E_{\tau \text{\textasciitilde}p_{\theta}(\tau)}[R(\tau)\triangledown log p_{\theta} (\tau)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;改為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}=E_{\tau \text{\textasciitilde}p_{\theta^{&amp;rsquo;}}(\tau)}[\frac{p_{\theta}(\tau)}{p_{\theta^{&amp;rsquo;}}(\tau)}R(\tau)\triangledown log p_{\theta} (\tau)]$&lt;/li&gt;
&lt;li&gt;從 $\theta^{&amp;rsquo;}$ sample 資料&lt;/li&gt;
&lt;li&gt;更新 $\theta$ 多次&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantage-function&#34;&gt;Advantage function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta}}[A^{\theta}(s_t,a_t)\triangledown log p_\theta(a_t^n|s_t^n)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;改為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{P_\theta(s_t,a_t)}{P_{\theta^{&amp;rsquo;}}(s_t,a_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)\triangledown log p_\theta(a_t^n|s_t^n)]$&lt;/li&gt;
&lt;li&gt;要注意 Advantage 的結果要由 $\theta^{&amp;rsquo;}$ 得出，是 $\theta^{&amp;rsquo;}$在和環境互動&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;新的 objective function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J^{\theta^{&amp;rsquo;}}(\theta)=E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{&amp;rsquo;}}(a_t|s_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ppo&#34;&gt;PPO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;確保 $\theta$ 和 $\theta^{&amp;rsquo;}$ 不會差太多&lt;/li&gt;
&lt;li&gt;$J_{PPO}^{\theta^{&amp;rsquo;}}(\theta)=J^{\theta^{&amp;rsquo;}}(\theta)-\beta KL(\theta, \theta^{&amp;rsquo;})$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;前身-trpo&#34;&gt;前身 TRPO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trust Region Policy Optimization&lt;/li&gt;
&lt;li&gt;$J_{TRPO}^{\theta^{&amp;rsquo;}}(\theta)=E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{&amp;rsquo;}}(a_t|s_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)], KL(\theta, \theta^{&amp;rsquo;})&amp;lt;\delta$&lt;/li&gt;
&lt;li&gt;constrain 很難處理&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kl-divergence&#34;&gt;KL divergence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;這邊不是 $\theta$ 和 $\theta^{&amp;rsquo;}$ 參數上的距離，而是 behavior 的距離
&lt;ul&gt;
&lt;li&gt;參數上的距離是指這兩個參數有多像&lt;/li&gt;
&lt;li&gt;是給同樣的 state 生出 action 的 distribution 要像&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;初始參數 $\theta^0$&lt;/li&gt;
&lt;li&gt;每個 iteration
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用 $\theta^k$ 和環境互動，蒐集{$s_t,a_t$}，並計算 advantage $A^{\theta^k}(s_t,a_t)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找出 theta 最佳化 $J_{PPO}(\theta)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta KL(\theta, \theta^{k})$&lt;/li&gt;
&lt;li&gt;可以更新很多次&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動態調整 $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adaptive KL Penalty&lt;/li&gt;
&lt;li&gt;設可接受的 KL 數值範圍&lt;/li&gt;
&lt;li&gt;if $KL(\theta,\theta^k)&amp;gt;KL_{max},\text{increase} \beta$&lt;/li&gt;
&lt;li&gt;if $KL(\theta,\theta^k)&amp;lt;KL_{min},\text{decrease} \beta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ppo2&#34;&gt;PPO2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta KL(\theta, \theta^{k})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PPO2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO2}^{\theta^{k}}(\theta)\approx \displaystyle\sum_{(s_t,a_t)}min(\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}A^{\theta^k}(s_t,a_t), \\
clip(\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}, 1-\varepsilon, 1+\varepsilon)A^{\theta^k}(s_t,a_t))$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/ppo/ppo2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Policy Gradient</title>
        <link>https://roykesydon.github.io/Blog/p/policy-gradient/</link>
        <pubDate>Sun, 19 Feb 2023 17:16:14 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/policy-gradient/</guid>
        <description>&lt;h1 id=&#34;basic-components&#34;&gt;Basic Components&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Actor
&lt;ul&gt;
&lt;li&gt;Policy $\pi$ is a network with parameter $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Env&lt;/li&gt;
&lt;li&gt;Reward Function&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;trajectory&#34;&gt;Trajectory&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/policy-gradient/aer.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在一場遊戲，把 env 輸出的 s 和 actor 輸出的 a 串起來，是一個 Trajectory&lt;/li&gt;
&lt;li&gt;Trajectory $\tau$ = {$s_1,a_1,s_2,a_2,&amp;hellip;,s_T,a_T$}&lt;/li&gt;
&lt;li&gt;$p_{\theta}(\tau)=p(s_1)\displaystyle\prod_{t=1}^Tp_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;update&#34;&gt;Update&lt;/h1&gt;
&lt;p&gt;$\theta \leftarrow \theta + \eta \triangledown \overline{R}_{\theta}$&lt;/p&gt;
&lt;p&gt;$\triangledown \overline{R_{\theta}} = \displaystyle\sum_{\tau} R(\tau) \triangledown p_{\theta} (\tau) \\
=\frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}R(\tau^n)\triangledown log p_{\theta} (a_t^n|s_t^n)$&lt;/p&gt;
&lt;h1 id=&#34;實作&#34;&gt;實作&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;常見公式
&lt;ul&gt;
&lt;li&gt;$\triangledown f(x)=f(x)\triangledown logf(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;用當前模型蒐集一堆 Trajectory&lt;/li&gt;
&lt;li&gt;更新模型&lt;/li&gt;
&lt;li&gt;回到第一步&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;細節
&lt;ul&gt;
&lt;li&gt;做一個分類問題，把 state 當作分類器的 Input，把 action 當作分類器的 ground truth 作訓練&lt;/li&gt;
&lt;li&gt;在實作分類問題的時候，objective function 都會寫成 minimize cross entropy，就是 maximize log likelihood&lt;/li&gt;
&lt;li&gt;RL 和一般分類的區別是，要記得在 loss 前面乘上 $R(\tau^n)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;tip&#34;&gt;Tip&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Add a Baseline
&lt;ul&gt;
&lt;li&gt;$R(\tau^n)$ 有可能永遠都為正
&lt;ul&gt;
&lt;li&gt;此時等於告訴 Model 說，今天不管是什麼 action，都要提高它的機率。不一定會有問題，因為雖然都是正的，但正的量有大有小，可能某些 action 上升的幅度會更大。因為我們是在做 sampling，不一定會 sample 到某些 action，本來想的情況是所有的 trajectory 都會出現才沒問題。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解法: 希望 reward 不要總是正的
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(R(\tau^n)-b)\triangledown log p_{\theta}(a_t^n|s_t^n)$&lt;/li&gt;
&lt;li&gt;$b \approx E[R(\tau)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Assign Suitable Credit
&lt;ul&gt;
&lt;li&gt;原本整場遊戲的所有 action 都會乘上 $R(\tau)$，但這不太公平，因為就算結果是好的，不代表所有 action 都是對的，反之亦然。在理想的情況下，如果 sample 夠多，就可以解決這問題。&lt;/li&gt;
&lt;li&gt;解法
&lt;ol&gt;
&lt;li&gt;只計算從這個 action 後的 reward 總和
&lt;ul&gt;
&lt;li&gt;因為前面的 reward 和你做了什麼沒關係&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;接續解法 1，把比較未來的 reward 做 discount
&lt;ul&gt;
&lt;li&gt;乘某個小於 1 的 $\gamma^{t^{&amp;rsquo;}-t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;advantage-function&#34;&gt;Advantage function&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;base 可以是 state-dependent，可以根據 network 得出，以後再說&lt;/li&gt;
&lt;li&gt;$(Reward-b)$ 可以合起來看做 Advantage function $A^{\theta}(s_t,a_t)$
&lt;ul&gt;
&lt;li&gt;這邊 Reward 不管你是什麼形式，有沒有 discount。&lt;/li&gt;
&lt;li&gt;它的意義是，這個 action 相較於其他的 action 有多好，而不是絕對好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;這個 A 通常可以由某個類神經網路估計，那個類神經網路叫做 critic，以後講 Actor-Critic 的時候再說&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
