<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>nlp on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/tags/nlp/</link>
        <description>Recent content in nlp on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 25 Dec 2023 00:00:13 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/tags/nlp/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ORQA 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/orqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 25 Dec 2023 00:00:13 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/orqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.00300&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Latent Retrieval for Weakly Supervised Open Domain Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;最近的工作常依賴於兩個假設，一個是對 supporting evidence 做 strong supervision，另一個是假設 blackbox information retrieval (IR) system 可以找到所有的 evidence candidates&lt;/p&gt;
&lt;p&gt;作者認為兩者都不是最理想的，因為 gold evidence 不總是可用，而且 QA 和 IR 有著根本上的不同&lt;/p&gt;
&lt;p&gt;作者首次證明，在沒有任何 IR 的情況下，可以從 question-answer pair 中共同學習 retriver 和 reader&lt;/p&gt;
&lt;p&gt;在這種 setting 下，來自 Wikipedia 的 evidence retrieval 被視作 latent variable&lt;/p&gt;
&lt;p&gt;由於 learn from scratch 不切實際，因此用 Inverse Cloze Task (ICT) 來預訓練 retriever&lt;/p&gt;
&lt;p&gt;作者對五個 QA 資料集進行評估&lt;/p&gt;
&lt;p&gt;在提問者已經知道答案的情況下，傳統的 IR 系統（比如 BM25）已足夠&lt;/p&gt;
&lt;p&gt;在使用者真正尋求答案的資料集上，作者顯示出 learned retrival 的重要性，在 exact match 上比 BM25 好 19 個點&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;由於閱讀理解系統的發展，人們對 open domain question answering (QA) 的興趣重新燃起&lt;/p&gt;
&lt;p&gt;其中 evidence 得從 open corpus 取得，而不是直接從輸入給入&lt;/p&gt;
&lt;p&gt;現有的方法需要 blackbox IR system 來完成大部分繁重的工作，即使它無法對下遊任務進行微調&lt;/p&gt;
&lt;p&gt;在 DrQA 推廣的強監督環境中，他們也假設了一個訓練在 question-answer-evidence triple 上的閱讀理解模型&lt;/p&gt;
&lt;p&gt;在某些人提出的 weakly supervised setting 中，他們假設 IR system 提供 noisy gold evidence&lt;/p&gt;
&lt;p&gt;這些方法利用 IR system 來大幅減少搜尋空間&lt;/p&gt;
&lt;p&gt;然而 QA 和 IR 有著根本性的差異&lt;/p&gt;
&lt;p&gt;雖然 IR 關心的是 lexical 和 semantic matching，但 question 的定義並不具體，而且需要更多 language understanding，因為 user 在找的是未知資訊&lt;/p&gt;
&lt;p&gt;我們應該直接 用 QA data 學習 retrieve，而不是受限於 blackbox IR system&lt;/p&gt;
&lt;p&gt;在本文中，作者介紹了第一個 OpenRetrieval Question Answering system (ORQA) 框架&lt;/p&gt;
&lt;p&gt;ORQA 學習從 open corpus retrieve evidence，並且只做 question-answer pair 的監督訓練&lt;/p&gt;
&lt;p&gt;雖然最近的工作在改進 evidence retrieval 上取得了巨大的進展，但是他們依然只是在 closed evidence set 下 rerank&lt;/p&gt;
&lt;p&gt;fully end-to-end 的挑戰是，open corpus 的 retrieval 必須被視為 latent variable，要 train from scratch 是不切實際的&lt;/p&gt;
&lt;p&gt;IR system 提供了一個合理但可能非最好的起點&lt;/p&gt;
&lt;p&gt;本文的一個關鍵是，如果用無監督 的 ICT 對 retriever 做預訓練，那 end-to-end learning 是有可能的&lt;/p&gt;
&lt;p&gt;在 ICT 中，一個句子被視作 pseudo-question，而它的 context 被視作 pseudo-evdience&lt;/p&gt;
&lt;p&gt;給定一個 pseudo-question，retriever 的目標是從 batch 中的 candidate 找出對應的 pseudo-evidence&lt;/p&gt;
&lt;p&gt;ICT pretraining 提供了強大的初始化，使 ORQA 可以簡單地優化&lt;/p&gt;
&lt;p&gt;作者在五個 QA 資料集上進行了評估，在問題作者已知答案的資料集上 (SQuAD、TriviaQA)，檢索問題類似傳統的 IR，並且 BM25 是 SOTA retrieval&lt;/p&gt;
&lt;p&gt;在問題作者不知道答案的資料集上 (Natural Questions、WebQuestions、CuratedTrec)，作者顯示出 learned retrieval 的重要性，比 BM25 在 exact match 上好 6~19 個點&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h3 id=&#34;task&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;在 open-domain QA 中，$q$ 是 question string，而 $a$ 是 answer string&lt;/p&gt;
&lt;p&gt;與閱讀理解不同，evidence 的來源是 modeling choice，而非 task definition 的一部分&lt;/p&gt;
&lt;h3 id=&#34;formal-definitions&#34;&gt;Formal Definitions&lt;/h3&gt;
&lt;p&gt;Model 把一個 unstructured text corpus 切成 B 塊的 evidence text&lt;/p&gt;
&lt;p&gt;一個 answer derivation 是一個 pair $(b,s)$&lt;/p&gt;
&lt;p&gt;$1 \le b \le B$ 是 block 的 index&lt;/p&gt;
&lt;p&gt;$s$ 是 block $b$ 的 span&lt;/p&gt;
&lt;p&gt;scoring function $S(b,s,q)$ 用來計算 $(b,s)$ 對於 $q$ 的分數&lt;/p&gt;
&lt;p&gt;一般來說 scoring function 會被分解成 retrieval component $S_{retr}(b,q)$ 和 $S_{read}(b,s,q)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S(b,s,q) = S_{retr}(b,q) + S_{read}(b,s,q)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在推論階段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a^* = TEXT(argmax_{b,s} S(b,s,q))$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;open-domain QA 的一個主要挑戰是 handling the scale&lt;/p&gt;
&lt;p&gt;作者在 English Wikipedia 上做實驗，包含超過 13M 個 blocks，每個 block 都有超過 2000 個可能的 spans&lt;/p&gt;
&lt;h3 id=&#34;existing-pipelined-models&#34;&gt;Existing Pipelined Models&lt;/h3&gt;
&lt;p&gt;在現有的 retrieval-based open-domain QA 模型中，blackbox IR system 先選擇一組 closed set of evidence candidates&lt;/p&gt;
&lt;p&gt;然後 DrQA 之後的多數工作都用 TF-IDF 來挑選 candidate，並專注於閱讀理解或 reranking 的部分&lt;/p&gt;
&lt;p&gt;reading component $S_{read}(b,s,q)$ 是從 gold answer 學習的&lt;/p&gt;
&lt;p&gt;在最接近我們的方法的工作中，reader 是透過 weak supervision 學習的&lt;/p&gt;
&lt;p&gt;retrieval system 會啟發式（heuristically）地刪除 spurious ambiguities，並把清理後的結果視為 gold evidence&lt;/p&gt;
&lt;h2 id=&#34;open-retrieval-question-answering-orqa&#34;&gt;Open-Retrieval Question Answering (ORQA)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;$BERT(x_1, [x_2]) = \{CLS: h_{CLS}, 1: h_1, 2: h_2,&amp;hellip;\}$&lt;/p&gt;
&lt;h3 id=&#34;retriever-component&#34;&gt;Retriever component&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$h_q = W_q BERT(q)[CLS]$&lt;/li&gt;
&lt;li&gt;$h_b = W_b BERT(b)[CLS]$&lt;/li&gt;
&lt;li&gt;$S_{retr}(b,q) = h_q^T h_b$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reader-component&#34;&gt;Reader component&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$h_{start} = BERT_R(q,b)[START(s)]$&lt;/li&gt;
&lt;li&gt;$h_{end} = BERT_R(q,b)[END(s)]$&lt;/li&gt;
&lt;li&gt;$S_{read}(b,s,q) = MLP([h_{start};h_{end}])$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inference--learning-challenges&#34;&gt;Inference &amp;amp; Learning Challenges&lt;/h3&gt;
&lt;p&gt;上面的模型概念很簡單&lt;/p&gt;
&lt;p&gt;但推論和學習上具有挑戰性，因為：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;open evidence corpus 有巨大的搜尋空間（超過 13M 個 blocks）&lt;/li&gt;
&lt;li&gt;要如何在空間 navigate 是 latent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此標準的 teacher forcing 不能用，latent variable 也不能用，因為存在大量 spuriously ambiguous derivations（比如答案是 &amp;ldquo;seven&amp;rdquo;，很多 evidence 都會有 &amp;ldquo;seven&amp;rdquo; 這個字眼）&lt;/p&gt;
&lt;p&gt;作者透過非監督預訓練來良好地初始化 retriever 來解決這些挑戰&lt;/p&gt;
&lt;p&gt;預訓練的 retriever 使作者能夠：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;pre-encode Wikipedia blocks，從而在 finetune 階段實現動態且快速的 top-k retrieval&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使 retrieval 可以遠離 spuriously ambiguities 並偏向 supportive evidence&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;inverse-cloze-task&#34;&gt;Inverse Cloze Task&lt;/h2&gt;
&lt;p&gt;作者提出的預訓練流程的目標是想讓 retriever 解決和 evidence retrieval for QA 相似的無監督任務&lt;/p&gt;
&lt;p&gt;直覺上，useful evidence 通常會討論問題中的 entities, events 和 relations&lt;/p&gt;
&lt;p&gt;還包含問題中不存在的額外資訊 (the answer)&lt;/p&gt;
&lt;p&gt;question-evidence pair 是 setence-context pair，句子的上下文在語意上是相關的，可以推論句子中缺少的資訊&lt;/p&gt;
&lt;p&gt;憑這種想法，作者建議使用 ICT 來預訓練 retriever&lt;/p&gt;
&lt;p&gt;在 standard cloze task 中，目標是根據上下文預測 masked-out text&lt;/p&gt;
&lt;p&gt;相反，ICT 要逆向預測，給定一個句子，預測上下文&lt;/p&gt;
&lt;p&gt;使用類似 downstream retrieval 的 discriminative objective：&lt;/p&gt;
&lt;p&gt;$P_{ICT}(b|q)=\frac{exp(S_{retr}(b,q))}{\sum_{b&amp;rsquo; \in BATCH} exp(S_{retr}(b&amp;rsquo;,q))}$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;預測哪個上下文是 query 的&lt;/p&gt;
&lt;p&gt;ICT 有個重點是，它要做的不僅僅是單字匹配，因為 evidence 中沒有 pseudo-question&lt;/p&gt;
&lt;p&gt;例如 fig.2 的 pseudo-question 完全沒有提及 zebra，但 retriever 要能夠選擇 zebra 的 context&lt;/p&gt;
&lt;p&gt;能夠從非指定的語言推論出語意是 QA 和傳統 IR 的差異&lt;/p&gt;
&lt;p&gt;然而作者也不想阻止 retriever 學習單字匹配，因為 lexical overlap 最終是一個非常有用的特徵&lt;/p&gt;
&lt;p&gt;因此，作者只在 90 % 的例子中把 sentence 從 context 中移除，鼓勵模型在需要的時候學習抽象表示，也在可用時學習 low-level word matching features&lt;/p&gt;
&lt;p&gt;ICT 預訓練實現兩個主要目標：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;儘管預訓練的句子和微調時的 question 不匹配，但作者預期 zero-shot evidence retrieval performance 足以引導 latent variable 的學習&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pretrained evidence blocks 和 downstream evidence blocks 間沒有這種不匹配的問題&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;因此可預期 $BERT_{B}(b)$ 無須進一步訓練即可正常工作&lt;/p&gt;
&lt;p&gt;只有 question encoder 需要針對下遊資料微調&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;由於 fixed block encoder 已經為 retrieval 提供了有用的 representation，可以預先計算所有 block 的 encoding&lt;/p&gt;
&lt;p&gt;因此在微調的時候不需要對大量 evidence block 重新 encode，並且可以使用比如 locality-sensitive hashing 之類的現有工具來建立索引&lt;/p&gt;
&lt;p&gt;透過 pre-compiled index，推理遵循 standard beam-search&lt;/p&gt;
&lt;p&gt;檢索 top-k evidence block，並只計算這 k 個 block 的 reader score&lt;/p&gt;
&lt;h2 id=&#34;learning&#34;&gt;Learning&lt;/h2&gt;
&lt;p&gt;這邊太複雜，建議看原文&lt;/p&gt;
&lt;h2 id=&#34;experimental-setup&#34;&gt;Experimental Setup&lt;/h2&gt;
&lt;h3 id=&#34;open-domain-qa-datasets&#34;&gt;Open Domain QA Datasets&lt;/h3&gt;
&lt;p&gt;對 5 個現有 question answering 或閱讀理解資料集進行評估&lt;/p&gt;
&lt;p&gt;並非所有資料集的原始形式都是 open-domain QA，因此作者遵循 DrQA 的做法，轉成 open format&lt;/p&gt;
&lt;p&gt;每個 example 都有一個 single question 和一「組」 reference answer&lt;/p&gt;
&lt;h4 id=&#34;natural-questions&#34;&gt;Natural Questions&lt;/h4&gt;
&lt;p&gt;包含了從 Google Search 的 aggregated queries 中的 question&lt;/p&gt;
&lt;p&gt;為了蒐集這個資料集的 open version，作者只保留 short answer 並丟棄 evidence document&lt;/p&gt;
&lt;p&gt;具有許多 token 的答案通常類似 extractive snippets 而不是 canonical answer，因此作者丟棄長度超過 5 個 token 的答案&lt;/p&gt;
&lt;h4 id=&#34;webquestions&#34;&gt;WebQuestions&lt;/h4&gt;
&lt;p&gt;包含從 Google Suggest API 抽取的問題&lt;/p&gt;
&lt;p&gt;答案是根據 Freebase 標註的，但是只保留 entities 的 string representation&lt;/p&gt;
&lt;h4 id=&#34;curatedtrec&#34;&gt;CuratedTrec&lt;/h4&gt;
&lt;p&gt;問題來自真實查詢的各種來源，比如 MSNSearch&lt;/p&gt;
&lt;h4 id=&#34;triviaqa&#34;&gt;TriviaQA&lt;/h4&gt;
&lt;p&gt;從網路上抓的 question-answer pairs&lt;/p&gt;
&lt;p&gt;作者使用 unfiltered set 並捨棄 distanly supervised evidence&lt;/p&gt;
&lt;h4 id=&#34;squad&#34;&gt;SQuAD&lt;/h4&gt;
&lt;p&gt;被設計來用作閱讀理解，而不是 open-domain QA&lt;/p&gt;
&lt;p&gt;答案範圍是從 Wikipedia 的段落中選擇的，問題由 annotators 編寫，annotators 被指示提出問題，要由給定的 context 中的 span 來回答&lt;/p&gt;
&lt;h3 id=&#34;dataset-biases&#34;&gt;Dataset Biases&lt;/h3&gt;
&lt;p&gt;在 Natural Questions、WebQuestions 和 CuratedTrec 中，提問者不知道答案，反映了真實的尋求問題的分佈&lt;/p&gt;
&lt;p&gt;但是，annotators 必須單獨找到正確的答案，因此需要 automatic tools，並可能會對這些工具的結果產生 bias&lt;/p&gt;
&lt;p&gt;在 TriviaQA 和 SQuAD 中，不需要 automatic tools，因為 annotators 是根據已知答案寫問題的&lt;/p&gt;
&lt;p&gt;然而這引入了另一組可能更成問題的 bias，就是撰寫問題並非出於資訊需求&lt;/p&gt;
&lt;p&gt;導致問題中有許多自然出現的問題中沒有的提示&lt;/p&gt;
&lt;p&gt;這在 SQuAD 中問題特別嚴重，使問題和 evidence 間人為地出現大量詞彙重疊&lt;/p&gt;
&lt;p&gt;但上述這些只是想表達資料集的屬性，而非可採取行動的批評，因為要取得大規模資料必定會遇到這些狀況，目前還不清楚如何在合理的成本下收集公正的資料集&lt;/p&gt;
&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h3&gt;
&lt;h4 id=&#34;evidence-corpus&#34;&gt;Evidence Corpus&lt;/h4&gt;
&lt;p&gt;corpus 被分成最多 288 個單字的 chunk，並且保留 sentence boundaries&lt;/p&gt;
&lt;p&gt;導致有超過 13M 個 blocks&lt;/p&gt;
&lt;h2 id=&#34;main-results&#34;&gt;Main Results&lt;/h2&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;h4 id=&#34;bm25&#34;&gt;BM25&lt;/h4&gt;
&lt;p&gt;BM25 是事實上的非監督搜索方法 SOTA
被證明對於傳統資訊檢索任務和 QA 的 evidence retrieval 任務都是 robust&lt;/p&gt;
&lt;h4 id=&#34;language-models&#34;&gt;Language Models&lt;/h4&gt;
&lt;p&gt;非監督的 neural retrieval 對於傳統 IR 來說很難改進，但這裡視作比較的 baseline&lt;/p&gt;
&lt;p&gt;作者對 LM 進行實驗，並且這已被證明是 SOTA unsupervised representation&lt;/p&gt;
&lt;p&gt;我們與兩種廣泛使用的 128-dimensional representation 進行比較：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NNLM
&lt;ul&gt;
&lt;li&gt;context-independent embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ELMO
&lt;ul&gt;
&lt;li&gt;context-dependent bidirectional LSTM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;就像 ICT 一樣，使用 alternate encoder 來預先計算 encoded evidence blocks 還有初始化經過 finetune 的 question encoding&lt;/p&gt;
&lt;p&gt;根據現有的 IR 文獻，還有 LM 沒有顯著優化 retrieval 的直覺，作者並不期望這些成為強大的 baseline，但是他們證明了將文本編碼為 128 維的難度&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/table5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在提問者已經知道答案的資料集中&lt;/p&gt;
&lt;p&gt;證實壓縮到 128維的向量無法與 BM25 精確表示 evidence 中每個單字的能力相符&lt;/p&gt;
&lt;p&gt;SQuAD 的 dev 和 test 間的顯著下降反映了資料集中的某個特性 - 10 萬個問題僅源自 536 個文件&lt;/p&gt;
&lt;p&gt;因此，SQuAD 的好的檢索目標，會和訓練範例高度相關，違反了 IID 假設，使其不適合學習檢索&lt;/p&gt;
&lt;p&gt;因此，作者強烈建議對 end-to-end open-domain QA models 有興趣的人不再使用 SQuAD 進行訓練和評估&lt;/p&gt;
&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;h3 id=&#34;strongly-supervised-comparison&#34;&gt;Strongly supervised comparison&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/table6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;為了證實作者的 BM25 Baseline 是 SOTA，提供了和 DrQA 的比較&lt;/p&gt;
&lt;p&gt;DrQA 的 reader 是 DocReader，用 TF-IDF 取得 top k documents&lt;/p&gt;
&lt;p&gt;還包括基於 TF-IDF retrieval 的 distant supervision&lt;/p&gt;
&lt;p&gt;BERTserini 的 reader 是一個基於 base BERT（類似作者的 reader），並用 BM25 搜索 top-k 個段落（像作者的 BM25 baseline）&lt;/p&gt;
&lt;p&gt;主要區別在 BERTserini 使用 Wikipedia 中的真實段落，而不是任意 block，從而由於長度不均導致更多 evidence blocks&lt;/p&gt;
&lt;p&gt;為了和這些強監督系統進行比較，作者在 SQuAD 上預訓練 reader&lt;/p&gt;
&lt;h3 id=&#34;masking-rate-in-the-inverse-cloze-task&#34;&gt;Masking Rate in the Inverse Cloze Task&lt;/h3&gt;
&lt;p&gt;pseudo-query 在 90% 的時間裡都從 evidence block 遮蔽&lt;/p&gt;
&lt;p&gt;如果總是屏蔽 pseudo-query，那麼 retriever 永遠不會知道 n-gram overlap 是一個強大的 retrieval signal，導致損失 10 個點&lt;/p&gt;
&lt;p&gt;如果從不屏蔽，問題就會簡化為記憶，導致不能很好地推廣到問題&lt;/p&gt;
&lt;h3 id=&#34;example-predictions&#34;&gt;Example Predictions&lt;/h3&gt;
&lt;p&gt;發現 ORQA 在具有高度詞彙重疊的文本更加 robust&lt;/p&gt;
&lt;p&gt;但是由於 128 維向量的資訊有限&lt;/p&gt;
&lt;p&gt;很難精確地表示極為具體的概念，比如準確日期&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DrQA 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/drqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 25 Dec 2023 00:00:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/drqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.00051&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Reading Wikipedia to Answer Open-Domain Questions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;本文提出以 Wikipedia 為知識來源來解決 open-domain question answering 問題&lt;/p&gt;
&lt;p&gt;任何問題的答案都是 Wikipedia 中的一段文字&lt;/p&gt;
&lt;p&gt;這項挑戰結合了文件檢索和理解文字的能力&lt;/p&gt;
&lt;p&gt;本文的作法基於一個 search component，由 bigram hashing 和 TF-IDF matching 構成，並結合 RNN&lt;/p&gt;
&lt;p&gt;對多個 QA 資料集做的實驗表明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;這兩個 components 對於現有的對應模塊具有高度競爭力&lt;/li&gt;
&lt;li&gt;在他們的組合上使用 distant supervision 十分有效&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;要把 wikipedia 當作知識來源，回答任何一個問題，都必須先從超過 5 百萬篇文章中找出少數相關的文章，並仔細掃描以找出答案。&lt;/p&gt;
&lt;p&gt;本文將這稱為 machine reading at scale (MRS)&lt;/p&gt;
&lt;p&gt;本文把 wikipedia 當作一個文章的集合，並且不依賴內部的 graph structure&lt;/p&gt;
&lt;p&gt;因此該方法是通用的，可以套到諸如新聞、網路論壇等等的資料集上&lt;/p&gt;
&lt;p&gt;本文開發了 DrQA，強大的維基百科問答系統，由以下部分組成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Document Retriever
&lt;ul&gt;
&lt;li&gt;一個用 bigram hashing 和 TF-IDF matching 的 module&lt;/li&gt;
&lt;li&gt;用於在給定問題的情況下，返回有效相關文章的子集&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Document Reader
&lt;ul&gt;
&lt;li&gt;RNN，用來 detect 文章中答案的位置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;實驗表明 Document Retriever 的性能優於 Wikipedia 的內部搜尋引擎，Document Reader 在 SQuAD 上達到 SOTA&lt;/p&gt;
&lt;p&gt;此外，與 single task training 相比，作者表明 multitask learning 和 distant supervision 有助於提高模型的性能&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;隨著 Knowledge Base (KB) 的發展，QA 出現了許多創新，但是 KB 具備固有限制（incompleteness, fixed schema），促使研究人員轉回從 raw text 中提取答案&lt;/p&gt;
&lt;p&gt;有些工作嘗試利用 multitask learning 來組合多個 QA 資料集，目標是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;透過 task transfer 來實現跨資料集的改進&lt;/li&gt;
&lt;li&gt;提供一個單一通用的系統，可以回答不同種類的問題，因為資料來源中不可避免地存在不同的資料分布&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文的工作在先 retrive 再 read 的 setting 下，沒有利用 KB，取得了正面成果&lt;/p&gt;
&lt;h2 id=&#34;drqa&#34;&gt;DrQA&lt;/h2&gt;
&lt;p&gt;由兩個 Components 組成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Document Retriever
&lt;ul&gt;
&lt;li&gt;用來尋找相關文章&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Document Reader
&lt;ul&gt;
&lt;li&gt;用於從單一文件或一小部分文件提取答案&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;document-retriever&#34;&gt;Document Retriever&lt;/h3&gt;
&lt;p&gt;遵循經典的 QA System，先用高效的 （非機器學習）的 document retrieval system 縮小搜索範圍，並專注於比較可能有關的文章&lt;/p&gt;
&lt;p&gt;與基於 ElasticSearch 的 Wikipedia Search API 相比，簡單的 inverted index lookup 和 term vector model scoring 表現的十分好&lt;/p&gt;
&lt;p&gt;文章和問題被表示為 TF-IDF weighted bag-of-words vectors&lt;/p&gt;
&lt;p&gt;作者考慮透過考慮 local word order 和 n-gram features 來進一步改善&lt;/p&gt;
&lt;p&gt;表現最佳的系統用 bigram counts，並利用 hashing 映射到 $2^{24}$ bins 來保持 speed 和 memory efficiency，用的是 unsigned murmur3 hash&lt;/p&gt;
&lt;p&gt;Document Retriever 作為模型的第一部分，設定為對任何問題返回 5 個相關的文章&lt;/p&gt;
&lt;p&gt;這些文章再交由 Document Reader 來處理&lt;/p&gt;
&lt;h3 id=&#34;document-reader&#34;&gt;Document Reader&lt;/h3&gt;
&lt;h4 id=&#34;paragraph-encoding&#34;&gt;Paragraph encoding&lt;/h4&gt;
&lt;p&gt;$p_i$ 是段落 $p$ 中的 token，期望 $p_i$ 可以被 encode 成帶有周圍資訊的向量&lt;/p&gt;
&lt;p&gt;採用的是 multi-layer bidirectional LSTM&lt;/p&gt;
&lt;p&gt;特徵向量 $p_i$ 由以下部分組成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Word embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用 300 維的 Glove word embedding，固定大部分預訓練的 word embedding，只 finetune 最常見的 1000 個 question words，例如 what, how, which&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exact match&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;採用三個簡單的 binary features，用來表示 $p_i$ 是否與問題中的 question word $q$ 精確匹配，無論是原始形式、小寫，還是 lemma form&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Token features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作者添加了一些 manual feature 好反應 $p_i$ 在 context 中的屬性，比如 part-of-speech (POS)、named entity recognition (NER) tags 和它的 (normalized) term frequency (TF)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aligned question embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_{align}(p_i)=\sum_j a_{i,j}E(q_j)$
&lt;ul&gt;
&lt;li&gt;$E$ 是 word embedding&lt;/li&gt;
&lt;li&gt;$a_{i,j}$ 是 attention score，計算 $p_i$ 和每個 $q_j$ 之間的 similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;question-encoding&#34;&gt;Question encoding&lt;/h4&gt;
&lt;p&gt;這個比較簡單，只是在 word embedding 上加上一層 RNN，並把 hidden units 重新結合成一個向量&lt;/p&gt;
&lt;h4 id=&#34;prediction&#34;&gt;Prediction&lt;/h4&gt;
&lt;p&gt;把 $\{p_1,&amp;hellip;,p_m\}$ 和 $q$ 作為 input，並個別單獨訓練兩個分類器預測開頭和結尾的位置&lt;/p&gt;
&lt;p&gt;具體來說，作者用 bilinear term 來計算每個 $p_i$ 和 $q$ 之間的相似度，並計算每個 $p_i$ 是開頭的機率和結尾的機率&lt;/p&gt;
&lt;p&gt;最後選擇最佳範圍，從 token $i$ 到 token $i&#39;$&lt;/p&gt;
&lt;p&gt;$i \le i&amp;rsquo; \le i+15$&lt;/p&gt;
&lt;p&gt;並且使 $P_{start}(i) \times P_{end}(i&amp;rsquo;)$ 最大化&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;本文的工作依賴三種資料：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wikipedia
&lt;ul&gt;
&lt;li&gt;尋找答案的來源&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SQuAD
&lt;ul&gt;
&lt;li&gt;用來訓練和評估模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;另外三個 QA 資料集 (WebQuestions, CuratedTREC, WikiMovies)
&lt;ul&gt;
&lt;li&gt;用來測試模型在 Open-domain QA 上的泛化能力，並評估模型從 multitask learning 和 distant supervision 中獲益的程度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;wikipedia-knowledge-source&#34;&gt;Wikipedia (Knowledge Source)&lt;/h3&gt;
&lt;p&gt;用 2016-12-21 dump2 的 English Wikipedia&lt;/p&gt;
&lt;p&gt;對於每頁，只提取文本，並刪除結構化的資料（lists and figures）&lt;/p&gt;
&lt;p&gt;丟棄 disambiguation pages, list, index 和 outline pages，保留了 5,075,182 篇文章&lt;/p&gt;
&lt;h3 id=&#34;squad&#34;&gt;SQuAD&lt;/h3&gt;
&lt;p&gt;使用兩個 evaluation metrics：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exact string match (EM)&lt;/li&gt;
&lt;li&gt;F1 score&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;為了評估 open-domain QA 的能力，作者只有使用 SQuAD 的 QA pairs，要求系統在無法存取相關段落的情況下發現正確的 answer span&lt;/p&gt;
&lt;p&gt;不像標準的 SQuAD setting 會給出相關段落&lt;/p&gt;
&lt;h3 id=&#34;open-domain-qa-evaluation-resources&#34;&gt;Open-domain QA Evaluation Resources&lt;/h3&gt;
&lt;p&gt;SQuAD 是目前可用的最大的 general purpose QA 資料集之一&lt;/p&gt;
&lt;p&gt;收集過程包括向每個 human annotator 展示一個段落，並寫一個問題&lt;/p&gt;
&lt;p&gt;因此，distribution 非常特定&lt;/p&gt;
&lt;p&gt;作者建議在其他 open-domain QA 資料集上評估系統，他們以不同的方式建構&lt;/p&gt;
&lt;h4 id=&#34;curatedtrec&#34;&gt;CuratedTREC&lt;/h4&gt;
&lt;p&gt;使用大版本，包含 TREC 1999, 2000, 2001 和 2002 的 2180 個問題&lt;/p&gt;
&lt;h4 id=&#34;webquestions&#34;&gt;WebQuestions&lt;/h4&gt;
&lt;p&gt;這資料集旨在回答 Freebase KB 的問題&lt;/p&gt;
&lt;p&gt;透過 Google Suggest API 抓問題，再用 Amazon Mechanical Turk 來獲取答案&lt;/p&gt;
&lt;p&gt;作者用 entity names 把每個 answer 轉成答案，以便不引入 Freebase IDs&lt;/p&gt;
&lt;h4 id=&#34;wikimovies&#34;&gt;WikiMovies&lt;/h4&gt;
&lt;p&gt;包含對電影領域的 96k 個問答對&lt;/p&gt;
&lt;h3 id=&#34;distantly-supervised-data&#34;&gt;Distantly Supervised Data&lt;/h3&gt;
&lt;p&gt;上面說的資料集除了 SQuAD 都沒有相關段落，因此不能直接訓練 Document Reader&lt;/p&gt;
&lt;p&gt;追隨之前已有的利用 distant supervision (DS) 來做 relation extraction 的工作，作者用一個程式自動把段落和此類訓練範例做相關聯&lt;/p&gt;
&lt;p&gt;對每個問答對使用以下過程來建立訓練集：&lt;/p&gt;
&lt;p&gt;首先，對問題用 Document Retriever 找到前 5 個相關的段落&lt;/p&gt;
&lt;p&gt;與已知答案沒有 exact match 的段落都被直接丟棄&lt;/p&gt;
&lt;p&gt;短於 25 個字元或長於 1500 個字元的段落也被丟棄&lt;/p&gt;
&lt;p&gt;如果在問題中找到 named entity，不包含該 named entity 的段落也被丟棄&lt;/p&gt;
&lt;p&gt;對於每個 retrived page 的每個段落，使用 question 和 20 token window 間的 unigram  和 bigram overlap 來計算相似度，保留重疊度最高的前五個段落&lt;/p&gt;
&lt;p&gt;將找到的每個 pair 加入到 DS 訓練集中&lt;/p&gt;
&lt;p&gt;大約一半的 DS 範例來自 SQuAD 以外的頁面&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;finding-relevant-articles&#34;&gt;Finding Relevant Articles&lt;/h3&gt;
&lt;p&gt;先檢查 Retriever 在所有 QA 資料集上的性能&lt;/p&gt;
&lt;p&gt;計算 ration 是根據特定的問題，考慮這些問題對應的文本在前 5 個相關文章中的比例&lt;/p&gt;
&lt;p&gt;結果表明，比 Wikipedia Search 還更好，尤其是使用 bigram hashing 的情況下&lt;/p&gt;
&lt;h3 id=&#34;reader-evaluation-on-squad&#34;&gt;Reader Evaluation on SQuAD&lt;/h3&gt;
&lt;h4 id=&#34;implementation-details&#34;&gt;Implementation details&lt;/h4&gt;
&lt;p&gt;使用 h=128 的 3 層雙向 LSTM，來做 paragraph encoding 和 question encoding&lt;/p&gt;
&lt;p&gt;使用 Stanford CoreNLP 來做 tokenization 並生成 lemma, part-of-speech 和 named entity tags&lt;/p&gt;
&lt;p&gt;Optimizer 使用 Adamax&lt;/p&gt;
&lt;p&gt;Dropout rate 為 0.3&lt;/p&gt;
&lt;h4 id=&#34;result-and-analysis&#34;&gt;Result and analysis&lt;/h4&gt;
&lt;p&gt;作者的系統可以在 SQuAD 上達到 SOTA&lt;/p&gt;
&lt;p&gt;做了 ablation study，結果表明所有功能都會影響性能&lt;/p&gt;
&lt;p&gt;有趣的是，單獨沒有 $f_{alignd}$ 或 $f_{exact_match}$ 對性能不會有極大的影響，但兩個都沒有就會急遽下降&lt;/p&gt;
&lt;h3 id=&#34;full-wikipedia-question-answering&#34;&gt;Full Wikipedia Question Answering&lt;/h3&gt;
&lt;p&gt;比較三個版本的 DrQA：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SQuAD
&lt;ul&gt;
&lt;li&gt;只在 SQuAD 上訓練，並用於所有評估資料集&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tune (DS)
&lt;ul&gt;
&lt;li&gt;先在 SQuAD 上預訓練，在用 DS 訓練集對每個資料集進行微調&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multitask (DS)
&lt;ul&gt;
&lt;li&gt;在 SQuAD 和 DS 訓練集上聯合訓練&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DrQA/table6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;兩個明顯的 angles of attack：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;把多個段落直接納入 Document Reader 的訓練，因為他目前獨立訓練每個段落&lt;/li&gt;
&lt;li&gt;實作一個 end-to-end training 的 pipeline，可以在一個模型中結合 Document Retriever 和 Document Reader&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Self-Instruct 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 30 Apr 2023 00:00:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2212.10560&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;大型 &amp;ldquo;instruction-tuned&amp;rdquo; 語言模型 (經過微調好回應 instruction) 已經展現出在新任務上 zero-shot 的能力。&lt;/p&gt;
&lt;p&gt;然而他們嚴重依賴人工編寫的指令，在數量、多樣性和創造力上都受到了限制，阻礙了模型的通用性。&lt;/p&gt;
&lt;p&gt;作者介紹了 Self-Instruct 這個框架，可以透過自己生成的指令，來增強預訓練模型遵循指令的能力。&lt;/p&gt;
&lt;p&gt;將作者的方法應用在 GPT3，在 SuperNaturalInstructions 獲得了比原始模型高 33% 的改進，與使用 private user data 和 human annotations 的 $InstructGPT_{001}$ 性能相當。&lt;/p&gt;
&lt;p&gt;為了進一步評估，我們為新任務整理一組專家編寫的指令，並通過人工評估，顯示出使用 Self-Instruction 調整 GPT3 的性能大大優於使用現有公共指令資料集，只比 $InstructGPT_{001}$ 落後 5% 的差距。&lt;/p&gt;
&lt;p&gt;Self-Instruct 提供一個幾乎 annotation-free 的方法，align 預訓練模型和 instructions，而且作者釋出了他們的大型合成資料集，以促進未來對 instruction tuning 的研究。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;最近的 NLP 文獻見證了「建構可以遵循自然語言指令的模型方面」的大量活動。&lt;/p&gt;
&lt;p&gt;這些發展由兩個關鍵部分組成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;大型預訓練語言模型 (LM)&lt;/li&gt;
&lt;li&gt;人工編寫的指令資料&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PromptSource 和 SuperNaturalInstructions 是最近兩個著名的資料集。
他們透過大量手動註釋來收集指令，以建造 T0 和 T$k$-Instruct。&lt;/p&gt;
&lt;p&gt;然而這過程代價高昂，而且由於大多數人往往生成的都是流行的 NLP 任務，使其未能涵蓋真正多樣的任務，也不能涵蓋各種描述任務的不同方式，因此多樣性受侷限。&lt;/p&gt;
&lt;p&gt;鑒於這些限制，想要繼續提升 instruction-tuned models 的品質，需要幫 supervising instruction-tuned models 發展替代方案。&lt;/p&gt;
&lt;p&gt;本文介紹了 Self-Instruct，這是一種 semi-automated 的過程，用模型自身的 instructional signals 對 pretrained LM 進行 instruction-tuning。&lt;/p&gt;
&lt;p&gt;整個流程是一種 iterative bootstrapping algorithm，從手動編寫的 limited seed set 引導生成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在第一階段，模型要幫新任務生成指令。
利用現有的指令集合，創建更廣泛的指令，好定義 (通常是新的) 任務。&lt;/p&gt;
&lt;p&gt;對於新生成的指令集，框架為他們創建 input-output instances，稍後可以透過 supervising 用於 instruction tuning。&lt;/p&gt;
&lt;p&gt;最後，透過各種手段，在低品質和重複的指令加到 task pool 前，把他們修剪掉。&lt;/p&gt;
&lt;p&gt;可以重複這個流程非常多次，直到獲得大量任務。&lt;/p&gt;
&lt;p&gt;該模型的跌代過程中產生了大約 52K 個指令，與大約 85K 個 instance inputs 和 target outputs 配對 (有些相同的指令會對應多種輸入輸出)。&lt;/p&gt;
&lt;p&gt;作者觀察到生成的資料提供了各種有創意的任務，其中超過 50% 的任務和 seed instructions 的 ROUGE-L overlap 小於 0.3。&lt;/p&gt;
&lt;p&gt;基於上述結果，作者通過微調 GPT3 (和生成指令資料是同個模型) 建構了 $GPT3_{SELF-INST}$。&lt;/p&gt;
&lt;p&gt;SuperNI 的結果表明，$GPT3_{SELF-INST}$ 性能大大優於 GPT3 (原始模型)，高了 33.1%，幾乎和 $InstructGPT_{001}$ 的性能相當。&lt;/p&gt;
&lt;p&gt;此外，作者在新創建的的指令集上進行人工評估，$GPT3_{SELF-INST}$ 顯示出廣泛的指令遵循能力，優於在其他公開可用指令數據集上訓練的模型，只比 InstrcutGPT001 落後 5%。&lt;/p&gt;
&lt;p&gt;本文貢獻：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Self-Instruct：一種用最少的人工標記數據引導指令遵循能力的作法&lt;/li&gt;
&lt;li&gt;通過大量的 instruction-tuning 實驗，證明了有效性。&lt;/li&gt;
&lt;li&gt;發布了一個包含 52K 指令的大型綜合資料集，還有一組手動編寫的新任務，用於建構和評估未來的 instruction-following models。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;instruction-following-language-models&#34;&gt;Instruction-following language models&lt;/h3&gt;
&lt;p&gt;一系列工作顯示，使用 annotated &amp;ldquo;instructional&amp;rdquo; data，可以使普通語言模型遵循一般語言的指令。&lt;/p&gt;
&lt;p&gt;也顯示出 &amp;ldquo;instructional&amp;rdquo; data 的大小和多樣性直接影響模型的泛化能力。&lt;/p&gt;
&lt;p&gt;本文的工作目的在減少對人工註釋者的依賴。&lt;/p&gt;
&lt;h3 id=&#34;language-models-for-data-generation-and-augmentation&#34;&gt;Language models for data generation and augmentation&lt;/h3&gt;
&lt;p&gt;許多工作依賴生成式 LM 來生成數據或做 augmentation。&lt;/p&gt;
&lt;p&gt;雖然作者的工作可被視為一種 augmentation，但和這些工作的差別在於不限於特定任務。&lt;/p&gt;
&lt;p&gt;Self-Instruct 的一個明顯動機是引導出新的任務定義，而這些任務可能還未被 NLP 的研究者定義過。&lt;/p&gt;
&lt;h3 id=&#34;self-training&#34;&gt;Self-training&lt;/h3&gt;
&lt;p&gt;一種典型的 self-training 框架透過經過訓練的模型，幫 unlabeled 資料進行 label，然後用這些資料改進模型。&lt;/p&gt;
&lt;p&gt;雖然 Self-Instruct 和 self-training 有一些相似之處，但多數 self-training 的方法都假設了一個特定的目標任務。&lt;/p&gt;
&lt;p&gt;相比之下，Self-Instruct 從頭開始生出各種任務。&lt;/p&gt;
&lt;h3 id=&#34;knowledge-distillation&#34;&gt;Knowledge distillation&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;這邊我想不太通為什麼可以和 Knowledge distillation 扯上關係&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Knowledge distillation 通常涉及知識從較大模型到較小模型的轉移&lt;/p&gt;
&lt;p&gt;Self-Instruct 也可以看做是 Knowledge distillation 的一種形式，但區別如下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;distillation 的來源和目標是相同的，即模型的知識被 distill 到他自己&lt;/li&gt;
&lt;li&gt;distill 的內容以 instruction task 的形式出現&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;標記大規模指令資料對人類來說可能具有挑戰性，因為他需要&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;創意，好提出新任務&lt;/li&gt;
&lt;li&gt;為每個任務編寫 labeled instances 的專業知識&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;defining-instruction-data&#34;&gt;Defining Instruction Data&lt;/h3&gt;
&lt;p&gt;我們要生成的指令資料集包含 {$I_t$}，每個指令用自然語言定義了任務 $t$。&lt;/p&gt;
&lt;p&gt;每個任務都有一個或多個 input-output instances ($X_t,Y_t$)。&lt;/p&gt;
&lt;p&gt;給定 task instruction $I_t$，還有 instance x，模型 M 要生出 y：&lt;/p&gt;
&lt;p&gt;$M(I_t,x)=y, for (x,y) \in (X_t,Y_t)$&lt;/p&gt;
&lt;p&gt;值得注意的是，instance input 和 instruction 沒有嚴格分界。&lt;/p&gt;
&lt;p&gt;比如 Instruction:&amp;ldquo;write an essay about school safety&amp;rdquo; x:&amp;quot;&amp;quot;，可以被改為 Instruction:&amp;ldquo;write an essay about the following topic&amp;rdquo; x:&amp;ldquo;school safety&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;automatic-instruction-data-generation&#34;&gt;Automatic Instruction Data Generation&lt;/h3&gt;
&lt;p&gt;生成指令資料的 pipeline 分成四個步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;指令生成&lt;/li&gt;
&lt;li&gt;辨識指令是否是分類任務&lt;/li&gt;
&lt;li&gt;用 input-first 或 output-first 做 instance generation&lt;/li&gt;
&lt;li&gt;過濾掉低品質的資料&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;instruction-generation&#34;&gt;Instruction Generation&lt;/h4&gt;
&lt;p&gt;Self-Instruct 是基於一個發現，也就是大型語言模型可以透過 context 中的現有指令，生出新穎的指令。&lt;/p&gt;
&lt;p&gt;為作者提供了一種從一小組人類編寫的指令中，使指令資料增長的做法。&lt;/p&gt;
&lt;p&gt;作者用他們編寫的 175 個任務 (每個任務 1 個 instruction 和 1 個 instance) 初始化 task pool。&lt;/p&gt;
&lt;p&gt;在每一個 step，作者從裡面 sample 8 個 instructions，作為 in-context 的範例。在這 8 個指令中，有 6 條來自人工編寫的任務，另外兩條來自前面步驟中模型生成的任務，以促進多樣性。&lt;/p&gt;
&lt;h4 id=&#34;classification-task-identification&#34;&gt;Classification Task Identification&lt;/h4&gt;
&lt;p&gt;因為對於分類和非分類的任務，作者會採取兩種做法，所以作者使用來自 seed taks 的 12 條分類指令和 19 條非分類指令，讓 GPT3 透過 few-shot 來判別。&lt;/p&gt;
&lt;h4 id=&#34;instance-generation&#34;&gt;Instance Generation&lt;/h4&gt;
&lt;p&gt;給予指令和他們的任務類別，作者獨立地為每條指令生成 instance。&lt;/p&gt;
&lt;p&gt;這具備挑戰性，原因在於他需要模型瞭解目標任務是什麼，根據指令找出需要那些額外的輸入內容，並生成他們。 (模型要根據 instruction 生出 instance input)&lt;/p&gt;
&lt;p&gt;作者發現，在 prompt 中放入其他包含 instruction-input-output 的任務範例的時候，模型可以實現這點。&lt;/p&gt;
&lt;p&gt;一種自然的方法是 Input-first Approach，可以要求語言模型先根據指令提出 input，再生出相應的 output。&lt;/p&gt;
&lt;p&gt;然而，這種方法在分類任務上，可能會偏向於生成某種 label。所以，對於分類任務，作者採用 Output-first Approach，先生成可能的 label，在每個 label 上再生成輸入。&lt;/p&gt;
&lt;h4 id=&#34;filtering-and-postprocessing&#34;&gt;Filtering and Postprocessing&lt;/h4&gt;
&lt;p&gt;為了鼓勵多樣性，只有當新的指令和任何現有的指令的 ROUGE-L overlapping 小於 0.7 的時候，才會被添加到 task pool。&lt;/p&gt;
&lt;p&gt;還排除了一些包含了通常不能被 LM 處理的關鍵字 (e.g. images, pictures, graphs) 的指令。&lt;/p&gt;
&lt;p&gt;在為每個指令生成新的 instance 的時候，會過濾掉完全相同或者是輸入相同但輸出不同的 instance。&lt;/p&gt;
&lt;h3 id=&#34;finetuning-the-lm-to-follow-instructions&#34;&gt;Finetuning the LM to Follow Instructions&lt;/h3&gt;
&lt;p&gt;在創建大規模指令資料後，用這些資料對原始語言模型進行 fine-tune。&lt;/p&gt;
&lt;p&gt;為此，將 instruction 和 instance input 連接起來，作為 prompt，然後訓練模型透過標準的監督式學習進行微調。&lt;/p&gt;
&lt;p&gt;為了讓模型對不同的格式 robust，使用多個模板將指令和輸入 encode 在一起。&lt;/p&gt;
&lt;p&gt;例如，指令可以有或沒有 Task: 前墜、輸入可以有或沒有 Input: 前墜，或是中間可以有不同數量的換行之類的。&lt;/p&gt;
&lt;h2 id=&#34;self-instruct-data-from-gpt3&#34;&gt;Self-Instruct Data from GPT3&lt;/h2&gt;
&lt;p&gt;作者透過 OpenAI API 訪問最大的 GPT3 (davinci)&lt;/p&gt;
&lt;h3 id=&#34;statistics&#34;&gt;Statistics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;diversity&#34;&gt;Diversity&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;quality&#34;&gt;Quality&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;h3 id=&#34;gpt3_self-inst-fine-tuning-gpt3-on-its-own-instruction-data&#34;&gt;$GPT3_{SELF-INST}$: fine-tuning GPT3 on its own instruction data&lt;/h3&gt;
&lt;p&gt;使用生出來的指令資料，對 GPT3 進行微調。&lt;/p&gt;
&lt;p&gt;微調是透過 OpenAI finetuning API&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;h4 id=&#34;off-the-shelf-language-models&#34;&gt;Off-the-shelf language models&lt;/h4&gt;
&lt;p&gt;T5-LM 和 GPT3 是普通 LM baselines (只有 pre-training，沒有額外 fine-tune)&lt;/p&gt;
&lt;p&gt;這些 baseline 將表明現成的 LM 在預訓練後，能夠立刻自然地遵循指令的程度。&lt;/p&gt;
&lt;h4 id=&#34;publicly-available-instruction-tuned-models&#34;&gt;Publicly-available instruction-tuned models&lt;/h4&gt;
&lt;p&gt;T0 和 $T_k$-Instruct 是兩個 instruction-tuned models。&lt;/p&gt;
&lt;p&gt;兩者都是從 T5 進行微調的，對這兩種模型，都使用具有 11B 參數的最大版本。&lt;/p&gt;
&lt;h4 id=&#34;instruction-tuned-gpt3-models&#34;&gt;Instruction-tuned GPT3 models&lt;/h4&gt;
&lt;p&gt;作者評估了 InstructGPT，它是 OpenAI 基於 GPT3 開發的。&lt;/p&gt;
&lt;p&gt;對於 SuperNI 的實驗，只與 text-davinci-001 engine 進行比較，因為更新的 engine 用最新的用戶資料，而且很可能已經看過 SuperNI。&lt;/p&gt;
&lt;p&gt;對於新編寫的指令，評估時則包含了 001、002 和 003，以確保完整性。&lt;/p&gt;
&lt;p&gt;為了進一步比較 Self-Instruct 在其他公開可用的指令訓練集資料，使用 PromptSource 和 SuperNI 的資料微調 GPT3，這些資料用於訓練 T0 和 $T_k$-Instruct 模型。&lt;/p&gt;
&lt;p&gt;分別簡稱為 T0 訓練和 SuperNI 訓練。&lt;/p&gt;
&lt;h3 id=&#34;experiment-1-zero-shot-generalization-on-superni-benchmark&#34;&gt;Experiment 1: Zero-Shot Generalization on SUPERNI benchmark&lt;/h3&gt;
&lt;p&gt;首先以 zero-shot 的方式評估典型 NLP 任務遵循指令的能力。&lt;/p&gt;
&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;experiment-2-generalization-to-user-oriented-instructions-on-novel-tasks&#34;&gt;Experiment 2: Generalization to User-oriented Instructions on Novel Tasks&lt;/h3&gt;
&lt;p&gt;盡管 SuperNI 在現有的 NLP 任務具有全面性，多數的這些任務是初於研究理由提出的，而且偏向分類。&lt;/p&gt;
&lt;p&gt;為了更好的獲取指令遵循模型的實用價值，作者中的一部分人策劃了一組面向用戶應用的新指令集。&lt;/p&gt;
&lt;p&gt;他們先針對 Large LM 可能可以應用到的領域進行 brainstorm，並且制定與每個領域相關的 instruction 和 instance。&lt;/p&gt;
&lt;p&gt;總共創建了 252 條指令，每條指令有 1 個 instance。&lt;/p&gt;
&lt;h4 id=&#34;human-evaluation-setup&#34;&gt;Human evaluation setup&lt;/h4&gt;
&lt;p&gt;評估模型在這些不同任務的測試集上的表現極具挑戰性，因為不同的任務需要不同的專業知識。&lt;/p&gt;
&lt;p&gt;為了獲得更忠實的評價，作者請了 instructions 的作者對模型的預測結果進行評估。&lt;/p&gt;
&lt;p&gt;實施一個 four-level rating system：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rating A
&lt;ul&gt;
&lt;li&gt;回覆有效且令人滿意&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating B
&lt;ul&gt;
&lt;li&gt;回覆可接受，但存在可以改進的地方&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating C
&lt;ul&gt;
&lt;li&gt;回覆相關，但在內容上有重大錯誤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating D
&lt;ul&gt;
&lt;li&gt;回覆不相關或無效，包含重複輸入的部分，完全無關的輸出。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;results-1&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如果把 Rating B 以上視為有效，$GPT_{SELF-INST}$ 只和 $InstructGPT_{001}$ 相差 5%&lt;/p&gt;
&lt;h2 id=&#34;discussion-and-limitation&#34;&gt;Discussion and Limitation&lt;/h2&gt;
&lt;h3 id=&#34;why-does-self-instruct-work&#34;&gt;Why does SELF-INSTRUCT work?&lt;/h3&gt;
&lt;p&gt;值得反思的是，在最近成功的 instruction-tuning LMs 中，高品質的 human feedback 扮演的角色。&lt;/p&gt;
&lt;p&gt;這裡有兩個極端的假設：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Human feedback 是 instruction-tuning 中必要且不可或缺的角色，因為 LM 需要了解在預訓練過程中沒完全了解到的問題。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Human feedback 是 instruction-tuning 一個可選的方向，因為 LM 在預訓練就已經很熟悉指令了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;雖然現實可能介於這兩個極端之間，作者推測可能更傾向於第二種假設，尤其是對於較大的模型。&lt;/p&gt;
&lt;p&gt;第二種，也是人類直覺，是 Self- Instruct 的關鍵動機，而且也從成功的結果獲得支持。&lt;/p&gt;
&lt;h3 id=&#34;broader-impact&#34;&gt;Broader Impact&lt;/h3&gt;
&lt;p&gt;除了本文的直接關注點外，作者相信 Self-Instruct 可能有助於揭露各種 instruction tuning 模型 &amp;ldquo;幕後&amp;rdquo; 發生的事情。&lt;/p&gt;
&lt;p&gt;不幸的是，由於他們的資料集尚未發布，這種業界模型仍處於 API 牆之後。&lt;/p&gt;
&lt;p&gt;人們對其結構以及為何能展現令人印象深刻的能力知之甚少。&lt;/p&gt;
&lt;h3 id=&#34;limitations-of-self-instruct&#34;&gt;Limitations of Self-Instruct&lt;/h3&gt;
&lt;h4 id=&#34;tail-phenomena&#34;&gt;Tail phenomena&lt;/h4&gt;
&lt;p&gt;Self-Instruct 依賴於 LM，繼承 LM 的所有限制。&lt;/p&gt;
&lt;p&gt;最近的研究顯示出 tail phenomena 對 LM 的成功構成嚴峻的挑戰。&lt;/p&gt;
&lt;p&gt;換句話說，LM 的最大收益出現於語言中最頻繁出現的部分 (語言分佈的頭部)，而低頻率出現的上下文中獲得的收益最小。&lt;/p&gt;
&lt;p&gt;同樣的，在這項工作背景下，如果 Self-Instruct 大部分的收益偏向預訓練 corpus 中頻繁出現的任務或指令，那也不令人感到意外。&lt;/p&gt;
&lt;p&gt;因此，該方法在不常見和有創意的指令下，可能會顯現出脆弱性。&lt;/p&gt;
&lt;h4 id=&#34;dependence-on-large-models&#34;&gt;Dependence on large models&lt;/h4&gt;
&lt;p&gt;因為 Self-Instruct 依賴於從 LM 中提取初的 inductive bias，因此它可能適合 larger model。&lt;/p&gt;
&lt;p&gt;如果這是對的，這會對那些沒有大量計算資源的人造成阻礙。&lt;/p&gt;
&lt;h4 id=&#34;reinforcing-lm-biases&#34;&gt;Reinforcing LM biases&lt;/h4&gt;
&lt;p&gt;作者擔心這種迭代作法可能會產生意料之外的結果，比如將有問題的社會偏見放大。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>InstructGPT</title>
        <link>https://roykesydon.github.io/Blog/p/instructgpt/</link>
        <pubDate>Fri, 27 Jan 2023 17:39:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/instructgpt/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;把語言模型變大不代表他們會更好地遵循用戶的意圖。&lt;/p&gt;
&lt;p&gt;大的語言模型有可能會生成 untruthful, toxic, not helpful 的答案。&lt;/p&gt;
&lt;p&gt;該論文透過 fine-tuning with human feedback 來解決這問題。&lt;/p&gt;
&lt;p&gt;一開始準備一系列人工標註的 prompts，然後用這 dataset 對 GPT-3 做 fine-tune。&lt;/p&gt;
&lt;p&gt;接下來再蒐集一個 dataset，存放 rankings of model outputs，由人工判斷輸出好壞，再用 RL 把剛剛 fine-tune 過的 model 繼續 fine-tune。&lt;/p&gt;
&lt;p&gt;最後有 1.3B 參數的 InstructGPT 表現的結果比 175B 參數的 GPT-3 還好。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Large language models(LMs) 可以透過 &amp;ldquo;prompt&amp;rdquo; 來執行各種 NLP 任務。&lt;/p&gt;
&lt;p&gt;但這些模型也常有一些非目的性的行為，諸如捏造事實等等。&lt;/p&gt;
&lt;p&gt;原因是出在目標函數上，多數 LMs 的目標函數是根據網路上的文本生出下一個字詞。&lt;/p&gt;
&lt;p&gt;這和「根據使用者指令生出安全且有幫助的答案不同」。&lt;/p&gt;
&lt;p&gt;上述的差異使語言模型的目標是 misaligned。&lt;/p&gt;
&lt;p&gt;作者的目標是生出 helpful、 honest(沒有誤導性資訊)、harmless 的 model。&lt;/p&gt;
&lt;p&gt;具體作法，使用 reinforcement learning from human feedback(RLHF)。&lt;/p&gt;
&lt;h2 id=&#34;訓練步驟&#34;&gt;訓練步驟&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-train-step.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labelers 明顯偏好 InstructGPT 的答案，勝過 GPT-3 的答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT 的答案在 truthfulness 勝過 GPT-3 的答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT 的答案在 toxicity 上小勝 GPT-3 的答案，但在 bias 上沒有&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;methods&#34;&gt;Methods&lt;/h1&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;標註人員寫很多 prompts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plain:
&lt;ul&gt;
&lt;li&gt;隨便寫任意任務&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Few-shot:
&lt;ul&gt;
&lt;li&gt;想個 instruction，並寫 multiple query/response pairs for that instruction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User-based:
&lt;ul&gt;
&lt;li&gt;根據一些申請使用 OpenAI API 的用戶，提出有關的 prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然後根據這個訓練初步模型，並把這個初步模型放到他們的 Playground 給用戶使用。&lt;/p&gt;
&lt;p&gt;再把用戶問的問題蒐集回來，並做篩選。&lt;/p&gt;
&lt;p&gt;訓練 SFT 的模型用 13k training prompts&lt;/p&gt;
&lt;p&gt;訓練 RM 的模型用 33k training prompts&lt;/p&gt;
&lt;p&gt;訓練 PPO 的模型用 31k training prompts&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Supervised fine-tuning(SFT)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拿 GPT-3 去訓練 16 個 epochs&lt;/li&gt;
&lt;li&gt;跑一個 epoch 就發現 overfitting，但發現訓練更多 epoches 對後面的 RM 有用，而且這個 model 也只是過渡產品&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward modeling(RM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;把 SFT 後面的 unembedding layer 去除掉，接上線性層，最後輸出一個 scalar reward&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用 6B RMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;這模型會吃 prompt 和 response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;人工標記的是排序，不是分數&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對每個 prompt 生出 9 個答案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原本是 4 個，但排 9 個花的時間可能不會到 4 個的兩倍，因為主要心力會花在讀 prompt。但標註訊息會多很多，因為都是兩兩比較。&lt;/li&gt;
&lt;li&gt;而且在 loss 中最多只要丟入 RM 9 次，因為可以重用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pairwise Ranking Loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對一個 prompt(假設是 x)，取出一對回覆(假設是 $y_w$ 和 $y_l$)，算出 RM(x, $y_w$) 和 RM(x, $y_l$)，假設 $y_w$ 比 $y_l$ 排序高，讓 RM(x, $y_w$) - RM(x, $y_l$) 的數值越大越好&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-reward-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement learning(RL)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-rl-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\beta$ 那項是 KL divergence&lt;/li&gt;
&lt;li&gt;$\gamma$ 那項是不想要讓這 model 太專注在微調的任務，而失去原本在其他 NLP 任務也表現很好的功能。
&lt;ul&gt;
&lt;li&gt;$D_{pretrain}$ 是 pretraining distribution&lt;/li&gt;
&lt;li&gt;如果 $\gamma$ 為 0，在該實驗中叫做 PPO，否則，稱為 PPO-ptx&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GPT 三部曲</title>
        <link>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</link>
        <pubDate>Thu, 19 Jan 2023 01:50:07 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</guid>
        <description>&lt;p&gt;GPT 本質上就是 Transformer 的 decoder&lt;/p&gt;
&lt;h1 id=&#34;gpt-1&#34;&gt;GPT-1&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用 semi-supervised，後來被歸為 self-supervised&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h2&gt;
&lt;p&gt;$L_1(U)=\sum_i logP(u_i|u_{i-k},&amp;hellip;,u_{i-1};\theta)$&lt;/p&gt;
&lt;p&gt;$U= \{ u_1,&amp;hellip;,u_n \}$&lt;/p&gt;
&lt;p&gt;$U$ 是一系列未標記的文本 token&lt;/p&gt;
&lt;p&gt;$k$ 是窗口大小&lt;/p&gt;
&lt;h3 id=&#34;模型大致架構&#34;&gt;模型大致架構&lt;/h3&gt;
&lt;p&gt;$h_0=UW_e+W_p$&lt;/p&gt;
&lt;p&gt;$h_1=transformer \_ block(h_{i-1})\forall i \in[1,n]$&lt;/p&gt;
&lt;p&gt;$P(u)=softmax(h_nW^T_e)$&lt;/p&gt;
&lt;p&gt;$U=\{u_{-k},&amp;hellip;,u_{-1}\}$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h2&gt;
&lt;p&gt;$P(y|x^1,&amp;hellip;,x^m)=softmax(h^m_lW_y)$&lt;/p&gt;
&lt;p&gt;$L2(C)=\sum_{(x,y)}log P(y|x^1,&amp;hellip;,x^m)$&lt;/p&gt;
&lt;p&gt;$L_3(C)=L_2(C)+\lambda*L_1(C)$&lt;/p&gt;
&lt;p&gt;$C$ 是 labeled 的資料集，微調基本上就是在後面加上線性層&lt;/p&gt;
&lt;p&gt;作者最大化 likelihood 的時候是用 $L_3$ 而非單純的 $L_2$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;微調應用範例&#34;&gt;微調應用範例&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-1-tasks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;資料集&#34;&gt;資料集&lt;/h2&gt;
&lt;p&gt;用 BooksCorpus 訓練出來的&lt;/p&gt;
&lt;p&gt;有超過 7000 本未出版的書&lt;/p&gt;
&lt;h2 id=&#34;模型結構&#34;&gt;模型結構&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;12 層 transformer 的 decoder&lt;/li&gt;
&lt;li&gt;768 維 word embedding&lt;/li&gt;
&lt;li&gt;12 個 attention heads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;和-bert-base-比較&#34;&gt;和 BERT BASE 比較&lt;/h2&gt;
&lt;p&gt;BERT 論文比較晚出，但 BASE 的模型架構和 GPT 有相似之處，&lt;/p&gt;
&lt;p&gt;BASE 是 12 層的 decoder，word embedding 和 attention head 的維度或數量和 GPT-1 相同&lt;/p&gt;
&lt;h1 id=&#34;gpt-2&#34;&gt;GPT-2&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/paper/language-models-are-unsupervised-multitask&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Unsupervised Multitask Learner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GPT-2 除了用更大的的模型和更大的資料集，把重點放在 zero-shot 上，雖然在 GPT-1 的論文就有提過 zero-shot&lt;/p&gt;
&lt;h2 id=&#34;資料集-1&#34;&gt;資料集&lt;/h2&gt;
&lt;p&gt;這次做了一個叫做 WebText 的資料集，有百萬級別的網頁&lt;/p&gt;
&lt;h3 id=&#34;common-crawl&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;大型爬蟲專案，有大量網頁資料，但充斥了垃圾訊息&lt;/p&gt;
&lt;h3 id=&#34;webtext&#34;&gt;WebText&lt;/h3&gt;
&lt;p&gt;WebText 的資料來源是 reddit 上的外部連結，只要有至少三個 karma，就會被採納，由此取得品質較好的網頁資料。透過這種方法，取得了 4500 萬個連結。並用Dragnet (Peters &amp;amp; Lecocq, 2013) and Newspaper content extractors 把文字訊息從 HTML 中抓出來&lt;/p&gt;
&lt;h2 id=&#34;架構&#34;&gt;架構&lt;/h2&gt;
&lt;p&gt;和原本差不多，變成有 1.5B 參數的 Transformer decoder&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h2&gt;
&lt;p&gt;不需要下游任務的標記資料&lt;/p&gt;
&lt;p&gt;改把任務輸入進模型&lt;/p&gt;
&lt;h3 id=&#34;目前問題&#34;&gt;目前問題&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;現在的模型泛化能力不太好&lt;/li&gt;
&lt;li&gt;Multitask learning
在 NLP 上不太常用，NLP 現在主流還是在預訓練模型上做微調以應對下游任務
&lt;ul&gt;
&lt;li&gt;對每個下游任務都得重新訓練模型&lt;/li&gt;
&lt;li&gt;得蒐集 labeled 資料&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;gpt-3&#34;&gt;GPT-3&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有 175B 的參數，由於模型極大，要在子任務微調會成本很大，所以不做任何梯度更新&lt;/li&gt;
&lt;li&gt;在很多 NLP 任務有傑出的成果&lt;/li&gt;
&lt;li&gt;可以生出人類難以區分的新聞文章&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;目前有的問題&#34;&gt;目前有的問題&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;要在子任務微調，需要資料集&lt;/li&gt;
&lt;li&gt;微調後在有些子任務上表現好不代表你預訓練模型一定泛化能力高&lt;/li&gt;
&lt;li&gt;人類不需要大量 labeled 資料去完成小任務&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;評估方式&#34;&gt;評估方式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;分為三種，few / one / zero-shot learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;架構-1&#34;&gt;架構&lt;/h2&gt;
&lt;p&gt;基本上 GPT-3 和 GPT-2 架構一樣&lt;/p&gt;
&lt;h3 id=&#34;相同&#34;&gt;相同&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;modified initialization&lt;/li&gt;
&lt;li&gt;pre-normalization&lt;/li&gt;
&lt;li&gt;reversible tokenization described therein&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;不同&#34;&gt;不同&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;把 Sparse Transformer 的一些修改拿過來用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;GPT-3 Small 是 GPT-1 的大小
GPT-3 Medium 是 BERT Large 的大小
GPT-3 XL 和 GPT-2 相近，比較淺也比較寬&lt;/p&gt;
&lt;h4 id=&#34;batch-size-大小&#34;&gt;Batch Size 大小&lt;/h4&gt;
&lt;p&gt;模型小的時候需要小一點，透過這種額外的 noise 來避免 overfitting(不確定是不是猜想)&lt;/p&gt;
&lt;h2 id=&#34;資料集-2&#34;&gt;資料集&lt;/h2&gt;
&lt;h3 id=&#34;common-crawl-1&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;架構比 GPT-2 大很多，所以回頭考慮這個資料集&lt;/p&gt;
&lt;h4 id=&#34;三步驟&#34;&gt;三步驟&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;先過濾，透過 reddit 那個高品質的資料集，來訓練一個模型分類高品質和低品質的網頁。&lt;/li&gt;
&lt;li&gt;透過 LSH 演算法把相似的文本過濾掉&lt;/li&gt;
&lt;li&gt;把一些已知高品質的資料集也加進來&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;這是一個 Batch 裡有 60% 來自 Common Crawl(filtered) 的意思
Wikipedia 雖然總量比較少，但也有 3% 的採樣率&lt;/p&gt;
&lt;h2 id=&#34;結果-1&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;計算量指數增長，loss 卻是線性的往下降&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;paper 裡有很多任務的實驗結果，這邊就不附上了&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;在文本生成上還是比較弱，生很長的東西，可能會重複自己說過的話、失去連貫性、自相矛盾等等&lt;/p&gt;
&lt;p&gt;在有些雙向性的任務上可能表現更差&lt;/p&gt;
&lt;h2 id=&#34;影響&#34;&gt;影響&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可能被用來散布不實消息、垃圾郵件等等&lt;/li&gt;
&lt;li&gt;偏見&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;p&gt;在很多 NLP 任務可以做到接近 SOTA 微調模型的成果&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
