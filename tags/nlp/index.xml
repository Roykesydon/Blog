<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>nlp on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/tags/nlp/</link>
        <description>Recent content in nlp on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 30 Apr 2023 00:00:12 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/tags/nlp/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Self-Instruct 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 30 Apr 2023 00:00:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2212.10560&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;大型 &amp;ldquo;instruction-tuned&amp;rdquo; 語言模型 (經過微調好回應 instruction) 已經展現出在新任務上 zero-shot 的能力。&lt;/p&gt;
&lt;p&gt;然而他們嚴重依賴人工編寫的指令，在數量、多樣性和創造力上都受到了限制，阻礙了模型的通用性。&lt;/p&gt;
&lt;p&gt;作者介紹了 Self-Instruct 這個框架，可以透過自己生成的指令，來增強預訓練模型遵循指令的能力。&lt;/p&gt;
&lt;p&gt;將作者的方法應用在 GPT3，在 SuperNaturalInstructions 獲得了比原始模型高 33% 的改進，與使用 private user data 和 human annotations 的 $InstructGPT_{001}$ 性能相當。&lt;/p&gt;
&lt;p&gt;為了進一步評估，我們為新任務整理一組專家編寫的指令，並通過人工評估，顯示出使用 Self-Instruction 調整 GPT3 的性能大大優於使用現有公共指令資料集，只比 $InstructGPT_{001}$ 落後 5% 的差距。&lt;/p&gt;
&lt;p&gt;Self-Instruct 提供一個幾乎 annotation-free 的方法，align 預訓練模型和 instructions，而且作者釋出了他們的大型合成資料集，以促進未來對 instruction tuning 的研究。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;最近的 NLP 文獻見證了「建構可以遵循自然語言指令的模型方面」的大量活動。&lt;/p&gt;
&lt;p&gt;這些發展由兩個關鍵部分組成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;大型預訓練語言模型 (LM)&lt;/li&gt;
&lt;li&gt;人工編寫的指令資料&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PromptSource 和 SuperNaturalInstructions 是最近兩個著名的資料集。
他們透過大量手動註釋來收集指令，以建造 T0 和 T$k$-Instruct。&lt;/p&gt;
&lt;p&gt;然而這過程代價高昂，而且由於大多數人往往生成的都是流行的 NLP 任務，使其未能涵蓋真正多樣的任務，也不能涵蓋各種描述任務的不同方式，因此多樣性受侷限。&lt;/p&gt;
&lt;p&gt;鑒於這些限制，想要繼續提升 instruction-tuned models 的品質，需要幫 supervising instruction-tuned models 發展替代方案。&lt;/p&gt;
&lt;p&gt;本文介紹了 Self-Instruct，這是一種 semi-automated 的過程，用模型自身的 instructional signals 對 pretrained LM 進行 instruction-tuning。&lt;/p&gt;
&lt;p&gt;整個流程是一種 iterative bootstrapping algorithm，從手動編寫的 limited seed set 引導生成。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在第一階段，模型要幫新任務生成指令。
利用現有的指令集合，創建更廣泛的指令，好定義 (通常是新的) 任務。&lt;/p&gt;
&lt;p&gt;對於新生成的指令集，框架為他們創建 input-output instances，稍後可以透過 supervising 用於 instruction tuning。&lt;/p&gt;
&lt;p&gt;最後，透過各種手段，在低品質和重複的指令加到 task pool 前，把他們修剪掉。&lt;/p&gt;
&lt;p&gt;可以重複這個流程非常多次，直到獲得大量任務。&lt;/p&gt;
&lt;p&gt;該模型的跌代過程中產生了大約 52K 個指令，與大約 85K 個 instance inputs 和 target outputs 配對 (有些相同的指令會對應多種輸入輸出)。&lt;/p&gt;
&lt;p&gt;作者觀察到生成的資料提供了各種有創意的任務，其中超過 50% 的任務和 seed instructions 的 ROUGE-L overlap 小於 0.3。&lt;/p&gt;
&lt;p&gt;基於上述結果，作者通過微調 GPT3 (和生成指令資料是同個模型) 建構了 $GPT3_{SELF-INST}$。&lt;/p&gt;
&lt;p&gt;SuperNI 的結果表明，$GPT3_{SELF-INST}$ 性能大大優於 GPT3 (原始模型)，高了 33.1%，幾乎和 $InstructGPT_{001}$ 的性能相當。&lt;/p&gt;
&lt;p&gt;此外，作者在新創建的的指令集上進行人工評估，$GPT3_{SELF-INST}$ 顯示出廣泛的指令遵循能力，優於在其他公開可用指令數據集上訓練的模型，只比 InstrcutGPT001 落後 5%。&lt;/p&gt;
&lt;p&gt;本文貢獻：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Self-Instruct：一種用最少的人工標記數據引導指令遵循能力的作法&lt;/li&gt;
&lt;li&gt;通過大量的 instruction-tuning 實驗，證明了有效性。&lt;/li&gt;
&lt;li&gt;發布了一個包含 52K 指令的大型綜合資料集，還有一組手動編寫的新任務，用於建構和評估未來的 instruction-following models。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;instruction-following-language-models&#34;&gt;Instruction-following language models&lt;/h3&gt;
&lt;p&gt;一系列工作顯示，使用 annotated &amp;ldquo;instructional&amp;rdquo; data，可以使普通語言模型遵循一般語言的指令。&lt;/p&gt;
&lt;p&gt;也顯示出 &amp;ldquo;instructional&amp;rdquo; data 的大小和多樣性直接影響模型的泛化能力。&lt;/p&gt;
&lt;p&gt;本文的工作目的在減少對人工註釋者的依賴。&lt;/p&gt;
&lt;h3 id=&#34;language-models-for-data-generation-and-augmentation&#34;&gt;Language models for data generation and augmentation&lt;/h3&gt;
&lt;p&gt;許多工作依賴生成式 LM 來生成數據或做 augmentation。&lt;/p&gt;
&lt;p&gt;雖然作者的工作可被視為一種 augmentation，但和這些工作的差別在於不限於特定任務。&lt;/p&gt;
&lt;p&gt;Self-Instruct 的一個明顯動機是引導出新的任務定義，而這些任務可能還未被 NLP 的研究者定義過。&lt;/p&gt;
&lt;h3 id=&#34;self-training&#34;&gt;Self-training&lt;/h3&gt;
&lt;p&gt;一種典型的 self-training 框架透過經過訓練的模型，幫 unlabeled 資料進行 label，然後用這些資料改進模型。&lt;/p&gt;
&lt;p&gt;雖然 Self-Instruct 和 self-training 有一些相似之處，但多數 self-training 的方法都假設了一個特定的目標任務。&lt;/p&gt;
&lt;p&gt;相比之下，Self-Instruct 從頭開始生出各種任務。&lt;/p&gt;
&lt;h3 id=&#34;knowledge-distillation&#34;&gt;Knowledge distillation&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;這邊我想不太通為什麼可以和 Knowledge distillation 扯上關係&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Knowledge distillation 通常涉及知識從較大模型到較小模型的轉移&lt;/p&gt;
&lt;p&gt;Self-Instruct 也可以看做是 Knowledge distillation 的一種形式，但區別如下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;distillation 的來源和目標是相同的，即模型的知識被 distill 到他自己&lt;/li&gt;
&lt;li&gt;distill 的內容以 instruction task 的形式出現&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;標記大規模指令資料對人類來說可能具有挑戰性，因為他需要&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;創意，好提出新任務&lt;/li&gt;
&lt;li&gt;為每個任務編寫 labeled instances 的專業知識&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;defining-instruction-data&#34;&gt;Defining Instruction Data&lt;/h3&gt;
&lt;p&gt;我們要生成的指令資料集包含 {$I_t$}，每個指令用自然語言定義了任務 $t$。&lt;/p&gt;
&lt;p&gt;每個任務都有一個或多個 input-output instances ($X_t,Y_t$)。&lt;/p&gt;
&lt;p&gt;給定 task instruction $I_t$，還有 instance x，模型 M 要生出 y：&lt;/p&gt;
&lt;p&gt;$M(I_t,x)=y, for (x,y) \in (X_t,Y_t)$&lt;/p&gt;
&lt;p&gt;值得注意的是，instance input 和 instruction 沒有嚴格分界。&lt;/p&gt;
&lt;p&gt;比如 Instruction:&amp;ldquo;write an essay about school safety&amp;rdquo; x:&amp;quot;&amp;quot;，可以被改為 Instruction:&amp;ldquo;write an essay about the following topic&amp;rdquo; x:&amp;ldquo;school safety&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;automatic-instruction-data-generation&#34;&gt;Automatic Instruction Data Generation&lt;/h3&gt;
&lt;p&gt;生成指令資料的 pipeline 分成四個步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;指令生成&lt;/li&gt;
&lt;li&gt;辨識指令是否是分類任務&lt;/li&gt;
&lt;li&gt;用 input-first 或 output-first 做 instance generation&lt;/li&gt;
&lt;li&gt;過濾掉低品質的資料&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;instruction-generation&#34;&gt;Instruction Generation&lt;/h4&gt;
&lt;p&gt;Self-Instruct 是基於一個發現，也就是大型語言模型可以透過 context 中的現有指令，生出新穎的指令。&lt;/p&gt;
&lt;p&gt;為作者提供了一種從一小組人類編寫的指令中，使指令資料增長的做法。&lt;/p&gt;
&lt;p&gt;作者用他們編寫的 175 個任務 (每個任務 1 個 instruction 和 1 個 instance) 初始化 task pool。&lt;/p&gt;
&lt;p&gt;在每一個 step，作者從裡面 sample 8 個 instructions，作為 in-context 的範例。在這 8 個指令中，有 6 條來自人工編寫的任務，另外兩條來自前面步驟中模型生成的任務，以促進多樣性。&lt;/p&gt;
&lt;h4 id=&#34;classification-task-identification&#34;&gt;Classification Task Identification&lt;/h4&gt;
&lt;p&gt;因為對於分類和非分類的任務，作者會採取兩種做法，所以作者使用來自 seed taks 的 12 條分類指令和 19 條非分類指令，讓 GPT3 透過 few-shot 來判別。&lt;/p&gt;
&lt;h4 id=&#34;instance-generation&#34;&gt;Instance Generation&lt;/h4&gt;
&lt;p&gt;給予指令和他們的任務類別，作者獨立地為每條指令生成 instance。&lt;/p&gt;
&lt;p&gt;這具備挑戰性，原因在於他需要模型瞭解目標任務是什麼，根據指令找出需要那些額外的輸入內容，並生成他們。 (模型要根據 instruction 生出 instance input)&lt;/p&gt;
&lt;p&gt;作者發現，在 prompt 中放入其他包含 instruction-input-output 的任務範例的時候，模型可以實現這點。&lt;/p&gt;
&lt;p&gt;一種自然的方法是 Input-first Approach，可以要求語言模型先根據指令提出 input，再生出相應的 output。&lt;/p&gt;
&lt;p&gt;然而，這種方法在分類任務上，可能會偏向於生成某種 label。所以，對於分類任務，作者採用 Output-first Approach，先生成可能的 label，在每個 label 上再生成輸入。&lt;/p&gt;
&lt;h4 id=&#34;filtering-and-postprocessing&#34;&gt;Filtering and Postprocessing&lt;/h4&gt;
&lt;p&gt;為了鼓勵多樣性，只有當新的指令和任何現有的指令的 ROUGE-L overlapping 小於 0.7 的時候，才會被添加到 task pool。&lt;/p&gt;
&lt;p&gt;還排除了一些包含了通常不能被 LM 處理的關鍵字 (e.g. images, pictures, graphs) 的指令。&lt;/p&gt;
&lt;p&gt;在為每個指令生成新的 instance 的時候，會過濾掉完全相同或者是輸入相同但輸出不同的 instance。&lt;/p&gt;
&lt;h3 id=&#34;finetuning-the-lm-to-follow-instructions&#34;&gt;Finetuning the LM to Follow Instructions&lt;/h3&gt;
&lt;p&gt;在創建大規模指令資料後，用這些資料對原始語言模型進行 fine-tune。&lt;/p&gt;
&lt;p&gt;為此，將 instruction 和 instance input 連接起來，作為 prompt，然後訓練模型透過標準的監督式學習進行微調。&lt;/p&gt;
&lt;p&gt;為了讓模型對不同的格式 robust，使用多個模板將指令和輸入 encode 在一起。&lt;/p&gt;
&lt;p&gt;例如，指令可以有或沒有 Task: 前墜、輸入可以有或沒有 Input: 前墜，或是中間可以有不同數量的換行之類的。&lt;/p&gt;
&lt;h2 id=&#34;self-instruct-data-from-gpt3&#34;&gt;Self-Instruct Data from GPT3&lt;/h2&gt;
&lt;p&gt;作者透過 OpenAI API 訪問最大的 GPT3 (davinci)&lt;/p&gt;
&lt;h3 id=&#34;statistics&#34;&gt;Statistics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;diversity&#34;&gt;Diversity&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;quality&#34;&gt;Quality&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;h3 id=&#34;gpt3_self-inst-fine-tuning-gpt3-on-its-own-instruction-data&#34;&gt;$GPT3_{SELF-INST}$: fine-tuning GPT3 on its own instruction data&lt;/h3&gt;
&lt;p&gt;使用生出來的指令資料，對 GPT3 進行微調。&lt;/p&gt;
&lt;p&gt;微調是透過 OpenAI finetuning API&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;h4 id=&#34;off-the-shelf-language-models&#34;&gt;Off-the-shelf language models&lt;/h4&gt;
&lt;p&gt;T5-LM 和 GPT3 是普通 LM baselines (只有 pre-training，沒有額外 fine-tune)&lt;/p&gt;
&lt;p&gt;這些 baseline 將表明現成的 LM 在預訓練後，能夠立刻自然地遵循指令的程度。&lt;/p&gt;
&lt;h4 id=&#34;publicly-available-instruction-tuned-models&#34;&gt;Publicly-available instruction-tuned models&lt;/h4&gt;
&lt;p&gt;T0 和 $T_k$-Instruct 是兩個 instruction-tuned models。&lt;/p&gt;
&lt;p&gt;兩者都是從 T5 進行微調的，對這兩種模型，都使用具有 11B 參數的最大版本。&lt;/p&gt;
&lt;h4 id=&#34;instruction-tuned-gpt3-models&#34;&gt;Instruction-tuned GPT3 models&lt;/h4&gt;
&lt;p&gt;作者評估了 InstructGPT，它是 OpenAI 基於 GPT3 開發的。&lt;/p&gt;
&lt;p&gt;對於 SuperNI 的實驗，只與 text-davinci-001 engine 進行比較，因為更新的 engine 用最新的用戶資料，而且很可能已經看過 SuperNI。&lt;/p&gt;
&lt;p&gt;對於新編寫的指令，評估時則包含了 001、002 和 003，以確保完整性。&lt;/p&gt;
&lt;p&gt;為了進一步比較 Self-Instruct 在其他公開可用的指令訓練集資料，使用 PromptSource 和 SuperNI 的資料微調 GPT3，這些資料用於訓練 T0 和 $T_k$-Instruct 模型。&lt;/p&gt;
&lt;p&gt;分別簡稱為 T0 訓練和 SuperNI 訓練。&lt;/p&gt;
&lt;h3 id=&#34;experiment-1-zero-shot-generalization-on-superni-benchmark&#34;&gt;Experiment 1: Zero-Shot Generalization on SUPERNI benchmark&lt;/h3&gt;
&lt;p&gt;首先以 zero-shot 的方式評估典型 NLP 任務遵循指令的能力。&lt;/p&gt;
&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;experiment-2-generalization-to-user-oriented-instructions-on-novel-tasks&#34;&gt;Experiment 2: Generalization to User-oriented Instructions on Novel Tasks&lt;/h3&gt;
&lt;p&gt;盡管 SuperNI 在現有的 NLP 任務具有全面性，多數的這些任務是初於研究理由提出的，而且偏向分類。&lt;/p&gt;
&lt;p&gt;為了更好的獲取指令遵循模型的實用價值，作者中的一部分人策劃了一組面向用戶應用的新指令集。&lt;/p&gt;
&lt;p&gt;他們先針對 Large LM 可能可以應用到的領域進行 brainstorm，並且制定與每個領域相關的 instruction 和 instance。&lt;/p&gt;
&lt;p&gt;總共創建了 252 條指令，每條指令有 1 個 instance。&lt;/p&gt;
&lt;h4 id=&#34;human-evaluation-setup&#34;&gt;Human evaluation setup&lt;/h4&gt;
&lt;p&gt;評估模型在這些不同任務的測試集上的表現極具挑戰性，因為不同的任務需要不同的專業知識。&lt;/p&gt;
&lt;p&gt;為了獲得更忠實的評價，作者請了 instructions 的作者對模型的預測結果進行評估。&lt;/p&gt;
&lt;p&gt;實施一個 four-level rating system：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rating A
&lt;ul&gt;
&lt;li&gt;回覆有效且令人滿意&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating B
&lt;ul&gt;
&lt;li&gt;回覆可接受，但存在可以改進的地方&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating C
&lt;ul&gt;
&lt;li&gt;回覆相關，但在內容上有重大錯誤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating D
&lt;ul&gt;
&lt;li&gt;回覆不相關或無效，包含重複輸入的部分，完全無關的輸出。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;results-1&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;如果把 Rating B 以上視為有效，$GPT_{SELF-INST}$ 只和 $InstructGPT_{001}$ 相差 5%&lt;/p&gt;
&lt;h2 id=&#34;discussion-and-limitation&#34;&gt;Discussion and Limitation&lt;/h2&gt;
&lt;h3 id=&#34;why-does-self-instruct-work&#34;&gt;Why does SELF-INSTRUCT work?&lt;/h3&gt;
&lt;p&gt;值得反思的是，在最近成功的 instruction-tuning LMs 中，高品質的 human feedback 扮演的角色。&lt;/p&gt;
&lt;p&gt;這裡有兩個極端的假設：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Human feedback 是 instruction-tuning 中必要且不可或缺的角色，因為 LM 需要了解在預訓練過程中沒完全了解到的問題。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Human feedback 是 instruction-tuning 一個可選的方向，因為 LM 在預訓練就已經很熟悉指令了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;雖然現實可能介於這兩個極端之間，作者推測可能更傾向於第二種假設，尤其是對於較大的模型。&lt;/p&gt;
&lt;p&gt;第二種，也是人類直覺，是 Self- Instruct 的關鍵動機，而且也從成功的結果獲得支持。&lt;/p&gt;
&lt;h3 id=&#34;broader-impact&#34;&gt;Broader Impact&lt;/h3&gt;
&lt;p&gt;除了本文的直接關注點外，作者相信 Self-Instruct 可能有助於揭露各種 instruction tuning 模型 &amp;ldquo;幕後&amp;rdquo; 發生的事情。&lt;/p&gt;
&lt;p&gt;不幸的是，由於他們的資料集尚未發布，這種業界模型仍處於 API 牆之後。&lt;/p&gt;
&lt;p&gt;人們對其結構以及為何能展現令人印象深刻的能力知之甚少。&lt;/p&gt;
&lt;h3 id=&#34;limitations-of-self-instruct&#34;&gt;Limitations of Self-Instruct&lt;/h3&gt;
&lt;h4 id=&#34;tail-phenomena&#34;&gt;Tail phenomena&lt;/h4&gt;
&lt;p&gt;Self-Instruct 依賴於 LM，繼承 LM 的所有限制。&lt;/p&gt;
&lt;p&gt;最近的研究顯示出 tail phenomena 對 LM 的成功構成嚴峻的挑戰。&lt;/p&gt;
&lt;p&gt;換句話說，LM 的最大收益出現於語言中最頻繁出現的部分 (語言分佈的頭部)，而低頻率出現的上下文中獲得的收益最小。&lt;/p&gt;
&lt;p&gt;同樣的，在這項工作背景下，如果 Self-Instruct 大部分的收益偏向預訓練 corpus 中頻繁出現的任務或指令，那也不令人感到意外。&lt;/p&gt;
&lt;p&gt;因此，該方法在不常見和有創意的指令下，可能會顯現出脆弱性。&lt;/p&gt;
&lt;h4 id=&#34;dependence-on-large-models&#34;&gt;Dependence on large models&lt;/h4&gt;
&lt;p&gt;因為 Self-Instruct 依賴於從 LM 中提取初的 inductive bias，因此它可能適合 larger model。&lt;/p&gt;
&lt;p&gt;如果這是對的，這會對那些沒有大量計算資源的人造成阻礙。&lt;/p&gt;
&lt;h4 id=&#34;reinforcing-lm-biases&#34;&gt;Reinforcing LM biases&lt;/h4&gt;
&lt;p&gt;作者擔心這種迭代作法可能會產生意料之外的結果，比如將有問題的社會偏見放大。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>InstructGPT</title>
        <link>https://roykesydon.github.io/Blog/p/instructgpt/</link>
        <pubDate>Fri, 27 Jan 2023 17:39:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/instructgpt/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;把語言模型變大不代表他們會更好地遵循用戶的意圖。&lt;/p&gt;
&lt;p&gt;大的語言模型有可能會生成 untruthful, toxic, not helpful 的答案。&lt;/p&gt;
&lt;p&gt;該論文透過 fine-tuning with human feedback 來解決這問題。&lt;/p&gt;
&lt;p&gt;一開始準備一系列人工標註的 prompts，然後用這 dataset 對 GPT-3 做 fine-tune。&lt;/p&gt;
&lt;p&gt;接下來再蒐集一個 dataset，存放 rankings of model outputs，由人工判斷輸出好壞，再用 RL 把剛剛 fine-tune 過的 model 繼續 fine-tune。&lt;/p&gt;
&lt;p&gt;最後有 1.3B 參數的 InstructGPT 表現的結果比 175B 參數的 GPT-3 還好。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Large language models(LMs) 可以透過 &amp;ldquo;prompt&amp;rdquo; 來執行各種 NLP 任務。&lt;/p&gt;
&lt;p&gt;但這些模型也常有一些非目的性的行為，諸如捏造事實等等。&lt;/p&gt;
&lt;p&gt;原因是出在目標函數上，多數 LMs 的目標函數是根據網路上的文本生出下一個字詞。&lt;/p&gt;
&lt;p&gt;這和「根據使用者指令生出安全且有幫助的答案不同」。&lt;/p&gt;
&lt;p&gt;上述的差異使語言模型的目標是 misaligned。&lt;/p&gt;
&lt;p&gt;作者的目標是生出 helpful、 honest(沒有誤導性資訊)、harmless 的 model。&lt;/p&gt;
&lt;p&gt;具體作法，使用 reinforcement learning from human feedback(RLHF)。&lt;/p&gt;
&lt;h2 id=&#34;訓練步驟&#34;&gt;訓練步驟&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-train-step.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labelers 明顯偏好 InstructGPT 的答案，勝過 GPT-3 的答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT 的答案在 truthfulness 勝過 GPT-3 的答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT 的答案在 toxicity 上小勝 GPT-3 的答案，但在 bias 上沒有&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;methods&#34;&gt;Methods&lt;/h1&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;標註人員寫很多 prompts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plain:
&lt;ul&gt;
&lt;li&gt;隨便寫任意任務&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Few-shot:
&lt;ul&gt;
&lt;li&gt;想個 instruction，並寫 multiple query/response pairs for that instruction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User-based:
&lt;ul&gt;
&lt;li&gt;根據一些申請使用 OpenAI API 的用戶，提出有關的 prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然後根據這個訓練初步模型，並把這個初步模型放到他們的 Playground 給用戶使用。&lt;/p&gt;
&lt;p&gt;再把用戶問的問題蒐集回來，並做篩選。&lt;/p&gt;
&lt;p&gt;訓練 SFT 的模型用 13k training prompts&lt;/p&gt;
&lt;p&gt;訓練 RM 的模型用 33k training prompts&lt;/p&gt;
&lt;p&gt;訓練 PPO 的模型用 31k training prompts&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Supervised fine-tuning(SFT)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拿 GPT-3 去訓練 16 個 epochs&lt;/li&gt;
&lt;li&gt;跑一個 epoch 就發現 overfitting，但發現訓練更多 epoches 對後面的 RM 有用，而且這個 model 也只是過渡產品&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward modeling(RM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;把 SFT 後面的 unembedding layer 去除掉，接上線性層，最後輸出一個 scalar reward&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用 6B RMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;這模型會吃 prompt 和 response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;人工標記的是排序，不是分數&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對每個 prompt 生出 9 個答案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原本是 4 個，但排 9 個花的時間可能不會到 4 個的兩倍，因為主要心力會花在讀 prompt。但標註訊息會多很多，因為都是兩兩比較。&lt;/li&gt;
&lt;li&gt;而且在 loss 中最多只要丟入 RM 9 次，因為可以重用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pairwise Ranking Loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對一個 prompt(假設是 x)，取出一對回覆(假設是 $y_w$ 和 $y_l$)，算出 RM(x, $y_w$) 和 RM(x, $y_l$)，假設 $y_w$ 比 $y_l$ 排序高，讓 RM(x, $y_w$) - RM(x, $y_l$) 的數值越大越好&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-reward-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement learning(RL)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-rl-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\beta$ 那項是 KL divergence&lt;/li&gt;
&lt;li&gt;$\gamma$ 那項是不想要讓這 model 太專注在微調的任務，而失去原本在其他 NLP 任務也表現很好的功能。
&lt;ul&gt;
&lt;li&gt;$D_{pretrain}$ 是 pretraining distribution&lt;/li&gt;
&lt;li&gt;如果 $\gamma$ 為 0，在該實驗中叫做 PPO，否則，稱為 PPO-ptx&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GPT 三部曲</title>
        <link>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</link>
        <pubDate>Thu, 19 Jan 2023 01:50:07 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</guid>
        <description>&lt;p&gt;GPT 本質上就是 Transformer 的 decoder&lt;/p&gt;
&lt;h1 id=&#34;gpt-1&#34;&gt;GPT-1&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用 semi-supervised，後來被歸為 self-supervised&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h2&gt;
&lt;p&gt;$L_1(U)=\sum_i logP(u_i|u_{i-k},&amp;hellip;,u_{i-1};\theta)$&lt;/p&gt;
&lt;p&gt;$U= \{ u_1,&amp;hellip;,u_n \}$&lt;/p&gt;
&lt;p&gt;$U$ 是一系列未標記的文本 token&lt;/p&gt;
&lt;p&gt;$k$ 是窗口大小&lt;/p&gt;
&lt;h3 id=&#34;模型大致架構&#34;&gt;模型大致架構&lt;/h3&gt;
&lt;p&gt;$h_0=UW_e+W_p$&lt;/p&gt;
&lt;p&gt;$h_1=transformer \_ block(h_{i-1})\forall i \in[1,n]$&lt;/p&gt;
&lt;p&gt;$P(u)=softmax(h_nW^T_e)$&lt;/p&gt;
&lt;p&gt;$U=\{u_{-k},&amp;hellip;,u_{-1}\}$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h2&gt;
&lt;p&gt;$P(y|x^1,&amp;hellip;,x^m)=softmax(h^m_lW_y)$&lt;/p&gt;
&lt;p&gt;$L2(C)=\sum_{(x,y)}log P(y|x^1,&amp;hellip;,x^m)$&lt;/p&gt;
&lt;p&gt;$L_3(C)=L_2(C)+\lambda*L_1(C)$&lt;/p&gt;
&lt;p&gt;$C$ 是 labeled 的資料集，微調基本上就是在後面加上線性層&lt;/p&gt;
&lt;p&gt;作者最大化 likelihood 的時候是用 $L_3$ 而非單純的 $L_2$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;微調應用範例&#34;&gt;微調應用範例&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-1-tasks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;資料集&#34;&gt;資料集&lt;/h2&gt;
&lt;p&gt;用 BooksCorpus 訓練出來的&lt;/p&gt;
&lt;p&gt;有超過 7000 本未出版的書&lt;/p&gt;
&lt;h2 id=&#34;模型結構&#34;&gt;模型結構&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;12 層 transformer 的 decoder&lt;/li&gt;
&lt;li&gt;768 維 word embedding&lt;/li&gt;
&lt;li&gt;12 個 attention heads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;和-bert-base-比較&#34;&gt;和 BERT BASE 比較&lt;/h2&gt;
&lt;p&gt;BERT 論文比較晚出，但 BASE 的模型架構和 GPT 有相似之處，&lt;/p&gt;
&lt;p&gt;BASE 是 12 層的 decoder，word embedding 和 attention head 的維度或數量和 GPT-1 相同&lt;/p&gt;
&lt;h1 id=&#34;gpt-2&#34;&gt;GPT-2&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/paper/language-models-are-unsupervised-multitask&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Unsupervised Multitask Learner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GPT-2 除了用更大的的模型和更大的資料集，把重點放在 zero-shot 上，雖然在 GPT-1 的論文就有提過 zero-shot&lt;/p&gt;
&lt;h2 id=&#34;資料集-1&#34;&gt;資料集&lt;/h2&gt;
&lt;p&gt;這次做了一個叫做 WebText 的資料集，有百萬級別的網頁&lt;/p&gt;
&lt;h3 id=&#34;common-crawl&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;大型爬蟲專案，有大量網頁資料，但充斥了垃圾訊息&lt;/p&gt;
&lt;h3 id=&#34;webtext&#34;&gt;WebText&lt;/h3&gt;
&lt;p&gt;WebText 的資料來源是 reddit 上的外部連結，只要有至少三個 karma，就會被採納，由此取得品質較好的網頁資料。透過這種方法，取得了 4500 萬個連結。並用Dragnet (Peters &amp;amp; Lecocq, 2013) and Newspaper content extractors 把文字訊息從 HTML 中抓出來&lt;/p&gt;
&lt;h2 id=&#34;架構&#34;&gt;架構&lt;/h2&gt;
&lt;p&gt;和原本差不多，變成有 1.5B 參數的 Transformer decoder&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h2&gt;
&lt;p&gt;不需要下游任務的標記資料&lt;/p&gt;
&lt;p&gt;改把任務輸入進模型&lt;/p&gt;
&lt;h3 id=&#34;目前問題&#34;&gt;目前問題&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;現在的模型泛化能力不太好&lt;/li&gt;
&lt;li&gt;Multitask learning
在 NLP 上不太常用，NLP 現在主流還是在預訓練模型上做微調以應對下游任務
&lt;ul&gt;
&lt;li&gt;對每個下游任務都得重新訓練模型&lt;/li&gt;
&lt;li&gt;得蒐集 labeled 資料&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;gpt-3&#34;&gt;GPT-3&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有 175B 的參數，由於模型極大，要在子任務微調會成本很大，所以不做任何梯度更新&lt;/li&gt;
&lt;li&gt;在很多 NLP 任務有傑出的成果&lt;/li&gt;
&lt;li&gt;可以生出人類難以區分的新聞文章&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;目前有的問題&#34;&gt;目前有的問題&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;要在子任務微調，需要資料集&lt;/li&gt;
&lt;li&gt;微調後在有些子任務上表現好不代表你預訓練模型一定泛化能力高&lt;/li&gt;
&lt;li&gt;人類不需要大量 labeled 資料去完成小任務&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;評估方式&#34;&gt;評估方式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;分為三種，few / one / zero-shot learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;架構-1&#34;&gt;架構&lt;/h2&gt;
&lt;p&gt;基本上 GPT-3 和 GPT-2 架構一樣&lt;/p&gt;
&lt;h3 id=&#34;相同&#34;&gt;相同&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;modified initialization&lt;/li&gt;
&lt;li&gt;pre-normalization&lt;/li&gt;
&lt;li&gt;reversible tokenization described therein&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;不同&#34;&gt;不同&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;把 Sparse Transformer 的一些修改拿過來用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;GPT-3 Small 是 GPT-1 的大小
GPT-3 Medium 是 BERT Large 的大小
GPT-3 XL 和 GPT-2 相近，比較淺也比較寬&lt;/p&gt;
&lt;h4 id=&#34;batch-size-大小&#34;&gt;Batch Size 大小&lt;/h4&gt;
&lt;p&gt;模型小的時候需要小一點，透過這種額外的 noise 來避免 overfitting(不確定是不是猜想)&lt;/p&gt;
&lt;h2 id=&#34;資料集-2&#34;&gt;資料集&lt;/h2&gt;
&lt;h3 id=&#34;common-crawl-1&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;架構比 GPT-2 大很多，所以回頭考慮這個資料集&lt;/p&gt;
&lt;h4 id=&#34;三步驟&#34;&gt;三步驟&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;先過濾，透過 reddit 那個高品質的資料集，來訓練一個模型分類高品質和低品質的網頁。&lt;/li&gt;
&lt;li&gt;透過 LSH 演算法把相似的文本過濾掉&lt;/li&gt;
&lt;li&gt;把一些已知高品質的資料集也加進來&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;這是一個 Batch 裡有 60% 來自 Common Crawl(filtered) 的意思
Wikipedia 雖然總量比較少，但也有 3% 的採樣率&lt;/p&gt;
&lt;h2 id=&#34;結果-1&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;計算量指數增長，loss 卻是線性的往下降&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;paper 裡有很多任務的實驗結果，這邊就不附上了&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;在文本生成上還是比較弱，生很長的東西，可能會重複自己說過的話、失去連貫性、自相矛盾等等&lt;/p&gt;
&lt;p&gt;在有些雙向性的任務上可能表現更差&lt;/p&gt;
&lt;h2 id=&#34;影響&#34;&gt;影響&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可能被用來散布不實消息、垃圾郵件等等&lt;/li&gt;
&lt;li&gt;偏見&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;p&gt;在很多 NLP 任務可以做到接近 SOTA 微調模型的成果&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
