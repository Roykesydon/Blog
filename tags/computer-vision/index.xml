<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>computer-vision on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/tags/computer-vision/</link>
        <description>Recent content in computer-vision on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 31 Mar 2024 00:27:55 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ğŸ¦‘SQUIDğŸ¦‘ è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 31 Mar 2024 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.13495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Radiography imaging protocols (æ”¾å°„ç·šæˆåƒå”å®š) æœƒå°ˆæ³¨æ–¼ç‰¹å®šçš„èº«é«”å€åŸŸï¼Œå› æ­¤æœƒåœ¨æ‚£è€…é–“ç”¢ç”Ÿå¤§é‡ç›¸ä¼¼çš„ç…§ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†åˆ©ç”¨é€™ç¨® structed informationï¼Œä½œè€…æå‡ºäº† Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (SQUID)ï¼Œå®ƒå¯ä»¥æŠŠå›ºæœ‰çš„äººé«”çµæ§‹åˆ†é¡ç‚ºåè¦†å‡ºç¾çš„ patternã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ¨ç†ç‹€æ…‹ä¸‹ï¼Œå®ƒå¯ä»¥è­˜åˆ¥åœ–ç‰‡ä¸­çš„ç•°å¸¸æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”è¼ƒå…©å€‹ chest X-ray benchmarkï¼ŒSQUID åœ¨éç›£ç£ç•°å¸¸æª¢æ¸¬ä¸Šè¶…è¶Šäº† 13 ç¨® SOTA æ–¹æ³•è‡³å°‘ 5 å€‹ç™¾åˆ†é»ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„å‰µå»ºäº†ä¸€å€‹æ–°çš„è³‡æ–™é›† (DigitAnatomy)ï¼Œè©²è³‡æ–™é›†çµåˆäº†èƒ¸è…”è§£å‰–å­¸ä¸­çš„ spatial correlation å’Œ consistent shape é€™å…©å€‹ç‰¹æ€§ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;æ”¾å°„ç·šæˆåƒå’Œä¸€èˆ¬åœ–ç‰‡çš„å·®åˆ¥
&lt;ul&gt;
&lt;li&gt;ä¸€èˆ¬çš„ photographic imaging å’Œ radiography imaging æ˜¯ä¸åŒçš„ã€‚ä¸€èˆ¬çš„åœ–ç‰‡ç‰©é«”ï¼Œæˆ‘å€‘æœƒå‡è¨­ translation invariance (å¹³ç§»ä¸è®Šæ€§)ï¼Œç„¡è«–è²“åœ¨å·¦å³ï¼Œéƒ½æ˜¯è²“ã€‚ä½†æ˜¯åœ¨æ”¾å°„ç·šæˆåƒä¸­ï¼Œçµæ§‹çš„ç›¸å°ä½ç½®å’Œæ–¹å‘æ˜¯è¾¨åˆ¥æ­£å¸¸å’Œç•°å¸¸çš„é‡è¦ç‰¹å¾µã€‚&lt;/li&gt;
&lt;li&gt;è€Œä¸”ç”±æ–¼ radiography imaging protocols ä»¥ç›¸ç•¶ä¸€è‡´çš„æ–¹å‘è©•ä¼°æ‚£è€…ï¼Œæˆåƒåœ¨ä¸åŒçš„è¨­å‚™è£½é€ å•†ã€è¨­æ–½ä½ç½®é‚„æœ‰æ‚£è€…çš„æƒ…æ³ä¸‹ï¼Œéƒ½å…·æœ‰å¾ˆå¤§çš„ç›¸ä¼¼æ€§ã€‚åƒé€™æ¨£åè¦†å‡ºç¾ä¸”ä¸€è‡´çš„çµæ§‹ï¼Œæœ‰åŠ©æ–¼åˆ†æå•é¡Œï¼Œæ˜¯æ”¾å°„ç·šæˆåƒçš„å„ªå‹¢ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æœ‰å¤šé …ç ”ç©¶è­‰æ˜äº†è¨±å¤šå…ˆé©—çŸ¥è­˜åœ¨å¢å¼·æ·±åº¦å­¸ç¿’æ¨¡å‹æ€§èƒ½ä¸Šçš„å„ªå‹¢ï¼Œæ¯”å¦‚æ·»åŠ  location featuresã€ä¿®æ”¹ç›®æ¨™å‡½æ•¸é‚„æœ‰ç´„æŸç›¸å°æ–¼ç…§ç‰‡ä¸­ landmarks çš„ç›¸å°åº§æ¨™ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æƒ³è§£æ±ºçš„å•é¡Œ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¤šé” 80% çš„è‡¨åºŠéŒ¯èª¤æ˜¯ç”±æ–¼æ”¾å°„ç§‘é†«ç”Ÿæ¼æ‰ç•°å¸¸è€Œé€ æˆã€‚&lt;/li&gt;
&lt;li&gt;æœ¬æ–‡æƒ³å›ç­”ä¸€å€‹é—œéµå•é¡Œï¼šæœ‰æ²’æœ‰è¾¦æ³•åˆ©ç”¨ anatomical patterns çš„ consistency å’Œ spatial informationï¼Œåœ¨æ²’æœ‰æ‰‹å‹•æ¨™è¨»çš„æƒ…æ³ä¸‹ï¼ŒåŠ å¼·æ·±åº¦å­¸ç¿’æ¨¡å‹çš„ç•°å¸¸æª¢æ¸¬èƒ½åŠ›ï¼Ÿéç›£ç£çš„ç•°å¸¸æª¢æ¸¬åªç”¨å¥åº·çš„åœ–ç‰‡é€²è¡Œè¨“ç·´ï¼Œä¸ç”¨ç–¾ç—…è¨ºæ–·æˆ–ä»»ä½• labelã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SQUID è§£æ±ºè¾¦æ³•&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ¬æ–‡ä¸åƒå…ˆå‰çš„ç•°å¸¸æª¢æ¸¬æ–¹æ³•ï¼Œæœ¬æ–‡æŠŠ task åˆ¶å®šç‚º in-painting task (åœ–åƒä¿®å¾©)ï¼Œå¥½åˆ©ç”¨æ”¾å°„ç·šæˆåƒçš„å¤–è§€ã€ä½ç½®ã€å¸ƒå±€ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½œè€…æå‡ºäº† SQUIDï¼Œåœ¨è¨“ç·´éç¨‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥é€éç©ºé–“ä¸­ç¶“å¸¸å‡ºç¾çš„ anoatomical patterns ä¾†å‹•æ…‹ç¶­è­·ä¸€å€‹ visual pattern dictionaryã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”±æ–¼è§£å‰–å­¸çš„ consistencyï¼Œå¥åº·æˆåƒä¸­çš„èº«é«”å€åŸŸæœƒå‘ˆç¾é¡ä¼¼çš„ visual patternï¼Œä½¿ unique pattern çš„æ•¸é‡æ˜¯å¯æ§çš„ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨æ¨ç†éšæ®µï¼Œç”±æ–¼ dictionary ä¸å­˜åœ¨ anomaly patternï¼Œå› æ­¤å¦‚æœå­˜åœ¨ç•°å¸¸ï¼Œç”¢ç”Ÿçš„æ”¾å°„ç·šæˆåƒæœƒå’Œç¾å¯¦æœ‰æ‰€å·®è·ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯ä»¥é€éå€åˆ†ä¿®å¾©ä»»å‹™çš„å“è³ªä¾†è­˜åˆ¥ç•°å¸¸ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯¦é©—å‡è¨­&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç•°å¸¸æª¢æ¸¬çš„æˆåŠŸåŸºæ–¼å…©å€‹å‡è¨­
&lt;ul&gt;
&lt;li&gt;è³‡æ–™ä¸­å¾ˆå°‘ç•°å¸¸åœ–ç‰‡&lt;/li&gt;
&lt;li&gt;ç•°å¸¸å’Œæ­£å¸¸æœ‰é¡¯è‘—ä¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯¦é©—&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨å…©å€‹å¤§è¦æ¨¡ã€å…¬é–‹çš„æ”¾å°„ç·šæˆåƒè³‡æ–™é›†ä¸Šå¯¦é©—
&lt;ul&gt;
&lt;li&gt;ZhangLab
&lt;ul&gt;
&lt;li&gt;åœ¨éç›£ç£æ–¹é¢è´ SOTA è¶…é 5 å€‹ç™¾åˆ†é»&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;æ¯”æœ€è¿‘çš„ 13 ç¨®æ–¹æ³•æé«˜ 10 å€‹ç™¾åˆ†é»&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ–°è³‡æ–™é›†&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‰µå»ºäº† DigitAnatomy è³‡æ–™é›†ï¼Œé—¡æ˜èƒ¸è…”è§£å‰–çµæ§‹çš„ spatial correlation å’Œ consistent shapeã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è²¢ç»ç¸½çµ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨èƒ¸è…”æ”¾å°„ç·šæˆåƒçš„æ–°éç›£ç£ SOTA ç•°å¸¸æª¢æ¸¬æ–¹æ³•&lt;/li&gt;
&lt;li&gt;æ–°çš„ç¶œåˆè³‡æ–™é›†&lt;/li&gt;
&lt;li&gt;ç™¼æ˜æ–°æ–¹æ³•æ‰“æ•—ä¸»æµéç›£ç£ç•°å¸¸æª¢æ¸¬æ–¹æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Anomaly detection in natural imaging
&lt;ul&gt;
&lt;li&gt;è­˜åˆ¥åé›¢æ­£å¸¸è³‡æ–™åˆ†ä½ˆçš„ç½•è¦‹äº‹ä»¶&lt;/li&gt;
&lt;li&gt;ç”±æ–¼ç•°å¸¸æ¨£æœ¬çš„ç¼ºä¹ï¼Œå¾Œä¾†çš„å·¥ä½œéƒ½åˆ¶å®šç‚ºéç›£ç£å­¸ç¿’å•é¡Œ&lt;/li&gt;
&lt;li&gt;å¤§è‡´åˆ†ç‚ºå…©é¡
&lt;ul&gt;
&lt;li&gt;reconstruction-based
&lt;ul&gt;
&lt;li&gt;æ¢å¾©åŸå§‹è¼¸å…¥ä¸¦åˆ†æé‡å»ºèª¤å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;density-based
&lt;ul&gt;
&lt;li&gt;é€éä¼°è¨ˆæ­£å¸¸è³‡æ–™çš„åˆ†ä½ˆä¾†é æ¸¬ç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä¸éé€™äº›æ–¹æ³•éƒ½æ²’è¾¦æ³•è§£é‡‹å¯èƒ½çš„ç•°å¸¸ï¼Œæœ¬æ–‡é€éç¶­è­· visual pattern memory ä¾†è§£æ±ºé€™å€‹å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anomaly detection in medical imaging
&lt;ul&gt;
&lt;li&gt;åŸºæ–¼ç›£ç£å­¸ç¿’çš„æ–¹æ³•å¤šåŠç”¨æ–¼æª¢æ¸¬ç‰¹å®šç¨®é¡çš„ç•°å¸¸ï¼Œæ¯”å¦‚è…«ç˜¤&lt;/li&gt;
&lt;li&gt;æœ€è¿‘æå‡ºäº†ä¸€äº›ç„¡ç›£ç£æ–¹æ³•ä¾†æª¢æ¸¬ä¸€èˆ¬ç•°å¸¸ï¼Œå’Œ GAN æœ‰é—œï¼Œä½†æ˜¯é€™äº›æ–¹æ³•éœ€è¦æœ‰é—œæ–¼ç•°å¸¸ç¨®é¡çš„å¼·å¤§å…ˆé©—çŸ¥è­˜å’Œå‡è¨­æ‰èƒ½ä½¿å¢å¼·æœ‰æ•ˆ&lt;/li&gt;
&lt;li&gt;å’Œä¸€èˆ¬çš„ç…§ç‰‡ä¸åŒï¼ŒRadiography imaging protocols ç”Ÿæˆå…·ä¸€è‡´æ€§çš„åœ–ç‰‡ï¼Œç•°å¸¸çš„è®ŠåŒ–æ¯”è¼ƒå¾®å¦™ (subtle)ï¼Œæª¢æ¸¬èµ·ä¾†æ›´å…·æŒ‘æˆ°ï¼Œä½œè€…åˆ©ç”¨æ”¾å°„ç·šæˆåƒçš„ç‰¹æ€§ï¼Œå¤§å¤§æé«˜æª¢æ¸¬æ€§èƒ½ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory networks
&lt;ul&gt;
&lt;li&gt;éå¾€æœ‰ä¸€äº›æœ‰é—œæ–¼æŠŠ Memory modules ç´å…¥ç¥ç¶“ç¶²è·¯çš„ç ”ç©¶ï¼Œå…¶ä¸­æœ‰æ¡ç”¨åˆ° Memory Matrixã€‚æœ¬æ–‡å…‹æœäº† Memory matrix çš„ä¾·é™æ€§ï¼Œä¸¦æå‡ºä¸€ç¨®æœ‰æ•ˆä¸”é«˜æ•ˆç‡çš„çš„ memory queueã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;squid&#34;&gt;SQUID&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Feature extraction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠåœ–ç‰‡åˆ‡æˆ N x N å€‹ non-overlapping patchesï¼Œç„¶å¾Œé¤µå…¥ä¸€å€‹ encoder åšç‰¹å¾µæå–ï¼Œé€™è£¡æ˜¯ç”¨ CNN æå–ï¼Œä½†è¦ç”¨å…¶ä»– backbone ä¹Ÿå¯ä»¥&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image reconstruction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é€™è£¡æœƒç”¨ teacher å’Œ student generator
&lt;ul&gt;
&lt;li&gt;teacher
&lt;ul&gt;
&lt;li&gt;ç›´æ¥ç”¨ encoder çš„ feature é‡å»ºåœ–ç‰‡&lt;/li&gt;
&lt;li&gt;æœ¬è³ªä¸Šæ˜¯ auto-encoder&lt;/li&gt;
&lt;li&gt;ä½œç‚º regularizer ä¾†é¿å… student generator é‡è¤‡ç”Ÿæˆç›¸åŒçš„æ­£å¸¸åœ–ç‰‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;student
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨ in-painting block å¢å¼·å¾Œçš„ feature ä¾†é‡å»ºï¼Œæœ€å¾Œæœƒè¢«ç”¨åœ¨ discrimination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å…©å€‹ generator æœƒåœ¨æ¯å€‹ up-sampling level ç”¨ knowledge distillation paradigm ä¾†çµåˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anomaly discrimination&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨ adversarial learning å¾Œï¼Œä½¿ç”¨ discriminator ä¾†å€åˆ†æ­£å¸¸å’Œç•°å¸¸&lt;/li&gt;
&lt;li&gt;ç”¨ 2 å€‹ generator ä¾†ç”Ÿæˆåœ–ç‰‡ï¼Œå†ç”¨ discriminator ä¾†å€åˆ†ï¼Œåªæœ‰ student generator æœƒæ¥æ”¶ discriminator çš„æ¢¯åº¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inventing-memory-queue-as-dictionary&#34;&gt;Inventing Memory Queue as Dictionary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Motivation
&lt;ul&gt;
&lt;li&gt;Memory Matrix è¢«å»£æ³›æ¡ç”¨
&lt;ul&gt;
&lt;li&gt;Feature æœƒé€éåœ¨ Memory matrix åšåŠ æ¬Šå¹³å‡ä¾†å¼·åŒ–&lt;/li&gt;
&lt;li&gt;ç¼ºé»
&lt;ul&gt;
&lt;li&gt;é€™æ¨£çš„å¢å¼·æ–¹æ³•æ˜¯å°æ•´å¼µåœ–ç‰‡çš„æå‡ºçš„ç‰¹å¾µåšçš„ï¼Œä¸Ÿæ£„äº†åœ–ç‰‡ä¸­çš„ spatial informationã€‚å°è‡´ä»–ç„¡æ³•æ„ŸçŸ¥åˆ°æ”¾å°„ç·šæˆåƒä¸­çš„ä¸€è‡´æ€§çµæ§‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Space-aware memory
&lt;ul&gt;
&lt;li&gt;ç‚ºäº†åˆ©ç”¨ç©ºé–“è³‡è¨Šï¼Œä½œè€…åªå°‡ patch è€Œä¸æ˜¯æ•´å¼µåœ–ç‰‡å‚³éåˆ° modelï¼Œè®“ patch åªèƒ½å­˜å– Memory matrix ä¸­å°æ‡‰åˆ°çš„å€æ®µï¼Œä½œè€…æŠŠé€™ç¨®ç­–ç•¥ç¨±ç‚º Space-aware memoryï¼Œè€Œä¸”é‚„å¯ä»¥åŠ å¿«é€Ÿåº¦ï¼Œå› ç‚ºä¸ç”¨å­˜å–æ•´å€‹ Memory matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory queue
&lt;ul&gt;
&lt;li&gt;åœ¨ learning-based Memory matrix ä¸­ï¼Œnormal patterns æ˜¯ç”± matrix ä¸­çš„ learned basis çµ„åˆè€Œæˆï¼Œä½†çµ„åˆå‡ºä¾†çš„æ±è¥¿å’Œç¾å¯¦ç…§ç‰‡çš„ç‰¹å¾µç¸½æœƒæœ‰åˆ†ä½ˆå·®è·ï¼Œä½¿å¾ŒçºŒçš„å½±åƒç”Ÿæˆè®Šå¾—å›°é›£&lt;/li&gt;
&lt;li&gt;ä½œè€…æå‡º memory queueï¼Œç”¨ä¾†åœ¨è¨“ç·´æœŸé–“å„²å­˜çœŸå¯¦çš„å½±åƒ featureï¼Œå¾è€Œå‘ˆç¾å’Œå½±åƒç‰¹å¾µç›¸åŒçš„åˆ†ä½ˆã€‚å®ƒåœ¨è¨“ç·´æœŸé–“æœƒæŠŠå…ˆå‰çœ‹åˆ°çš„ç‰¹å¾µç›´æ¥è¤‡è£½åˆ° queue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gumbel shrinkage
&lt;ul&gt;
&lt;li&gt;æ§åˆ¶ memory matrix ä¸­ activated pattern çš„æ•¸é‡æ˜¯æœ‰åˆ©çš„ï¼Œä½†å¦‚æœç”¨ hard shrinkage threashold æœƒç„¡æ³•è™•ç†æ‰¾ä¸åˆ°åˆé© entry çš„æƒ…æ³ã€‚ä¸€ç¨®è‡ªç„¶çš„è§£æ³•æ˜¯è®“æ¢¯åº¦æµéå‰ k å€‹ç›¸ä¼¼çš„ entryï¼Œå…¶é¤˜çš„ä¸æ›´æ–°ã€‚ä½†é€™æ¨£åˆæœƒå°è‡´æœªå•Ÿå‹•çš„ entry ç„¡æ³•æ¥æ”¶ä»»ä½•æ¢¯åº¦ä¸¦æ›´æ–°ï¼Œå› æ­¤æå‡ºäº† Gumbel shrinkage schema
&lt;ul&gt;
&lt;li&gt;$w&amp;rsquo; = sg(hs(w,topk(w)) - \phi(w)) + \phi(w)$
&lt;ul&gt;
&lt;li&gt;$w$ ä»£è¡¨ feature å’Œ entry çš„ç›¸ä¼¼åº¦&lt;/li&gt;
&lt;li&gt;$sg(\cdot)$ ä»£è¡¨ stop-gradientï¼Œä¸è¨ˆç®—è¼¸å…¥çš„æ¢¯åº¦&lt;/li&gt;
&lt;li&gt;$hs(\cdot, t)$ ä»£è¡¨ hard shrinkageï¼Œæœ‰å€‹ threshold $t$&lt;/li&gt;
&lt;li&gt;$\phi(\cdot)$ ä»£è¡¨ softmax&lt;/li&gt;
&lt;li&gt;é€™æ¨£ä¿ç•™äº† top k ä½œç‚º w çš„æœ€çµ‚çµæœï¼Œåˆç”¨ softmax å°æ‰€æœ‰ entry é€²è¡Œæ›´æ–°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;formulating-anomaly-detection-as-in-painting&#34;&gt;Formulating Anomaly Detection as In-painting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image in-painting æœ€åˆæ˜¯ç”¨ä¾†æ¢å¾©å…·æœ‰ neighboring context çš„åœ–ç‰‡å€å¡Šï¼Œå› æ­¤æ ¹æ“šæ­¤ç›´è¦ºï¼Œæƒ³æŠŠç•°å¸¸åœ–ç‰‡ä¿®å¾©æˆæ­£å¸¸åœ–ç‰‡ä¾†å¯¦ç¾æª¢æ¸¬&lt;/li&gt;
&lt;li&gt;åœ¨ä¿®å¾©åƒç´ çš„æ™‚å€™ï¼Œç‰¹åˆ¥æ˜¯ç”¨æ·±åº¦ç¶²è·¯ï¼Œå®¹æ˜“æœ‰ boundary artifactsï¼Œåœ¨ pixel ç´šåˆ¥çš„ä¿®å¾©ä¸­ï¼Œé€™äº› boundary artifacts æœƒå°è‡´å¤§é‡èª¤å ±
&lt;ul&gt;
&lt;li&gt;artifact ä¸­ç¿»å¥½åƒæ˜¯ã€Œå½å½±ã€ï¼Œå°±æ˜¯é‡å»ºçš„æ™‚å€™æœƒå‘ˆç¾æœ‰é»åƒæ£‹ç›¤çš„æ•ˆæ‡‰&lt;/li&gt;
&lt;li&gt;ä½œè€…é¸æ“‡åœ¨ feature level é€²è¡Œ in-paintingï¼Œé¿é–‹é€™å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In-painting block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æœƒå…ˆæŠŠæ¯å€‹ patch $F_{1,1}$ ~ $F_{w,h}$ éƒ½å…ˆæ‰¾åˆ°æœ€æ¥è¿‘çš„ normal patterns $N_{1,1}$ ~ $N_{w,h}$&lt;/li&gt;
&lt;li&gt;å› ç‚º N æ˜¯ä¹‹å‰è¨“ç·´è³‡æ–™ä¸­æå–çš„ç‰¹å¾µçµ„æˆçš„ï¼Œä¸å—ç•¶å‰è¼¸å…¥å½±åƒçš„å½±éŸ¿ã€‚ç‚ºäº†å°å…¥è¼¸å…¥åœ–ç‰‡çš„ç‰¹å¾µï¼Œä½œè€…æŠŠ F å’Œ N ç”¨ transformer block ä¾†çµåˆ
&lt;ul&gt;
&lt;li&gt;å°æ–¼æ¯å€‹ patch $F_{i,j}$ï¼ŒæœƒæŠŠå…¶ç•¶ä½œä¸­å¿ƒï¼Œç”¨ç›¸é„°çš„ 8 å€‹ N patch ä¾†é‡æ–°å®šç¾© $F_{i,j}$ï¼ŒæŠŠé€™ 8 å€‹ N patch ä½œç‚º key å’Œ valueï¼Œ$F_{i,j}$ ä½œç‚º query&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æœ€å¾Œæœƒåœ¨ in-painting block çš„å‰å¾Œåš point-wise convolution (1x1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Masked shortcut&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯¦é©—çµæœè¡¨æ˜ï¼Œç›´æ¥åš residual connection æœƒé™ä½ä¿®å¾©çš„æ€§èƒ½ï¼Œä½œè€…æ¡ç”¨ random binary mask åœ¨ training æœŸé–“ gate shortcut feature
&lt;ul&gt;
&lt;li&gt;$F&amp;rsquo;=(1-\delta)\cdot F + \delta \cdot inpaint(F)$
&lt;ul&gt;
&lt;li&gt;$\delta$~$Bernoulli(\rho)$
&lt;ul&gt;
&lt;li&gt;$\rho$ gating probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç²å¾— F&amp;rsquo; å¾Œï¼ŒåŸå§‹çš„ F æœƒè¢«æ›´æ–°é€² memory&lt;/li&gt;
&lt;li&gt;åœ¨æ¨è«–éšæ®µï¼Œæœƒ disable shortcutï¼Œä½¿ $F&amp;rsquo;=inpaint(F)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anomaly-discrimination&#34;&gt;Anomaly Discrimination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discriminator è©•ä¼°åœ–ç‰‡ç¾ä¸ç¾å¯¦ï¼Œä¸ç¾å¯¦è¡¨ç¤ºç•°å¸¸&lt;/li&gt;
&lt;li&gt;å› ç‚º Generator åªåœ¨æ­£å¸¸åœ–ç‰‡è¨“ç·´ï¼Œæ‰€ä»¥ Memory Queue ä¹Ÿåªæœ‰ normal pattern&lt;/li&gt;
&lt;li&gt;ç¨å¾®ç¸½çµ
&lt;ul&gt;
&lt;li&gt;in-painting block æœƒæŠŠ patch å¼·åŒ–ç‚ºç›¸ä¼¼çš„ normal feature&lt;/li&gt;
&lt;li&gt;student generator æœƒæ ¹æ“š &amp;ldquo;normal&amp;rdquo; feature é‡å»ºå‡º &amp;ldquo;normal&amp;rdquo; image&lt;/li&gt;
&lt;li&gt;å¦‚æœæ²’æœ‰ç•°å¸¸çš„è©±ï¼Œé‚£ input å’Œé‡å»ºçš„ image åœ¨èªæ„ä¸Šæ‡‰è©²ç›¸å·®å¾ˆå°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç•°å¸¸åˆ†æ•¸ $A$ çš„ç®—æ³•
&lt;ul&gt;
&lt;li&gt;$A=\phi(\frac{D(G_s(E(I)))-\mu}{\sigma})$
&lt;ul&gt;
&lt;li&gt;$\phi(\cdot)$ æ˜¯ sigmoid function&lt;/li&gt;
&lt;li&gt;$\mu$ å’Œ $\sigma$ æ˜¯æ ¹æ“š training samples ç®—å‡ºçš„ç•°å¸¸åˆ†æ•¸çš„å¹³å‡å€¼å’Œæ¨™æº–å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_t = (I-G_t (E(I)))^2$&lt;/li&gt;
&lt;li&gt;$\mathcal L_s = (I-G_s (E(I)))^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge distillation
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dist} = \sum_{l}^{i=1} (F^i_t-F^i_s)^2$
&lt;ul&gt;
&lt;li&gt;$l$ æ˜¯ levels of features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial loss
&lt;ul&gt;
&lt;li&gt;é¡ä¼¼ DCGAN&lt;/li&gt;
&lt;li&gt;$\mathcal L_{gen} = log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discriminator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dis} = log(D(I)) + log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;li&gt;æŠŠ real image æ©Ÿç‡æ‹‰é«˜ï¼ŒæŠŠ fake image æ©Ÿç‡æ‹‰ä½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Total loss
&lt;ul&gt;
&lt;li&gt;minimize generative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_t \mathcal L_t + \lambda_s \mathcal L_s + \lambda_{dist} \mathcal L_{dist} + \lambda_{gen} \mathcal L_{gen}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;maximize discriminative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_{dis} \mathcal L_{dis}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;new-benchmark&#34;&gt;New Benchmark&lt;/h3&gt;
&lt;p&gt;æå‡ºä¸€å€‹æ–°è³‡æ–™é›† - DigitAnatomyã€‚ã€‚å¦‚æœåŒ…å«æ­£ç¢ºé †åºçš„é˜¿æ‹‰ä¼¯æ•¸å­— 1~9 å‰‡è¦–ç‚ºæ­£å¸¸ï¼Œç•°å¸¸åŒ…æ‹¬ç¼ºå¤±ã€äº‚åºã€ç¿»è½‰å’Œ zero digitã€‚&lt;/p&gt;
&lt;p&gt;è©²è³‡æ–™é›†å°æ–¼æ”¾å°„ç·šæˆåƒå°¤å…¶æœ‰åˆ©ï¼ŒåŸå› å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial correlation and consistent shape&lt;/li&gt;
&lt;li&gt;æ”¾å°„ç·šæˆåƒè¦æ¨™è¨˜éœ€è¦å°ˆæ¥­çŸ¥è­˜ï¼Œä½†æ•¸å­—å®¹æ˜“ debug&lt;/li&gt;
&lt;li&gt;è©²è³‡æ–™é›†å¾ˆå®¹æ˜“å°±å¯ä»¥ç²å¾—æ¨¡æ“¬ç•°å¸¸çš„ ground truth&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;public-benchmarks&#34;&gt;Public Benchmarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ZhangLab Chest X-ray
&lt;ul&gt;
&lt;li&gt;åŒ…å«å¥åº·å’Œè‚ºç‚çš„å½±åƒ&lt;/li&gt;
&lt;li&gt;è¨“ç·´é›†
&lt;ul&gt;
&lt;li&gt;1349 å¼µæ­£å¸¸&lt;/li&gt;
&lt;li&gt;3883 å¼µç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ¸¬è©¦é›†
&lt;ul&gt;
&lt;li&gt;234 å¼µæ­£å¸¸&lt;/li&gt;
&lt;li&gt;390 å¼µç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä½œè€…å¾è¨“ç·´é›†éš¨æ©ŸæŒ‘ 200 å¼µåšç‚ºèª¿æ•´è¶…åƒæ•¸çš„ validation set&lt;/li&gt;
&lt;li&gt;å½±åƒéƒ½èª¿æ•´ç‚º 128x128&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;å° front-view PA å½±åƒé€²è¡Œè©•ä¼°ï¼Œå…±æœ‰ 12 ç¨®ç•°å¸¸&lt;/li&gt;
&lt;li&gt;æœ‰ 5249 å¼µæ­£å¸¸å’Œ 23671 å¼µç•°å¸¸ç”¨ä½œè¨“ç·´
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨å’Œ ZhangLab ç›¸åŒçš„è¶…åƒæ•¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç”¨è¨“ç·´é›†çš„ 250 å¼µæ­£å¸¸å’Œ 250 å¼µç•°å¸¸é€²è¡Œæ¸¬è©¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baselines-and-metrics&#34;&gt;Baselines and Metrics&lt;/h3&gt;
&lt;p&gt;è€ƒæ…® 13 å€‹ä¸»è¦çš„ baseline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç¶“å…¸ UAD (unsupervised anomaly detection)
&lt;ul&gt;
&lt;li&gt;Auto-encoderã€VAE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;é†«å­¸å½±åƒçš„ SOTA
&lt;ul&gt;
&lt;li&gt;Ganomalyã€f-AnoGANã€IFã€SALAD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æœ€è¿‘çš„ UAD
&lt;ul&gt;
&lt;li&gt;MemAEã€CutPasteã€M-KDã€PANDAã€PaDiMã€IGD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;é™¤éæœ‰ç‰¹åˆ¥è¨»æ˜ï¼Œä¸ç„¶éƒ½æ˜¯å¾é ­ç¨ç«‹è¨“ç·´è‡³å°‘ä¸‰æ¬¡&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;interpreting-squid-on-digitanatomy&#34;&gt;Interpreting SQUID on DigitAnatomy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ DigitAnatomy çš„å¯¦é©—ä¸­ï¼Œæ•…æ„æ³¨å…¥ç•°å¸¸åˆ°æ­£å¸¸åœ–ç‰‡ä¸­ï¼Œæ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ä»¥é‡å»ºæ­£å¸¸åœ–ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;SQUID é‡å»ºå‡ºçš„åœ–ç‰‡æ¯”å…¶ä»– baseline æœ‰æ›´å¤šæœ‰æ„ç¾©çš„è¨Šæ¯ï¼Œä¸»è¦æ­¸åŠŸæ–¼ space-aware memoryï¼Œå…¶ç”¢ç”Ÿç¨ç‰¹çš„ patternï¼Œè€Œä¸”å’Œç©ºé–“è¨Šæ¯ç›¸é—œè¯ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€æ—¦å‡ºç¾ç•°å¸¸ï¼Œin-painting block æœƒå¾å­—å…¸ä¸­æ‰¾å‡ºå‰ k å€‹ç›¸è¿‘çš„ï¼ŒæŠŠç•°å¸¸ç‰¹å¾µå¢å¼·åˆ°å…¶å°æ‡‰çš„æ­£å¸¸ç‰¹å¾µï¼Œå…¶ä»–æ–¹æ³•ä¸å…·å‚™æ­¤èƒ½åŠ›ï¼Œæ‰€ä»¥ä»–å€‘é‡å»ºå‡ºæœ‰ç¼ºé™·çš„åœ–åƒã€‚&lt;/p&gt;
&lt;p&gt;GAN å‚¾å‘æ–¼é‡å»ºè¨“ç·´æ¨£æœ¬å¹³å‡å¾—åˆ°çš„å½±åƒã€‚
MemAE å—ç›Šæ–¼ Memory matrixï¼Œè¡¨ç¾è¼ƒå¥½ï¼Œä½†å°æ–¼ç¼ºå¤±æ•¸å­—çš„ç•°å¸¸æ•ˆæœä¸ä½³ã€‚&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-squid-on-chest-radiography&#34;&gt;Benchmarking SQUID on Chest Radiography&lt;/h3&gt;
&lt;h4 id=&#34;limitation&#34;&gt;Limitation&lt;/h4&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ç›®å‰çš„ SQUID æ²’è¾¦æ³•åœ¨åƒç´ å±¤ç´šç²¾ç¢ºå®šä½ç•°å¸¸ã€‚é€™å¯ä»¥ç†è§£ï¼Œå› ç‚º SQUID æ˜¯ä¸€ç¨®éç›£ç£æ–¹æ³•ï¼Œä¸éœ€è¦æ¨™è¨»ã€‚&lt;/p&gt;
&lt;p&gt;é‚£äº›åƒç´ ç´šåˆ¥çš„ç•°å¸¸æª¢æ¸¬æœƒé­é‡æ”¾å¤§é›œè¨Šçš„å½±éŸ¿ï¼Œä½†æ˜¯ç”±æ–¼ SQUID æ˜¯åœ¨ç‰¹å¾µå±¤ç´šé€²è¡Œçš„ï¼Œæ¯”åƒç´ ç´šåˆ¥æ›´åŠ  robustã€‚&lt;/p&gt;
&lt;h3 id=&#34;ablating-key-properties-in-squid&#34;&gt;Ablating Key Properties in SQUID&lt;/h3&gt;
&lt;h4 id=&#34;component-study&#34;&gt;Component study&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;hyper-parameter-robustness&#34;&gt;Hyper-parameter robustness&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig9.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;disease-free-training-requirement&#34;&gt;Disease-free training requirement?&lt;/h4&gt;
&lt;p&gt;ç”¨æ–¼é†«å­¸ç•°å¸¸æª¢æ¸¬çš„éç›£ç£æ–¹æ³•ä¸¦ä¸å¸¸è¦‹ï¼Œå› ç‚ºæ‰€è¬‚çš„ UAD æ–¹æ³•ä¸¦ä¸æ˜¯ã€Œéç›£ç£ã€çš„ï¼Œå› ç‚ºä»–å€‘å¿…é ˆåªåœ¨ç„¡ç–¾ç—…å½±åƒä¸Šä½œè¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å¯¦è¸ä¸­ï¼Œè¦ç²å¾—å¥åº·åœ–ç‰‡éœ€è¦ manual annotationã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨“ç·´é›†ä¸­è€ƒæ…® disease-free å¾ 100% - 50% çš„æƒ…æ³ï¼ŒæŠŠ SQUID çš„ robust å’Œå¦å¤–ä¸‰å€‹ baseline é€²è¡Œæ¯”è¼ƒã€‚&lt;/p&gt;
&lt;p&gt;SQUID çš„ memory queue å¯ä»¥è‡ªå‹•å¿½ç•¥å°‘æ•¸çš„ anatomical patternsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>CLIP è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Tue, 21 Nov 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.00020&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç¾æœ‰çš„ SOTA CV system å¯ä»¥ç¶“éè¨“ç·´é æ¸¬ä¸€çµ„å›ºå®šçš„é¡åˆ¥ã€‚
ä½†é€™ç¨®ç›£ç£å¼çš„æ–¹æ³•ä¹Ÿå—é™äº†é€šç”¨æ€§ï¼Œå› ç‚ºéœ€è¦é¡å¤–çš„ labeled data ä¾†æ“´å±•ã€‚&lt;/p&gt;
&lt;p&gt;ç›´æ¥å¾ raw text å­¸ç¿’ image æ˜¯å€‹æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è­‰æ˜äº†ã€Œé æ¸¬å“ªå€‹æ˜¯åœ–ç‰‡çš„ captionã€é€™ç¨®å½¢å¼çš„é è¨“ç·´æ˜¯ä¸€ç¨®é«˜æ•ˆä¸”å¯æ“´å±•çš„æ–¹æ³•ï¼Œå¯ä»¥å¾ internet ä¸Šè’é›†çš„ 4 å„„å°è³‡æ–™å¾é ­å­¸ç¿’åˆ° SOTA image representationã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´å¾Œï¼Œé€éè‡ªç„¶èªè¨€ä¾†å¼•å°ï¼Œå°±å¯ä»¥åœ¨ä¸‹æ¸¸ä»»å‹™åç·š zero-shotã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å° 30 å€‹ä¸åŒçš„ç¾æœ‰é›»è…¦è¦–è¦ºè³‡æ–™é›†é€²è¡Œæ¯”è¼ƒï¼Œå¯ä»¥åœ¨å¤šæ•¸ä»»å‹™å’Œç›£ç£å¼å­¸ç¿’çš„ baseline ç«¶çˆ­ï¼Œè€Œä¸”ç„¡é ˆä»»è³‡æ–™é›†ä¾†åšç‰¹åˆ¥çš„è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚åœ¨ ImageNet ä¸Šåš zero-shot å¯ä»¥å’Œ ResNet-50 å–å¾—ç›¸è¿‘çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-motivating-work&#34;&gt;Introduction and Motivating Work&lt;/h2&gt;
&lt;p&gt;ç›´æ¥å¾åŸå§‹æ–‡æœ¬å­¸ç¿’çš„é è¨“ç·´æ–¹æ³•åœ¨éå»å¹¾å¹´å¾¹åº•æ”¹è®Šäº† NLPã€‚&lt;/p&gt;
&lt;p&gt;Task-agnostic (èˆ‡ä¸‹æ¸¸ä»»å‹™ç„¡é—œ) objectivesï¼Œæ¯”å¦‚ autoregressive å’Œ masked language modelingï¼Œè®“æ¨¡å‹å¾—ä»¥éš¨è‘— compute, model capacity, å’Œ data è¦æ¨¡çš„å¢é•·ï¼Œä½¿èƒ½åŠ›ä¹Ÿé€æ­¥æå‡ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ &amp;ldquo;text-to-text&amp;rdquo; é€™ç¨®è¼¸å…¥è¼¸å‡ºå½¢å¼çš„é è¨“ç·´ï¼Œä½¿æ¨¡å‹è½‰ç§»åˆ°ä¸‹æ¸¸ä»»å‹™çš„æ™‚å€™ï¼Œä¸ç”¨ç‰¹åœ°å®¢è£½åŒ– output headï¼Œæˆ–å°è³‡æ–™é›†åšç‰¹åˆ¥åœ°è™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›çµæœè¡¨æ˜ï¼Œç¾ä»£çš„é è¨“ç·´æ–¹æ³•åœ¨ web-scale çš„æ–‡å­—é›†åˆçš„è¡¨ç¾å·²ç¶“è¶…éäº†ç”¨é«˜å“è³ªçš„äººç‚ºæ¨™è¨˜ NLP è³‡æ–™é›†ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œåœ¨ CV ç­‰é ˜åŸŸï¼Œåœ¨ ImageNet é€™ç¨®äººç‚ºæ¨™è¨˜çš„è³‡æ–™é›†ä¸Šåšé è¨“ç·´å»ä¾ç„¶æ˜¯æ¨™æº–åšæ³•ã€‚&lt;/p&gt;
&lt;p&gt;ç›´æ¥å¾ç¶²è·¯æ–‡æœ¬å­¸ç¿’çš„å¯æ“´å±•é è¨“ç·´æ–¹æ³•æˆ–è¨±èƒ½åœ¨ CV å¸¶ä¾†é¡ä¼¼çš„çªç ´ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥å¾€æœ‰ä¸€äº›å·¥ä½œå˜—è©¦åˆ©ç”¨å¹¾ä¹ç„¡é™é‡çš„åŸå§‹æ–‡æœ¬è€Œä¸æ˜¯æœ‰é™æ•¸é‡çš„ &amp;ldquo;gold-labels&amp;rdquo;ï¼Œ
ä½†æ˜¯é€™äº›æ–¹æ³•éƒ½æœ‰ä¸€äº›å¦¥å”ï¼Œæ¯”å¦‚éƒ½åˆ©ç”¨ softmax ä¾†åŸ·è¡Œé æ¸¬ï¼Œä½¿å…¶æ²’è¾¦æ³•æ‡‰ä»˜æ–°é¡åˆ¥ï¼Œåš´é‡é™åˆ¶äº† zero-shot çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æäº†å¹¾å€‹å¼±ç›£ç£å­¸ç¿’çš„ä¾‹å­ï¼Œä»–å€‘åˆ©ç”¨é¡å¤–çš„è³‡æ–™çµåˆé è¨“ç·´ï¼Œä¾†å¹«å¿™æ”¹å–„ç›£ç£å¼å­¸ç¿’çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;ä¹Ÿæäº†å¹¾å€‹å’Œ CLIP é¡ä¼¼çš„å·¥ä½œ VirTex, ICMLM, ConVIRTï¼Œæƒ³åˆ©ç”¨ Transformerï¼Œå¾ Natural Language ä¸­å­¸ç¿’ image representationã€‚&lt;/p&gt;
&lt;p&gt;é€™äº› weakly supervised model å’Œæœ€è¿‘å¾ NLP å­¸ç¿’ image representation çš„æ–¹æ³•æœ‰ä¸€å€‹é‡å¤§å·®ç•°ï¼Œè¦æ¨¡ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ä¸€äº›ç ”ç©¶ï¼Œæ¯”å¦‚ä¸€äº›å¼±ç›£ç£å­¸ç¿’åœ¨æ•¸ç™¾è¬åˆ°æ•¸åå„„å¼µç…§ç‰‡ä¸Šè¨“ç·´äº†å¤šå€‹ accelerator yearsã€‚ä½†æ˜¯å’Œ CLIP ç›¸ä¼¼çš„ç ”ç©¶åªåœ¨äºŒåè¬å¼µåœ–ç‰‡ä¸Šè¨“ç·´äº†å¹¾å¤©ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å°‡è¦æ¨¡æ‹‰é«˜ï¼Œä»¥ç¸®çŸ­è¦æ¨¡ä¸Šçš„å·®è·ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ internet ä¸Šè’é›†äº† 4 å„„å°åœ–ç‰‡å’Œæ–‡å­—çš„è³‡æ–™ï¼Œåšæˆæ–°çš„è³‡æ–™é›†ï¼Œä¸¦æå‡ºäº† CLIPï¼ŒConVIRT çš„ç°¡åŒ–ç‰ˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ 30 å¹¾å€‹è³‡æ–™é›†ä¸Šæ¸¬è©¦ï¼ŒåŸºæœ¬ä¸Šèƒ½å’Œç›£ç£å¼çš„æ¨¡å‹ç«¶çˆ­ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœç”¨ linear-probeï¼Œæ¯”å…¬é–‹å¯ç”¨çš„ SOTA ImageNet model é‚„æ›´å¥½ã€‚&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/ex1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;natural-language-supervision&#34;&gt;Natural Language Supervision&lt;/h3&gt;
&lt;p&gt;æ ¸å¿ƒæƒ³æ³•æ˜¯åˆ©ç”¨ natural language ä¾†å­¸ç¿’ perceptionã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç¨±é€™ä¸æ˜¯ä¸€å€‹æ–°æƒ³æ³•ï¼Œä½†ä»¥å¾€ç›¸ä¼¼çš„æ–¹æ³•çš„ç”¨èªå¤šæ¨£ï¼Œä»–ä»‹ç´¹äº†å››ç¯‡æ–‡ç« ï¼Œä½†æŠŠå¾æ–‡å­—å’Œåœ–ç‰‡ä¸­å­¸ç¿’ image representation çš„æ–¹æ³•å€‹åˆ¥ç¨±ç‚ºï¼šç„¡ç›£ç£ã€è‡ªç›£ç£ã€å¼±ç›£ç£ã€ç›£ç£å¼ã€‚&lt;/p&gt;
&lt;p&gt;æ“´å±• natural language supervision æ¯”èµ·åœ–åƒåˆ†é¡ç°¡å–®çš„å¤šï¼Œä¸å¿…å®šå¥½é¡åˆ¥ï¼Œå†å»æ¨™è¨»æ¯å¼µç…§ç‰‡çš„é¡åˆ¥ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸” natural language supervision é‚„æœ‰å€‹å„ªå‹¢ï¼Œä»–ä¸åªèƒ½å­¸ç¿’ image representationï¼Œé‚„èƒ½å°‡å…¶å’Œæ–‡å­—ç›¸é—œè¯ï¼Œä½¿å…¶æ›´å¥½åš zero-shot çš„é·ç§»ã€‚&lt;/p&gt;
&lt;h3 id=&#34;creating-a-sufficiently-large-dataset&#34;&gt;Creating a Sufficiently Large Dataset&lt;/h3&gt;
&lt;p&gt;ç¾æœ‰å·¥ä½œä¸»è¦ç”¨ä¸‰å€‹è³‡æ–™é›†:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MS-COCO&lt;/li&gt;
&lt;li&gt;Visual Genome&lt;/li&gt;
&lt;li&gt;YFCC100M&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MS-COCO å’Œ Visual Genome éƒ½æ˜¯é«˜å“è³ªçš„äººç‚ºæ¨™è¨˜è³‡æ–™é›†ï¼Œä½†æ˜¯æŒ‰ç…§ç¾ä»£æ¨™æº–ä¾†çœ‹ï¼Œå®ƒå€‘å¾ˆå°ï¼Œæ¯å€‹è³‡æ–™é›†å¤§ç´„æœ‰ 100,000 å¼µè¨“ç·´ç…§ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸è¼ƒä¹‹ä¸‹ï¼Œä½œè€…èˆ‰äº†ä¸€å€‹æœ€è¿‘çš„ç ”ç©¶ï¼Œç”¨äº† 3.5 Billion å¼µ Instagram ç…§ç‰‡ä½œç‚ºè¨“ç·´è³‡æ–™ã€‚&lt;/p&gt;
&lt;p&gt;YFCC100M æ˜¯ä¸€å€‹å¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒæœ‰ 100 million å¼µç…§ç‰‡ï¼Œä½†æ¯å¼µç…§ç‰‡çš„ metadata è³‡æ–™ç¨€ç–ï¼Œè€Œä¸”è‰¯è ä¸é½Šã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚è¨±å¤šæª”åæ˜¯è‡ªå‹•ç”¢ç”Ÿçš„ï¼Œå¯èƒ½æ˜¯æ™‚é–“ï¼Œæˆ–æ˜¯ç›¸æ©Ÿçš„åƒæ•¸ã€‚&lt;/p&gt;
&lt;p&gt;ç¶“ééæ¿¾ï¼Œä¿ç•™å¸¶æœ‰è‡ªç„¶èªè¨€çš„æ¨™é¡Œæˆ–æè¿°çš„åœ–åƒï¼Œè³‡æ–™é›†ç¸®å°äº† 6 å€ï¼Œåªå‰© 15000 è¬å¼µç…§ç‰‡ï¼Œå’Œ ImageNet çš„å¤§å°ç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;natural language supervision çš„ä¸€å€‹ä¸»è¦å‹•æ©Ÿæ˜¯ç¶²è·¯ä¸Šå…¬é–‹è‘—å¤§é‡é€™ç¨®å½¢å¼çš„ dataã€‚
ç”±æ–¼ç¾æœ‰è³‡æ–™é›†æ²’æœ‰åæ˜ é€™ç¨®å¯èƒ½æ€§ï¼Œå› æ­¤åªè€ƒæ…®é€™äº›è³‡æ–™é›†æœƒä½ä¼°é€™æ–¹é¢ç ”ç©¶çš„æ½›åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥ä½œè€…å»ºç«‹äº†ä¸€å€‹æ–°çš„åŒ…å« 400 million pairs çš„è³‡æ–™é›†ï¼Œå¾ç¶²è·¯ä¸Šå„ç¨®å…¬é–‹çš„ä¾†æºè’é›†çš„ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç›¡å¯èƒ½æ¶µè“‹æ‰€æœ‰çš„ visual conceptsï¼Œä½œè€…åœ¨å»ºæ§‹è³‡æ–™é›†çš„æ™‚å€™æº–å‚™äº† 50 è¬çµ„ç‰¹å®šçš„ queryï¼Œæ¯çµ„ query æœ€å¤šåŒ…å« 20,000 å€‹ pairï¼Œä¾†é€²è¡Œ class balanceã€‚&lt;/p&gt;
&lt;p&gt;ç”¢ç”Ÿçš„è³‡æ–™é›†çš„ç¸½å­—æ•¸å’Œ GPT-2 ç”¨çš„ WebText å·®ä¸å¤šã€‚&lt;/p&gt;
&lt;p&gt;å°‡æ­¤è³‡æ–™é›†ç¨±ç‚º WITï¼Œå…¨åæ˜¯ WebImageTextã€‚&lt;/p&gt;
&lt;h3 id=&#34;selecting-an-efficient-pre-training-method&#34;&gt;Selecting an Efficient Pre-Training Method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æœ€å…ˆé€²çš„ CV System éœ€è¦å¤§é‡çš„è¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…èˆ‰äº†å…©å€‹è¨ˆç®—é‡éƒ½éå¸¸ææ€–çš„æ¨¡å‹ï¼Œè€Œä¸”ä»–å€‘åªèƒ½é æ¸¬ 1000 å€‹ ImageNet çš„é¡åˆ¥ã€‚
å…¶ä¸­ä¸€å€‹èŠ±äº† 19 å€‹ GPU yearsï¼Œå¦ä¸€å€‹èŠ±äº† 33 å€‹ TPUv3 core-yearsã€‚
ä¹çœ‹ä¹‹ä¸‹ï¼Œå¾è‡ªç„¶èªè¨€ä¸­å­¸ç¿’ä¸€çµ„é–‹æ”¾çš„è¦–è¦ºæ¦‚å¿µä¼¼ä¹ä»¤äººç”Ÿç•ã€‚&lt;/p&gt;
&lt;p&gt;ä½†åœ¨ä½œè€…åŠªåŠ›çš„éç¨‹ä¸­ï¼Œä»–å€‘ç™¼ç¾è¨“ç·´æ•ˆç‡æ˜¯æˆåŠŸæ“´å±•è‡ªç„¶èªè¨€ç›£ç£çš„é—œéµï¼Œä¹Ÿæ ¹æ“šè©²æŒ‡æ¨™é¸å®šæœ€çµ‚çš„é è¨“ç·´æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;æœ€åˆçš„æ–¹æ³•å’Œ VirTex ç›¸ä¼¼ï¼Œå¾é ­é–‹å§‹è¨“ç·´ä¸€å€‹ CNNï¼Œå’Œ text transformer ä¾†é æ¸¬ captionã€‚&lt;/p&gt;
&lt;p&gt;Fig.2 å±•ç¤ºçš„ Transformer èªè¨€æ¨¡å‹çš„è¨ˆç®—é‡æ˜¯ ResNet-50 Image encoder çš„å…©å€ã€‚
é æ¸¬ caption æ¯”é æ¸¬ caption ä½†æ¡ç”¨è©è¢‹çš„æ–¹å¼é‚„æ…¢ä¸‰å€ã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£é æ¸¬ caption æ˜¯ä¸€å€‹å›°é›£çš„ä»»å‹™ï¼ŒåŒä¸€å¼µç…§ç‰‡å°æ‡‰çš„ caption å¯èƒ½å‡ºç¾çš„æè¿°ç”šè‡³æœ‰éå¸¸å¤šç¨®ã€‚
æœ€è¿‘åœ¨ Contrastive representation learning æ–¹é¢çš„ç ”ç©¶ç™¼ç¾ contrastive objectives æœ‰ä¸éŒ¯çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ä½œè€…æ¢ç´¢ä¸€ç¨®æ–¹æ³•æ˜¯ï¼Œåªé æ¸¬æ–‡æœ¬å’Œå“ªä¸€å€‹åœ–ç‰‡é…å°ï¼Œè€Œä¸æ˜¯é æ¸¬ç¢ºåˆ‡çš„å–®å­—ã€‚&lt;/p&gt;
&lt;p&gt;å› ç‚ºè³‡æ–™é›†è¶…ç´šå¤§ï¼Œoverfitting çš„å•é¡Œå½±éŸ¿ä¸å¤§ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…ç™¼ç¾å°æ–¼ encoder çš„ representationï¼Œè¦è½‰æ›åˆ° multi-model embedding spaceï¼Œåªéœ€è¦ä½¿ç”¨ linear projection å³å¯ï¼Œä¸éœ€è¦ non-linearï¼Œå…©è€…ä¹‹é–“å·®åˆ¥ä¸å¤§ã€‚&lt;/p&gt;
&lt;p&gt;Data augmentation åªæœ‰ä½¿ç”¨ random cropï¼Œè€Œæ²’æœ‰ä½¿ç”¨å…¶ä»–çš„ã€‚&lt;/p&gt;
&lt;h3 id=&#34;choosing-and-scaling-a-model&#34;&gt;Choosing and Scaling a Model&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# image_encoder - ResNet or Vision Transformer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# text_encoder - CBOW or Text Transformer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# I[n, h, w, c] - minibatch of aligned images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# T[n, l] - minibatch of aligned texts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# W_i[d_i, d_e] - learned proj of image to embed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# W_t[d_t, d_e] - learned proj of text to embed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# t - learned temperature parameter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# extract feature representations of each modality&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;I_f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image_encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#[n, d_i]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;T_f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text_encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#[n, d_t]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# joint multimodal embedding [n, d_e]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;I_e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2_normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;T_e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2_normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# scaled pairwise cosine similarities [n, n]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I_e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T_e&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# symmetric loss function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;prompt-engineering-and-ensembling&#34;&gt;Prompt Engineering and Ensembling&lt;/h3&gt;
&lt;p&gt;ä¸€ç¨®å¸¸è¦‹çš„å•é¡Œæ˜¯ polysemyï¼Œä¸€å€‹å–®å­—å¯èƒ½æœ‰å¤šç¨®æ„æ€ï¼Œæ¯”å¦‚ &amp;ldquo;boxer&amp;rdquo; å¯èƒ½æ˜¯ä¸€ç¨®ç‹—ï¼Œæˆ–æ˜¯æ‹³æ“Šæ‰‹ã€‚
å¦‚æœä¸€å¼µåœ–ç‰‡å°æ‡‰ä¸€å€‹å–®å­—å°±æœƒé¢è‡¨é€™å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€ç¨®æ˜¯ distribution gapï¼Œæ¯”å¦‚è¨“ç·´ç”¨å¥å­ï¼Œä½†æ¸¬è©¦ç”¨å–®å­—ã€‚
ç‚ºäº†ç·©è§£é€™å•é¡Œï¼Œä½œè€…ç™¼ç¾ç”¨ prompt template &amp;ldquo;A photo of a {label}.&amp;rdquo; æ¯”ç›´æ¥ç”¨ label å¥½ã€‚&lt;/p&gt;
&lt;p&gt;å…‰ç”¨é€™å€‹ prompt template å°±æé«˜ 1.3 % åœ¨ ImageNet ä¸Šçš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœå¯ä»¥çµ¦å…¶ä»–é¡å¤–è¨Šæ¯æœƒæ›´æœ‰å¹«åŠ©ï¼Œæ¯”å¦‚å°æ–¼å¯µç‰©çš„è³‡æ–™é›†ï¼Œå¯ä»¥ç”¨ &amp;ldquo;A photo of a {label}, a type of pet.&amp;quot;ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ OCR è³‡æ–™é›†ï¼Œä½œè€…ç™¼ç¾åœ¨è¦è­˜åˆ¥çš„æ–‡å­—æˆ–æ•¸å­—å‰å¾ŒåŠ ä¸Šå¼•è™Ÿå¯ä»¥æé«˜æ•ˆèƒ½ã€‚&lt;/p&gt;
&lt;p&gt;å†ä¾†æ˜¯ prompt ensemblingï¼Œä½œè€…ç™¼ç¾ç”¨å¤šå€‹ prompt template ä¾†é æ¸¬ï¼Œç„¶å¾Œç¶œåˆçµæœï¼Œå¯ä»¥æé«˜æ•ˆèƒ½ã€‚
ä½œè€…ç”¨äº† 80 å€‹ templateã€‚åœ¨ ImageNet ä¸Šæ¯”ç”¨å–®ä¸€çš„ prompt template æé«˜ 3.5 % çš„ performanceã€‚&lt;/p&gt;
&lt;p&gt;ç¶œåˆè€ƒæ…® prompt engineering å’Œ prompt ensemblingï¼Œä½œè€…åœ¨ ImageNet ä¸Šçš„æº–ç¢ºåº¦æé«˜å¤§æ¦‚ 5%ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é€™è£¡åˆ—å¹¾å€‹ä½œè€…ç”¨çš„ prompt template:
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;a bad photo of a {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a photo of many {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a sculpture of a {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a photo of the hard to see {}.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analysis-of-zero-shot-clip-performance&#34;&gt;Analysis of Zero-Shot CLIP Performance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å°æ–¼ä¸€èˆ¬çš„ç‰©é«”åˆ†é¡çš„è³‡æ–™é›†ï¼ŒCLIP è¡¨ç¾è¼ƒå¥½ã€‚&lt;/p&gt;
&lt;p&gt;ä¸‹é¢æœ‰äº›è¤‡é›œã€å°ˆé–€ã€æŠ½è±¡çš„ä»»å‹™ï¼ŒCLIP å‰‡è¡¨ç¾çš„å¾ˆå·®ï¼Œæ¯”å¦‚è¨ˆç®—å ´æ™¯ä¸­æœ‰å¤šå°‘ç‰©é«”çš„ ï¼ˆCLEVRCountsï¼‰ã€è¡›æ˜Ÿåœ–åƒåˆ†é¡ï¼ˆEuroSATï¼‰æˆ–æ˜¯ è­˜åˆ¥æœ€è¿‘çš„æ±½è»Šè·é›¢ï¼ˆKITTI Distanceï¼‰&lt;/p&gt;
&lt;p&gt;å°æ–¼é€™ç¨®ç‰¹åˆ¥é›£çš„ä»»å‹™ï¼Œè®“ CLIP åš zero-shot ä¸å¤ªåˆç†ã€‚
å¯èƒ½ç”¨ few-shot çš„æ–¹å¼æœƒæ¯”è¼ƒå¥½ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;BiT æ˜¯ google ç‚º Transfer Learning è¨­è¨ˆçš„é è¨“ç·´æ¨¡å‹ï¼Œåœ¨åˆ†é¡å•é¡Œï¼ŒFew-shot learning ä¸Šæœ‰è‰¯å¥½çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;h3 id=&#34;representation-learning&#34;&gt;Representation Learning&lt;/h3&gt;
&lt;p&gt;é€™ç¯€æ¢è¨å®Œå…¨ä½¿ç”¨ä¸‹æ¸¸ä»»å‹™è³‡æ–™é›†è€Œé Zero-shot æˆ– few-shot çš„æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é¸ç”¨ linear-probe è€Œä¸æ˜¯ finetune ä¾†åšä¸‹æ¸¸ä»»å‹™çš„è©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;å› ç‚ºä»–å€‘çš„é‡é»æ˜¯é–‹ç™¼èˆ‡è³‡æ–™é›†ç„¡é—œçš„é è¨“ç·´æ–¹æ³•ï¼Œfinetune æœ‰å¯èƒ½è®“ä¸€å€‹é è¨“ç·´å­¸ç¿’ representation å¤±æ•—çš„æ¨¡å‹åœ¨å¾®èª¿éç¨‹ä¸­è®Šå¥½ã€‚
è€Œ linear-probe çš„é™åˆ¶å¯ä»¥å‡¸é¡¯é€™äº›å¤±æ•—ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;comparison-to-human-performance&#34;&gt;Comparison to Human Performance&lt;/h2&gt;
&lt;p&gt;å†ä¾†æ˜¯ CLIP å’Œäººé¡ç›¸æ¯”çš„çµæœã€‚
æŒ‘é¸äº†äº”å€‹äººåœ¨å¯µç‰©è³‡æ–™é›†ä¸Šæ¯”è¼ƒçš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;data-overlap-analysis&#34;&gt;Data Overlap Analysis&lt;/h2&gt;
&lt;p&gt;å¯èƒ½æœƒæœ‰äººè³ªç–‘ï¼ŒCLIP çš„è¡¨ç¾æ˜¯å› ç‚ºè¨“ç·´è³‡æ–™é›†å’Œæ¸¬è©¦è³‡æ–™é›†æœ‰é‡ç–Šã€‚
ä½†ä½œè€…åšäº†ä¸€äº›å¯¦é©—ï¼Œæœ‰äº›è³‡æ–™é›†å®Œå…¨æ²’æœ‰åµæ¸¬åˆ°é‡ç–Šã€‚
å°æœ‰é‡ç–Šçš„åšå¯¦é©—ï¼Œç™¼ç¾æœ‰é‡ç–Šçš„å°æ•ˆæœæå‡å½±éŸ¿å¾ˆå°ã€‚&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;CLIP é›–ç„¶å¯ä»¥å’Œä½œç‚º Baseline çš„ ResNet-50 æ‰“å¹³æ‰‹ï¼Œä½†ç¾åœ¨çš„ SOTA é é«˜æ–¼è©² Baselineã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾å†ç¹¼çºŒåŠ å¤§æ¨¡å‹å’Œè³‡æ–™æ˜¯å¯ä»¥ç¹¼çºŒæå‡æ€§èƒ½çš„ï¼Œä½†ä½œè€…ä¼°è¨ˆè¦é”åˆ°ç¾æœ‰çš„ SOTA éœ€è¦å¢åŠ å¤§æ¦‚ 1000 å€çš„è¨ˆç®—é‡æ‰èƒ½é”åˆ°ï¼Œä½¿ç”¨ç¾æœ‰çš„ç¡¬é«”æ˜¯ä¸å¯è¡Œçš„ã€‚&lt;/p&gt;
&lt;p&gt;CLIP å°ç´°åˆ†é¡ã€æŠ½è±¡æˆ–æ›´é›£çš„ä»»å‹™è¡¨ç¾ä¸å¥½ï¼Œä½œè€…ç›¸ä¿¡é‚„æœ‰è¨±å¤šä»»å‹™æ˜¯ CLIP ç”¨ zero-shot åªèƒ½é”åˆ°äº‚çŒœç­‰ç´šçš„ã€‚&lt;/p&gt;
&lt;p&gt;Zero-Shot çš„ CLIP å¾ˆé›£æ³›åŒ–åˆ° out-of-distribution çš„è³‡æ–™ï¼Œæ¯”å¦‚åœ¨ MNIST ä¸Šåªèƒ½é”åˆ° 88% çš„æº–ç¢ºåº¦ã€‚
ä½œè€…ç™¼ç¾é è¨“ç·´è³‡æ–™å¹¾ä¹æ²’æœ‰é¡ä¼¼ MNIST çš„åœ–ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ CLIP å¯ä»¥éˆæ´»æ‡‰ç”¨å„ç¨® Zero-Shot çš„åˆ†é¡ï¼Œä½†åŸºæœ¬ä¸Šé‚„æ˜¯å¾ä½ çµ¦å®šçš„åˆ†é¡é¸æ“‡ã€‚
å’ŒçœŸæ­£éˆæ´»çš„æ–¹æ³•ï¼ˆç”Ÿæˆ image captionï¼‰ç›¸æ¯”ï¼Œæ˜¯é‡å¤§çš„é™åˆ¶ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹å€¼å¾—å˜—è©¦çš„ç°¡å–®æƒ³æ³•æ˜¯æŠŠ contrastive objective å’Œ generative objectiveï¼Œçµåˆã€‚&lt;/p&gt;
&lt;p&gt;CLIP ä¹Ÿæ²’æœ‰è§£æ±ºæ·±åº¦å­¸ç¿’è³‡æ–™æ•ˆç‡ä½ä¸‹çš„å•é¡Œï¼ŒCLIP è¨“ç·´äº† 32 å€‹ epochï¼Œå¦‚æœæŠŠé è¨“ç·´æœŸé–“çš„ç…§ç‰‡ä»¥ä¸€ç§’ä¸€å¼µä¾†å‘ˆç¾ï¼Œéœ€è¦ 405 å¹´ã€‚
æŠŠ CLIP å’Œ self-supervision æˆ–è€…å’Œ self-training åšçµåˆæ˜¯æœ‰å‰é€”çš„æ–¹å‘ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ä½œè€…å¼·èª¿ Zero-Shot Learningï¼Œä½†æ˜¯ä½œè€…é‚„æ˜¯æœ‰åè¦†æª¢æŸ¥ä¸‹æ¸¸ä»»å‹™æ¸¬è©¦é›†çš„è¡¨ç¾ï¼Œä¾†èª¿æ•´ CLIPã€‚
æ¯æ¬¡éƒ½ç”¨ ImageNet ä¾†ç¢ºèªï¼Œä¸¦ä¸ç®—çœŸæ­£çš„ zero-shot çš„æƒ…æ³ã€‚
å¦‚æœèƒ½å†å‰µä¸€å€‹æ–°çš„è³‡æ–™é›†ï¼Œå°ˆé–€ç”¨ä¾†è©•ä¼° zero-shot é·ç§»çš„èƒ½åŠ›æœƒæ›´æ°ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;çˆ¬ä¸‹ä¾†çš„è³‡æ–™æœ‰å¯èƒ½å¸¶æœ‰ç¤¾æœƒåè¦‹ã€‚&lt;/p&gt;
&lt;p&gt;æœ‰ä¸€äº›è¤‡é›œçš„ä»»å‹™å¾ˆé›£ç”¨æ–‡å­—ä¾†å‚³é”ï¼Œé›–ç„¶å¯¦éš›çš„è¨“ç·´æ¨£æœ¬æœ‰ç”¨ï¼Œä½† CLIP ä¸¦ä¸æœƒé‡å° few-shot æœ€ä½³åŒ–ã€‚æœ‰å€‹é•åç›´è¦ºçš„çµæœï¼Œå¯ä»¥æ³¨æ„åˆ°åœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œfew-shot ä¸è¦‹å¾—æ¯” zero-shot å¥½ã€‚&lt;/p&gt;
&lt;h2 id=&#34;é¡å¤–æ‡‰ç”¨&#34;&gt;é¡å¤–æ‡‰ç”¨&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;åœ–ç‰‡ç”Ÿæˆ
&lt;ul&gt;
&lt;li&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery
&lt;ul&gt;
&lt;li&gt;ç”¨æ–‡å­—å¼•å°ç”Ÿæˆåœ–ç‰‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç‰©ä»¶åµæ¸¬
&lt;ul&gt;
&lt;li&gt;Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation
&lt;ul&gt;
&lt;li&gt;å°‡åŸºç¤é¡åˆ¥å†åšç´°åˆ†é¡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OCR
&lt;ul&gt;
&lt;li&gt;Contrastive Language-Image Forensic Search
&lt;ul&gt;
&lt;li&gt;æœç´¢å½±ç‰‡ä¸­æœ‰æ²’æœ‰æ–‡æœ¬æè¿°çš„ç‰©é«”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ç­†è¨˜&#34;&gt;ç­†è¨˜&lt;/h2&gt;
&lt;p&gt;prompt engineering
prompt ensemble&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion å…¥é–€</title>
        <link>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</link>
        <pubDate>Mon, 23 Oct 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</guid>
        <description>&lt;h2 id=&#34;å¤§è‡´æ¦‚å¿µ&#34;&gt;å¤§è‡´æ¦‚å¿µ&lt;/h2&gt;
&lt;p&gt;å±¬æ–¼ç”Ÿæˆå¼ AIï¼Œä¸€é–‹å§‹ç”¨åœ¨ç”Ÿæˆåœ–ç‰‡ï¼Œå¾Œä¾†ä¹Ÿæœ‰æ‡‰ç”¨åˆ°è«¸å¦‚ NLP ç­‰é ˜åŸŸã€‚&lt;/p&gt;
&lt;p&gt;ä¸‹æ–‡ç¨±å‘¼åŸåœ–ç‚º spriteã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡ AutoEncoder æœ‰é»é¡ä¼¼ï¼Œå…ˆå–å¾—ä¸€å¼µ spriteï¼Œéš¨è‘—æ™‚é–“æ¨é€²ï¼Œæ¯æ¬¡éƒ½åœ¨åœ–ç‰‡ä¸ŠåŠ ä¸€å±¤é›œè¨Šï¼Œåè¦†ç–ŠåŠ ï¼Œè¿­ä»£å¤šæ¬¡å¾Œï¼Œå°±æœƒå¾—åˆ°ä¸€å¼µé›£ä»¥çœ‹å‡ºåŸåœ–çš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;å¾ sprite åˆ°åªèƒ½çœ‹å‡ºæ˜¯ä¸€åœ˜é›œè¨Šä¸¦éæ˜¯ä¸€æ­¥åˆ°ä½çš„éç¨‹ã€‚ä¸€é–‹å§‹æ²’æœ‰é›œè¨Šæ™‚å¯ä»¥çœ‹å‡ºåŸæœ¬çš„ spriteï¼Œä¸€å€‹è¿­ä»£å¾Œå¯èƒ½å¯ä»¥å‹‰å¼·çœ‹å‡ºåŸæœ¬çš„ spriteï¼Œå†å¹¾å€‹è¿­ä»£å¾Œå¯èƒ½ä¹Ÿé‚„èƒ½çœ‹å‡ºåŸæœ¬çš„ outlineï¼Œç¶“éè¨±å¤šæ¬¡å¾Œæ‰æœƒè®Šæˆå®Œå…¨è¾¨è­˜ä¸äº†çš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘æœŸæœ›æ¨¡å‹åšçš„äº‹æƒ…å‰‡æ˜¯å¾ gaussian noise é€æ­¥æ¨å› spriteï¼ŒåŒæ¨£ä¸æ˜¯ä¸€æ­¥åˆ°ä½ï¼Œè€Œæ˜¯è®“æ¨¡å‹é æ¸¬ä¸Šä¸€å€‹æ™‚é–“é»çš„é›œè¨Šï¼Œç›¸æ¸›å¾Œå†é€æ­¥æ¨å› spriteï¼Œé€™éç¨‹ç¨±ç‚º denoiseã€‚&lt;/p&gt;
&lt;h2 id=&#34;ddpm&#34;&gt;DDPM&lt;/h2&gt;
&lt;p&gt;å¯¦ç¾ Diffusion å¯èƒ½æœƒæœ‰é» confusingï¼Œå› ç‚ºä»–å¯¦ä½œä¸Šå’Œä¸Šé¢èªªçš„ä¸å¤ªç›¸åŒã€‚
åœ¨è¨“ç·´çš„æ™‚å€™ï¼Œæˆ‘å€‘æœƒæ¡æ¨£ä¸‰å€‹æ±è¥¿ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;è¨“ç·´åœ–ç‰‡ (sprite)&lt;/li&gt;
&lt;li&gt;é›œè¨Š&lt;/li&gt;
&lt;li&gt;æ™‚é–“é» (t)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è¨“ç·´éšæ®µçš„æ™‚å€™ï¼Œæˆ‘å€‘æœƒæŠŠã€ŒåŸå§‹ä¹¾æ·¨çš„åœ–ç‰‡ã€å’Œã€Œé›œè¨Šã€æ ¹æ“šæ™‚é–“é€²è¡Œä¸åŒæ¯”ä¾‹çš„ç›¸åŠ  (æ··åˆ)ï¼Œt è¶Šå¤§ï¼Œé›œè¨Šçš„æ¯”ä¾‹è¶Šå¤§ã€‚&lt;/p&gt;
&lt;p&gt;æ¨¡å‹é æ¸¬çš„ç›®æ¨™æ˜¯å‰é¢ sample å‡ºçš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;é€™èˆ‡å‰é¢èªªçš„æ¦‚å¿µç›¸æ‚–ã€‚æŒ‰ç…§å‰é¢çš„èªªæ³•ï¼Œå°æ–¼æ™‚é–“é» tï¼Œæ‡‰è©²æ˜¯ä»¥ä¸€å¼µåŠ äº† t-1 æ¬¡é›œè¨Šçš„ sprite ä½œç‚ºè¼¸å…¥ï¼Œå†åŠ ä¸Š t æ‰€ sample å‡ºçš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;ç¾åœ¨å¯¦ä½œå»æ˜¯åŸå§‹ä¹¾æ·¨çš„ sprite ç›´æ¥æ ¹æ“šæ™‚é–“é»æ··å’ŒæŸå€‹é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;é€™èƒŒå¾Œçš„æ•¸å­¸æ¨å°ååˆ†å†—é•·ï¼Œé€™è£¡ä¸æ•˜è¿°ï¼Œä½†éœ€çŸ¥é“å¯¦ä½œå·®ç•°ã€‚&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;åœ¨æ¨è«–éšæ®µçš„æ™‚å€™ï¼Œæ¯æ¬¡ denoise å¾Œéœ€è¦æŠŠåœ–ç‰‡å’Œé¡å¤– sample çš„ noise ç›¸åŠ ã€‚é€™å€‹ noise å’Œå‰é¢çš„ noise ä¸€æ¨£ï¼Œéƒ½æ˜¯å¾ mean=0, std=1 çš„ gaussian distribution ä¸­ sample å‡ºä¾†çš„ã€‚&lt;/p&gt;
&lt;p&gt;ä¸åŠ çš„è©±ä¼¼ä¹é‚„å®¹æ˜“æœ‰ Mode Collapse çš„ç¾è±¡ã€‚
çœ‹åˆ°ä¸€å€‹èªªæ³•æ˜¯ï¼Œæ¨¡å‹å–œæ­¡åƒåœ–ç‰‡åŠ ä¸Šé›œè¨Šçš„åœ–åƒä½œç‚ºåœ–ç‰‡ï¼Œåœ¨åœ–ç‰‡ä¸ŠåŠ ä¸Š noise ä¼¼ä¹æœƒæ›´ç¬¦åˆæ¨¡å‹é æœŸçš„è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;çœ‹äº†æå¼˜æ¯…çš„å½±ç‰‡ï¼Œä¹Ÿæœ‰åŸºæ–¼éš¨æ©Ÿæ€§çš„è§€é»ã€‚&lt;/p&gt;
&lt;p&gt;ç”Ÿæˆå¼ Model ç”Ÿæˆæ–‡ç« æ™‚æ°¸é å–æ©Ÿç‡æœ€å¤§çš„ï¼Œä¸è¦‹å¾—æœ‰æ›´å¥½çš„æ•ˆæœï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ‰ç ”ç©¶æ˜¯è®“ Model é¸æ©Ÿç‡æœ€å¤§çš„ï¼Œçµæœå®¹æ˜“ç”Ÿå‡ºåè¦†è·³é‡çš„æ–‡ç« ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¹Ÿæœ‰æŠŠäººé¡å¯«çš„æ–‡ç« å»é¤µçµ¦ Model çœ‹ï¼Œå¾ä»–çš„è§’åº¦çœ‹äººé¡å¯«çš„ä¸‹ä¸€å€‹å­—çš„æ©Ÿç‡æ˜¯å¤šå°‘ï¼Œç™¼ç¾äººé¡å¯«çš„æ–‡ç« å¾ˆå¸¸å‡ºç¾ä¸€ä¸‹æ©Ÿç‡é«˜ä¸€ä¸‹æ©Ÿç‡ä½çš„å­—ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æŸç¯‡èªéŸ³åˆæˆçš„æ–‡ç« éœ€è¦åœ¨æ¨è«–éšæ®µã€Œå•Ÿç”¨ã€dropout æ‰å¯ä»¥æœ‰å¥½çš„çµæœã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Diffusion ä¹Ÿæœ‰å¯èƒ½æˆåŠŸçš„é»æ˜¯åœ¨æ–¼ä¸¦éã€Œä¸€æ¬¡åˆ°ä½ã€è€Œæ˜¯ã€ŒN æ¬¡åˆ°ä½ã€ã€‚
å¾é€™æ¨£çš„è§’åº¦çœ‹ï¼ŒDiffusion æ˜¯ autoregressive æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;é¡ä¼¼çš„ä½œæ³•ä¹Ÿæœ‰ Mask-Predictï¼Œå¤§è‡´æ¦‚å¿µæ˜¯å¾åŸæœ¬éƒ½æ˜¯ Mask çš„æƒ…å¢ƒé–‹å§‹ï¼Œå°‡ä¸€äº›ä¿¡å¿ƒé«˜çš„é æ¸¬ç•™ä½ï¼Œä¿¡å¿ƒä½çš„ä¿æŒç‚º Maskï¼Œä¸€æ­¥æ­¥é æ¸¬å‡ºæ‰€æœ‰è³‡è¨Šã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DETR è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 10 Aug 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.12872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ä½œè€…æŠŠ object detection è¦–ä½œä¸€å€‹ set prediction å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ç°¡åŒ–äº† pipelineï¼Œæ¶ˆé™¤äº†è¨±å¤š hand-designed componentsï¼Œæ¯”å¦‚ non-maximum suppression å’Œ anchor generationï¼Œé€™äº› component ç”±æˆ‘å€‘å°æ–¼ä»»å‹™çš„å…ˆé©—çŸ¥è­˜æ§‹æˆã€‚&lt;/p&gt;
&lt;p&gt;æå‡ºäº†ä¸€å€‹æ–°çš„ç›®æ¨™å‡½æ•¸ï¼Œé€éäºŒåˆ†åŒ¹é…ï¼ˆbipartite matchingï¼‰é€²è¡Œé æ¸¬ï¼Œä¹Ÿç”¨ Transformer encoder-decoder æ¶æ§‹ã€‚&lt;/p&gt;
&lt;p&gt;çµ¦äºˆä¸€çµ„å›ºå®šçš„ learned object queryï¼ŒDETR å¯ä»¥æ¨ç† objects å’Œ globol image context çš„é—œä¿‚ï¼Œä¸¦ã€Œä¸¦è¡Œã€è¼¸å‡ºä¸€çµ„é æ¸¬é›†ã€‚&lt;/p&gt;
&lt;p&gt;DETR æ¦‚å¿µéå¸¸ç°¡å–®ã€‚&lt;/p&gt;
&lt;p&gt;DETR åœ¨ COCO ä¸Šå’Œ Faster RCNN baseline åœ¨æº–ç¢ºåº¦å’Œ performance ä¸Šç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;DETR å¯ä»¥å¾ˆç°¡å–®åœ°æ¨å»£åˆ° Panoptic Segmentationã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ç›®æ¨™æª¢æ¸¬çš„ç›®æ¨™å°±æ˜¯é›†åˆé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç›®å‰éƒ½ç”¨ä¸€äº›å¾ˆé–“æ¥çš„æ–¹å¼å»åšï¼Œåƒæ˜¯ç”¨ proposals, anchors æˆ– window centersã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯é€™äº›æ–¹æ³•æ€§èƒ½æ˜é¡¯å—é™æ–¼å¾Œè™•ç†æ­¥é©Ÿï¼Œæ¯”å¦‚ non-maximum suppressionï¼Œå› ç‚ºä»–å€‘æœƒç”¢ç”Ÿå¤§é‡å†—é¤˜çš„æ¡†ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç°¡åŒ– pipelineï¼Œä½œè€…æå‡ºäº†ä¸€ç¨® End-to-End çš„æ–¹æ³•ï¼Œä»¥å¾€ä¹Ÿæœ‰ä¸€äº›å˜—è©¦ï¼Œä½†ä»–å€‘è¦ä¸æ·»åŠ äº†å…¶ä»–çš„å…ˆé©—çŸ¥è­˜ï¼Œä¸ç„¶å°±æ˜¯åœ¨å…·æœ‰æŒ‘æˆ°æ€§çš„ benchmark ä¸Šè¡¨ç¾ä¸å¥½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ COCO ä¸Šå’Œ Faster R-CNN çš„æ€§èƒ½ç›¸ç•¶ï¼Œè¡¨ç¾å’Œé€Ÿåº¦éƒ½å·®ä¸å¤šã€‚&lt;/p&gt;
&lt;p&gt;DETR åœ¨å¤§ç‰©é«”è¡¨ç¾å¾ˆå¥½ï¼Œå¯èƒ½æ˜¯æ­¸åŠŸæ–¼ Transformer non-local çš„è¨ˆç®—èƒ½åŠ›ã€‚
é›–ç„¶ DETR åœ¨å°ç‰©é«”ä¸Šè¡¨ç¾å€’ä¸æ€éº¼æ¨£ã€‚&lt;/p&gt;
&lt;p&gt;DETR éœ€è¦è¶…é•·çš„è¨“ç·´æ™‚é–“ï¼Œä½† DETR çš„è¨­è¨ˆç†å¿µå¯ä»¥æ‹“å±•åˆ° Panoptic Segmentationã€‚&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;set-prediction&#34;&gt;Set Prediction&lt;/h3&gt;
&lt;p&gt;æ²’æœ‰è¦ç¯„çš„æ·±åº¦å­¸ç¿’æ¨¡å‹å¯ä»¥ç›´æ¥é æ¸¬é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›ä»»å‹™ä¸­çš„ä¸€å€‹å›°é›£é»æ˜¯é¿å… near-dulicatesï¼ˆç›¸è¿‘çš„é‡è¤‡æª¢æ¸¬æ¡†ï¼‰ ç•¶å‰å¤šæ•¸æª¢æ¸¬å™¨ç”¨ NMS ä¾†è§£æ±ºæ­¤å•é¡Œï¼Œå¦‚æœæ˜¯ direct set prediction å°±ä¸ç”¨å¾Œè™•ç†ã€‚&lt;/p&gt;
&lt;h3 id=&#34;transformers-and-parallel-decoding&#34;&gt;Transformers and Parallel Decoding&lt;/h3&gt;
&lt;p&gt;Transformer åœ¨å„ç¨®åœ°æ–¹è¡¨ç¾å‡ºè‰²ï¼Œä½†æ¨ç†æˆæœ¬ä»¤äººæœ›è€Œç”Ÿç•ã€‚&lt;/p&gt;
&lt;h3 id=&#34;object-detection&#34;&gt;Object detection&lt;/h3&gt;
&lt;p&gt;ç¾åœ¨å¤šæ•¸çš„ç›®æ¨™æª¢æ¸¬æ–¹æ³•æ˜¯åŸºæ–¼ä¸€äº›åˆå§‹çš„çŒœæ¸¬ï¼Œå†å»åšé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚å°æ–¼ two-stage çš„æ–¹æ³•ï¼Œå°±æ˜¯å°æ–¼ proposals å¾€ä¸‹åšé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ single-stageï¼Œåˆå§‹çŒœæ¸¬å°±æ˜¯ anchorsã€‚&lt;/p&gt;
&lt;h4 id=&#34;set-based-loss&#34;&gt;Set-based loss&lt;/h4&gt;
&lt;p&gt;ä»¥å‰çš„ä¸€äº›ä½œæ³•æ¯”å¦‚ Learnable NMS æˆ– relation networks éƒ½å¯ä»¥é€é attention ä¾†è™•ç†ä¸åŒé æ¸¬ä¹‹é–“çš„é—œä¿‚ã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ direct set lossesï¼Œä»–å€‘ä¸éœ€è¦ä»»ä½•å¾Œè™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯é€™äº›æ–¹æ³•å¾€å¾€ç”¨é¡å¤–çš„ hand-crafted context featureï¼Œæ¯”å¦‚ proposal box coordinatesã€‚ä½œè€…å°‹æ‰¾æ¸›å°‘æ¨¡å‹ä¸­å…ˆé©—çŸ¥è­˜çš„æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;h4 id=&#34;recurrent-detectors&#34;&gt;Recurrent detectors&lt;/h4&gt;
&lt;p&gt;ä»¥å¾€æœ‰é¡ä¼¼çš„å·¥ä½œï¼Œä½†ä»–å€‘æ˜¯ç”¨ RNNã€‚&lt;/p&gt;
&lt;h2 id=&#34;the-detr-model&#34;&gt;The DETR model&lt;/h2&gt;
&lt;h4 id=&#34;object-detection-set-prediction-loss&#34;&gt;Object detection set prediction loss&lt;/h4&gt;
&lt;p&gt;DETE æœƒçµ¦ N å€‹å›ºå®šå¤§å°çš„é›†åˆé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;è¦è§£äºŒåˆ†åœ–åŒ¹é…ï¼Œæœ¬æ–‡ç”¨ scipy çš„ linear_sum_assignment è™•ç†ï¼Œä»–èƒŒå¾Œæ˜¯åŒˆç‰™åˆ©æ¼”ç®—æ³•ã€‚&lt;/p&gt;
&lt;p&gt;å…¶å¯¦é€™ç¨®æ–¹æ³•å’Œ proposals å’Œ anchors æœ‰å·®ä¸å¤šçš„ä½œç”¨ï¼Œå·®åˆ¥åœ¨æ–¼é€™è£¡æœƒæ‰¾ä¸€å°ä¸€çš„åŒ¹é…ï¼Œè€Œä¸ç”¨é‡è¤‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›®æ¨™å‡½æ•¸ï¼š&lt;/p&gt;
&lt;p&gt;$L_{Hungarian}(y, \text{\^{y}}) = \displaystyle\sum^{N}_{i=1} [-log \text{\^{p}} $
$_{\^{\sigma}(i)}(c_i) + \text{1}$
$_\{$
$_{c_i \neq \text{\o}}$
$_\}$
$\mathcal{L}$
$_{\text{box}} (b_i, \text{\^{b}}$
$_{\^{\sigma}}(i))]$&lt;/p&gt;
&lt;p&gt;å‰é¢æ˜¯åˆ†é¡çš„ lossï¼Œå¾Œé¢æ˜¯ bounding box çš„ lossã€‚&lt;/p&gt;
&lt;p&gt;é€™é‚Šæœ‰å…©å€‹æ”¹å‹•ï¼Œç¬¬ä¸€å€‹æ˜¯åˆ†é¡é‚£é‚Šä¸ç”¨ logï¼Œä½¿å€¼å’Œ bounding box çš„ loss æ¯”è¼ƒæ¥è¿‘ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹æ˜¯ bounding box é‚£é‚Šä¸¦ä¸æ˜¯ç”¨æœ€å¸¸è¦‹çš„ L1ï¼Œå› ç‚º L1 å°æ–¼å¤§çš„ç›®æ¨™ loss æ¯”è¼ƒé«˜ï¼Œé€™è£¡é™¤äº† L1 é‚„é¸ç”¨ generalized IoU lossï¼Œå®ƒåœ¨å°ºåº¦ä¸Šèˆ‡ loss ç„¡é—œã€‚&lt;/p&gt;
&lt;h4 id=&#34;detr-architecture&#34;&gt;DETR architecture&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ç”¨ CNN å¾åœ–ç‰‡æŠ½ç‰¹å¾µï¼Œæ‹‰ç›´ï¼Œé¤µçµ¦ Transformer encoder-decoderï¼Œå¾—åˆ°ä¸€çµ„é æ¸¬é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;é€™è£¡ encoder æœ‰åŠ©æ–¼ç‰¹å¾µé–“å½¼æ­¤äº¤äº’ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´çš„æ™‚å€™ï¼Œé æ¸¬çš„æ¡†å’Œ GT åšåŒ¹é…ï¼Œæ²’åŒ¹é…åˆ°çš„å°±æ”¾åˆ° &amp;ldquo;no object&amp;rdquo; classã€‚&lt;/p&gt;
&lt;p&gt;decoder æœƒé¤µå…¥ object queriesï¼Œé€™äº›æ˜¯ learnable positional encodingsã€‚&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;h4 id=&#34;number-of-encoder-layers&#34;&gt;Number of encoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…é€éæ”¹è®Š Encoder layer çš„æ•¸é‡ä¾†è©•ä¼° global imagelevel self-attention çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æ¨è«– encoder å¯èƒ½å°æ–¼åˆ¤æ–·åˆ†é–‹å°è±¡å¾ˆé‡è¦ï¼Œåœ– 3 å¯è¦–åŒ–äº†æœ€å¾Œä¸€å€‹ encoder layer çš„ attention mapã€‚&lt;/p&gt;
&lt;p&gt;encoder çœ‹ä¼¼å·²ç¶“åˆ†é›¢äº† instanceï¼Œå¯èƒ½ç°¡åŒ–äº† decoder å°æ–¼ object extraction å’Œ localization çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h4 id=&#34;number-of-decoder-layers&#34;&gt;Number of decoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨åœ– 6 åšäº† decoder çš„æ³¨æ„åŠ›å¯è¦–åŒ–ï¼Œå¯ä»¥æ³¨æ„åˆ°è§€å¯Ÿçš„æ³¨æ„åŠ›ç›¸ç•¶å±€éƒ¨ã€‚&lt;/p&gt;
&lt;p&gt;æ¨è«–æ˜¯ encoder ä¸»è¦åˆ†é›¢å¯¦é«”ï¼Œdecoder åªéœ€è¦é—œæ³¨å››è‚¢å³å¯æå–å‡ºå°è±¡çš„é‚Šç•Œå’Œåˆ†é¡ã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig7.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ– 7 æŠŠ 100 å€‹é æ¸¬æ§½ä¸­çš„ 20 å€‹åšå¯è¦–åŒ–ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹é æ¸¬æ¡†ä»£è¡¨ä¸€é»ï¼Œå¯ä»¥æ³¨æ„åˆ°ä¸åŒçš„æ§½ä½æœƒå°ˆæ³¨åœ¨ä¸åŒå€åŸŸã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>I3D è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/i3d-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 23 Jul 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/i3d-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1705.07750&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç›®å‰çš„å‹•ä½œåˆ†é¡è³‡æ–™é›† (UCF-101 å’Œ HMDB-51) çš„å½±ç‰‡éå¸¸ç¼ºä¹ï¼Œä½¿è¾¨è­˜ã€Œè‰¯å¥½çš„å½±åƒæ¶æ§‹ã€è®Šå¾—å›°é›£ï¼Œ
ä½¿å¤šæ•¸æ–¹æ³•åœ¨ç¾æœ‰çš„å°è¦æ¨¡ benchmark çš„è¡¨ç¾å·®ä¸å¤šã€‚ç‚ºæ­¤æœ¬æ–‡æ ¹æ“šæ–°çš„ Kinetics Human Action Video dataset å° SOTA æ¶æ§‹é€²è¡Œäº†é‡æ–°è©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;Kinetics æœ‰ 400 å€‹äººé¡å‹•ä½œé¡åˆ¥ã€‚æ¯å€‹é¡åˆ¥æœ‰ 400 å€‹ clipã€‚å¾ YouTube ä¸Šç²å–çš„ï¼Œè€Œä¸”æ¯å€‹ clip ä¾†è‡ª unique çš„ youtube å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡åˆ†æäº†ç•¶å‰æ¶æ§‹åœ¨ Kinetics ä¸Šå‹•ä½œåˆ†é¡ä»»å‹™çš„è¡¨ç¾ï¼Œä¹Ÿè©•ä¼° Kinetcis ç”¨ä½œé è¨“ç·´çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº†ä¸€ç¨®åŸºæ–¼ 2D ConvNet inflation çš„ Two-Stream Inflated 3D ConvNet (I3D)ã€‚&lt;/p&gt;
&lt;p&gt;I3D çš„æ“´å±•æ–¹æ³•è®“ ImageNet ä¸Šå·²ç¶“å–å¾—æˆåŠŸçš„æ¶æ§‹å¯ä»¥è¢«åˆ©ç”¨åœ¨è§£æ±ºå½±åƒä»»å‹™ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;çµæœè¡¨æ˜ï¼Œç¶“éåœ¨ Kinetics ä¸Šé è¨“ç·´å¾Œï¼ŒI3D åœ¨å‹•ä½œåˆ†é¡æ–¹é¢é¡¯è‘—æé«˜äº† SOTAï¼Œåœ¨ HMDB-51 ä¸Šé”åˆ° 80.9%ï¼Œåœ¨ UCF-101 ä¸Šé”åˆ° 98.0%ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;åœ¨ ImageNet ä¸Šé è¨“ç·´æ¨¡å‹çš„æ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨å½±ç‰‡é ˜åŸŸï¼Œé è¨“ç·´æˆæ•ˆä¸€ç›´æ˜¯ä¸€å€‹æœªçŸ¥çš„å•é¡Œã€‚å› ç‚ºæµè¡Œçš„å‹•ä½œè­˜åˆ¥ benchmark éƒ½éå¸¸å°ï¼Œç´„ç•¥åªæœ‰ 10k å€‹å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;Kinetics æœ‰ 400 å€‹äººé¡å‹•ä½œé¡åˆ¥ã€‚æ¯å€‹é¡åˆ¥æœ‰ 400 å€‹ clipï¼Œè€Œä¸”æ¯å€‹ clip éƒ½ä¾†è‡ªä¸€å€‹ unique çš„ Youtube å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å¯¦é©—ç­–ç•¥æ˜¯åœ¨ Kinetics ä¸Šé è¨“ç·´ï¼Œå†åœ¨ HMDB-51 å’Œ USC-101 ä¸Šå¾®èª¿ï¼Œçµæœé¡¯ç¤ºå‡ºé è¨“ç·´ç¸½æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œä½†æå‡å¤šå¯¡å› æ¶æ§‹è€Œç•°ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºæ–°æ¶æ§‹ï¼Œç¨±ç‚ºã€ŒTwo-Stream Inflated 3D ConvNetsã€(I3D)ï¼Œå»ºç«‹åœ¨ SOTA çš„å½±åƒåˆ†é¡æ¶æ§‹ä¸Šï¼Œä¸¦å°‡ filters å’Œ pooling kernel è†¨è„¹æˆ 3Dã€‚&lt;/p&gt;
&lt;p&gt;åŸºæ–¼ Inceptionv1 çš„ I3D åœ¨ Knetics ä¸Šé è¨“ç·´å¾Œï¼Œæ€§èƒ½é è¶…éç•¶å‰çš„ SOTA æ¶æ§‹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬æ–‡çš„æ¨¡å‹ä¸­ï¼Œä¸¦æ²’æœ‰è€ƒæ…®æ›´å¤šç¶“å…¸æ–¹æ³•ï¼Œæ¯”å¦‚ bag-of-visual-words representationï¼Œä½† Kinetics æ˜¯å…¬é–‹çš„ï¼Œå› æ­¤å…¶ä»–äººå¯ä»¥é€²è¡Œå¾ŒçºŒç ”ç©¶ã€‚&lt;/p&gt;
&lt;h2 id=&#34;action-classification-architectures&#34;&gt;Action Classification Architectures&lt;/h2&gt;
&lt;p&gt;ç›®å‰å½±ç‰‡æ¶æ§‹ä¸­çš„ä¸€äº›ä¸»è¦å€åˆ¥å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å·ç©æ˜¯ 2D é‚„ 3D çš„&lt;/li&gt;
&lt;li&gt;æ˜¯å¦åªæ˜¯ RGB å½±ç‰‡ï¼Œé‚„æ˜¯åŒ…å«äº‹å…ˆè¨ˆç®—çš„ optical flow&lt;/li&gt;
&lt;li&gt;å°æ–¼ 2D ConvNetsï¼Œè¨Šæ¯æ˜¯æ€éº¼åœ¨ frame ä¹‹é–“å‚³éçš„
&lt;ul&gt;
&lt;li&gt;é€™éƒ¨åˆ†å¯ä»¥ä½¿ç”¨ temporally-recurrent layersï¼Œæ¯”å¦‚ LSTMï¼Œæˆ–æ˜¯ç”¨éš¨æ™‚é–“çš„ feature aggregation ä¾†å®Œæˆã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œè€ƒæ…®äº†æ¶µè“‹å¤§éƒ¨åˆ†ç¾æœ‰æ¶æ§‹çš„æ¨¡å‹å­é›†ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D ConvNets
&lt;ul&gt;
&lt;li&gt;é ‚éƒ¨æœ‰ LSTM çš„ ConvNet&lt;/li&gt;
&lt;li&gt;æœ‰å…©ç¨® stream fusion çš„ two-stream networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3D ConvNets
&lt;ul&gt;
&lt;li&gt;C3D&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç”±æ–¼åƒæ•¸ç¶­åº¦è¼ƒé«˜ï¼Œä»¥åŠç¼ºä¹ labeled video dataï¼Œä»¥å‰çš„ 3D ConvNet ç›¸å°è¼ƒæ·ºï¼ˆæœ€å¤š 8 å±¤ï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç™¼ç¾è«¸å¦‚ VGG-16 å’Œ ResNet ç­‰å¾ˆæ·±çš„å½±åƒåˆ†é¡ç¶²è·¯å¯ä»¥è¼•é¬†æ“´å±•æˆ spatio-temporal feature extractorsï¼Œä¸¦ä¸”ä»–å€‘çš„é è¨“ç·´æ¬Šé‡ä¹Ÿå¯ä»¥æä¾›æœ‰åƒ¹å€¼çš„åˆå§‹åŒ–ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ä¹Ÿç™¼ç¾ two-stream çš„ä½œæ³•ä¾ç„¶æœ‰ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;
fig2. K æ˜¯å½±ç‰‡ä¸­çš„ frame çš„ç¸½æ•¸ï¼ŒN æ˜¯ç›¸é„° frames çš„å­é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–æ˜¯æœ¬æ–‡å¯¦é©—çš„äº”ç¨®æ¶æ§‹ï¼Œå‰å››ç¨®æ˜¯ä¹‹å‰çš„åšæ³•ï¼Œæœ€å¾Œä¸€ç¨®æ˜¯æå‡ºçš„æ–°ä½œæ³•ã€‚
ä¸Šåœ–ä¸­é™¤äº† C3D å¤–éƒ½æœ‰ç”¨åˆ° ImageNet é è¨“ç·´çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;æ™‚é–“æ˜¯æ ¹æ“š input çš„ frame æ›ç®—å‡ºä¾†çš„ï¼Œfps æ˜¯ 25ï¼Œé™¤äº† LSTM é‚£å€‹æ¯”è¼ƒç‰¹åˆ¥ï¼Œå› ç‚º LSTM é‚£å€‹æ˜¯æ¯ 5 frame å– 1 frameï¼Œæ‰€ä»¥æ™‚é–“æ˜¯ 5 å€ã€‚&lt;/p&gt;
&lt;h3 id=&#34;ä¹‹å‰çš„åšæ³•&#34;&gt;ä¹‹å‰çš„åšæ³•&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ConvNet+LSTM&lt;/p&gt;
&lt;p&gt;æœ‰ä¸€ç¨®åšæ³•æ˜¯æŠŠæ¯å€‹ frame ç¨ç«‹é¤µçµ¦ 2D Convï¼Œç„¶å¾Œå†æŠŠé æ¸¬åšå½™æ•´ï¼Œç¬¦åˆ bag of words image modeling çš„ç²¾ç¥ï¼Œä½†é€™æ¨£æœƒå¿½ç•¥æ™‚é–“çµæ§‹ä¸Šçš„è³‡è¨Šï¼Œæ¯”å¦‚ç„¡æ³•åˆ¤æ–·é–‹é–€æˆ–é—œé–€ã€‚&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥æœ€å¥½åœ¨å¾Œé¢åŠ ä¸€å€‹ recurrent layerï¼Œæ‰€ä»¥é€™é‚Šå°±ç”¨ Inception-V1 çµåˆ LSTMã€‚&lt;/p&gt;
&lt;p&gt;åŸå§‹çš„å½±ç‰‡ stream æ˜¯ 25 fpsï¼Œé€™é‚Šæ¯ 5 frame æ¡æ¨£ä¸€æ¬¡ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3D ConvNets&lt;/p&gt;
&lt;p&gt;å’Œä¸€èˆ¬çš„å·ç©ç¥ç¶“ç¶²è·¯å·®ä¸å¤šï¼Œåªæ˜¯å…·æœ‰ spatio-temporal filtersã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç”±æ–¼é¡å¤–çš„ kernel ç¶­åº¦ï¼Œç›¸æ¯” 2D Conv æœƒæœ‰æ›´å¤šåƒæ•¸ï¼Œä¹Ÿä½¿ä»–å€‘æ›´é›£è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”é€™æ¨£æœƒç„¡æ³•ç™¼æ® ImageNet é è¨“ç·´çš„å¥½è™•ï¼Œå› æ­¤ä¹‹å‰çš„å·¥ä½œéƒ½å®šç¾©äº†ç›¸å°æ·ºå±¤çš„æ¶æ§‹ï¼Œä¸¦ä¸”å¾é ­è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;benchmark ä¸­çš„è¡¨ç¾å‚™å—æœŸå¾…ï¼Œä½†å’Œ SOTA æ¯”æ²’æœ‰ç«¶çˆ­åŠ›ï¼Œä¹Ÿå› æ­¤æˆç‚ºæœ¬æ–‡å¯¦é©—çš„è‰¯å¥½å€™é¸è€…ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç”¨çš„æ˜¯ C3D çš„å°è®Šé«”ï¼Œå·®ç•°åœ¨æ–¼æ‰€æœ‰å·ç©å±¤å’Œ FC å±¤çš„å¾Œé¢éƒ½ç”¨äº† BNã€‚
è€Œä¸”åœ¨ç¬¬ä¸€å€‹ pooling layer ç”¨çš„ stride æ˜¯ 2ï¼Œå¥½æ¸›å°‘è¨˜æ†¶é«”çš„ä½¿ç”¨ï¼Œæ¯”ç”¨æ›´å¤§çš„ batchï¼Œé€™åœ¨ BN ä¸­éå¸¸é‡è¦ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two-Stream Networks&lt;/p&gt;
&lt;p&gt;Royï¼šé€™è£¡ç”±æ–¼æ¯”è¼ƒè¤‡é›œï¼Œæˆ‘è¦æ”¹æ two-stream çš„åŸå§‹è«–æ–‡ï¼ˆ&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2199&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;ï¼‰èªªæ˜é€™æ±è¥¿æ˜¯ä»€éº¼&lt;/p&gt;
&lt;p&gt;ç°¡è€Œè¨€ä¹‹å°±æ˜¯åˆ†æˆå…©å€‹éƒ¨åˆ†ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ç©ºé–“è³‡è¨Šï¼š&lt;/p&gt;
&lt;p&gt;ç”¨å½±ç‰‡çš„ä¸€å€‹ frameã€€ç¶“éå·ç©ç¥ç¶“ç¶²è·¯é”æˆï¼Œé€™å€‹ frame ç”¨ä¾†æå–å½±åƒä¸­çš„ç‰©ä»¶è³‡è¨Šï¼Œæ¯”å¦‚æ‰“æ’çƒé€™å‹•ä½œå¯èƒ½è¾¨è­˜å‡ºæ’çƒå°±éå¸¸å¥½åˆ¤å®šï¼Œæ‰€ä»¥ç”¨æŸå€‹ frame ä¾†æå–ç©ºé–“è³‡è¨Šã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å‹•ä½œè³‡è¨Šï¼š&lt;/p&gt;
&lt;p&gt;é€™é‚Šç”¨ä¸€é€£ä¸²çš„å…‰æµï¼ˆoptical flowï¼‰åœ–ä¾†é”æˆï¼Œå…‰æµæ˜¯ç‰©é«”ï¼ˆpixelï¼‰åœ¨å…©å€‹ frame é–“çš„ä½ç§»å‘é‡ï¼Œä¼°è¨ˆæ–¹æ³•æœ‰å¾ˆå¤šï¼Œé€™è£¡ä¸ä¸€ä¸€èˆ‰ä¾‹ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/ex-fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–å‡ºè‡ª &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2199&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;ï¼Œåœ– c å°±æ˜¯å…‰æµï¼Œå…·æœ‰å…©å€‹æ–¹å‘ï¼ŒæŒ‡å‡ºåƒç´ çš„ä½ç§»ï¼Œåœ– d æ˜¯æ°´å¹³æ–¹å‘çš„è¦–è¦ºåŒ–ï¼Œåœ– e æ˜¯å‚ç›´æ–¹å‘çš„è¦–è¦ºåŒ–ã€‚&lt;/p&gt;
&lt;p&gt;å†æŠŠé€™äº›å…‰æµåœ–é¤µçµ¦å·ç©ç¥ç¶“ç¶²è·¯ï¼Œç”¨ä½œå‹•ä½œè³‡è¨Šçš„åˆ¤åˆ¥ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å€¼å¾—ä¸€æçš„æ˜¯ä»–æ˜¯ late fusionï¼Œè€Œä¸”æ˜¯ç”¨åŠ æ¬Šå¹³å‡ï¼Œä¸æ˜¯åƒä¸€èˆ¬æƒ³çš„æŠŠç‰¹å¾µçµåˆå†åšå…¶ä»–è™•ç†ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-new-two-stream-inflated-3d-convnets&#34;&gt;The New: Two-Stream Inflated 3D ConvNets&lt;/h3&gt;
&lt;p&gt;ä½œè€…æŠŠæˆåŠŸçš„ 2D åˆ†é¡æ¨¡å‹ç°¡å–®åœ°è½‰æ›ç‚º 3D&lt;/p&gt;
&lt;h4 id=&#34;inflating&#34;&gt;Inflating&lt;/h4&gt;
&lt;p&gt;åšæ³•æ˜¯æŠŠæ–¹å½¢çš„ filter æ”¹æˆç«‹æ–¹é«”ï¼ŒæŠŠ N x N çš„ filter æ”¹æˆ N x N x N çš„ filterï¼Œä½†é€™åªæœ‰æ¶æ§‹ä¸Šçš„åƒè€ƒã€‚&lt;/p&gt;
&lt;h4 id=&#34;bootstraping&#34;&gt;Bootstraping&lt;/h4&gt;
&lt;p&gt;æŠŠæ¬Šé‡ä¹Ÿçµ¦è½‰æ›åˆ° 3D æ¶æ§‹çš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…è§€å¯Ÿåˆ°å½±åƒå¯ä»¥é€éåè¦†è¤‡è£½è²¼ä¸Šä¾†ç”Ÿå‡ºä¸€å€‹ã€Œä¸æœƒå‹•çš„ç„¡èŠå½±ç‰‡ã€ï¼Œ
é€éé€™äº›å½±ç‰‡ï¼Œ3D æ¨¡å‹å¯ä»¥é€éé€™ç¨®æ–¹å¼åœ¨ ImageNet ä¸Š implicitly pretrainï¼Œåšæ³•å°±æ˜¯è®“ 3D filter åƒç„¡èŠå½±ç‰‡çš„è¼¸å‡ºå’Œ 2D filter åƒå–®ä¸€ frame çš„è¼¸å‡ºç›¸åŒï¼Œåšæ³•å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘å¯ä»¥æ²¿æ™‚é–“ç¶­åº¦é‡è¤‡ 2D filter N æ¬¡ï¼ŒæŠŠé€™æ¬Šé‡çµ¦ 3D filterï¼ŒåŒæ™‚æŠŠæ¬Šé‡é™¤ä»¥ Nï¼Œé”åˆ°é€™ç¨®æ•ˆæœã€‚&lt;/p&gt;
&lt;h4 id=&#34;pacing-receptive-field-growth-in-space-time-and-network-depth&#34;&gt;Pacing receptive field growth in space, time and network depth&lt;/h4&gt;
&lt;p&gt;ä»¥å¾€åœ¨åœ–ç‰‡ä¸Šå°æ°´å¹³å’Œå‚ç›´è»¸çš„å°å¾…æ˜¯å¹³ç­‰çš„ï¼Œpooling kernel å’Œ stride éƒ½ä¸€æ¨£ã€‚
ä½¿æ„Ÿå—é‡åœ¨å…©å€‹ç¶­åº¦ä¸Šéš¨è‘—æ¨¡å‹è¶Šä¾†è¶Šæ·±ï¼Œæ…¢æ…¢å¹³ç­‰å¢é•·ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯æ™‚é–“è»¸ç”¨å°ç¨±çš„æ„Ÿå—é‡ä¸ä¸€å®šæœ€å¥½ï¼Œè€Œè©²å–æ±ºæ–¼ frame rate å’Œ image dimensinosã€‚
å¦‚æœæ™‚é–“ç›¸å°æ–¼ç©ºé–“å¢é•·å¤ªå¿«ï¼Œå¯èƒ½æœƒæ··æ·†ä¸åŒå°è±¡çš„é‚Šç·£ï¼Œå½±éŸ¿æ—©æœŸçš„ç‰¹å¾µæª¢æ¸¬ã€‚å¦‚æœå¢é•·å¤ªæ…¢ï¼Œå¯èƒ½ç„¡æ³•å¾ˆå¥½åœ°æ•æ‰å ´æ™¯å‹•æ…‹ã€‚&lt;/p&gt;
&lt;p&gt;å¯¦é©—ä¸­ï¼Œè¼¸å…¥å½±ç‰‡çš„ fps æ˜¯ 25ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾åœ¨å‰å…©å€‹ max pooling layer ä¸åœ¨æ™‚é–“è»¸ poolingï¼ˆé€éç”¨ 1 x 3 x 3 çš„ kernelï¼Œä¸¦ä¸”æ™‚é–“è»¸çš„ stride æ˜¯ 1ï¼‰ï¼Œä¸¦åœ¨å…¶ä»– max pooling layer éƒ½ç”¨ symmetric kernels å’Œ stride æ˜¯æœ‰å¹«åŠ©çš„ã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œçš„ average pooling layer æ˜¯ç”¨ 2 x 7 x 7 çš„ kernelã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ 64 frame è¨“ç·´ï¼Œä½†ç”¨æ•´å€‹å½±ç‰‡æ¸¬è©¦ã€‚ï¼ˆaveraging predictions temporallyï¼‰&lt;/p&gt;
&lt;p&gt;æˆ‘æƒ³äº†ä¸€ä¸‹ï¼Œ250 / 64 é™¤ä¸é€²ï¼Œä½†æ˜¯æˆ‘çœ‹ code ç™¼ç¾ä»–å¥½åƒå¯¬é«˜ 224 * 224 çš„ç…§ç‰‡æœƒåœ¨æœ€å¾Œç¶“é Average pool å¾Œè®Šæˆ 1 * 1ï¼Œæ‰€ä»¥ä»–å¯ä»¥ç›´æ¥ç”¨ 1 * 1 * 1 çš„å·ç©æ ¸æŠŠè¼¸å…¥é€šé“æ”¹æˆåˆ†é¡æ•¸ï¼Œå†æŠŠæ™‚é–“è»¸çš„çµæœå¹³å‡ã€‚&lt;/p&gt;
&lt;h4 id=&#34;two-3d-streams&#34;&gt;Two 3D Streams&lt;/h4&gt;
&lt;p&gt;åˆ†åˆ¥è¨“ç·´å…©å€‹ç¶²è·¯ï¼Œä¸¦åœ¨æ¸¬è©¦éšæ®µå°é æ¸¬é€²è¡Œå¹³å‡ã€‚&lt;/p&gt;
&lt;p&gt;é€™é‚Šä½œè€…èªªå…‰æµçš„æ¼”ç®—æ³•æŸç¨®æ„ç¾©ä¸Šæ˜¯ recurrentï¼ˆä¾‹å¦‚ï¼Œå°æ–¼ flow fields é€²è¡Œ iterative optimizationï¼‰ï¼Œæˆ‘ä¸å¤ªæ‡‚é€™é‚Šæ˜¯ä»€éº¼æ„æ€ï¼Œæˆ‘æƒ³ä½œè€…ç”¨çš„å…‰æµæ¼”ç®—æ³•æ‡‰è©²æ˜¯é€éæŸç¨®é¡ä¼¼ EM æ¼”ç®—æ³•é‚£ç¨®ä¸æ–·è¿­ä»£å»é€¼è¿‘æ•¸å€¼çš„æ¼”ç®—æ³•ï¼Œä½†ä½œè€…æåˆ°ã€Œæˆ–è¨±æ˜¯å› ç‚ºç¼ºä¹ recurrenceï¼Œæˆ‘å€‘ç™¼ç¾é›™æµæœ‰åƒ¹å€¼ã€ï¼Œæˆ‘ä¸å¤ªæ‡‚ç‚ºä»€éº¼éœ€è¦ recurrence æ•ˆæœæ‰æœƒå¥½ã€‚&lt;/p&gt;
&lt;p&gt;ä½†çµè«–æ˜¯ two-stream ä¾ç„¶å…·å‚™åƒ¹å€¼ã€‚&lt;/p&gt;
&lt;h4 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h4&gt;
&lt;p&gt;é€™é‚Šè¬›æ»¿è©³ç´°çš„ï¼Œæœ‰èˆˆè¶£å¯ä»¥å»åŸæ–‡çœ‹ã€‚
åªæä¸€ä¸‹å¹¾é»:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å…‰æµæ¼”ç®—æ³•æ˜¯ç”¨ TV-L1ã€‚&lt;/li&gt;
&lt;li&gt;é™¤äº†é¡ä¼¼ C3D çš„ 3D ConvNet éƒ½ç”¨ä½¿ç”¨ ImageNet é è¨“ç·´çš„ Inception-V1 ä½œç‚º base networkã€‚&lt;/li&gt;
&lt;li&gt;å°æ–¼è¼ƒçŸ­çš„å½±ç‰‡ï¼Œæœƒé‡è¤‡å¾ªç’°ä»¥æ»¿è¶³æ¨¡å‹çš„è¼¸å…¥ä»‹é¢&lt;/li&gt;
&lt;li&gt;æ¸¬è©¦æ™‚æœƒåœ¨ä¸­é–“å‰ªè£ 224 x 224&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-kinetics-human-action-video-dataset&#34;&gt;The Kinetics Human Action Video Dataset&lt;/h2&gt;
&lt;p&gt;Kinetics æœ‰ 400 å€‹äººé¡å‹•ä½œé¡åˆ¥ã€‚æ¯å€‹é¡åˆ¥æœ‰ 400 å€‹ clipï¼Œè€Œä¸”æ¯å€‹ clip éƒ½ä¾†è‡ªä¸€å€‹ unique çš„ Youtube å½±ç‰‡ï¼Œå…±æœ‰ 24 è¬å€‹è¨“ç·´å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹ clip éƒ½å¤§ç´„ 10 ç§’ï¼Œè€Œä¸”æ²’æœ‰æœªå‰ªçš„å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æ¸¬è©¦é›†æ¯å€‹ class åŒ…å« 100 å€‹ clipã€‚&lt;/p&gt;
&lt;h2 id=&#34;experimental-comparison-of-architectures&#34;&gt;Experimental Comparison of Architectures&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table2and3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;I3D åœ¨æ‰€æœ‰è³‡æ–™é›†ä¸Šéƒ½è¡¨ç¾æœ€å¥½ï¼Œç”šè‡³æ˜¯åœ¨ UCF-101 å’Œ HMDB-51 é€™ç¨®å°è³‡æ–™é›†ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ï¼Œé€™æ„å‘³è‘— ImageNet é è¨“ç·´çš„å¥½è™•æœ‰æˆåŠŸæ“´å±•åˆ° 3D ConvNetã€‚&lt;/p&gt;
&lt;p&gt;å¤šæ•¸æ¨¡å‹åœ¨ UCF ä¸Šéƒ½è¡¨ç¾å¾—æ¯” Kinetics ä¸Šå¥½ï¼Œé¡¯ç¾å‡ºè³‡æ–™é›†çš„é›£åº¦å·®è·ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯åœ¨ HMDB è¡¨ç¾å¾—è¼ƒå·®ï¼ŒåŸå› å¯èƒ½æ˜¯ HMDB æ•…æ„å¼„å¾—å¾ˆé›£ï¼Œä½œè€…æœ‰èˆ‰ä¾‹ï¼Œå¾ˆå¤š clip åœ¨å®Œå…¨ç›¸åŒçš„å ´æ™¯æœƒæœ‰ä¸åŒçš„å‹•ä½œã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æœ‰æåˆ°èªª I3D ç‰¹å¾µæ¯”è¼ƒå¥½é·ç§»çš„ä¸€ç¨®è§£é‡‹æ˜¯å®ƒå…·å‚™ high temporal resolutionï¼Œ
I3D åœ¨ 25 fps çš„å½±ç‰‡ä¸­ç”¨ 64 frames åšè¨“ç·´ï¼Œä½¿å®ƒèƒ½æ•æ‰å‹•ä½œçš„ fine-grained æ™‚é–“çµæ§‹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;experimental-evaluation-of-features&#34;&gt;Experimental Evaluation of Features&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table4and5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Kinetics ä¸Šåšé è¨“ç·´æ•ˆæœæ˜é¡¯æ¯” ImageNet å¥½ã€‚&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Kinetics ä¸Šçš„é è¨“ç·´å°æ–¼é·ç§»å­¸ç¿’æœ‰æ˜é¡¯å¥½è™•ï¼Œä½†å°æ–¼å…¶ä»–å½±åƒä»»å‹™ï¼Œæ¯”å¦‚å½±åƒèªç¾©åˆ†å‰²æ˜¯å¦æœ‰å¥½è™•ä»å¾…è§€å¯Ÿã€‚&lt;/p&gt;
&lt;p&gt;ç›®å‰å°æ–¼æ¶æ§‹æ²’æœ‰å…¨é¢æ¢ç´¢ï¼Œæ¯”å¦‚æ²’æœ‰æ¡ç”¨ action tubes æˆ–æ˜¯ attention æ©Ÿåˆ¶ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Swin Transformer è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Fri, 14 Apr 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.14030&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€å€‹æ–°çš„ vision Transformerï¼Œç¨±ä½œ Swin Transformerï¼Œå¯ä»¥è¢«ç”¨ä½œ computer vision ä¸­çš„ general-purpose backboneã€‚&lt;/p&gt;
&lt;p&gt;æŠŠ Transformer å¾ language ç§»åˆ° vision å…·å‚™æŒ‘æˆ°æ€§ï¼Œæ¯”å¦‚åŒä¸€å€‹ visual entity åœ¨å¤§å°ä¸Šå…·å‚™å¾ˆå¤§çš„ varianceã€‚é‚„æœ‰ high resolution ä¸‹ pixel å’Œ word çš„æ•¸é‡å·®ç•°å¤ªå¤§ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è§£æ±ºé€™äº›å·®ç•°ï¼Œä½œè€…æå‡º hierachical Transformerï¼Œç”¨ shifted windows ä¾†ç®—å‡º representationã€‚&lt;/p&gt;
&lt;p&gt;shifted windowing é€éæŠŠ self-attention é™åˆ¶åœ¨ non-overlapping çš„ local window å’Œå…è¨± cross-windows connection ä¾†æé«˜æ•ˆç‡ã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨® hierarchical architecture å¯ä»¥éˆæ´»åœ°åœ¨å„ç¨® scale ä¸‹æ“´å±• modelï¼Œé‚„å¯ä»¥å°åœ–åƒå¤§å°æœ‰ç·šæ€§çš„è¨ˆç®—æ™‚é–“è¤‡é›œåº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ViT æŠŠåœ–ç‰‡æ‰“æˆ patchï¼Œæ¯å€‹ patch æ˜¯ 16*16ï¼Œfeature maps ç”± single low resolution çš„è¼¸å…¥ç”Ÿæˆï¼Œè€Œä¸”ç”±æ–¼è‡ªæ³¨æ„åŠ›å§‹çµ‚éƒ½æ˜¯åœ¨å…¨å±€ä¸Šè¨ˆç®—çš„ (patch å’Œ patch é–“åšè‡ªæ³¨æ„åŠ›)ï¼Œæ‰€ä»¥æ™‚é–“è¤‡é›œåº¦æ˜¯ quadratic computation complexityã€‚&lt;/p&gt;
&lt;p&gt;Swin Transformer å¾å° patch é–‹å§‹ï¼Œä¸¦åœ¨æ›´æ·±çš„ Transformer layers åˆä½µç›¸é„°çš„ patchesã€‚&lt;/p&gt;
&lt;p&gt;æœ‰äº†é€™äº› hierarchical feature mapsï¼Œå¯ä»¥ç”¨åœ¨åƒæ˜¯ FPN æˆ–æ˜¯ U-Netã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹ Swin Transformer çš„é—œéµè¨­è¨ˆå› ç´ æ˜¯ shifted windowã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;é€é bridge ä¸åŒ layer çš„ windows ä¾†æä¾›ä»–å€‘é€£æ¥ã€‚&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;overall-architecture&#34;&gt;Overall Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Patch Merging
&lt;ul&gt;
&lt;li&gt;åŸæœ¬ç‰¹å¾µåœ–æ˜¯ H * W * C&lt;/li&gt;
&lt;li&gt;ä»¥ä¸Šä¸‹ stride=2 è¡Œèµ°ï¼Œæœƒå¾—åˆ°å››å¼µ H/2 * W/2 * C&lt;/li&gt;
&lt;li&gt;concatenate èµ·ä¾†ï¼Œè®Šæˆ H/2 * W/2 * 4C&lt;/li&gt;
&lt;li&gt;åš linearï¼Œè®Šæˆ H/2 * W/2 * 2C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;swin-transformer-block&#34;&gt;Swin Transformer block&lt;/h4&gt;
&lt;p&gt;Swin Transformer æ˜¯é€éæŠŠ Transformer block ä¸­çš„ multi-head self attention(MSA) æ›æˆåŸºæ–¼ shifted windows çš„ module æ§‹æˆã€‚&lt;/p&gt;
&lt;h3 id=&#34;shifted-window-based-self-attention&#34;&gt;Shifted Window based Self-Attention&lt;/h3&gt;
&lt;p&gt;æ¨™æº–çš„ Transformer æ¶æ§‹æœƒç®— global self-attentionï¼Œè¨ˆç®—æ‰€æœ‰ token é–“å½¼æ­¤çš„é—œä¿‚ï¼Œå°è‡´ quadratic complexityï¼Œä½¿å…¶ä¸é©ç”¨æ–¼éœ€è¦å¤§é‡ token çš„è¨±å¤š CV å•é¡Œ&lt;/p&gt;
&lt;h4 id=&#34;self-attention-in-non-overlapped-windows&#34;&gt;Self-attention in non-overlapped windows&lt;/h4&gt;
&lt;p&gt;åŸä¾†çš„åœ–ç‰‡æœƒä»¥ non-overlapping çš„æ–¹å¼åˆ‡å‰²ã€‚&lt;/p&gt;
&lt;p&gt;å‡è¨­æ¯å€‹ windows æœ‰ M * M å€‹ patchesï¼Œç„¶å¾Œä¸€å¼µåœ–åƒæœ‰ h * w å¡Š patchesï¼Œè¨ˆç®—è¤‡é›œåº¦å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega(MSA)=4hwC^2+2(hw)^2C$&lt;/li&gt;
&lt;li&gt;$\Omega(W-MSA)=4hwC^2+2M^2hwC$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;shifted-window-partitioning-in-successive-blocks&#34;&gt;Shifted window partitioning in successive blocks&lt;/h4&gt;
&lt;p&gt;window-based self-attention module ç¼ºä¹äº† windows é–“å½¼æ­¤çš„é€£æ¥ï¼Œæœƒé™åˆ¶æ¨¡å‹èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºäº†ä¸€ç¨® shifted window çš„æ–¹æ³•ï¼Œä¿æŒ non-overlapping windows çš„é«˜æ•ˆè¨ˆç®—ï¼ŒåŒæ™‚å¼•å…¥ windows é–“çš„é€£æ¥ã€‚&lt;/p&gt;
&lt;p&gt;å†å…©å€‹é€£çºŒçš„ windows é–“ï¼Œæœƒç§»å‹• $(âŒŠ \frac{M}{2} âŒ‹, âŒŠ \frac{M}{2} âŒ‹)$&lt;/p&gt;
&lt;h4 id=&#34;efficient-batch-computation-for-shifted-configuration&#34;&gt;Efficient batch computation for shifted configuration&lt;/h4&gt;
&lt;p&gt;shifted window æœ‰å€‹å•é¡Œæ˜¯ï¼Œæœƒå°è‡´æ›´å¤šçš„ windowsï¼Œå¾ $âŒˆ \frac{h}{M} âŒ‰ * âŒˆ \frac{w}{M} âŒ‰$ åˆ° $(âŒˆ \frac{h}{M} âŒ‰+1) * (âŒˆ \frac{w}{M} âŒ‰+1)$ï¼Œè€Œä¸”æœ‰äº› window æœƒå°æ–¼ M*Mã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£æœƒå°è‡´ç„¡æ³•æŠŠé€™äº›çµ¦å£“æˆä¸€å€‹ batch å¿«é€Ÿè¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨® naive çš„è§£æ³•å°±æ˜¯ç›´æ¥åœ¨å¤–é¢åŠ  zero paddingï¼Œä½†æœƒå¢åŠ è¨ˆç®—é‡ï¼Œç•¶ windows æ•¸é‡è¼ƒå°‘æ™‚ï¼Œè¨ˆç®—é‡æœƒè®Šå¾ˆå¯è§€ (å¾ 2 * 2 å€‹ windows è®Šæˆ 3 * 3 å€‹ windowsï¼Œå¢åŠ äº† 2.25 å€)&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºå¦å¤–ä¸€ç¨®å·§å¦™çš„åšæ³•ï¼ŒæŠŠä¸€äº›éƒ¨åˆ†æŒªç§»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½†ç¾åœ¨æœ‰äº› window è£¡æœ‰å¤šå€‹ä¸è©²ç›¸äº’åš attention çš„éƒ¨åˆ†ï¼Œæ‰€ä»¥è¦ç”¨ mask çš„æ–¹å¼è¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä¸åŒ windowsï¼Œåš self-attention å¾Œï¼ŒæŠŠä¸ç›¸å¹²çš„éƒ¨åˆ†åšçš„ attention æ¸›å»ä¸€å€‹å¾ˆå¤§çš„æ•¸å€¼ï¼Œæœ€å¾Œå†é softmaxã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/mask.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–ä¾†è‡ªä½œè€…åœ¨ github æä¾›çš„å¯è¦–åŒ–&lt;/p&gt;
&lt;p&gt;æœ€å¾Œå†æŠŠå®ƒæŒªå›åŸæœ¬çš„ä½ç½®ã€‚&lt;/p&gt;
&lt;h4 id=&#34;relative-position-bias&#34;&gt;Relative position bias&lt;/h4&gt;
&lt;p&gt;åƒè€ƒé€™å€‹: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_37541097/article/details/121119988&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/qq_37541097/article/details/121119988&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;architecture-variants&#34;&gt;Architecture Variants&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;window size é è¨­æ˜¯ M = 7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;query dimension of each head æ˜¯ d = 32&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;expansion layer of each MLP is $\alpha$ = 4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C æ˜¯ first stage çš„ hidden layers çš„ channel numbers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-T&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 6, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 0.25 å€&lt;/li&gt;
&lt;li&gt;complexity æ¥è¿‘ ResNet-50&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-S&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 0.5 å€&lt;/li&gt;
&lt;li&gt;complexity æ¥è¿‘ ResNet-101&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-B&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 128&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-L&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 192&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 2 å€&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;image-classification-on-imagenet-1k&#34;&gt;Image Classification on ImageNet-1K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;object-detection-on-coco&#34;&gt;Object Detection on COCO&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;semantic-segmentation-on-ade20k&#34;&gt;Semantic Segmentation on ADE20K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;åŸºæ–¼ self-attention çš„ shifted window æ˜¯ Swin Transformer é—œéµéƒ¨åˆ†ï¼Œè¢«é¡¯ç¤ºå‡ºä»–åœ¨ CV é ˜åŸŸæœ‰æ•ˆç‡ä¸”æœ‰æ•ˆï¼Œä¸¦æœŸæœ›æœªä¾†æŠŠå®ƒæ‡‰ç”¨åœ¨ NLPã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GIT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 29 Mar 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.14100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GIT: A Generative Image-to-text Transformer for Vision and Language&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•   
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;è¨­è¨ˆäº†ä¸€å€‹ Generative Image-to-text Transformerï¼Œçµ±ä¸€ vision-language tasksï¼Œåƒæ˜¯ image/video captioning æˆ–æ˜¯å•ç­”ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ generative models åœ¨é è¨“ç·´å’Œå¾®èª¿çš„æ™‚å€™æ˜¯åŒæ¨£çš„ç¶²è·¯æ¶æ§‹ï¼Œç¾æœ‰çš„å·¥ä½œé€šå¸¸éƒ½åŒ…å«è¤‡é›œçš„æ¶æ§‹ (uni/multi-modal encoder/decoder)ï¼Œ
è€Œä¸”ä¾è³´æ–¼å¤–éƒ¨æ¨¡çµ„ï¼Œæ¯”å¦‚ç‰©ä»¶åµæ¸¬æˆ– optical character recognition (OCR)ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ GITï¼Œæˆ‘å€‘ç°¡åŒ–ç‚º single language modeling task ä¸‹çš„ä¸€å€‹ image encoder å’Œä¸€å€‹ text decoderã€‚&lt;/p&gt;
&lt;p&gt;æ“´å¤§äº†é è¨“ç·´è³‡æ–™å’Œæ¨¡å‹å¤§å°ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨±å¤šå…·æœ‰æŒ‘æˆ°æ€§çš„ benchmarks ä¸Šå–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚é¦–æ¬¡åœ¨ TextCpas ä¸Šè¶…è¶Šäººé¡çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;æå‡ºäº†ä¸€ç¨® generation-based image classification and scene text recognition çš„æ–°æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;è¿‘å¹´ä¾†åœ¨ vision-languageï¼ˆVLï¼‰é è¨“ç·´æ–¹é¢å–å¾—äº†å·¨å¤§é€²å±•ï¼Œç‰¹åˆ¥æ˜¯åŸºæ–¼ image-text pairs çš„å¤§è¦æ¨¡æ•¸æ“šï¼Œä¾‹å¦‚ CLIPã€Florence å’Œ SimVLMã€‚&lt;/p&gt;
&lt;p&gt;å­¸ç¿’åˆ°çš„ representation å¾ˆå¥½çš„æé«˜äº†ä¸‹æ¸¸ä»»å‹™çš„æ€§èƒ½ï¼Œæ¯”å¦‚ image captioningã€visual question answering å’Œ image-text retrievalã€‚&lt;/p&gt;
&lt;p&gt;åœ¨é è¨“ç·´éç¨‹ä¸­ï¼ŒMasked Language Modeling (MLM) å’Œ Image-Text Matching (ITM) è¢«å»£æ³›ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™äº› loss å’Œä¸‹æ¸¸ä»»å‹™ä¸åŒï¼Œå¿…é ˆåš task-specific adaptationã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼Œ image captioning è¦ç§»é™¤ ITMï¼ŒVQA éœ€è¦é¡å¤–éš¨æ©Ÿåˆå§‹çš„ MLPã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ¸›å°‘é€™ç¨®å·®ç•°ï¼Œæœ€è¿‘çš„ç ”ç©¶è©¦åœ–ç‚ºé è¨“ç·´æ¨¡å‹è¨­è¨ˆ unified generative models ä¾†é è¨“ç·´ï¼Œå› ç‚ºå¤§å¤šæ•¸ VL çš„å•é¡Œå¯ä»¥è½‰åŒ–ç‚ºç”Ÿæˆå•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›æ–¹æ³•é€šå¸¸åˆ©ç”¨ multi-modal encoder å’Œ text decoderï¼Œä¸¦ç²¾å¿ƒè¨­è¨ˆ text input å’Œ text targetã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æ¨å‹•é€™æ–¹å‘çš„ç ”ç©¶ï¼Œä½œè€…è¨­è¨ˆäº†ä¸€å€‹ç°¡å–®çš„ Generative Image-to-text Transformerï¼Œç¨±ä½œ GITï¼ŒåªåŒ…å«ä¸€å€‹ image encoder å’Œ text decoderã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´ä»»å‹™åªæ˜¯æŠŠè¼¸å…¥çš„åœ–åƒæ˜ å°„åˆ°ç›¸é—œè¯çš„æ–‡å­—æè¿°ã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ä»–å¾ˆç°¡å–®ï¼Œä½†é‚„æ˜¯åœ¨çœ¾å¤šå…·æœ‰æŒ‘æˆ°æ€§çš„ benchmark å–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;image encoder æ˜¯ Swin-like vision transformerï¼Œåœ¨å¤§é‡çš„ image-text pairs ä¸Šåš pretrainï¼ŒåŸºæ–¼ contrastive taskã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¶ˆé™¤äº†ç¾æœ‰è¨±å¤šæ–¹æ³•ä¸­å° object detector çš„ä¾è³´ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†å°‡å…¶æ“´å±•åˆ°å½±ç‰‡é ˜åŸŸï¼Œæˆ‘å€‘æŠŠå¤šå€‹ frame çš„ç‰¹å¾µ concatenateï¼Œä½œç‚º video è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;text decoder æ˜¯ä¸€å€‹ç”¨ä¾†é æ¸¬ç›¸é—œè¯æ–‡å­—çš„ transformerã€‚&lt;/p&gt;
&lt;p&gt;æ•´å€‹ç¶²è·¯éƒ½æ˜¯åŸºæ–¼ language modeling task ä¾†è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ VQAï¼Œinput question è¢«çœ‹ä½œ text prefixï¼Œä¸¦ä»¥ auto-regressive çš„æ–¹æ³•ç”Ÿå‡ºç­”æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç¨® generation-based çš„ ImageNet classification æ–°æ–¹æ¡ˆï¼Œé æ¸¬æ¨™ç±¤ç›´æ¥æ ¹æ“šä½œè€…çš„ç”Ÿæˆæ¨¡å‹ï¼Œè€Œä¸ç”¨é å…ˆå®šç¾©è©å½™è¡¨ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘çš„ä½œæ³•å¾ˆç°¡å–®ï¼Œä½†åœ¨æ“´å¤§é è¨“ç·´è³‡æ–™å’Œæ¨¡å‹å¤§å°å¾Œï¼Œæˆæœé©šäººã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦è²¢ç»å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘å±•ç¤ºäº† GITï¼Œåƒ…ç”±ä¸€å€‹ image encoder å’Œä¸€å€‹ text decoder çµ„æˆï¼Œé€é language modeling taskï¼Œåœ¨ 0.8 billion image-text pairs ä¸Š pretrainã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨ image/video captioning å’Œ QA ä¸Šï¼Œæ²’æœ‰åŸºæ–¼ object detectorsï¼Œobject tags å’Œ OCRï¼Œå°±åœ¨å¤šå€‹ä»»å‹™ä¸Šå–å¾— SOTAã€‚è­‰æ˜ç°¡å–®çš„ç¶²è·¯æ¶æ§‹ä¹Ÿå¯ä»¥é€é scaling å–å¾—å¼·å¤§çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘è­‰æ˜ GIT é›–ç„¶ pretrain åœ¨ image-text pairsï¼Œä¹Ÿèƒ½åœ¨ video tasks ä¸Šå–å¾— SOTAï¼Œä¸éœ€è¦ video dedicated encodersã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘æå‡ºäº†ä¸€ç¨®æ–°çš„ generation-based image classification æ–¹æ¡ˆï¼Œåœ¨ ImageNet-1K ä¸Šï¼Œå–å¾—ä¸éŒ¯çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/table1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ VL pre-training ä¸­ï¼Œå¤š multi-task pre-training è¢«å»£æ³›ä½¿ç”¨ï¼Œè³¦äºˆç¶²è·¯å¤šç¨®æˆ–å¢å¼·çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼ŒMLM å’Œ ITM æ˜¯å»£æ³›æ¡ç”¨çš„é è¨“ç·´ä»»å‹™ï¼Œæœ€è¿‘ä¹Ÿæœ‰ç ”ç©¶åŠ å…¥ image-text contrastive lossã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼å¤šæ•¸ VL ä»»å‹™éƒ½å¯ä»¥è¡¨ç¤ºæˆ text generation taskï¼Œæ‰€ä»¥å¯ä»¥è¨“ç·´ä¸€å€‹ç”Ÿæˆæ¨¡å‹ä¾†æ”¯æŒå„ç¨®ä¸‹æ¸¸ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;è¼¸å…¥å’Œè¼¸å‡ºæ–‡æœ¬é€šå¸¸éƒ½æœƒç¶“éç²¾å¿ƒè¨­è¨ˆï¼Œä»¥é è¨“ç·´é€™æ¨£çš„ç”Ÿæˆæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ image representationï¼ŒFaster RCNN è¢«å¤§å¤šæ•¸ç¾æœ‰æ–¹æ³•ç”¨ä¾†æå–å€åŸŸç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;åŒæ™‚ï¼Œä¹Ÿå¾ˆå®¹æ˜“ä»¥ end-to-end çš„æ–¹æ³•è¨“ç·´æ•´å€‹ç¶²è·¯ã€‚&lt;/p&gt;
&lt;p&gt;é™¤äº† feature mapï¼Œobject tagsï¼Œä¹Ÿå¾ˆå¸¸è¢«ç”¨ä¾†æ–¹ä¾¿ transformer ç†è§£ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ¥æ˜¯ novel objectsã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼èˆ‡å ´æ™¯æ–‡æœ¬ç›¸é—œçš„ä»»å‹™ï¼Œèª¿ç”¨ OCR ä»¥ç”Ÿæˆå ´æ™¯æ–‡æœ¬ä½œç‚ºé™„åŠ ç¶²è·¯è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ text predictionï¼Œå¸¸ç”¨ transformer networkï¼Œçµåˆ cross-attention module ä¾†èåˆ image tokensã€‚&lt;/p&gt;
&lt;p&gt;æˆ–è€…åªæ˜¯å–®ç´” concatenate text tokens å’Œ image tokensï¼Œç„¶å¾Œç”¨ self-attentionã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘æœ‰ 9 å€‹ä¸åŒçš„ benchmarkï¼Œ3 ç¨®ä¸åŒæ¨¡å‹å¤§å°å’Œ 3 ç¨®ä¸åŒé è¨“ç·´è³‡æ–™è¦æ¨¡ã€‚&lt;/p&gt;
&lt;h2 id=&#34;generative-image-to-text-transformer&#34;&gt;Generative Image-to-text Transformer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;network-architecture&#34;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;image encoder åŸºæ–¼ contrastive pre-trained modelã€‚&lt;/p&gt;
&lt;p&gt;è¼¸å…¥æ˜¯åŸå§‹åœ–åƒï¼Œè¼¸å‡ºæ˜¯ compact 2D feature mapï¼Œè¢« flatten æˆ list of featuresã€‚&lt;/p&gt;
&lt;p&gt;é€éä¸€å€‹é¡å¤–çš„ linear layer å’Œä¸€å€‹ layernorm layerï¼Œimage features è¢« project åˆ° D dimensionsï¼Œä¹Ÿå°±æ˜¯ text encoder çš„ inputã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä½¿ç”¨åš contrastive tasks pretraining çš„ image encoderï¼Œå› ç‚ºæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜é€™ç¨® image encoder æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å¾Œé¢çš„ç« ç¯€ï¼Œé‚„è§€å¯Ÿåˆ° VL performence æ˜é¡¯åœ°éš¨è‘—æ›´å¼·çš„ image encoder è€Œæœ‰æ‰€æå‡ã€‚
é€™å’Œ object detection-based çš„æ–¹æ³•è§€å¯Ÿåˆ°çš„çµæœä¸€è‡´ã€‚&lt;/p&gt;
&lt;p&gt;CoCa çš„ concurrent work çµ±ä¸€äº† contrastive task å’Œ the generation taskï¼Œä½œç‚ºä¸€å€‹é è¨“ç·´éšæ®µã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„æ–¹æ³•ç›¸ç•¶æ–¼æ˜¯æŒ‰é †åºåˆ†é›¢å…©å€‹ä»»å‹™:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨ contrastive task è¨“ç·´ image encoder&lt;/li&gt;
&lt;li&gt;ç”¨ generation task pretrain image encoder å’Œ text decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;text decoder æ˜¯ä¸€å€‹ç”¨æ–¼é æ¸¬æ–‡æœ¬æè¿°çš„ transformer moduleï¼Œç”±å¤šå€‹ transformer block çµ„æˆï¼Œæ¯å€‹ transformer block ç”±ä¸€å€‹ self-attention layer å’Œ feed-forward layer çµ„æˆã€‚&lt;/p&gt;
&lt;p&gt;text è¢« tokenize å’Œ embed åˆ° D dimensionsï¼Œä¸¦æ·»åŠ  positional encoding å’Œ layernorm layerã€‚&lt;/p&gt;
&lt;p&gt;image features å’Œ text embeddings è¢« concatenate èµ·ä¾†ä½œç‚º transformer module çš„è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;text ä»¥ [BOS] é–‹å§‹ï¼Œä¸¦ä»¥ auto regressive çš„æ–¹å¼ decodeï¼Œç›´åˆ° [EOS] æˆ– maximum stepsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;attention mask æ ¹æ“šä¸Šåœ–è¨­è¨ˆï¼Œä½¿çš„ text token åªèƒ½ä¾è³´æ–¼å‰é¢çš„ text token å’Œ image tokenï¼Œè€Œ image token å¯ä»¥äº’ç›¸åš attentionã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œ unidirectional attention mask ä¸åŒï¼Œunidirectional attention mask ä¸¦éæ¯å€‹ image token éƒ½å¯ä»¥ä¾è³´æ–¼å…¶ä»–çš„ Image tokenã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…å¾ˆå¥½åœ°åˆå§‹åŒ– image encoderï¼Œå»éš¨æ©Ÿåˆå§‹åŒ– text decoderã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨®è¨­è¨ˆå‹•æ©Ÿæ˜¯åŸºæ–¼[MiniVLM: A Smaller and Faster Vision-Language Model]ï¼Œè©²ç ”ç©¶éš¨æ©Ÿåˆå§‹åŒ–é¡¯ç¤ºå‡ºèˆ‡ BERT åˆå§‹åŒ–ç›¸ä¼¼åœ°æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åŸå› å¯èƒ½åœ¨æ–¼ BERT åœ°åˆå§‹åŒ–ç„¡æ³•ç†è§£åœ–åƒä¿¡è™Ÿï¼Œé€™å°æ–¼ VL ä»»å‹™è‡³é—œé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;[Flamingo: a Visual Language Model for Few-Shot Learning] æ¡ç”¨äº†é¡ä¼¼çš„ image encoder + text decoderï¼Œä½†æ˜¯ä»–å€‘çš„ decoder ç¶“é pretrainï¼Œä¸¦ä¸”æœ‰ freezeï¼Œå¥½ä¿ç•™å¤§å‹èªè¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;GIT çš„æ‰€æœ‰åƒæ•¸éƒ½æœƒæ›´æ–°ï¼Œä»¥æ›´å¥½åœ°é©æ‡‰ VL çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€ç¨®æ¶æ§‹æ˜¯ cross-attention-based çš„ decoderï¼Œç”¨æ–¼ incorporate image signalsï¼Œè€Œä¸æ˜¯ concatenation å†ç”¨ self-attentionã€‚&lt;/p&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œlarge-scale çš„ pre-trainingï¼Œself-attention-based æœƒæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå°è¦æ¨¡çš„å‰‡æ˜¯ cross-attention-basedã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹åˆç†çš„è§£é‡‹æ˜¯ï¼Œç¶“éå……åˆ†è¨“ç·´ï¼Œdecoder å¯ä»¥å¾ˆå¥½åœ°è™•ç†åœ–åƒå’Œæ–‡æœ¬ï¼Œè€Œä¸” image token å¯ä»¥ç‚ºäº† text generation æ›´å¥½åœ°æ›´æ–°ã€‚&lt;/p&gt;
&lt;p&gt;è€Œ cross-attention è®“ image token æ²’è¾¦æ³• attend å½¼æ­¤ã€‚&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h3&gt;
&lt;p&gt;è¨“ç·´æ¡ç”¨ language modeling (LM) lossã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/for1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$I$ æ˜¯ image&lt;/li&gt;
&lt;li&gt;$y_i,i \in $ { $ 1,&amp;hellip;,N $ } æ˜¯æ–‡å­— tokenï¼Œ$y_0$ æ˜¯ [BOS]ï¼Œ$y_{N+1}$ æ˜¯ [EOS]&lt;/li&gt;
&lt;li&gt;CE æ˜¯æœ‰ 0.1 label smoothing çš„ cross-entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¦ä¸€ç¨®é¸æ“‡æ˜¯ MLMï¼Œåœ¨æ¯å€‹ epoch ä¸­é æ¸¬ 15% çš„è¼¸å…¥ tokenï¼Œè¦é æ¸¬æ‰€æœ‰ token è‡³å°‘éœ€è¦ 1 / 0.15 = 6.7 å€‹ epochsï¼Œå°æ–¼ LMï¼Œæ¯å€‹ epoch éƒ½å¯ä»¥é æ¸¬æ‰€æœ‰ tokenï¼Œå°æ–¼å¤§è¦æ¨¡é è¨“ç·´è³‡æ–™ä¾†èªªæ•ˆç‡æ›´é«˜ã€‚&lt;/p&gt;
&lt;p&gt;ablation studies é¡¯ç¤ºå‡º LM å¯ä»¥åœ¨æœ‰é™çš„ epoch å…§å¯¦ç¾æ›´å¥½çš„æ€§èƒ½ã€‚
åœ¨å¤§è¦æ¨¡è¨“ç·´ä¸­ï¼Œç”±æ–¼è¨ˆç®—è³‡è¨Šçš„é™åˆ¶ï¼Œåªæœ‰å…©å€‹ epochï¼Œæ‰€ä»¥é¸æ“‡ LMã€‚
èˆ‡æ­¤åŒæ™‚ï¼Œå¤§éƒ¨åˆ†æœ€è¿‘çš„ large-scale language model ä¹Ÿæ˜¯åŸºæ–¼ LMã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœæ²’æœ‰åœ–åƒè¼¸å…¥ï¼Œè©²æ¨¡å‹å°‡ç°¡åŒ–ç‚º decoder-only çš„èªè¨€æ¨¡å‹ï¼Œæ¶æ§‹é¡ä¼¼æ–¼ GPT-3ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œé€™ç¨®è¨­è¨ˆé‚„å¯ä»¥åˆ©ç”¨ text-only çš„è³‡æ–™ä¾†æå‡ scaled-up decoder çš„èƒ½åŠ›ï¼ŒæŠŠé€™ä¿ç•™çµ¦æœªä¾†çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;å°æ–¼ image captioningï¼Œç”±æ–¼è¨“ç·´æ•¸æ“šæ ¼å¼å’Œé è¨“ç·´ç›¸åŒï¼Œæ‰€ä»¥ç”¨åŒæ¨£çš„ LM task ä¾†å¾®èª¿ GITã€‚
å°æ–¼ visual question answeringï¼Œå•é¡Œå’Œ GT åœ¨å¾®èª¿çš„æ™‚å€™è¢«çœ‹åš special captionï¼Œä½† LM loss åƒ…ç”¨æ–¼ç­”æ¡ˆå’Œ [EOS]ã€‚&lt;/p&gt;
&lt;p&gt;æ¨ç†éç¨‹ä¸­ï¼Œquestion è¢«ç•¶ä½œ caption çš„ prefixï¼Œå®Œæˆçš„éƒ¨åˆ†æ˜¯é æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;VQAv2 ç¾æœ‰çš„å·¥ä½œæ”¶é›†å€™é¸ç­”æ¡ˆï¼Œå†é‡æ§‹æˆåˆ†é¡å•é¡Œï¼Œé æ¸¬ä¸€æ¬¡ã€‚
ä½œè€…çš„å·¥ä½œæœ‰æ›´å¤šæŒ‘æˆ°ï¼Œå› ç‚ºæ˜¯ç”Ÿæˆå¼çš„ï¼Œéœ€è¦ç”Ÿå‡ºè‡³å°‘å…©å€‹æ­£ç¢ºçš„ tokenï¼Œç­”æ¡ˆå’Œ [EOS]ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œè€ƒæ…®åˆ°è‡ªç”±å½¢å¼ç­”æ¡ˆçš„å¥½è™•ï¼Œä½œè€…é¸æ“‡äº†ç”Ÿæˆæ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ç”Ÿæˆæ¨¡å‹çš„é›£åº¦ï¼ŒVQAv2 æ¯”ç¾æœ‰çš„åˆ¤åˆ¥å·¥ä½œç•¥å·®ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼å’Œ scene-text related VQA ä»»å‹™ï¼Œç¾æœ‰æ–¹æ³•é€šå¸¸åˆ©ç”¨ OCR ç”Ÿæˆ 5 å€‹ scene text ä¸¦ç”¨ dynamic pointer network æ±ºå®šç•¶å‰è¼¸å‡ºæ‡‰è©²æ˜¯ OCR é‚„æ˜¯ general textã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç”±æ–¼ä½œè€…çš„æ–¹æ³•ä¸ä¾è³´æ–¼ OCRï¼Œå› æ­¤ä¹Ÿä¸ä¾è³´æ–¼ dynamic pointer networkã€‚&lt;/p&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œä½œè€…ç™¼ç¾æ¨¡å‹é€éå¤§è¦æ¨¡é è¨“ç·´è³‡æ–™å­¸æœƒå¦‚ä½•é–±è®€å ´æ™¯æ–‡æœ¬ï¼Œä¸¦ä¸”ä½œè€…çš„æ¨¡å‹ä¸æ˜¯å°ˆé–€ç‚ºäº†å½±ç‰‡é ˜åŸŸè¨­è¨ˆçš„ï¼Œä½†å¯ä»¥é€éç°¡å–®çš„æ¶æ§‹æ›´æ”¹å°±å–å¾—å…·æœ‰ç«¶çˆ­åŠ›æˆ–ç”šè‡³ SOTA çš„æˆæœï¼Œä¹Ÿå°±æ˜¯ä½œè€…å¯ä»¥å¾æ¯å€‹å½±ç‰‡æ¡æ¨£å¤šå€‹ frameï¼Œä¸¦é€é image encoder ç¨ç«‹åœ°ç‚ºæ¯å€‹ frame ç·¨ç¢¼ã€‚
æœ€å¾Œæ·»åŠ ä¸€å€‹ learnable temporal embedding (åˆå§‹åŒ–ç‚º 0)ï¼Œä¸¦ concatenate sampled frames çš„ç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„ç”¨æ–¼åœ–ç‰‡åˆ†é¡ï¼ŒæŠŠ class name ç”¨æ–¼ captionã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œç¾æœ‰å·¥ä½œä¸ä¸€æ¨£ï¼Œç¾æœ‰å·¥ä½œé€šå¸¸å…ˆå®šç¾©è©å½™è¡¨ï¼Œä¸¦ç”¨ç·šæ€§å±¤é æ¸¬æ¯å€‹é¡åˆ¥çš„å¯èƒ½æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç•¶æ–°æ•¸æ“šå’Œæ–°é¡åˆ¥è¢«æ·»åŠ åˆ°ç¾æœ‰æ•¸æ“šçš„æ™‚å€™ï¼Œé€™ç¨®æ–°ä¸€ä»£çš„æ–¹æ¡ˆæ˜¯æœ‰ç›Šçš„ï¼Œå› ç‚ºé€™æ¨£å¯ä»¥åœ¨ä¸å¼•å…¥æ–°åƒæ•¸çš„æƒ…æ³ä¸‹å°æ–°æ•¸æ“šé€²è¡Œè¨“ç·´ã€‚&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æ”¶é›† 0.8B çš„ image-text pairs ä¾†é è¨“ç·´ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image encoder æ˜¯æ ¹æ“š  pre-trained contrastive model åˆå§‹åŒ–çš„ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hidden dimension (D) = 768&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;text decoder æœ‰ 6 å€‹ randomly-initialized transformer blocks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å…±æœ‰ 0.7b çš„åƒæ•¸&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image decoder å’Œ text encoder çš„ learning rate å„åˆ¥æ˜¯ 1e-5 å’Œ 5e-5ï¼Œéƒ½ cosine decay åˆ° 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ¨è«–éšæ®µ beam size æ˜¯ 4ï¼Œlength penalty æ˜¯ 0.6ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supplementary materials å±•ç¤ºäº†å°æ¨¡å‹è®Šé«” (GITB and GITL) å’Œæ›´å¤§æ¨¡å‹ (GIT2) çš„çµæœ&lt;/p&gt;
&lt;h3 id=&#34;results-on-image-classification&#34;&gt;Results on Image Classification&lt;/h3&gt;
&lt;p&gt;è¼¸å‡ºå¿…é ˆèˆ‡é¡åˆ¥åç¨±å®Œå…¨åŒ¹é…ï¼Œç”šè‡³è€ƒæ…®å¤šæˆ–å°‘çš„ç©ºæ ¼ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ä¸çŸ¥é“è©å½™è¡¨ï¼Œç²¾ç¢ºåŒ¹è¢«æº–ç¢ºåº¦åªæœ‰ 1.93%ï¼Œå¦‚æœé æ¸¬åŒ…å« GT å°±å°ï¼Œé‚£æœ‰ 40.88%ã€‚&lt;/p&gt;
&lt;p&gt;é€šéå¾®èª¿æ¯å€‹é¡åˆ¥åªæœ‰ 1 shot æˆ– 5 shotï¼Œæº–ç¢ºåº¦æœƒé¡¯è‘—æé«˜ï¼Œ
è¡¨æ˜åªç”¨å°‘é‡è¨“ç·´æ¨£æœ¬ï¼Œä¹Ÿå¯ä»¥è¼•é¬†é©æ‡‰ä¸‹æ¸¸ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡ Flamingo ç›¸æ¯”ï¼ŒGIT å¯¦ç¾æ›´é«˜çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;p&gt;Flamingo åœ¨æ²’æœ‰åƒæ•¸æ›´æ–°çš„æƒ…æ³ä¸‹é€²è¡Œå°æ¨£æœ¬å­¸ç¿’ï¼Œä½†éœ€è¦é¡å¤–çš„ç¶²è·¯è¼¸å…¥ï¼Œå¯èƒ½æœƒå¢åŠ æ¨ç†æˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGIT é€éä¸€æ¬¡ lightweight fine-tuningï¼Œæ¨ç†éç¨‹ä¸­ä¸éœ€è¦é€™äº› training shotã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;h4 id=&#34;model-and-data-scaling&#34;&gt;Model and data scaling&lt;/h4&gt;
&lt;p&gt;å°æ–¼ç¶²è·¯æ¶æ§‹ï¼Œä½œè€…çš„æ¨¡å‹è¢«ç¨±ä½œ Hugeï¼ŒæŠŠ image encoder æ›æˆ CLIP çš„ ViT-B/16 å’Œ ViT-L/14 çš„å‰‡æ˜¯ Base å’Œ Largeã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å¯ä»¥çœ‹å‡ºè¼ƒå¤§çš„ image encoder å¸¶ä¾†çš„å¥½è™•ï¼Œä½†æ ¹æ“šå¯¦é©—ï¼Œ
ä½œè€…ç™¼ç¾å¾ˆé›£æœ‰æ•ˆåœ°æ“´å±• text decoderï¼ŒåŸå› å¯èƒ½æ˜¯ LM å¾ˆé›£ç”¨ limited amount of text ä¾†è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹å¯èƒ½çš„åŸå› æ˜¯ image encoder è² è²¬ object recognitionï¼Œè€Œ decoder è² è²¬ä»¥ NLP çš„æ–¹æ³•çµ„ç¹” object termsã€‚
å¾Œä¸€é …ä»»å‹™å¯èƒ½å¾ˆå®¹æ˜“ï¼Œå› ç‚ºå¤§å¤šæ•¸æè¿°éƒ½éµå¾ªç›¸ä¼¼çš„æ¨¡å¼ï¼Œæ¯”å¦‚ Object + verb + subjectï¼Œæ‰€ä»¥åªè¦ä¸€å€‹ small decoderï¼Œè¼ƒå¤§çš„ decoder å¯èƒ½æœƒå¢åŠ å­¸ç¿’é›£åº¦ã€‚&lt;/p&gt;
&lt;p&gt;Flamingo çš„ç ”ç©¶é¡¯ç¤ºæ›´å¤§çš„ Decoder å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†æ˜¯ä»–å€‘çš„ decoder æœ‰ pretrain éï¼Œè€Œä¸”åœ¨ VL é è¨“ç·´çš„æ™‚å€™ frozenï¼Œé¿é–‹äº†å¦‚ä½•æœ‰æ•ˆè¨“ç·´ decoder çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;LEMON çš„ transformer å¯ä»¥æ“´å±•åˆ° 32 å±¤ï¼Œå¯èƒ½æ˜¯å› ç‚ºä»–å€‘ä½¿ç”¨ MLM è€Œä¸æ˜¯ LMï¼Œå¾Œè€…å¯èƒ½æ›´åŠ å›°é›£ã€‚&lt;/p&gt;
&lt;h4 id=&#34;scene-text-in-pre-training-data&#34;&gt;Scene text in pre-training data&lt;/h4&gt;
&lt;p&gt;ç‚ºäº†ç­è§£ scene text comprehension çš„èƒ½åŠ›ï¼Œä½œè€…æª¢æŸ¥äº† pretrain data æœ‰å¤šå°‘ image-text pairs æœ‰ scene textã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ Microsoft Azure OCR API4 å°ä¸€äº›è³‡æ–™åš OCRï¼Œç„¶å¾ŒæŠŠ OCR çµæœå’Œ associated text åšæ¯”å°ï¼Œåªæœ‰åŒ…å«é•·åº¦è¶…é 5 å€‹å­—å…ƒçš„ OCR çµæœæ‰æœƒç®—æ¯”å°ã€‚
æœ‰ 15% çš„ CC12M å’Œ 31% çš„ä¸‹è¼‰åœ–åƒ(500K) åŒ…å« scene text æè¿°ã€‚
ç”±æ–¼ä»»å‹™æ˜¯è¨“ç·´é æ¸¬ textï¼Œç¶²è·¯é€æ¼¸å­¸æœƒé–±è®€ scene textã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;h3 id=&#34;limitations&#34;&gt;Limitations&lt;/h3&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œç›®å‰ä¸æ¸…æ¥šå¦‚ä½•æ§åˆ¶ç”Ÿæˆçš„ caption ä»¥åŠå¦‚ä½•åœ¨ä¸æ›´æ–°åƒæ•¸çš„æƒ…æ³ä¸‹åŸ·è¡Œ in-context learningï¼ŒæŠŠé€™ç•™çµ¦æœªä¾†çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h3 id=&#34;societal-impact&#34;&gt;Societal impact&lt;/h3&gt;
&lt;p&gt;è©²æ¨¡å‹åœ¨å¤§è¦æ¨¡æ•¸æ“šé›†ä¸Šé è¨“ç·´ï¼Œä¸èƒ½ä¿è­‰æ•¸æ“šä¸å« toxic languageï¼Œå¯èƒ½æœƒ poison outputã€‚&lt;/p&gt;
&lt;h2 id=&#34;å…¶ä»–&#34;&gt;å…¶ä»–&lt;/h2&gt;
&lt;h3 id=&#34;a3-network&#34;&gt;A.3 Network&lt;/h3&gt;
&lt;p&gt;è¬›è¶…åƒæ•¸&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/model.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MAE è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</link>
        <pubDate>Wed, 15 Feb 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.06377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;é€™ç¯‡è«–æ–‡é¡¯ç¤ºå‡º MAE æ˜¯ CV ä¸­çš„ scalable self-supervised learnersã€‚&lt;/p&gt;
&lt;p&gt;MAE çš„æ–¹æ³•å¾ˆç°¡å–®&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;éš¨æ©Ÿè“‹ä½è¼¸å…¥å½±åƒçš„ä¸€äº› patch&lt;/li&gt;
&lt;li&gt;é‡å»º missing pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å…·å‚™å…©å€‹æ ¸å¿ƒè¨­è¨ˆ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;éå°ç¨±çš„ encoder-decoder æ¶æ§‹ï¼Œencoder åªä½œç”¨æ–¼å¯è¦‹çš„ patch å­é›†åˆ(æ²’æœ‰ mask tokens)ï¼Œlightweight decoder å‰‡æ ¹æ“š latent representation å’Œ make tokens ä¾†é‡å»ºåœ–ç‰‡ã€‚&lt;/li&gt;
&lt;li&gt;ç•¶é®ä½é«˜æ¯”ä¾‹(æ¯”å¦‚ 75%)çš„å½±åƒæ™‚ï¼Œæœƒå¾—åˆ°ä¸€å€‹ nontrivial å’Œ meaningful çš„ self-supervisory task&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;çµåˆé€™å…©é»è¨­è¨ˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¨“ç·´å¤§æ¨¡å‹ã€‚
ä»¥ ViT-Huge ç”¨ ImageNet-1K è¨“ç·´(è¨“ç·´é›†ä¸€ç™¾å¤šè¬å¼µç…§ç‰‡)å¯é”åˆ° 87.8% çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/intro.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ CV ä¸­ï¼Œå¸¸éœ€è¦å¤§é‡ labeled imagesã€‚
NLP ä¸­ï¼Œè‡ªç›£ç£é è¨“ç·´è™•ç†äº†éœ€è¦å¤§é‡æ¨™è¨»è³‡æ–™çš„å•é¡Œã€‚
masked autoencoders æ˜¯ä¸€ç¨®æ›´ general çš„ denoising autoencoders çš„å½¢å¼ã€‚
BERT éå¸¸æˆåŠŸï¼Œautoencoding methods åœ¨ CV çš„ç ”ç©¶å»è½å¾Œ NLPï¼Œä½œè€…æ€è€ƒæ˜¯ä»€éº¼è®“ masked autoencoding åœ¨ CV å’Œ NLP ç”¢ç”Ÿä¸åŒã€‚
æœ‰ä»¥ä¸‹è§€é»&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç›´åˆ°å‰é™£å­ï¼ŒCV ä¸­çš„ CNN æ˜¯ä¸»æµï¼Œä½†å·ç©å±¤ä¸å¥½å¼•å…¥ mask tokens æˆ– positional embedding é€™äº› indicatorã€‚ä½†é€™äº›å¯ä»¥é€é ViT ä¾†è§£æ±ºï¼Œä¸æ‡‰æˆç‚ºå•é¡Œã€‚&lt;/li&gt;
&lt;li&gt;èªè¨€å’Œè¦–è¦ºçš„ Information density ä¸åŒï¼Œèªè¨€æ˜¯ highly semantic å’Œ information-denseï¼Œä½¿å¡«å­—æœ¬èº«ä¸æ˜¯å¾ˆç°¡å–®çš„äº‹æƒ…ï¼Œä½†å½±åƒå«æœ‰å¤§é‡å†—é¤˜çš„è¨Šæ¯ï¼Œç¼ºå¤±çš„éƒ¨åˆ†æ¯”è¼ƒå¥½å¾ç›¸é„°çš„ patch é‡å»ºï¼Œæ¯”å¦‚ç›´æ¥æ’å€¼ï¼Œæ‰€ä»¥ä½œè€…ç”¨ä¸€ç¨®ç°¡å–®çš„ç­–ç•¥ï¼Œéš¨æ©Ÿ mask å¾ˆå¤§ä¸€éƒ¨åˆ†çš„ patchï¼Œå‰µé€ ä¸€å€‹å…·æœ‰æŒ‘æˆ°æ€§çš„è‡ªç›£ç£ä»»å‹™ï¼Œå¼·è¿«æ¨¡å‹é—œæ³¨ global çš„è³‡è¨Šã€‚&lt;/li&gt;
&lt;li&gt;é—œæ–¼ decoderï¼ŒCV é‚„åŸ pixelï¼Œpixel å±¬æ–¼ lower semantic levelï¼ŒNLP é‚„åŸ wordï¼Œword çš„ semantic information è¼ƒé«˜ã€‚ä½œè€…ç™¼ç¾ï¼Œé›–ç„¶åœ¨ BERT ä¸­ï¼Œå¯ä»¥ç”¨ç°¡å–®çš„ decoder é‚„åŸ(ä¸€å€‹ MLP)ï¼Œä½† CV ä¸­ decoder çš„è¨­è¨ˆå°±å¾ˆé‡è¦ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åŸºæ–¼ä»¥ä¸Šè§€é»ï¼Œä½œè€…æå‡º MAEï¼Œéš¨æ©Ÿé®ä½å¤§é‡çš„ patchï¼Œä¸¦åœ¨ pixel space é‡å»ºå¤±å»çš„ patchã€‚è€Œä¸”æ˜¯éå°ç¨± encoder-decoder æ¶æ§‹ï¼Œencoder åªæœƒçœ‹åˆ°å¯è¦‹çš„ patchï¼Œä½† docoder é™¤äº† latent representationï¼Œé‚„æœƒçœ‹åˆ° mask tokensã€‚é€™ç¨®è¨­è¨ˆåœ¨éå¸¸é«˜çš„æ©è“‹ç‡(æ¯”å¦‚ 75%)ä¸‹ä¸ä½†å¯ä»¥æé«˜æº–ç¢ºåº¦ï¼Œé‚„å¯ä»¥è®“ encoder åªè™•ç†è¼ƒå°‘æ¯”ä¾‹(æ¯”å¦‚ 25%)çš„ patchï¼Œå°‡è¨“ç·´æ™‚é–“æ¸›å°‘ 3 å€æˆ–æ›´å¤šï¼Œä½¿ MAE å¯ä»¥è¼•é¬†æ“´å±•æˆæ›´å¤§çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨é€™æ¨£çš„æ¶æ§‹ä¸‹ï¼Œç”¨ MAE çš„ pre-trainingï¼Œå¯ä»¥è¨“ç·´éå¸¸åƒ data çš„æ¨¡å‹ï¼Œæ¯”å¦‚ ViT-Large/-Hugeï¼Œè€Œåªä½¿ç”¨ ImageNet-1Kã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ ImageNet-1K åœ¨ vanilla ViT-Huge ä¸Š fine-tune å¯é”åˆ° 87.8% æº–ç¢ºåº¦ï¼Œæ¯”ä»¥å¾€åªä½¿ç”¨ ImageNet-1K çš„çµæœéƒ½é«˜ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ obejct detectionã€instance segmentationã€semantic segmentation ä¸Šåš transfer learning éƒ½é”åˆ°ä¸éŒ¯çš„æ•ˆæœï¼Œå¯ä»¥æ‰“æ•—ç”¨ç›£ç£å¼é è¨“ç·´æ¨¡å‹çš„å°æ‰‹ã€‚&lt;/p&gt;
&lt;h1 id=&#34;ç›¸é—œå·¥ä½œ&#34;&gt;ç›¸é—œå·¥ä½œ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoding
&lt;ul&gt;
&lt;li&gt;MAE æ˜¯ä¸€ç¨® denoising autoencoding çš„å½¢å¼ï¼Œä½†å’Œ DAE é‚„æ˜¯å·®åˆ¥å¾ˆå¤§ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masked image encoding
&lt;ul&gt;
&lt;li&gt;iGPTã€ViTã€BEiT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Masking&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å’Œ ViT ä¸€æ¨£ï¼ŒæŠŠåœ–ç‰‡åˆ‡æˆå¤šå€‹ patchï¼Œå°æ–¼ patch å‡å‹»éš¨æ©Ÿåœ°æ¡æ¨£ä¿ç•™ï¼Œå‰©ä¸‹åœ°é®ä½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViT&lt;/li&gt;
&lt;li&gt;ä¹Ÿæœ‰ positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer block&lt;/li&gt;
&lt;li&gt;è¼¸å…¥
&lt;ul&gt;
&lt;li&gt;encoded visible patches&lt;/li&gt;
&lt;li&gt;mask tokens
&lt;ul&gt;
&lt;li&gt;shared, learned vector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;éƒ½æœƒåŠ å…¥ positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç”¨ç›¸è¼ƒ encoder è¼•é‡çš„è§£ç¢¼å™¨ï¼Œæ‰€æœ‰çš„ patch ç”±é€™å€‹è¼•é‡çš„ decoder è™•ç†ï¼Œæ¸›å°‘é è¨“ç·´æ™‚é–“&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconstruction target&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decoder çš„æœ€å¾Œä¸€å±¤æ˜¯ linear projectionï¼Œä¹‹å¾Œå† reshape æˆä½ è¦çš„  patch&lt;/li&gt;
&lt;li&gt;loss function
&lt;ul&gt;
&lt;li&gt;mean squared error(MSE)&lt;/li&gt;
&lt;li&gt;åªç®— masked patched çš„ MSEï¼Œåƒ BERT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple implementation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å…ˆå–å¾—ä¸€ç³»åˆ— token(patch åš linear projection + positional embedding)&lt;/li&gt;
&lt;li&gt;randomly shuffleï¼Œæ ¹æ“šæ¯”ä¾‹ç§»é™¤å°¾ç«¯ä¸€éƒ¨ä»½&lt;/li&gt;
&lt;li&gt;encoding å¾Œï¼Œå°¾ç«¯æ¥ä¸Š mask tokensï¼Œä¸¦ä¸” unshuffle&lt;/li&gt;
&lt;li&gt;åŠ ä¸Š positional embedding å¾Œï¼Œçµ¦ decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;imagenet-experiments&#34;&gt;ImageNet Experiments&lt;/h1&gt;
&lt;p&gt;åœ¨ ImageNet-1K ä¸Šåšè‡ªç›£ç£çš„é è¨“ç·´ï¼Œç„¶å¾Œåš&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;end-to-end fine-tuning
&lt;ul&gt;
&lt;li&gt;æ‰€æœ‰åƒæ•¸éƒ½å¯æ”¹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;linear probing
&lt;ul&gt;
&lt;li&gt;åªæ”¹æœ€å¾Œä¸€å±¤ç·šæ€§å±¤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/vit-mae.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/ratio-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;optimal masking ratio æ„å¤–åœ°é«˜ï¼Œç›¸æ¯” BERT åªæœ‰ 15%&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/fine-tune-blocks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;è¨è«–å’Œçµè«–&#34;&gt;è¨è«–å’Œçµè«–&lt;/h1&gt;
&lt;p&gt;åœ¨ CV å¯¦ç”¨çš„é è¨“ç·´åšæ³•ä¸»æµæ˜¯ç›£ç£å¼çš„ï¼ŒCV ä¸­è‡ªç›£ç£çš„åšæ³•å¯èƒ½æ­£è·Ÿè‘— NLP çš„è»Œè·¡èµ°ã€‚&lt;/p&gt;
&lt;p&gt;è¦ä»”ç´°è™•ç†åœ–åƒå’Œèªè¨€çš„å€åˆ¥ï¼Œä½œè€…å»é™¤åœ–ç‰‡ä¸­å¾ˆå¯èƒ½ä¸æ§‹æˆ semantic segment çš„éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ç§»é™¤æŸå€‹ objectã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ViT è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 12 Feb 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.11929&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;åœ¨ CV é ˜åŸŸ transformer è¡¨ç¾æœ‰é™ï¼Œç›®å‰ attention å¸¸å¸¸æ˜¯å’Œå·ç©ç¥ç¶“ç¶²è·¯ä¸€èµ·ç”¨ï¼Œæˆ–æ˜¯ç”¨ä¾†æŠŠä¸€äº›å·ç©å±¤æ›æˆ self-attentionï¼Œä½†æ•´é«”æ¶æ§‹ä¸è®Šã€‚é€™ç¯‡è«–æ–‡æƒ³å±•ç¾ä¸€å€‹ç´” Transformer å¯ä»¥ç›´æ¥åœ¨å½±åƒåˆ†é¡ä¸Šè¡¨ç¾å¾ˆå¥½ã€‚å¦‚æœç”¨å¤§é‡è³‡æ–™ä½œé è¨“ç·´ï¼Œå†é·ç§»åˆ°ä¸­å°å‹çš„è³‡æ–™é›†ï¼Œå¯ä»¥å’Œ SOTA çš„ CNN è¡¨ç¾å¾—ä¸€æ¨£å¥½ï¼Œé‚„éœ€è¦è¼ƒå°‘çš„è¨“ç·´è³‡æºä½œè¨“ç·´ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;self-attention-based æ¶æ§‹ï¼Œç‰¹åˆ¥æ˜¯ Transformerï¼Œå·²ç¶“æ˜¯ NLP çš„é‡è¦é¸æ“‡ã€‚ä¸»æµçš„ä½œæ³•æ˜¯åœ¨å¤§å‹æ–‡å­—è³‡æ–™é›†ä¸Šä½œè¨“ç·´ï¼Œå†é‡å°å°å‹ä»»å‹™è³‡æ–™é›†ä½œ fine-tuneã€‚ç”±æ–¼ Transformer çš„è¨ˆç®—æ•ˆç‡é«˜ï¼Œé‚„æœ‰å¯æ“´å±•æ€§ï¼Œå¯ä»¥ train ä¸€äº›å¾ˆå¤§çš„ modelï¼Œéš¨è‘— model å’Œè³‡æ–™é›†å¢å¤§ï¼Œç›®å‰é‚„æ²’çœ‹å‡ºé£½å’Œçš„ç¾è±¡ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œåœ¨ CVï¼ŒCNN é‚„æ˜¯ä¸»æµï¼Œä¸€äº›å·¥ä½œå˜—è©¦ç”¨ self-attention çµåˆ CNN-like çš„æ¶æ§‹ï¼Œæ¯”å¦‚æŠŠ feature map ç•¶ transformer çš„è¼¸å…¥ï¼Œå› ç‚ºåŸå§‹ pixel å¤ªå¤šï¼Œæˆ–ç”šè‡³æŠŠå·ç©å±¤å…¨æ›æˆ self-attentionï¼Œé›–ç„¶å¾Œè€…ç†è«–ä¸Šæ•ˆç‡å¾ˆé«˜(åŸè«–æ–‡ä¸­æœ‰å¦å¤– cite å…©ç¯‡ä½œæ³•)ï¼Œä½†å› ç‚ºä»–å€‘åšæ³•ç‰¹æ®Šï¼Œåœ¨ç¾ä»£ç¡¬é«”ä¸Šå¾ˆé›£åŠ é€Ÿï¼Œæ‰€ä»¥ç„¡æ³•å¾ˆæœ‰æ•ˆåœ°æ“´å±•ã€‚åœ¨ large-scale çš„å½±åƒè­˜åˆ¥ä¸Šï¼Œ ResNet-like çš„æ¶æ§‹é‚„æ˜¯ SOTAã€‚&lt;/p&gt;
&lt;p&gt;è©²å¯¦é©—ç›´æ¥æŠŠä¸€å€‹æ¨™æº–çš„ Transformer ä½œç”¨æ–¼åœ–ç‰‡ä¸Šï¼Œåªä½œæœ€å°‘çš„ä¿®æ”¹ã€‚æŠŠå½±åƒåˆ†æˆå¤šå€‹ patchï¼Œä¸¦æŠŠå®ƒå€‘è®Šæˆä¸€ç³»åˆ—çš„ linear embeddingï¼Œç•¶ä½œ NLP ä¸­çš„ tokens(words) ä¾†è™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;ç•¶åœ¨ä¸­å‹å¤§å°çš„è³‡æ–™é›†(e.g. ImageNet)ä¸Šè¨“ç·´ï¼Œå¦‚æœæ²’æœ‰ strong regularizationï¼ŒViT æœƒç•¥è¼¸åŒç­‰å¤§å°çš„ ResNets&lt;/p&gt;
&lt;p&gt;é€™ç¯‡è«–æ–‡åœ¨æ›´å¤§çš„è³‡æ–™é›†(14M-300M çš„å½±åƒ)ä¸Šè¨“ç·´ï¼Œå°±æ‰“æ•—äº† inductive biasã€‚åœ¨å¤§é‡è³‡æ–™ä¸Šä½œé è¨“ç·´å°±å¾ˆè®šã€‚&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;å¤§å‹çš„ Transformer-based æ¨¡å‹å¸¸å¸¸æ˜¯å…ˆåœ¨å¤§è³‡æ–™é›†ä¸Šé è¨“ç·´ç„¶å¾Œæ ¹æ“šä»»å‹™ fine-tuneï¼Œæ¯”å¦‚ BERT å’Œ GPTã€‚&lt;/p&gt;
&lt;p&gt;è¦æŠŠ self-attention ç”¨åœ¨ CV ä¸Šï¼Œæœ€ç°¡å–®çš„åšæ³•å°±æ˜¯æŠŠæ¯å€‹ Pixel ç•¶ä¸€å€‹å…ƒç´ ï¼Œä½† self-attention æ˜¯å¹³æ–¹è¤‡é›œåº¦ï¼Œåœ¨ç¾å¯¦çš„åœ–ç‰‡å¾ˆé›£æ‡‰ç”¨ã€‚ä¸€å€‹æ‡‰ç”¨ Transformer çš„åšæ³•æ˜¯åªæŠŠ self-attention ç”¨åœ¨ local neighborhoodï¼Œå¦å¤–ä¸€å€‹æ˜¯ç”¨ Sparse Transformerï¼Œé‚„æœ‰ä¸€å †ç‰¹æ®Šçš„æ–¹æ³•ï¼Œé›–ç„¶è¡¨ç¾ä¸éŒ¯ï¼Œä½†è¦ç”¨ç¡¬é«”åŠ é€Ÿèµ·ä¾†ä¸å®¹æ˜“ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹æœ‰é—œçš„æ¨¡å‹æ˜¯ iGPTï¼Œåœ¨ reduce image resolution å’Œ color space å¾ŒæŠŠ transformer æ‡‰ç”¨åœ¨ image pixels ä¸Šã€‚å®ƒç”¨éç›£ç£å¼è¨“ç·´å¾Œï¼Œå† fine-tune æˆ–åš linear probing(åªæ›´æ–°æœ€å¾Œçš„ linear layer) åˆ†é¡ä»»å‹™ï¼Œè¡¨ç¾å¾ˆå¥½ã€‚&lt;/p&gt;
&lt;p&gt;å·²ç¶“æœ‰é¡ä¼¼çš„å·¥ä½œäº†ï¼ŒæŠ½å– patches of size 2 * 2ï¼Œæœ€å¾Œå†æ¥ full self-attentionï¼ŒåŸºæœ¬ä¸Šå’Œ ViT éå¸¸åƒï¼Œé€™ç¯‡è«–æ–‡é€²ä¸€æ­¥è­‰æ˜äº†ä½œå¤§è¦æ¨¡çš„é è¨“ç·´å¯ä»¥è®“ Transformer å’Œ SOTA çš„ CNN ç›¸æ¯”ï¼Œè€Œä¸” ViT å› ç‚º patch æ¯”è¼ƒå¤§ï¼Œå¯ä»¥è™•ç† medium-resolution çš„åœ–ç‰‡ã€‚é€™å•é¡Œæ˜¯å¯é æœŸçš„ï¼Œå› ç‚º Transformer ç¼ºå°‘äº†ä¸€äº› inductive biasesã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inductive biases
&lt;ul&gt;
&lt;li&gt;ä¸€äº›å‡è¨­&lt;/li&gt;
&lt;li&gt;æ¯”å¦‚ CNN å¸¸æœ‰å››å€‹å‡è¨­
&lt;ul&gt;
&lt;li&gt;locality&lt;/li&gt;
&lt;li&gt;translation invariance with pooling layers
&lt;ul&gt;
&lt;li&gt;å¹³ç§»ä¸è®Šæ€§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation equivariance
&lt;ul&gt;
&lt;li&gt;f(g(x)) = g(f(x))&lt;/li&gt;
&lt;li&gt;å·ç©å’Œå¹³ç§»çš„å…ˆå¾Œé †åºæ²’å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;æ¨¡å‹ç›¡å¯èƒ½é¡ä¼¼åŸå§‹ Transformerï¼Œé€™æ¨£å¯ä»¥æŠŠä¸€äº› NLP ä¸ŠæˆåŠŸçš„ Transformer æ¶æ§‹æ‹¿ä¾†ç”¨ï¼Œé‚„å¯ä»¥ç”¨ä¸€äº›å¾ˆæœ‰æ•ˆç‡çš„ implementation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-process.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;embedding ç¶­åº¦æ˜¯ 768 = 16 * 16 * 3
position embedding çš„åšæ³•æ˜¯ standard learnable 1D positional embeddingsï¼Œå°±æ˜¯ BERT çš„åšæ³•ï¼Œç°¡å–®ä¾†èªªå°±æ˜¯ç”Ÿå‡ºä¸€å¼µå¯ä»¥è¨“ç·´çš„è¡¨ï¼Œ(åºåˆ—é•·åº¦, embedding size)ï¼Œä½œè€…ä¹Ÿæœ‰å˜—è©¦å…¶ä»–æ–¹æ³•ï¼Œä½†ç™¼ç¾æˆæ•ˆå·®ä¸å¤šï¼Œæ¯”å¦‚ 2D positional embeddingï¼Œæ¦‚å¿µå°±æ˜¯å¾ç”Ÿå‡º(åºåˆ—é•·åº¦, embedding size)è®Šæˆç”Ÿå‡º 2 å€‹(sqrt(åºåˆ—é•·åº¦), embedding size)ã€‚&lt;/p&gt;
&lt;p&gt;[class] çš„æ¦‚å¿µæ˜¯ NLP å‡ºä¾†çš„ï¼ŒResNet-like çš„æ¶æ§‹å¸¸è¦‹çš„åšæ³•ä¹Ÿæœ‰é€šé globally average-pooling (GAP)ä¾†ç”Ÿå‡ºå‘é‡ï¼Œå†æ¥ä¸Šåˆ†é¡å™¨åšé æ¸¬ã€‚å¯¦é©—ç™¼ç¾ç›´æ¥åœ¨ transformer çš„è¼¸å‡ºåš GAP å’Œ [class] éƒ½å¯ä»¥é”åˆ°ä¸éŒ¯çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-gap.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-acc.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;æ‹¿æ¨™æº–çš„ Transformer ä¾†ä½œ Image recognitionï¼Œå’Œä»¥å¾€ç”¨ self-attention åœ¨ CV çš„æ–¹æ³•ä¸ä¸€æ¨£ï¼Œé™¤äº†ä¸€é–‹å§‹çš„ initial patch extractionï¼Œæ²’æœ‰å¼•å…¥å…¶ä»–å½±åƒç‰¹æœ‰çš„ inductive biasesã€‚ç›´æ¥æŠŠåœ–ç‰‡ç•¶æˆæ˜¯ä¸€ç³»åˆ—çš„ patchï¼Œç„¶å¾Œç›´æ¥ç”¨ Transformer encoder ç•¶ä¸€èˆ¬ NLP ä»»å‹™è™•ç†ã€‚åœ¨å¾ˆå¤šå½±åƒåˆ†é¡è¨“ç·´é›†ä¸Šè¡¨ç¾å¾—æ›´å¥½é‚„åœ¨ pre-train ä¸Šç›¸å°ä¾¿å®œã€‚&lt;/p&gt;
&lt;p&gt;é‚„æœ‰ä¸€äº›å€¼å¾—æŒ‘æˆ°çš„åœ°æ–¹ï¼Œæ¯”å¦‚æŠŠ ViT æ‡‰ç”¨åœ¨å…¶ä»– CV ä»»å‹™ï¼Œæ¯”å¦‚ detection å’Œ segmentationã€‚å¦ä¸€å€‹æŒ‘æˆ°æ˜¯æ¢ç´¢è‡ªç›£ç£é è¨“ç·´çš„æ–¹æ³•ã€‚é€™ç¯‡è«–æ–‡å…¶å¯¦æœ‰å¯¦é©—è‡ªç›£ç£ï¼Œè¡¨ç¾ OKï¼Œä½†å’Œç›£ç£å¼é‚„æ˜¯æœ‰å¾ˆå¤§çš„è½å·®ã€‚æ“´å¤§ ViT å¯èƒ½æœ‰æ›´å¥½çš„çµæœã€‚&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
