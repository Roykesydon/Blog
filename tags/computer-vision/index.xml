<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>computer-vision on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/tags/computer-vision/</link>
        <description>Recent content in computer-vision on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 31 Mar 2024 00:27:55 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>🦑SQUID🦑 論文</title>
        <link>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 31 Mar 2024 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.13495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Radiography imaging protocols (放射線成像協定) 會專注於特定的身體區域，因此會在患者間產生大量相似的照片。&lt;/p&gt;
&lt;p&gt;為了利用這種 structed information，作者提出了 Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (SQUID)，它可以把固有的人體結構分類為反覆出現的 pattern。&lt;/p&gt;
&lt;p&gt;在推理狀態下，它可以識別圖片中的異常情況。&lt;/p&gt;
&lt;p&gt;比較兩個 chest X-ray benchmark，SQUID 在非監督異常檢測上超越了 13 種 SOTA 方法至少 5 個百分點。&lt;/p&gt;
&lt;p&gt;作者還創建了一個新的資料集 (DigitAnatomy)，該資料集結合了胸腔解剖學中的 spatial correlation 和 consistent shape 這兩個特性。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;放射線成像和一般圖片的差別
&lt;ul&gt;
&lt;li&gt;一般的 photographic imaging 和 radiography imaging 是不同的。一般的圖片物體，我們會假設 translation invariance (平移不變性)，無論貓在左右，都是貓。但是在放射線成像中，結構的相對位置和方向是辨別正常和異常的重要特徵。&lt;/li&gt;
&lt;li&gt;而且由於 radiography imaging protocols 以相當一致的方向評估患者，成像在不同的設備製造商、設施位置還有患者的情況下，都具有很大的相似性。像這樣反覆出現且一致的結構，有助於分析問題，是放射線成像的優勢。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有多項研究證明了許多先驗知識在增強深度學習模型性能上的優勢，比如添加 location features、修改目標函數還有約束相對於照片中 landmarks 的相對座標。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;想解決的問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多達 80% 的臨床錯誤是由於放射科醫生漏掉異常而造成。&lt;/li&gt;
&lt;li&gt;本文想回答一個關鍵問題：有沒有辦法利用 anatomical patterns 的 consistency 和 spatial information，在沒有手動標註的情況下，加強深度學習模型的異常檢測能力？非監督的異常檢測只用健康的圖片進行訓練，不用疾病診斷或任何 label。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SQUID 解決辦法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;本文不像先前的異常檢測方法，本文把 task 制定為 in-painting task (圖像修復)，好利用放射線成像的外觀、位置、布局。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;作者提出了 SQUID，在訓練過程中，模型可以透過空間中經常出現的 anoatomical patterns 來動態維護一個 visual pattern dictionary。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由於解剖學的 consistency，健康成像中的身體區域會呈現類似的 visual pattern，使 unique pattern 的數量是可控的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在推理階段，由於 dictionary 不存在 anomaly pattern，因此如果存在異常，產生的放射線成像會和現實有所差距。因此，模型可以透過區分修復任務的品質來識別異常。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;實驗假設&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;異常檢測的成功基於兩個假設
&lt;ul&gt;
&lt;li&gt;資料中很少異常圖片&lt;/li&gt;
&lt;li&gt;異常和正常有顯著不同&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;實驗&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在兩個大規模、公開的放射線成像資料集上實驗
&lt;ul&gt;
&lt;li&gt;ZhangLab
&lt;ul&gt;
&lt;li&gt;在非監督方面贏 SOTA 超過 5 個百分點&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;比最近的 13 種方法提高 10 個百分點&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;新資料集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;創建了 DigitAnatomy 資料集，闡明胸腔解剖結構的 spatial correlation 和 consistent shape。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;貢獻總結&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在胸腔放射線成像的新非監督 SOTA 異常檢測方法&lt;/li&gt;
&lt;li&gt;新的綜合資料集&lt;/li&gt;
&lt;li&gt;發明新方法打敗主流非監督異常檢測方法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Anomaly detection in natural imaging
&lt;ul&gt;
&lt;li&gt;識別偏離正常資料分佈的罕見事件&lt;/li&gt;
&lt;li&gt;由於異常樣本的缺乏，後來的工作都制定為非監督學習問題&lt;/li&gt;
&lt;li&gt;大致分為兩類
&lt;ul&gt;
&lt;li&gt;reconstruction-based
&lt;ul&gt;
&lt;li&gt;恢復原始輸入並分析重建誤差&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;density-based
&lt;ul&gt;
&lt;li&gt;透過估計正常資料的分佈來預測異常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不過這些方法都沒辦法解釋可能的異常，本文透過維護 visual pattern memory 來解決這個問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anomaly detection in medical imaging
&lt;ul&gt;
&lt;li&gt;基於監督學習的方法多半用於檢測特定種類的異常，比如腫瘤&lt;/li&gt;
&lt;li&gt;最近提出了一些無監督方法來檢測一般異常，和 GAN 有關，但是這些方法需要有關於異常種類的強大先驗知識和假設才能使增強有效&lt;/li&gt;
&lt;li&gt;和一般的照片不同，Radiography imaging protocols 生成具一致性的圖片，異常的變化比較微妙 (subtle)，檢測起來更具挑戰，作者利用放射線成像的特性，大大提高檢測性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory networks
&lt;ul&gt;
&lt;li&gt;過往有一些有關於把 Memory modules 納入神經網路的研究，其中有採用到 Memory Matrix。本文克服了 Memory matrix 的侷限性，並提出一種有效且高效率的的 memory queue。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;squid&#34;&gt;SQUID&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Feature extraction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把圖片切成 N x N 個 non-overlapping patches，然後餵入一個 encoder 做特徵提取，這裡是用 CNN 提取，但要用其他 backbone 也可以&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image reconstruction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這裡會用 teacher 和 student generator
&lt;ul&gt;
&lt;li&gt;teacher
&lt;ul&gt;
&lt;li&gt;直接用 encoder 的 feature 重建圖片&lt;/li&gt;
&lt;li&gt;本質上是 auto-encoder&lt;/li&gt;
&lt;li&gt;作為 regularizer 來避免 student generator 重複生成相同的正常圖片&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;student
&lt;ul&gt;
&lt;li&gt;使用 in-painting block 增強後的 feature 來重建，最後會被用在 discrimination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;兩個 generator 會在每個 up-sampling level 用 knowledge distillation paradigm 來結合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anomaly discrimination&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 adversarial learning 後，使用 discriminator 來區分正常和異常&lt;/li&gt;
&lt;li&gt;用 2 個 generator 來生成圖片，再用 discriminator 來區分，只有 student generator 會接收 discriminator 的梯度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inventing-memory-queue-as-dictionary&#34;&gt;Inventing Memory Queue as Dictionary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Motivation
&lt;ul&gt;
&lt;li&gt;Memory Matrix 被廣泛採用
&lt;ul&gt;
&lt;li&gt;Feature 會透過在 Memory matrix 做加權平均來強化&lt;/li&gt;
&lt;li&gt;缺點
&lt;ul&gt;
&lt;li&gt;這樣的增強方法是對整張圖片的提出的特徵做的，丟棄了圖片中的 spatial information。導致他無法感知到放射線成像中的一致性結構&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Space-aware memory
&lt;ul&gt;
&lt;li&gt;為了利用空間資訊，作者只將 patch 而不是整張圖片傳遞到 model，讓 patch 只能存取 Memory matrix 中對應到的區段，作者把這種策略稱為 Space-aware memory，而且還可以加快速度，因為不用存取整個 Memory matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory queue
&lt;ul&gt;
&lt;li&gt;在 learning-based Memory matrix 中，normal patterns 是由 matrix 中的 learned basis 組合而成，但組合出來的東西和現實照片的特徵總會有分佈差距，使後續的影像生成變得困難&lt;/li&gt;
&lt;li&gt;作者提出 memory queue，用來在訓練期間儲存真實的影像 feature，從而呈現和影像特徵相同的分佈。它在訓練期間會把先前看到的特徵直接複製到 queue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gumbel shrinkage
&lt;ul&gt;
&lt;li&gt;控制 memory matrix 中 activated pattern 的數量是有利的，但如果用 hard shrinkage threashold 會無法處理找不到合適 entry 的情況。一種自然的解法是讓梯度流過前 k 個相似的 entry，其餘的不更新。但這樣又會導致未啟動的 entry 無法接收任何梯度並更新，因此提出了 Gumbel shrinkage schema
&lt;ul&gt;
&lt;li&gt;$w&amp;rsquo; = sg(hs(w,topk(w)) - \phi(w)) + \phi(w)$
&lt;ul&gt;
&lt;li&gt;$w$ 代表 feature 和 entry 的相似度&lt;/li&gt;
&lt;li&gt;$sg(\cdot)$ 代表 stop-gradient，不計算輸入的梯度&lt;/li&gt;
&lt;li&gt;$hs(\cdot, t)$ 代表 hard shrinkage，有個 threshold $t$&lt;/li&gt;
&lt;li&gt;$\phi(\cdot)$ 代表 softmax&lt;/li&gt;
&lt;li&gt;這樣保留了 top k 作為 w 的最終結果，又用 softmax 對所有 entry 進行更新&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;formulating-anomaly-detection-as-in-painting&#34;&gt;Formulating Anomaly Detection as In-painting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image in-painting 最初是用來恢復具有 neighboring context 的圖片區塊，因此根據此直覺，想把異常圖片修復成正常圖片來實現檢測&lt;/li&gt;
&lt;li&gt;在修復像素的時候，特別是用深度網路，容易有 boundary artifacts，在 pixel 級別的修復中，這些 boundary artifacts 會導致大量誤報
&lt;ul&gt;
&lt;li&gt;artifact 中翻好像是「偽影」，就是重建的時候會呈現有點像棋盤的效應&lt;/li&gt;
&lt;li&gt;作者選擇在 feature level 進行 in-painting，避開這問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In-painting block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;會先把每個 patch $F_{1,1}$ ~ $F_{w,h}$ 都先找到最接近的 normal patterns $N_{1,1}$ ~ $N_{w,h}$&lt;/li&gt;
&lt;li&gt;因為 N 是之前訓練資料中提取的特徵組成的，不受當前輸入影像的影響。為了導入輸入圖片的特徵，作者把 F 和 N 用 transformer block 來結合
&lt;ul&gt;
&lt;li&gt;對於每個 patch $F_{i,j}$，會把其當作中心，用相鄰的 8 個 N patch 來重新定義 $F_{i,j}$，把這 8 個 N patch 作為 key 和 value，$F_{i,j}$ 作為 query&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最後會在 in-painting block 的前後做 point-wise convolution (1x1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Masked shortcut&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;實驗結果表明，直接做 residual connection 會降低修復的性能，作者採用 random binary mask 在 training 期間 gate shortcut feature
&lt;ul&gt;
&lt;li&gt;$F&amp;rsquo;=(1-\delta)\cdot F + \delta \cdot inpaint(F)$
&lt;ul&gt;
&lt;li&gt;$\delta$~$Bernoulli(\rho)$
&lt;ul&gt;
&lt;li&gt;$\rho$ gating probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;獲得 F&amp;rsquo; 後，原始的 F 會被更新進 memory&lt;/li&gt;
&lt;li&gt;在推論階段，會 disable shortcut，使 $F&amp;rsquo;=inpaint(F)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anomaly-discrimination&#34;&gt;Anomaly Discrimination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discriminator 評估圖片現不現實，不現實表示異常&lt;/li&gt;
&lt;li&gt;因為 Generator 只在正常圖片訓練，所以 Memory Queue 也只有 normal pattern&lt;/li&gt;
&lt;li&gt;稍微總結
&lt;ul&gt;
&lt;li&gt;in-painting block 會把 patch 強化為相似的 normal feature&lt;/li&gt;
&lt;li&gt;student generator 會根據 &amp;ldquo;normal&amp;rdquo; feature 重建出 &amp;ldquo;normal&amp;rdquo; image&lt;/li&gt;
&lt;li&gt;如果沒有異常的話，那 input 和重建的 image 在語意上應該相差很小&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;異常分數 $A$ 的算法
&lt;ul&gt;
&lt;li&gt;$A=\phi(\frac{D(G_s(E(I)))-\mu}{\sigma})$
&lt;ul&gt;
&lt;li&gt;$\phi(\cdot)$ 是 sigmoid function&lt;/li&gt;
&lt;li&gt;$\mu$ 和 $\sigma$ 是根據 training samples 算出的異常分數的平均值和標準差&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_t = (I-G_t (E(I)))^2$&lt;/li&gt;
&lt;li&gt;$\mathcal L_s = (I-G_s (E(I)))^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge distillation
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dist} = \sum_{l}^{i=1} (F^i_t-F^i_s)^2$
&lt;ul&gt;
&lt;li&gt;$l$ 是 levels of features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial loss
&lt;ul&gt;
&lt;li&gt;類似 DCGAN&lt;/li&gt;
&lt;li&gt;$\mathcal L_{gen} = log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discriminator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dis} = log(D(I)) + log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;li&gt;把 real image 機率拉高，把 fake image 機率拉低&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Total loss
&lt;ul&gt;
&lt;li&gt;minimize generative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_t \mathcal L_t + \lambda_s \mathcal L_s + \lambda_{dist} \mathcal L_{dist} + \lambda_{gen} \mathcal L_{gen}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;maximize discriminative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_{dis} \mathcal L_{dis}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;new-benchmark&#34;&gt;New Benchmark&lt;/h3&gt;
&lt;p&gt;提出一個新資料集 - DigitAnatomy。。如果包含正確順序的阿拉伯數字 1~9 則視為正常，異常包括缺失、亂序、翻轉和 zero digit。&lt;/p&gt;
&lt;p&gt;該資料集對於放射線成像尤其有利，原因如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial correlation and consistent shape&lt;/li&gt;
&lt;li&gt;放射線成像要標記需要專業知識，但數字容易 debug&lt;/li&gt;
&lt;li&gt;該資料集很容易就可以獲得模擬異常的 ground truth&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;public-benchmarks&#34;&gt;Public Benchmarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ZhangLab Chest X-ray
&lt;ul&gt;
&lt;li&gt;包含健康和肺炎的影像&lt;/li&gt;
&lt;li&gt;訓練集
&lt;ul&gt;
&lt;li&gt;1349 張正常&lt;/li&gt;
&lt;li&gt;3883 張異常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;測試集
&lt;ul&gt;
&lt;li&gt;234 張正常&lt;/li&gt;
&lt;li&gt;390 張異常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;作者從訓練集隨機挑 200 張做為調整超參數的 validation set&lt;/li&gt;
&lt;li&gt;影像都調整為 128x128&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;對 front-view PA 影像進行評估，共有 12 種異常&lt;/li&gt;
&lt;li&gt;有 5249 張正常和 23671 張異常用作訓練
&lt;ul&gt;
&lt;li&gt;使用和 ZhangLab 相同的超參數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;用訓練集的 250 張正常和 250 張異常進行測試&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baselines-and-metrics&#34;&gt;Baselines and Metrics&lt;/h3&gt;
&lt;p&gt;考慮 13 個主要的 baseline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;經典 UAD (unsupervised anomaly detection)
&lt;ul&gt;
&lt;li&gt;Auto-encoder、VAE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;醫學影像的 SOTA
&lt;ul&gt;
&lt;li&gt;Ganomaly、f-AnoGAN、IF、SALAD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最近的 UAD
&lt;ul&gt;
&lt;li&gt;MemAE、CutPaste、M-KD、PANDA、PaDiM、IGD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除非有特別註明，不然都是從頭獨立訓練至少三次&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;interpreting-squid-on-digitanatomy&#34;&gt;Interpreting SQUID on DigitAnatomy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者在 DigitAnatomy 的實驗中，故意注入異常到正常圖片中，測試模型是否可以重建正常圖片。&lt;/p&gt;
&lt;p&gt;SQUID 重建出的圖片比其他 baseline 有更多有意義的訊息，主要歸功於 space-aware memory，其產生獨特的 pattern，而且和空間訊息相關聯。&lt;/p&gt;
&lt;p&gt;一旦出現異常，in-painting block 會從字典中找出前 k 個相近的，把異常特徵增強到其對應的正常特徵，其他方法不具備此能力，所以他們重建出有缺陷的圖像。&lt;/p&gt;
&lt;p&gt;GAN 傾向於重建訓練樣本平均得到的影像。
MemAE 受益於 Memory matrix，表現較好，但對於缺失數字的異常效果不佳。&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-squid-on-chest-radiography&#34;&gt;Benchmarking SQUID on Chest Radiography&lt;/h3&gt;
&lt;h4 id=&#34;limitation&#34;&gt;Limitation&lt;/h4&gt;
&lt;p&gt;作者發現目前的 SQUID 沒辦法在像素層級精確定位異常。這可以理解，因為 SQUID 是一種非監督方法，不需要標註。&lt;/p&gt;
&lt;p&gt;那些像素級別的異常檢測會遭遇放大雜訊的影響，但是由於 SQUID 是在特徵層級進行的，比像素級別更加 robust。&lt;/p&gt;
&lt;h3 id=&#34;ablating-key-properties-in-squid&#34;&gt;Ablating Key Properties in SQUID&lt;/h3&gt;
&lt;h4 id=&#34;component-study&#34;&gt;Component study&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;hyper-parameter-robustness&#34;&gt;Hyper-parameter robustness&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig9.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;disease-free-training-requirement&#34;&gt;Disease-free training requirement?&lt;/h4&gt;
&lt;p&gt;用於醫學異常檢測的非監督方法並不常見，因為所謂的 UAD 方法並不是「非監督」的，因為他們必須只在無疾病影像上作訓練。&lt;/p&gt;
&lt;p&gt;在實踐中，要獲得健康圖片需要 manual annotation。&lt;/p&gt;
&lt;p&gt;在訓練集中考慮 disease-free 從 100% - 50% 的情況，把 SQUID 的 robust 和另外三個 baseline 進行比較。&lt;/p&gt;
&lt;p&gt;SQUID 的 memory queue 可以自動忽略少數的 anatomical patterns。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>CLIP 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Tue, 21 Nov 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.00020&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;現有的 SOTA CV system 可以經過訓練預測一組固定的類別。
但這種監督式的方法也受限了通用性，因為需要額外的 labeled data 來擴展。&lt;/p&gt;
&lt;p&gt;直接從 raw text 學習 image 是個有前途的替代方案。&lt;/p&gt;
&lt;p&gt;本文證明了「預測哪個是圖片的 caption」這種形式的預訓練是一種高效且可擴展的方法，可以從 internet 上蒐集的 4 億對資料從頭學習到 SOTA image representation。&lt;/p&gt;
&lt;p&gt;預訓練後，透過自然語言來引導，就可以在下游任務十線 zero-shot。&lt;/p&gt;
&lt;p&gt;本文對 30 個不同的現有電腦視覺資料集進行比較，可以在多數任務和監督式學習的 baseline 競爭，而且無須任資料集來做特別的訓練。&lt;/p&gt;
&lt;p&gt;例如在 ImageNet 上做 zero-shot 可以和 ResNet-50 取得相近的準確度。&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-motivating-work&#34;&gt;Introduction and Motivating Work&lt;/h2&gt;
&lt;p&gt;直接從原始文本學習的預訓練方法在過去幾年徹底改變了 NLP。&lt;/p&gt;
&lt;p&gt;Task-agnostic (與下游任務無關) objectives，比如 autoregressive 和 masked language modeling，讓模型得以隨著 compute, model capacity, 和 data 規模的增長，使能力也逐步提升。&lt;/p&gt;
&lt;p&gt;在 &amp;ldquo;text-to-text&amp;rdquo; 這種輸入輸出形式的預訓練，使模型轉移到下游任務的時候，不用特地客製化 output head，或對資料集做特別地處理。&lt;/p&gt;
&lt;p&gt;這些結果表明，現代的預訓練方法在 web-scale 的文字集合的表現已經超過了用高品質的人為標記 NLP 資料集。&lt;/p&gt;
&lt;p&gt;然而在 CV 等領域，在 ImageNet 這種人為標記的資料集上做預訓練卻依然是標準做法。&lt;/p&gt;
&lt;p&gt;直接從網路文本學習的可擴展預訓練方法或許能在 CV 帶來類似的突破。&lt;/p&gt;
&lt;p&gt;以往有一些工作嘗試利用幾乎無限量的原始文本而不是有限數量的 &amp;ldquo;gold-labels&amp;rdquo;，
但是這些方法都有一些妥協，比如都利用 softmax 來執行預測，使其沒辦法應付新類別，嚴重限制了 zero-shot 的能力。&lt;/p&gt;
&lt;p&gt;作者提了幾個弱監督學習的例子，他們利用額外的資料結合預訓練，來幫忙改善監督式學習的結果。&lt;/p&gt;
&lt;p&gt;也提了幾個和 CLIP 類似的工作 VirTex, ICMLM, ConVIRT，想利用 Transformer，從 Natural Language 中學習 image representation。&lt;/p&gt;
&lt;p&gt;這些 weakly supervised model 和最近從 NLP 學習 image representation 的方法有一個重大差異，規模。&lt;/p&gt;
&lt;p&gt;最近的一些研究，比如一些弱監督學習在數百萬到數十億張照片上訓練了多個 accelerator years。但是和 CLIP 相似的研究只在二十萬張圖片上訓練了幾天。&lt;/p&gt;
&lt;p&gt;本文將規模拉高，以縮短規模上的差距。&lt;/p&gt;
&lt;p&gt;作者在 internet 上蒐集了 4 億對圖片和文字的資料，做成新的資料集，並提出了 CLIP，ConVIRT 的簡化版本。&lt;/p&gt;
&lt;p&gt;作者在 30 幾個資料集上測試，基本上能和監督式的模型競爭。&lt;/p&gt;
&lt;p&gt;如果用 linear-probe，比公開可用的 SOTA ImageNet model 還更好。&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/ex1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;natural-language-supervision&#34;&gt;Natural Language Supervision&lt;/h3&gt;
&lt;p&gt;核心想法是利用 natural language 來學習 perception。&lt;/p&gt;
&lt;p&gt;作者稱這不是一個新想法，但以往相似的方法的用語多樣，他介紹了四篇文章，但把從文字和圖片中學習 image representation 的方法個別稱為：無監督、自監督、弱監督、監督式。&lt;/p&gt;
&lt;p&gt;擴展 natural language supervision 比起圖像分類簡單的多，不必定好類別，再去標註每張照片的類別。&lt;/p&gt;
&lt;p&gt;而且 natural language supervision 還有個優勢，他不只能學習 image representation，還能將其和文字相關聯，使其更好做 zero-shot 的遷移。&lt;/p&gt;
&lt;h3 id=&#34;creating-a-sufficiently-large-dataset&#34;&gt;Creating a Sufficiently Large Dataset&lt;/h3&gt;
&lt;p&gt;現有工作主要用三個資料集:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MS-COCO&lt;/li&gt;
&lt;li&gt;Visual Genome&lt;/li&gt;
&lt;li&gt;YFCC100M&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MS-COCO 和 Visual Genome 都是高品質的人為標記資料集，但是按照現代標準來看，它們很小，每個資料集大約有 100,000 張訓練照片。&lt;/p&gt;
&lt;p&gt;相較之下，作者舉了一個最近的研究，用了 3.5 Billion 張 Instagram 照片作為訓練資料。&lt;/p&gt;
&lt;p&gt;YFCC100M 是一個可能的替代方案，它有 100 million 張照片，但每張照片的 metadata 資料稀疏，而且良莠不齊。&lt;/p&gt;
&lt;p&gt;比如許多檔名是自動產生的，可能是時間，或是相機的參數。&lt;/p&gt;
&lt;p&gt;經過過濾，保留帶有自然語言的標題或描述的圖像，資料集縮小了 6 倍，只剩 15000 萬張照片，和 ImageNet 的大小相當。&lt;/p&gt;
&lt;p&gt;natural language supervision 的一個主要動機是網路上公開著大量這種形式的 data。
由於現有資料集沒有反映這種可能性，因此只考慮這些資料集會低估這方面研究的潛力。&lt;/p&gt;
&lt;p&gt;所以作者建立了一個新的包含 400 million pairs 的資料集，從網路上各種公開的來源蒐集的。&lt;/p&gt;
&lt;p&gt;為了盡可能涵蓋所有的 visual concepts，作者在建構資料集的時候準備了 50 萬組特定的 query，每組 query 最多包含 20,000 個 pair，來進行 class balance。&lt;/p&gt;
&lt;p&gt;產生的資料集的總字數和 GPT-2 用的 WebText 差不多。&lt;/p&gt;
&lt;p&gt;將此資料集稱為 WIT，全名是 WebImageText。&lt;/p&gt;
&lt;h3 id=&#34;selecting-an-efficient-pre-training-method&#34;&gt;Selecting an Efficient Pre-Training Method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;最先進的 CV System 需要大量的計算。&lt;/p&gt;
&lt;p&gt;作者舉了兩個計算量都非常恐怖的模型，而且他們只能預測 1000 個 ImageNet 的類別。
其中一個花了 19 個 GPU years，另一個花了 33 個 TPUv3 core-years。
乍看之下，從自然語言中學習一組開放的視覺概念似乎令人生畏。&lt;/p&gt;
&lt;p&gt;但在作者努力的過程中，他們發現訓練效率是成功擴展自然語言監督的關鍵，也根據該指標選定最終的預訓練方法。&lt;/p&gt;
&lt;p&gt;最初的方法和 VirTex 相似，從頭開始訓練一個 CNN，和 text transformer 來預測 caption。&lt;/p&gt;
&lt;p&gt;Fig.2 展示的 Transformer 語言模型的計算量是 ResNet-50 Image encoder 的兩倍。
預測 caption 比預測 caption 但採用詞袋的方式還慢三倍。&lt;/p&gt;
&lt;p&gt;這樣預測 caption 是一個困難的任務，同一張照片對應的 caption 可能出現的描述甚至有非常多種。
最近在 Contrastive representation learning 方面的研究發現 contrastive objectives 有不錯的表現。&lt;/p&gt;
&lt;p&gt;因此作者探索一種方法是，只預測文本和哪一個圖片配對，而不是預測確切的單字。&lt;/p&gt;
&lt;p&gt;因為資料集超級大，overfitting 的問題影響不大。&lt;/p&gt;
&lt;p&gt;此外，作者發現對於 encoder 的 representation，要轉換到 multi-model embedding space，只需要使用 linear projection 即可，不需要 non-linear，兩者之間差別不大。&lt;/p&gt;
&lt;p&gt;Data augmentation 只有使用 random crop，而沒有使用其他的。&lt;/p&gt;
&lt;h3 id=&#34;choosing-and-scaling-a-model&#34;&gt;Choosing and Scaling a Model&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# image_encoder - ResNet or Vision Transformer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# text_encoder - CBOW or Text Transformer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# I[n, h, w, c] - minibatch of aligned images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# T[n, l] - minibatch of aligned texts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# W_i[d_i, d_e] - learned proj of image to embed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# W_t[d_t, d_e] - learned proj of text to embed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# t - learned temperature parameter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# extract feature representations of each modality&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;I_f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image_encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#[n, d_i]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;T_f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text_encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#[n, d_t]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# joint multimodal embedding [n, d_e]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;I_e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2_normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;T_e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2_normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# scaled pairwise cosine similarities [n, n]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I_e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T_e&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# symmetric loss function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;prompt-engineering-and-ensembling&#34;&gt;Prompt Engineering and Ensembling&lt;/h3&gt;
&lt;p&gt;一種常見的問題是 polysemy，一個單字可能有多種意思，比如 &amp;ldquo;boxer&amp;rdquo; 可能是一種狗，或是拳擊手。
如果一張圖片對應一個單字就會面臨這問題。&lt;/p&gt;
&lt;p&gt;另一種是 distribution gap，比如訓練用句子，但測試用單字。
為了緩解這問題，作者發現用 prompt template &amp;ldquo;A photo of a {label}.&amp;rdquo; 比直接用 label 好。&lt;/p&gt;
&lt;p&gt;光用這個 prompt template 就提高 1.3 % 在 ImageNet 上的準確度。&lt;/p&gt;
&lt;p&gt;如果可以給其他額外訊息會更有幫助，比如對於寵物的資料集，可以用 &amp;ldquo;A photo of a {label}, a type of pet.&amp;quot;。&lt;/p&gt;
&lt;p&gt;對於 OCR 資料集，作者發現在要識別的文字或數字前後加上引號可以提高效能。&lt;/p&gt;
&lt;p&gt;再來是 prompt ensembling，作者發現用多個 prompt template 來預測，然後綜合結果，可以提高效能。
作者用了 80 個 template。在 ImageNet 上比用單一的 prompt template 提高 3.5 % 的 performance。&lt;/p&gt;
&lt;p&gt;綜合考慮 prompt engineering 和 prompt ensembling，作者在 ImageNet 上的準確度提高大概 5%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這裡列幾個作者用的 prompt template:
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;a bad photo of a {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a photo of many {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a sculpture of a {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a photo of the hard to see {}.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analysis-of-zero-shot-clip-performance&#34;&gt;Analysis of Zero-Shot CLIP Performance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;對於一般的物體分類的資料集，CLIP 表現較好。&lt;/p&gt;
&lt;p&gt;下面有些複雜、專門、抽象的任務，CLIP 則表現的很差，比如計算場景中有多少物體的 （CLEVRCounts）、衛星圖像分類（EuroSAT）或是 識別最近的汽車距離（KITTI Distance）&lt;/p&gt;
&lt;p&gt;對於這種特別難的任務，讓 CLIP 做 zero-shot 不太合理。
可能用 few-shot 的方式會比較好。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;BiT 是 google 為 Transfer Learning 設計的預訓練模型，在分類問題，Few-shot learning 上有良好的表現。&lt;/p&gt;
&lt;h3 id=&#34;representation-learning&#34;&gt;Representation Learning&lt;/h3&gt;
&lt;p&gt;這節探討完全使用下游任務資料集而非 Zero-shot 或 few-shot 的情況。&lt;/p&gt;
&lt;p&gt;作者選用 linear-probe 而不是 finetune 來做下游任務的評估。&lt;/p&gt;
&lt;p&gt;因為他們的重點是開發與資料集無關的預訓練方法，finetune 有可能讓一個預訓練學習 representation 失敗的模型在微調過程中變好。
而 linear-probe 的限制可以凸顯這些失敗。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;comparison-to-human-performance&#34;&gt;Comparison to Human Performance&lt;/h2&gt;
&lt;p&gt;再來是 CLIP 和人類相比的結果。
挑選了五個人在寵物資料集上比較的結果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;data-overlap-analysis&#34;&gt;Data Overlap Analysis&lt;/h2&gt;
&lt;p&gt;可能會有人質疑，CLIP 的表現是因為訓練資料集和測試資料集有重疊。
但作者做了一些實驗，有些資料集完全沒有偵測到重疊。
對有重疊的做實驗，發現有重疊的對效果提升影響很小。&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;CLIP 雖然可以和作為 Baseline 的 ResNet-50 打平手，但現在的 SOTA 遠高於該 Baseline。&lt;/p&gt;
&lt;p&gt;作者發現再繼續加大模型和資料是可以繼續提升性能的，但作者估計要達到現有的 SOTA 需要增加大概 1000 倍的計算量才能達到，使用現有的硬體是不可行的。&lt;/p&gt;
&lt;p&gt;CLIP 對細分類、抽象或更難的任務表現不好，作者相信還有許多任務是 CLIP 用 zero-shot 只能達到亂猜等級的。&lt;/p&gt;
&lt;p&gt;Zero-Shot 的 CLIP 很難泛化到 out-of-distribution 的資料，比如在 MNIST 上只能達到 88% 的準確度。
作者發現預訓練資料幾乎沒有類似 MNIST 的圖片。&lt;/p&gt;
&lt;p&gt;盡管 CLIP 可以靈活應用各種 Zero-Shot 的分類，但基本上還是從你給定的分類選擇。
和真正靈活的方法（生成 image caption）相比，是重大的限制。&lt;/p&gt;
&lt;p&gt;一個值得嘗試的簡單想法是把 contrastive objective 和 generative objective，結合。&lt;/p&gt;
&lt;p&gt;CLIP 也沒有解決深度學習資料效率低下的問題，CLIP 訓練了 32 個 epoch，如果把預訓練期間的照片以一秒一張來呈現，需要 405 年。
把 CLIP 和 self-supervision 或者和 self-training 做結合是有前途的方向。&lt;/p&gt;
&lt;p&gt;雖然作者強調 Zero-Shot Learning，但是作者還是有反覆檢查下游任務測試集的表現，來調整 CLIP。
每次都用 ImageNet 來確認，並不算真正的 zero-shot 的情況。
如果能再創一個新的資料集，專門用來評估 zero-shot 遷移的能力會更恰當。&lt;/p&gt;
&lt;p&gt;爬下來的資料有可能帶有社會偏見。&lt;/p&gt;
&lt;p&gt;有一些複雜的任務很難用文字來傳達，雖然實際的訓練樣本有用，但 CLIP 並不會針對 few-shot 最佳化。有個違反直覺的結果，可以注意到在某些情況下，few-shot 不見得比 zero-shot 好。&lt;/p&gt;
&lt;h2 id=&#34;額外應用&#34;&gt;額外應用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;圖片生成
&lt;ul&gt;
&lt;li&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery
&lt;ul&gt;
&lt;li&gt;用文字引導生成圖片&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;物件偵測
&lt;ul&gt;
&lt;li&gt;Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation
&lt;ul&gt;
&lt;li&gt;將基礎類別再做細分類&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OCR
&lt;ul&gt;
&lt;li&gt;Contrastive Language-Image Forensic Search
&lt;ul&gt;
&lt;li&gt;搜索影片中有沒有文本描述的物體&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;筆記&#34;&gt;筆記&lt;/h2&gt;
&lt;p&gt;prompt engineering
prompt ensemble&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion 入門</title>
        <link>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</link>
        <pubDate>Mon, 23 Oct 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</guid>
        <description>&lt;h2 id=&#34;大致概念&#34;&gt;大致概念&lt;/h2&gt;
&lt;p&gt;屬於生成式 AI，一開始用在生成圖片，後來也有應用到諸如 NLP 等領域。&lt;/p&gt;
&lt;p&gt;下文稱呼原圖為 sprite。&lt;/p&gt;
&lt;p&gt;與 AutoEncoder 有點類似，先取得一張 sprite，隨著時間推進，每次都在圖片上加一層雜訊，反覆疊加，迭代多次後，就會得到一張難以看出原圖的雜訊。&lt;/p&gt;
&lt;p&gt;從 sprite 到只能看出是一團雜訊並非是一步到位的過程。一開始沒有雜訊時可以看出原本的 sprite，一個迭代後可能可以勉強看出原本的 sprite，再幾個迭代後可能也還能看出原本的 outline，經過許多次後才會變成完全辨識不了的雜訊。&lt;/p&gt;
&lt;p&gt;我們期望模型做的事情則是從 gaussian noise 逐步推回 sprite，同樣不是一步到位，而是讓模型預測上一個時間點的雜訊，相減後再逐步推回 sprite，這過程稱為 denoise。&lt;/p&gt;
&lt;h2 id=&#34;ddpm&#34;&gt;DDPM&lt;/h2&gt;
&lt;p&gt;實現 Diffusion 可能會有點 confusing，因為他實作上和上面說的不太相同。
在訓練的時候，我們會採樣三個東西：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;訓練圖片 (sprite)&lt;/li&gt;
&lt;li&gt;雜訊&lt;/li&gt;
&lt;li&gt;時間點 (t)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;訓練階段的時候，我們會把「原始乾淨的圖片」和「雜訊」根據時間進行不同比例的相加 (混合)，t 越大，雜訊的比例越大。&lt;/p&gt;
&lt;p&gt;模型預測的目標是前面 sample 出的雜訊。&lt;/p&gt;
&lt;p&gt;這與前面說的概念相悖。按照前面的說法，對於時間點 t，應該是以一張加了 t-1 次雜訊的 sprite 作為輸入，再加上 t 所 sample 出的雜訊。&lt;/p&gt;
&lt;p&gt;現在實作卻是原始乾淨的 sprite 直接根據時間點混和某個雜訊。&lt;/p&gt;
&lt;p&gt;這背後的數學推導十分冗長，這裡不敘述，但需知道實作差異。&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;在推論階段的時候，每次 denoise 後需要把圖片和額外 sample 的 noise 相加。這個 noise 和前面的 noise 一樣，都是從 mean=0, std=1 的 gaussian distribution 中 sample 出來的。&lt;/p&gt;
&lt;p&gt;不加的話似乎還容易有 Mode Collapse 的現象。
看到一個說法是，模型喜歡吃圖片加上雜訊的圖像作為圖片，在圖片上加上 noise 似乎會更符合模型預期的輸入。&lt;/p&gt;
&lt;p&gt;看了李弘毅的影片，也有基於隨機性的觀點。&lt;/p&gt;
&lt;p&gt;生成式 Model 生成文章時永遠取機率最大的，不見得有更好的效果：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;有研究是讓 Model 選機率最大的，結果容易生出反覆跳針的文章。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;也有把人類寫的文章去餵給 Model 看，從他的角度看人類寫的下一個字的機率是多少，發現人類寫的文章很常出現一下機率高一下機率低的字。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;某篇語音合成的文章需要在推論階段「啟用」dropout 才可以有好的結果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Diffusion 也有可能成功的點是在於並非「一次到位」而是「N 次到位」。
從這樣的角度看，Diffusion 是 autoregressive 模型。&lt;/p&gt;
&lt;p&gt;類似的作法也有 Mask-Predict，大致概念是從原本都是 Mask 的情境開始，將一些信心高的預測留住，信心低的保持為 Mask，一步步預測出所有資訊。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DETR 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 10 Aug 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.12872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;作者把 object detection 視作一個 set prediction 問題。&lt;/p&gt;
&lt;p&gt;簡化了 pipeline，消除了許多 hand-designed components，比如 non-maximum suppression 和 anchor generation，這些 component 由我們對於任務的先驗知識構成。&lt;/p&gt;
&lt;p&gt;提出了一個新的目標函數，透過二分匹配（bipartite matching）進行預測，也用 Transformer encoder-decoder 架構。&lt;/p&gt;
&lt;p&gt;給予一組固定的 learned object query，DETR 可以推理 objects 和 globol image context 的關係，並「並行」輸出一組預測集。&lt;/p&gt;
&lt;p&gt;DETR 概念非常簡單。&lt;/p&gt;
&lt;p&gt;DETR 在 COCO 上和 Faster RCNN baseline 在準確度和 performance 上相當。&lt;/p&gt;
&lt;p&gt;DETR 可以很簡單地推廣到 Panoptic Segmentation。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;目標檢測的目標就是集合預測。&lt;/p&gt;
&lt;p&gt;但目前都用一些很間接的方式去做，像是用 proposals, anchors 或 window centers。&lt;/p&gt;
&lt;p&gt;但是這些方法性能明顯受限於後處理步驟，比如 non-maximum suppression，因為他們會產生大量冗餘的框。&lt;/p&gt;
&lt;p&gt;為了簡化 pipeline，作者提出了一種 End-to-End 的方法，以往也有一些嘗試，但他們要不添加了其他的先驗知識，不然就是在具有挑戰性的 benchmark 上表現不好。&lt;/p&gt;
&lt;p&gt;在 COCO 上和 Faster R-CNN 的性能相當，表現和速度都差不多。&lt;/p&gt;
&lt;p&gt;DETR 在大物體表現很好，可能是歸功於 Transformer non-local 的計算能力。
雖然 DETR 在小物體上表現倒不怎麼樣。&lt;/p&gt;
&lt;p&gt;DETR 需要超長的訓練時間，但 DETR 的設計理念可以拓展到 Panoptic Segmentation。&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;set-prediction&#34;&gt;Set Prediction&lt;/h3&gt;
&lt;p&gt;沒有規範的深度學習模型可以直接預測集合。&lt;/p&gt;
&lt;p&gt;這些任務中的一個困難點是避免 near-dulicates（相近的重複檢測框） 當前多數檢測器用 NMS 來解決此問題，如果是 direct set prediction 就不用後處理。&lt;/p&gt;
&lt;h3 id=&#34;transformers-and-parallel-decoding&#34;&gt;Transformers and Parallel Decoding&lt;/h3&gt;
&lt;p&gt;Transformer 在各種地方表現出色，但推理成本令人望而生畏。&lt;/p&gt;
&lt;h3 id=&#34;object-detection&#34;&gt;Object detection&lt;/h3&gt;
&lt;p&gt;現在多數的目標檢測方法是基於一些初始的猜測，再去做預測。&lt;/p&gt;
&lt;p&gt;比如對於 two-stage 的方法，就是對於 proposals 往下做預測。&lt;/p&gt;
&lt;p&gt;對於 single-stage，初始猜測就是 anchors。&lt;/p&gt;
&lt;h4 id=&#34;set-based-loss&#34;&gt;Set-based loss&lt;/h4&gt;
&lt;p&gt;以前的一些作法比如 Learnable NMS 或 relation networks 都可以透過 attention 來處理不同預測之間的關係。&lt;/p&gt;
&lt;p&gt;用 direct set losses，他們不需要任何後處理。&lt;/p&gt;
&lt;p&gt;但是這些方法往往用額外的 hand-crafted context feature，比如 proposal box coordinates。作者尋找減少模型中先驗知識的方案。&lt;/p&gt;
&lt;h4 id=&#34;recurrent-detectors&#34;&gt;Recurrent detectors&lt;/h4&gt;
&lt;p&gt;以往有類似的工作，但他們是用 RNN。&lt;/p&gt;
&lt;h2 id=&#34;the-detr-model&#34;&gt;The DETR model&lt;/h2&gt;
&lt;h4 id=&#34;object-detection-set-prediction-loss&#34;&gt;Object detection set prediction loss&lt;/h4&gt;
&lt;p&gt;DETE 會給 N 個固定大小的集合預測。&lt;/p&gt;
&lt;p&gt;要解二分圖匹配，本文用 scipy 的 linear_sum_assignment 處理，他背後是匈牙利演算法。&lt;/p&gt;
&lt;p&gt;其實這種方法和 proposals 和 anchors 有差不多的作用，差別在於這裡會找一對一的匹配，而不用重複。&lt;/p&gt;
&lt;p&gt;目標函數：&lt;/p&gt;
&lt;p&gt;$L_{Hungarian}(y, \text{\^{y}}) = \displaystyle\sum^{N}_{i=1} [-log \text{\^{p}} $
$_{\^{\sigma}(i)}(c_i) + \text{1}$
$_\{$
$_{c_i \neq \text{\o}}$
$_\}$
$\mathcal{L}$
$_{\text{box}} (b_i, \text{\^{b}}$
$_{\^{\sigma}}(i))]$&lt;/p&gt;
&lt;p&gt;前面是分類的 loss，後面是 bounding box 的 loss。&lt;/p&gt;
&lt;p&gt;這邊有兩個改動，第一個是分類那邊不用 log，使值和 bounding box 的 loss 比較接近。&lt;/p&gt;
&lt;p&gt;另一個是 bounding box 那邊並不是用最常見的 L1，因為 L1 對於大的目標 loss 比較高，這裡除了 L1 還選用 generalized IoU loss，它在尺度上與 loss 無關。&lt;/p&gt;
&lt;h4 id=&#34;detr-architecture&#34;&gt;DETR architecture&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;用 CNN 從圖片抽特徵，拉直，餵給 Transformer encoder-decoder，得到一組預測集合。&lt;/p&gt;
&lt;p&gt;這裡 encoder 有助於特徵間彼此交互。&lt;/p&gt;
&lt;p&gt;訓練的時候，預測的框和 GT 做匹配，沒匹配到的就放到 &amp;ldquo;no object&amp;rdquo; class。&lt;/p&gt;
&lt;p&gt;decoder 會餵入 object queries，這些是 learnable positional encodings。&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;h4 id=&#34;number-of-encoder-layers&#34;&gt;Number of encoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者透過改變 Encoder layer 的數量來評估 global imagelevel self-attention 的重要性。&lt;/p&gt;
&lt;p&gt;作者推論 encoder 可能對於判斷分開對象很重要，圖 3 可視化了最後一個 encoder layer 的 attention map。&lt;/p&gt;
&lt;p&gt;encoder 看似已經分離了 instance，可能簡化了 decoder 對於 object extraction 和 localization 的工作。&lt;/p&gt;
&lt;h4 id=&#34;number-of-decoder-layers&#34;&gt;Number of decoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在圖 6 做了 decoder 的注意力可視化，可以注意到觀察的注意力相當局部。&lt;/p&gt;
&lt;p&gt;推論是 encoder 主要分離實體，decoder 只需要關注四肢即可提取出對象的邊界和分類。&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig7.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;圖 7 把 100 個預測槽中的 20 個做可視化。&lt;/p&gt;
&lt;p&gt;每個預測框代表一點，可以注意到不同的槽位會專注在不同區域。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>I3D 論文</title>
        <link>https://roykesydon.github.io/Blog/p/i3d-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 23 Jul 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/i3d-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1705.07750&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;目前的動作分類資料集 (UCF-101 和 HMDB-51) 的影片非常缺乏，使辨識「良好的影像架構」變得困難，
使多數方法在現有的小規模 benchmark 的表現差不多。為此本文根據新的 Kinetics Human Action Video dataset 對 SOTA 架構進行了重新評估。&lt;/p&gt;
&lt;p&gt;Kinetics 有 400 個人類動作類別。每個類別有 400 個 clip。從 YouTube 上獲取的，而且每個 clip 來自 unique 的 youtube 影片。&lt;/p&gt;
&lt;p&gt;本文分析了當前架構在 Kinetics 上動作分類任務的表現，也評估 Kinetcis 用作預訓練的效果。&lt;/p&gt;
&lt;p&gt;本文提出了一種基於 2D ConvNet inflation 的 Two-Stream Inflated 3D ConvNet (I3D)。&lt;/p&gt;
&lt;p&gt;I3D 的擴展方法讓 ImageNet 上已經取得成功的架構可以被利用在解決影像任務上。&lt;/p&gt;
&lt;p&gt;結果表明，經過在 Kinetics 上預訓練後，I3D 在動作分類方面顯著提高了 SOTA，在 HMDB-51 上達到 80.9%，在 UCF-101 上達到 98.0%。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;在 ImageNet 上預訓練模型的效果很好，但在影片領域，預訓練成效一直是一個未知的問題。因為流行的動作識別 benchmark 都非常小，約略只有 10k 個影片。&lt;/p&gt;
&lt;p&gt;Kinetics 有 400 個人類動作類別。每個類別有 400 個 clip，而且每個 clip 都來自一個 unique 的 Youtube 影片。&lt;/p&gt;
&lt;p&gt;本文實驗策略是在 Kinetics 上預訓練，再在 HMDB-51 和 USC-101 上微調，結果顯示出預訓練總是能提高性能，但提升多寡因架構而異。&lt;/p&gt;
&lt;p&gt;本文提出新架構，稱為「Two-Stream Inflated 3D ConvNets」(I3D)，建立在 SOTA 的影像分類架構上，並將 filters 和 pooling kernel 膨脹成 3D。&lt;/p&gt;
&lt;p&gt;基於 Inceptionv1 的 I3D 在 Knetics 上預訓練後，性能遠超過當前的 SOTA 架構。&lt;/p&gt;
&lt;p&gt;在本文的模型中，並沒有考慮更多經典方法，比如 bag-of-visual-words representation，但 Kinetics 是公開的，因此其他人可以進行後續研究。&lt;/p&gt;
&lt;h2 id=&#34;action-classification-architectures&#34;&gt;Action Classification Architectures&lt;/h2&gt;
&lt;p&gt;目前影片架構中的一些主要區別如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;卷積是 2D 還 3D 的&lt;/li&gt;
&lt;li&gt;是否只是 RGB 影片，還是包含事先計算的 optical flow&lt;/li&gt;
&lt;li&gt;對於 2D ConvNets，訊息是怎麼在 frame 之間傳遞的
&lt;ul&gt;
&lt;li&gt;這部分可以使用 temporally-recurrent layers，比如 LSTM，或是用隨時間的 feature aggregation 來完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在本文中，考慮了涵蓋大部分現有架構的模型子集：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D ConvNets
&lt;ul&gt;
&lt;li&gt;頂部有 LSTM 的 ConvNet&lt;/li&gt;
&lt;li&gt;有兩種 stream fusion 的 two-stream networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3D ConvNets
&lt;ul&gt;
&lt;li&gt;C3D&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於參數維度較高，以及缺乏 labeled video data，以前的 3D ConvNet 相對較淺（最多 8 層）。&lt;/p&gt;
&lt;p&gt;本文發現諸如 VGG-16 和 ResNet 等很深的影像分類網路可以輕鬆擴展成 spatio-temporal feature extractors，並且他們的預訓練權重也可以提供有價值的初始化。&lt;/p&gt;
&lt;p&gt;本文也發現 two-stream 的作法依然有用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;
fig2. K 是影片中的 frame 的總數，N 是相鄰 frames 的子集合。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上圖是本文實驗的五種架構，前四種是之前的做法，最後一種是提出的新作法。
上圖中除了 C3D 外都有用到 ImageNet 預訓練的模型。&lt;/p&gt;
&lt;p&gt;時間是根據 input 的 frame 換算出來的，fps 是 25，除了 LSTM 那個比較特別，因為 LSTM 那個是每 5 frame 取 1 frame，所以時間是 5 倍。&lt;/p&gt;
&lt;h3 id=&#34;之前的做法&#34;&gt;之前的做法&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ConvNet+LSTM&lt;/p&gt;
&lt;p&gt;有一種做法是把每個 frame 獨立餵給 2D Conv，然後再把預測做彙整，符合 bag of words image modeling 的精神，但這樣會忽略時間結構上的資訊，比如無法判斷開門或關門。&lt;/p&gt;
&lt;p&gt;所以最好在後面加一個 recurrent layer，所以這邊就用 Inception-V1 結合 LSTM。&lt;/p&gt;
&lt;p&gt;原始的影片 stream 是 25 fps，這邊每 5 frame 採樣一次。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3D ConvNets&lt;/p&gt;
&lt;p&gt;和一般的卷積神經網路差不多，只是具有 spatio-temporal filters。&lt;/p&gt;
&lt;p&gt;但由於額外的 kernel 維度，相比 2D Conv 會有更多參數，也使他們更難訓練。&lt;/p&gt;
&lt;p&gt;而且這樣會無法發揮 ImageNet 預訓練的好處，因此之前的工作都定義了相對淺層的架構，並且從頭訓練。&lt;/p&gt;
&lt;p&gt;benchmark 中的表現備受期待，但和 SOTA 比沒有競爭力，也因此成為本文實驗的良好候選者。&lt;/p&gt;
&lt;p&gt;本文用的是 C3D 的小變體，差異在於所有卷積層和 FC 層的後面都用了 BN。
而且在第一個 pooling layer 用的 stride 是 2，好減少記憶體的使用，比用更大的 batch，這在 BN 中非常重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two-Stream Networks&lt;/p&gt;
&lt;p&gt;Roy：這裡由於比較複雜，我要改提 two-stream 的原始論文（&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2199&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;）說明這東西是什麼&lt;/p&gt;
&lt;p&gt;簡而言之就是分成兩個部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;空間資訊：&lt;/p&gt;
&lt;p&gt;用影片的一個 frame　經過卷積神經網路達成，這個 frame 用來提取影像中的物件資訊，比如打排球這動作可能辨識出排球就非常好判定，所以用某個 frame 來提取空間資訊。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動作資訊：&lt;/p&gt;
&lt;p&gt;這邊用一連串的光流（optical flow）圖來達成，光流是物體（pixel）在兩個 frame 間的位移向量，估計方法有很多，這裡不一一舉例。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/ex-fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上圖出自 &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2199&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;，圖 c 就是光流，具有兩個方向，指出像素的位移，圖 d 是水平方向的視覺化，圖 e 是垂直方向的視覺化。&lt;/p&gt;
&lt;p&gt;再把這些光流圖餵給卷積神經網路，用作動作資訊的判別。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;值得一提的是他是 late fusion，而且是用加權平均，不是像一般想的把特徵結合再做其他處理。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-new-two-stream-inflated-3d-convnets&#34;&gt;The New: Two-Stream Inflated 3D ConvNets&lt;/h3&gt;
&lt;p&gt;作者把成功的 2D 分類模型簡單地轉換為 3D&lt;/p&gt;
&lt;h4 id=&#34;inflating&#34;&gt;Inflating&lt;/h4&gt;
&lt;p&gt;做法是把方形的 filter 改成立方體，把 N x N 的 filter 改成 N x N x N 的 filter，但這只有架構上的參考。&lt;/p&gt;
&lt;h4 id=&#34;bootstraping&#34;&gt;Bootstraping&lt;/h4&gt;
&lt;p&gt;把權重也給轉換到 3D 架構的方法。&lt;/p&gt;
&lt;p&gt;作者觀察到影像可以透過反覆複製貼上來生出一個「不會動的無聊影片」，
透過這些影片，3D 模型可以透過這種方式在 ImageNet 上 implicitly pretrain，做法就是讓 3D filter 吃無聊影片的輸出和 2D filter 吃單一 frame 的輸出相同，做法如下：&lt;/p&gt;
&lt;p&gt;我們可以沿時間維度重複 2D filter N 次，把這權重給 3D filter，同時把權重除以 N，達到這種效果。&lt;/p&gt;
&lt;h4 id=&#34;pacing-receptive-field-growth-in-space-time-and-network-depth&#34;&gt;Pacing receptive field growth in space, time and network depth&lt;/h4&gt;
&lt;p&gt;以往在圖片上對水平和垂直軸的對待是平等的，pooling kernel 和 stride 都一樣。
使感受野在兩個維度上隨著模型越來越深，慢慢平等增長。&lt;/p&gt;
&lt;p&gt;但是時間軸用對稱的感受野不一定最好，而該取決於 frame rate 和 image dimensinos。
如果時間相對於空間增長太快，可能會混淆不同對象的邊緣，影響早期的特徵檢測。如果增長太慢，可能無法很好地捕捉場景動態。&lt;/p&gt;
&lt;p&gt;實驗中，輸入影片的 fps 是 25。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者發現在前兩個 max pooling layer 不在時間軸 pooling（透過用 1 x 3 x 3 的 kernel，並且時間軸的 stride 是 1），並在其他 max pooling layer 都用 symmetric kernels 和 stride 是有幫助的。&lt;/p&gt;
&lt;p&gt;最後的 average pooling layer 是用 2 x 7 x 7 的 kernel。&lt;/p&gt;
&lt;p&gt;作者用 64 frame 訓練，但用整個影片測試。（averaging predictions temporally）&lt;/p&gt;
&lt;p&gt;我想了一下，250 / 64 除不進，但是我看 code 發現他好像寬高 224 * 224 的照片會在最後經過 Average pool 後變成 1 * 1，所以他可以直接用 1 * 1 * 1 的卷積核把輸入通道改成分類數，再把時間軸的結果平均。&lt;/p&gt;
&lt;h4 id=&#34;two-3d-streams&#34;&gt;Two 3D Streams&lt;/h4&gt;
&lt;p&gt;分別訓練兩個網路，並在測試階段對預測進行平均。&lt;/p&gt;
&lt;p&gt;這邊作者說光流的演算法某種意義上是 recurrent（例如，對於 flow fields 進行 iterative optimization），我不太懂這邊是什麼意思，我想作者用的光流演算法應該是透過某種類似 EM 演算法那種不斷迭代去逼近數值的演算法，但作者提到「或許是因為缺乏 recurrence，我們發現雙流有價值」，我不太懂為什麼需要 recurrence 效果才會好。&lt;/p&gt;
&lt;p&gt;但結論是 two-stream 依然具備價值。&lt;/p&gt;
&lt;h4 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h4&gt;
&lt;p&gt;這邊講滿詳細的，有興趣可以去原文看。
只提一下幾點:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;光流演算法是用 TV-L1。&lt;/li&gt;
&lt;li&gt;除了類似 C3D 的 3D ConvNet 都用使用 ImageNet 預訓練的 Inception-V1 作為 base network。&lt;/li&gt;
&lt;li&gt;對於較短的影片，會重複循環以滿足模型的輸入介面&lt;/li&gt;
&lt;li&gt;測試時會在中間剪裁 224 x 224&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-kinetics-human-action-video-dataset&#34;&gt;The Kinetics Human Action Video Dataset&lt;/h2&gt;
&lt;p&gt;Kinetics 有 400 個人類動作類別。每個類別有 400 個 clip，而且每個 clip 都來自一個 unique 的 Youtube 影片，共有 24 萬個訓練影片。&lt;/p&gt;
&lt;p&gt;每個 clip 都大約 10 秒，而且沒有未剪的影片。&lt;/p&gt;
&lt;p&gt;測試集每個 class 包含 100 個 clip。&lt;/p&gt;
&lt;h2 id=&#34;experimental-comparison-of-architectures&#34;&gt;Experimental Comparison of Architectures&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table2and3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;I3D 在所有資料集上都表現最好，甚至是在 UCF-101 和 HMDB-51 這種小資料集上也是如此，這意味著 ImageNet 預訓練的好處有成功擴展到 3D ConvNet。&lt;/p&gt;
&lt;p&gt;多數模型在 UCF 上都表現得比 Kinetics 上好，顯現出資料集的難度差距。&lt;/p&gt;
&lt;p&gt;但是在 HMDB 表現得較差，原因可能是 HMDB 故意弄得很難，作者有舉例，很多 clip 在完全相同的場景會有不同的動作。&lt;/p&gt;
&lt;p&gt;作者有提到說 I3D 特徵比較好遷移的一種解釋是它具備 high temporal resolution，
I3D 在 25 fps 的影片中用 64 frames 做訓練，使它能捕捉動作的 fine-grained 時間結構。&lt;/p&gt;
&lt;h2 id=&#34;experimental-evaluation-of-features&#34;&gt;Experimental Evaluation of Features&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table4and5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Kinetics 上做預訓練效果明顯比 ImageNet 好。&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Kinetics 上的預訓練對於遷移學習有明顯好處，但對於其他影像任務，比如影像語義分割是否有好處仍待觀察。&lt;/p&gt;
&lt;p&gt;目前對於架構沒有全面探索，比如沒有採用 action tubes 或是 attention 機制。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Swin Transformer 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Fri, 14 Apr 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.14030&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;本文提出一個新的 vision Transformer，稱作 Swin Transformer，可以被用作 computer vision 中的 general-purpose backbone。&lt;/p&gt;
&lt;p&gt;把 Transformer 從 language 移到 vision 具備挑戰性，比如同一個 visual entity 在大小上具備很大的 variance。還有 high resolution 下 pixel 和 word 的數量差異太大。&lt;/p&gt;
&lt;p&gt;為了解決這些差異，作者提出 hierachical Transformer，用 shifted windows 來算出 representation。&lt;/p&gt;
&lt;p&gt;shifted windowing 透過把 self-attention 限制在 non-overlapping 的 local window 和允許 cross-windows connection 來提高效率。&lt;/p&gt;
&lt;p&gt;這種 hierarchical architecture 可以靈活地在各種 scale 下擴展 model，還可以對圖像大小有線性的計算時間複雜度。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ViT 把圖片打成 patch，每個 patch 是 16*16，feature maps 由 single low resolution 的輸入生成，而且由於自注意力始終都是在全局上計算的 (patch 和 patch 間做自注意力)，所以時間複雜度是 quadratic computation complexity。&lt;/p&gt;
&lt;p&gt;Swin Transformer 從小 patch 開始，並在更深的 Transformer layers 合併相鄰的 patches。&lt;/p&gt;
&lt;p&gt;有了這些 hierarchical feature maps，可以用在像是 FPN 或是 U-Net。&lt;/p&gt;
&lt;p&gt;一個 Swin Transformer 的關鍵設計因素是 shifted window。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;透過 bridge 不同 layer 的 windows 來提供他們連接。&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;overall-architecture&#34;&gt;Overall Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Patch Merging
&lt;ul&gt;
&lt;li&gt;原本特徵圖是 H * W * C&lt;/li&gt;
&lt;li&gt;以上下 stride=2 行走，會得到四張 H/2 * W/2 * C&lt;/li&gt;
&lt;li&gt;concatenate 起來，變成 H/2 * W/2 * 4C&lt;/li&gt;
&lt;li&gt;做 linear，變成 H/2 * W/2 * 2C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;swin-transformer-block&#34;&gt;Swin Transformer block&lt;/h4&gt;
&lt;p&gt;Swin Transformer 是透過把 Transformer block 中的 multi-head self attention(MSA) 換成基於 shifted windows 的 module 構成。&lt;/p&gt;
&lt;h3 id=&#34;shifted-window-based-self-attention&#34;&gt;Shifted Window based Self-Attention&lt;/h3&gt;
&lt;p&gt;標準的 Transformer 架構會算 global self-attention，計算所有 token 間彼此的關係，導致 quadratic complexity，使其不適用於需要大量 token 的許多 CV 問題&lt;/p&gt;
&lt;h4 id=&#34;self-attention-in-non-overlapped-windows&#34;&gt;Self-attention in non-overlapped windows&lt;/h4&gt;
&lt;p&gt;原來的圖片會以 non-overlapping 的方式切割。&lt;/p&gt;
&lt;p&gt;假設每個 windows 有 M * M 個 patches，然後一張圖像有 h * w 塊 patches，計算複雜度如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega(MSA)=4hwC^2+2(hw)^2C$&lt;/li&gt;
&lt;li&gt;$\Omega(W-MSA)=4hwC^2+2M^2hwC$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;shifted-window-partitioning-in-successive-blocks&#34;&gt;Shifted window partitioning in successive blocks&lt;/h4&gt;
&lt;p&gt;window-based self-attention module 缺乏了 windows 間彼此的連接，會限制模型能力。&lt;/p&gt;
&lt;p&gt;作者提出了一種 shifted window 的方法，保持 non-overlapping windows 的高效計算，同時引入 windows 間的連接。&lt;/p&gt;
&lt;p&gt;再兩個連續的 windows 間，會移動 $(⌊ \frac{M}{2} ⌋, ⌊ \frac{M}{2} ⌋)$&lt;/p&gt;
&lt;h4 id=&#34;efficient-batch-computation-for-shifted-configuration&#34;&gt;Efficient batch computation for shifted configuration&lt;/h4&gt;
&lt;p&gt;shifted window 有個問題是，會導致更多的 windows，從 $⌈ \frac{h}{M} ⌉ * ⌈ \frac{w}{M} ⌉$ 到 $(⌈ \frac{h}{M} ⌉+1) * (⌈ \frac{w}{M} ⌉+1)$，而且有些 window 會小於 M*M。&lt;/p&gt;
&lt;p&gt;這樣會導致無法把這些給壓成一個 batch 快速計算。&lt;/p&gt;
&lt;p&gt;一種 naive 的解法就是直接在外面加 zero padding，但會增加計算量，當 windows 數量較少時，計算量會變很可觀 (從 2 * 2 個 windows 變成 3 * 3 個 windows，增加了 2.25 倍)&lt;/p&gt;
&lt;p&gt;作者提出另外一種巧妙的做法，把一些部分挪移。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;但現在有些 window 裡有多個不該相互做 attention 的部分，所以要用 mask 的方式計算。&lt;/p&gt;
&lt;p&gt;不同 windows，做 self-attention 後，把不相干的部分做的 attention 減去一個很大的數值，最後再過 softmax。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/mask.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上圖來自作者在 github 提供的可視化&lt;/p&gt;
&lt;p&gt;最後再把它挪回原本的位置。&lt;/p&gt;
&lt;h4 id=&#34;relative-position-bias&#34;&gt;Relative position bias&lt;/h4&gt;
&lt;p&gt;參考這個: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_37541097/article/details/121119988&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/qq_37541097/article/details/121119988&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;architecture-variants&#34;&gt;Architecture Variants&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;window size 預設是 M = 7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;query dimension of each head 是 d = 32&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;expansion layer of each MLP is $\alpha$ = 4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C 是 first stage 的 hidden layers 的 channel numbers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-T&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 6, 2}&lt;/li&gt;
&lt;li&gt;大小和計算量是 Base 的大約 0.25 倍&lt;/li&gt;
&lt;li&gt;complexity 接近 ResNet-50&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-S&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;大小和計算量是 Base 的大約 0.5 倍&lt;/li&gt;
&lt;li&gt;complexity 接近 ResNet-101&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-B&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 128&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-L&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 192&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;大小和計算量是 Base 的大約 2 倍&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;image-classification-on-imagenet-1k&#34;&gt;Image Classification on ImageNet-1K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;object-detection-on-coco&#34;&gt;Object Detection on COCO&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;semantic-segmentation-on-ade20k&#34;&gt;Semantic Segmentation on ADE20K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;基於 self-attention 的 shifted window 是 Swin Transformer 關鍵部分，被顯示出他在 CV 領域有效率且有效，並期望未來把它應用在 NLP。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GIT 論文閱讀</title>
        <link>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 29 Mar 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.14100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GIT: A Generative Image-to-text Transformer for Vision and Language&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; ██████╗ ██╗████████╗
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;██╔════╝ ██║╚══██╔══╝
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;██║  ███╗██║   ██║   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;██║   ██║██║   ██║   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;╚██████╔╝██║   ██║   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; ╚═════╝ ╚═╝   ╚═╝   
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;設計了一個 Generative Image-to-text Transformer，統一 vision-language tasks，像是 image/video captioning 或是問答。&lt;/p&gt;
&lt;p&gt;雖然 generative models 在預訓練和微調的時候是同樣的網路架構，現有的工作通常都包含複雜的架構 (uni/multi-modal encoder/decoder)，
而且依賴於外部模組，比如物件偵測或 optical character recognition (OCR)。&lt;/p&gt;
&lt;p&gt;在 GIT，我們簡化為 single language modeling task 下的一個 image encoder 和一個 text decoder。&lt;/p&gt;
&lt;p&gt;擴大了預訓練資料和模型大小以提高模型性能。&lt;/p&gt;
&lt;p&gt;在許多具有挑戰性的 benchmarks 上取得 SOTA。&lt;/p&gt;
&lt;p&gt;比如首次在 TextCpas 上超越人類的表現。&lt;/p&gt;
&lt;p&gt;提出了一種 generation-based image classification and scene text recognition 的新方案。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;近年來在 vision-language（VL）預訓練方面取得了巨大進展，特別是基於 image-text pairs 的大規模數據，例如 CLIP、Florence 和 SimVLM。&lt;/p&gt;
&lt;p&gt;學習到的 representation 很好的提高了下游任務的性能，比如 image captioning、visual question answering 和 image-text retrieval。&lt;/p&gt;
&lt;p&gt;在預訓練過程中，Masked Language Modeling (MLM) 和 Image-Text Matching (ITM) 被廣泛使用。&lt;/p&gt;
&lt;p&gt;然而這些 loss 和下游任務不同，必須做 task-specific adaptation。&lt;/p&gt;
&lt;p&gt;比如， image captioning 要移除 ITM，VQA 需要額外隨機初始的 MLP。&lt;/p&gt;
&lt;p&gt;為了減少這種差異，最近的研究試圖為預訓練模型設計 unified generative models 來預訓練，因為大多數 VL 的問題可以轉化為生成問題。&lt;/p&gt;
&lt;p&gt;這些方法通常利用 multi-modal encoder 和 text decoder，並精心設計 text input 和 text target。&lt;/p&gt;
&lt;p&gt;為了進一步推動這方向的研究，作者設計了一個簡單的 Generative Image-to-text Transformer，稱作 GIT，只包含一個 image encoder 和 text decoder。&lt;/p&gt;
&lt;p&gt;預訓練任務只是把輸入的圖像映射到相關聯的文字描述。&lt;/p&gt;
&lt;p&gt;盡管他很簡單，但還是在眾多具有挑戰性的 benchmark 取得 SOTA。&lt;/p&gt;
&lt;p&gt;image encoder 是 Swin-like vision transformer，在大量的 image-text pairs 上做 pretrain，基於 contrastive task。&lt;/p&gt;
&lt;p&gt;這消除了現有許多方法中對 object detector 的依賴。&lt;/p&gt;
&lt;p&gt;為了將其擴展到影片領域，我們把多個 frame 的特徵 concatenate，作為 video 表示。&lt;/p&gt;
&lt;p&gt;text decoder 是一個用來預測相關聯文字的 transformer。&lt;/p&gt;
&lt;p&gt;整個網路都是基於 language modeling task 來訓練。&lt;/p&gt;
&lt;p&gt;對於 VQA，input question 被看作 text prefix，並以 auto-regressive 的方法生出答案。&lt;/p&gt;
&lt;p&gt;此外，作者提出了一種 generation-based 的 ImageNet classification 新方案，預測標籤直接根據作者的生成模型，而不用預先定義詞彙表。&lt;/p&gt;
&lt;p&gt;我們的作法很簡單，但在擴大預訓練資料和模型大小後，成果驚人。&lt;/p&gt;
&lt;p&gt;主要貢獻如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;我們展示了 GIT，僅由一個 image encoder 和一個 text decoder 組成，透過 language modeling task，在 0.8 billion image-text pairs 上 pretrain。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 image/video captioning 和 QA 上，沒有基於 object detectors，object tags 和 OCR，就在多個任務上取得 SOTA。證明簡單的網路架構也可以透過 scaling 取得強大的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我們證明 GIT 雖然 pretrain 在 image-text pairs，也能在 video tasks 上取得 SOTA，不需要 video dedicated encoders。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我們提出了一種新的 generation-based image classification 方案，在 ImageNet-1K 上，取得不錯的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/table1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在 VL pre-training 中，多 multi-task pre-training 被廣泛使用，賦予網路多種或增強的能力。&lt;/p&gt;
&lt;p&gt;比如，MLM 和 ITM 是廣泛採用的預訓練任務，最近也有研究加入 image-text contrastive loss。&lt;/p&gt;
&lt;p&gt;由於多數 VL 任務都可以表示成 text generation task，所以可以訓練一個生成模型來支持各種下游任務。&lt;/p&gt;
&lt;p&gt;輸入和輸出文本通常都會經過精心設計，以預訓練這樣的生成模型。&lt;/p&gt;
&lt;p&gt;對於 image representation，Faster RCNN 被大多數現有方法用來提取區域特徵。&lt;/p&gt;
&lt;p&gt;同時，也很容易以 end-to-end 的方法訓練整個網路。&lt;/p&gt;
&lt;p&gt;除了 feature map，object tags，也很常被用來方便 transformer 理解上下文，特別是 novel objects。&lt;/p&gt;
&lt;p&gt;對於與場景文本相關的任務，調用 OCR 以生成場景文本作為附加網路輸入。&lt;/p&gt;
&lt;p&gt;對於 text prediction，常用 transformer network，結合 cross-attention module 來融合 image tokens。&lt;/p&gt;
&lt;p&gt;或者只是單純 concatenate text tokens 和 image tokens，然後用 self-attention。&lt;/p&gt;
&lt;p&gt;在本文中，我們有 9 個不同的 benchmark，3 種不同模型大小和 3 種不同預訓練資料規模。&lt;/p&gt;
&lt;h2 id=&#34;generative-image-to-text-transformer&#34;&gt;Generative Image-to-text Transformer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;network-architecture&#34;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;image encoder 基於 contrastive pre-trained model。&lt;/p&gt;
&lt;p&gt;輸入是原始圖像，輸出是 compact 2D feature map，被 flatten 成 list of features。&lt;/p&gt;
&lt;p&gt;透過一個額外的 linear layer 和一個 layernorm layer，image features 被 project 到 D dimensions，也就是 text encoder 的 input。&lt;/p&gt;
&lt;p&gt;作者使用做 contrastive tasks pretraining 的 image encoder，因為最近的研究表明這種 image encoder 有更好的性能。&lt;/p&gt;
&lt;p&gt;在後面的章節，還觀察到 VL performence 明顯地隨著更強的 image encoder 而有所提升。
這和 object detection-based 的方法觀察到的結果一致。&lt;/p&gt;
&lt;p&gt;CoCa 的 concurrent work 統一了 contrastive task 和 the generation task，作為一個預訓練階段。&lt;/p&gt;
&lt;p&gt;作者的方法相當於是按順序分離兩個任務:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用 contrastive task 訓練 image encoder&lt;/li&gt;
&lt;li&gt;用 generation task pretrain image encoder 和 text decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;text decoder 是一個用於預測文本描述的 transformer module，由多個 transformer block 組成，每個 transformer block 由一個 self-attention layer 和 feed-forward layer 組成。&lt;/p&gt;
&lt;p&gt;text 被 tokenize 和 embed 到 D dimensions，並添加 positional encoding 和 layernorm layer。&lt;/p&gt;
&lt;p&gt;image features 和 text embeddings 被 concatenate 起來作為 transformer module 的輸入。&lt;/p&gt;
&lt;p&gt;text 以 [BOS] 開始，並以 auto regressive 的方式 decode，直到 [EOS] 或 maximum steps。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;attention mask 根據上圖設計，使的 text token 只能依賴於前面的 text token 和 image token，而 image token 可以互相做 attention。&lt;/p&gt;
&lt;p&gt;這和 unidirectional attention mask 不同，unidirectional attention mask 並非每個 image token 都可以依賴於其他的 Image token。&lt;/p&gt;
&lt;p&gt;作者很好地初始化 image encoder，卻隨機初始化 text decoder。&lt;/p&gt;
&lt;p&gt;這種設計動機是基於[MiniVLM: A Smaller and Faster Vision-Language Model]，該研究隨機初始化顯示出與 BERT 初始化相似地性能。&lt;/p&gt;
&lt;p&gt;原因可能在於 BERT 地初始化無法理解圖像信號，這對於 VL 任務至關重要。&lt;/p&gt;
&lt;p&gt;[Flamingo: a Visual Language Model for Few-Shot Learning] 採用了類似的 image encoder + text decoder，但是他們的 decoder 經過 pretrain，並且有 freeze，好保留大型語言模型的泛化能力。&lt;/p&gt;
&lt;p&gt;GIT 的所有參數都會更新，以更好地適應 VL 的任務。&lt;/p&gt;
&lt;p&gt;另一種架構是 cross-attention-based 的 decoder，用於 incorporate image signals，而不是 concatenation 再用 self-attention。&lt;/p&gt;
&lt;p&gt;根據實驗，large-scale 的 pre-training，self-attention-based 會有更好的性能，小規模的則是 cross-attention-based。&lt;/p&gt;
&lt;p&gt;一個合理的解釋是，經過充分訓練，decoder 可以很好地處理圖像和文本，而且 image token 可以為了 text generation 更好地更新。&lt;/p&gt;
&lt;p&gt;而 cross-attention 讓 image token 沒辦法 attend 彼此。&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h3&gt;
&lt;p&gt;訓練採用 language modeling (LM) loss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/for1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$I$ 是 image&lt;/li&gt;
&lt;li&gt;$y_i,i \in $ { $ 1,&amp;hellip;,N $ } 是文字 token，$y_0$ 是 [BOS]，$y_{N+1}$ 是 [EOS]&lt;/li&gt;
&lt;li&gt;CE 是有 0.1 label smoothing 的 cross-entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另一種選擇是 MLM，在每個 epoch 中預測 15% 的輸入 token，要預測所有 token 至少需要 1 / 0.15 = 6.7 個 epochs，對於 LM，每個 epoch 都可以預測所有 token，對於大規模預訓練資料來說效率更高。&lt;/p&gt;
&lt;p&gt;ablation studies 顯示出 LM 可以在有限的 epoch 內實現更好的性能。
在大規模訓練中，由於計算資訊的限制，只有兩個 epoch，所以選擇 LM。
與此同時，大部分最近的 large-scale language model 也是基於 LM。&lt;/p&gt;
&lt;p&gt;如果沒有圖像輸入，該模型將簡化為 decoder-only 的語言模型，架構類似於 GPT-3。&lt;/p&gt;
&lt;p&gt;因此，這種設計還可以利用 text-only 的資料來提升 scaled-up decoder 的能力，把這保留給未來的工作。&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;對於 image captioning，由於訓練數據格式和預訓練相同，所以用同樣的 LM task 來微調 GIT。
對於 visual question answering，問題和 GT 在微調的時候被看做 special caption，但 LM loss 僅用於答案和 [EOS]。&lt;/p&gt;
&lt;p&gt;推理過程中，question 被當作 caption 的 prefix，完成的部分是預測。&lt;/p&gt;
&lt;p&gt;VQAv2 現有的工作收集候選答案，再重構成分類問題，預測一次。
作者的工作有更多挑戰，因為是生成式的，需要生出至少兩個正確的 token，答案和 [EOS]。&lt;/p&gt;
&lt;p&gt;然而考慮到自由形式答案的好處，作者選擇了生成方法。&lt;/p&gt;
&lt;p&gt;由於生成模型的難度，VQAv2 比現有的判別工作略差。&lt;/p&gt;
&lt;p&gt;對於和 scene-text related VQA 任務，現有方法通常利用 OCR 生成 5 個 scene text 並用 dynamic pointer network 決定當前輸出應該是 OCR 還是 general text。&lt;/p&gt;
&lt;p&gt;但由於作者的方法不依賴於 OCR，因此也不依賴於 dynamic pointer network。&lt;/p&gt;
&lt;p&gt;根據實驗，作者發現模型透過大規模預訓練資料學會如何閱讀場景文本，並且作者的模型不是專門為了影片領域設計的，但可以透過簡單的架構更改就取得具有競爭力或甚至 SOTA 的成果，也就是作者可以從每個影片採樣多個 frame，並透過 image encoder 獨立地為每個 frame 編碼。
最後添加一個 learnable temporal embedding (初始化為 0)，並 concatenate sampled frames 的特徵。&lt;/p&gt;
&lt;p&gt;作者還用於圖片分類，把 class name 用於 caption。&lt;/p&gt;
&lt;p&gt;這和現有工作不一樣，現有工作通常先定義詞彙表，並用線性層預測每個類別的可能性。&lt;/p&gt;
&lt;p&gt;當新數據和新類別被添加到現有數據的時候，這種新一代的方案是有益的，因為這樣可以在不引入新參數的情況下對新數據進行訓練。&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;收集 0.8B 的 image-text pairs 來預訓練。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image encoder 是根據  pre-trained contrastive model 初始化的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hidden dimension (D) = 768&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;text decoder 有 6 個 randomly-initialized transformer blocks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;共有 0.7b 的參數&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image decoder 和 text encoder 的 learning rate 各別是 1e-5 和 5e-5，都 cosine decay 到 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;推論階段 beam size 是 4，length penalty 是 0.6。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supplementary materials 展示了小模型變體 (GITB and GITL) 和更大模型 (GIT2) 的結果&lt;/p&gt;
&lt;h3 id=&#34;results-on-image-classification&#34;&gt;Results on Image Classification&lt;/h3&gt;
&lt;p&gt;輸出必須與類別名稱完全匹配，甚至考慮多或少的空格。&lt;/p&gt;
&lt;p&gt;由於不知道詞彙表，精確匹被準確度只有 1.93%，如果預測包含 GT 就對，那有 40.88%。&lt;/p&gt;
&lt;p&gt;通過微調每個類別只有 1 shot 或 5 shot，準確度會顯著提高，
表明只用少量訓練樣本，也可以輕鬆適應下游任務。&lt;/p&gt;
&lt;p&gt;與 Flamingo 相比，GIT 實現更高的準確度。&lt;/p&gt;
&lt;p&gt;Flamingo 在沒有參數更新的情況下進行小樣本學習，但需要額外的網路輸入，可能會增加推理成本。&lt;/p&gt;
&lt;p&gt;相比之下，GIT 透過一次 lightweight fine-tuning，推理過程中不需要這些 training shot。&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;h4 id=&#34;model-and-data-scaling&#34;&gt;Model and data scaling&lt;/h4&gt;
&lt;p&gt;對於網路架構，作者的模型被稱作 Huge，把 image encoder 換成 CLIP 的 ViT-B/16 和 ViT-L/14 的則是 Base 和 Large。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看出較大的 image encoder 帶來的好處，但根據實驗，
作者發現很難有效地擴展 text decoder，原因可能是 LM 很難用 limited amount of text 來訓練。&lt;/p&gt;
&lt;p&gt;另一個可能的原因是 image encoder 負責 object recognition，而 decoder 負責以 NLP 的方法組織 object terms。
後一項任務可能很容易，因為大多數描述都遵循相似的模式，比如 Object + verb + subject，所以只要一個 small decoder，較大的 decoder 可能會增加學習難度。&lt;/p&gt;
&lt;p&gt;Flamingo 的研究顯示更大的 Decoder 可以提高性能，但是他們的 decoder 有 pretrain 過，而且在 VL 預訓練的時候 frozen，避開了如何有效訓練 decoder 的問題。&lt;/p&gt;
&lt;p&gt;LEMON 的 transformer 可以擴展到 32 層，可能是因為他們使用 MLM 而不是 LM，後者可能更加困難。&lt;/p&gt;
&lt;h4 id=&#34;scene-text-in-pre-training-data&#34;&gt;Scene text in pre-training data&lt;/h4&gt;
&lt;p&gt;為了瞭解 scene text comprehension 的能力，作者檢查了 pretrain data 有多少 image-text pairs 有 scene text。&lt;/p&gt;
&lt;p&gt;作者用 Microsoft Azure OCR API4 對一些資料做 OCR，然後把 OCR 結果和 associated text 做比對，只有包含長度超過 5 個字元的 OCR 結果才會算比對。
有 15% 的 CC12M 和 31% 的下載圖像(500K) 包含 scene text 描述。
由於任務是訓練預測 text，網路逐漸學會閱讀 scene text。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;h3 id=&#34;limitations&#34;&gt;Limitations&lt;/h3&gt;
&lt;p&gt;根據實驗，目前不清楚如何控制生成的 caption 以及如何在不更新參數的情況下執行 in-context learning，把這留給未來的工作。&lt;/p&gt;
&lt;h3 id=&#34;societal-impact&#34;&gt;Societal impact&lt;/h3&gt;
&lt;p&gt;該模型在大規模數據集上預訓練，不能保證數據不含 toxic language，可能會 poison output。&lt;/p&gt;
&lt;h2 id=&#34;其他&#34;&gt;其他&lt;/h2&gt;
&lt;h3 id=&#34;a3-network&#34;&gt;A.3 Network&lt;/h3&gt;
&lt;p&gt;講超參數&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/model.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MAE 論文</title>
        <link>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</link>
        <pubDate>Wed, 15 Feb 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.06377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;這篇論文顯示出 MAE 是 CV 中的 scalable self-supervised learners。&lt;/p&gt;
&lt;p&gt;MAE 的方法很簡單&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隨機蓋住輸入影像的一些 patch&lt;/li&gt;
&lt;li&gt;重建 missing pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具備兩個核心設計&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;非對稱的 encoder-decoder 架構，encoder 只作用於可見的 patch 子集合(沒有 mask tokens)，lightweight decoder 則根據 latent representation 和 make tokens 來重建圖片。&lt;/li&gt;
&lt;li&gt;當遮住高比例(比如 75%)的影像時，會得到一個 nontrivial 和 meaningful 的 self-supervisory task&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;結合這兩點設計，可以有效地訓練大模型。
以 ViT-Huge 用 ImageNet-1K 訓練(訓練集一百多萬張照片)可達到 87.8% 的準確度。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/intro.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在 CV 中，常需要大量 labeled images。
NLP 中，自監督預訓練處理了需要大量標註資料的問題。
masked autoencoders 是一種更 general 的 denoising autoencoders 的形式。
BERT 非常成功，autoencoding methods 在 CV 的研究卻落後 NLP，作者思考是什麼讓 masked autoencoding 在 CV 和 NLP 產生不同。
有以下觀點&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;直到前陣子，CV 中的 CNN 是主流，但卷積層不好引入 mask tokens 或 positional embedding 這些 indicator。但這些可以透過 ViT 來解決，不應成為問題。&lt;/li&gt;
&lt;li&gt;語言和視覺的 Information density 不同，語言是 highly semantic 和 information-dense，使填字本身不是很簡單的事情，但影像含有大量冗餘的訊息，缺失的部分比較好從相鄰的 patch 重建，比如直接插值，所以作者用一種簡單的策略，隨機 mask 很大一部分的 patch，創造一個具有挑戰性的自監督任務，強迫模型關注 global 的資訊。&lt;/li&gt;
&lt;li&gt;關於 decoder，CV 還原 pixel，pixel 屬於 lower semantic level，NLP 還原 word，word 的 semantic information 較高。作者發現，雖然在 BERT 中，可以用簡單的 decoder 還原(一個 MLP)，但 CV 中 decoder 的設計就很重要。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基於以上觀點，作者提出 MAE，隨機遮住大量的 patch，並在 pixel space 重建失去的 patch。而且是非對稱 encoder-decoder 架構，encoder 只會看到可見的 patch，但 docoder 除了 latent representation，還會看到 mask tokens。這種設計在非常高的掩蓋率(比如 75%)下不但可以提高準確度，還可以讓 encoder 只處理較少比例(比如 25%)的 patch，將訓練時間減少 3 倍或更多，使 MAE 可以輕鬆擴展成更大的模型。&lt;/p&gt;
&lt;p&gt;在這樣的架構下，用 MAE 的 pre-training，可以訓練非常吃 data 的模型，比如 ViT-Large/-Huge，而只使用 ImageNet-1K。&lt;/p&gt;
&lt;p&gt;用 ImageNet-1K 在 vanilla ViT-Huge 上 fine-tune 可達到 87.8% 準確度，比以往只使用 ImageNet-1K 的結果都高。&lt;/p&gt;
&lt;p&gt;在 obejct detection、instance segmentation、semantic segmentation 上做 transfer learning 都達到不錯的效果，可以打敗用監督式預訓練模型的對手。&lt;/p&gt;
&lt;h1 id=&#34;相關工作&#34;&gt;相關工作&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoding
&lt;ul&gt;
&lt;li&gt;MAE 是一種 denoising autoencoding 的形式，但和 DAE 還是差別很大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masked image encoding
&lt;ul&gt;
&lt;li&gt;iGPT、ViT、BEiT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Masking&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;和 ViT 一樣，把圖片切成多個 patch，對於 patch 均勻隨機地採樣保留，剩下地遮住&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViT&lt;/li&gt;
&lt;li&gt;也有 positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer block&lt;/li&gt;
&lt;li&gt;輸入
&lt;ul&gt;
&lt;li&gt;encoded visible patches&lt;/li&gt;
&lt;li&gt;mask tokens
&lt;ul&gt;
&lt;li&gt;shared, learned vector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;都會加入 positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;用相較 encoder 輕量的解碼器，所有的 patch 由這個輕量的 decoder 處理，減少預訓練時間&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconstruction target&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decoder 的最後一層是 linear projection，之後再 reshape 成你要的  patch&lt;/li&gt;
&lt;li&gt;loss function
&lt;ul&gt;
&lt;li&gt;mean squared error(MSE)&lt;/li&gt;
&lt;li&gt;只算 masked patched 的 MSE，像 BERT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple implementation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先取得一系列 token(patch 做 linear projection + positional embedding)&lt;/li&gt;
&lt;li&gt;randomly shuffle，根據比例移除尾端一部份&lt;/li&gt;
&lt;li&gt;encoding 後，尾端接上 mask tokens，並且 unshuffle&lt;/li&gt;
&lt;li&gt;加上 positional embedding 後，給 decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;imagenet-experiments&#34;&gt;ImageNet Experiments&lt;/h1&gt;
&lt;p&gt;在 ImageNet-1K 上做自監督的預訓練，然後做&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;end-to-end fine-tuning
&lt;ul&gt;
&lt;li&gt;所有參數都可改&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;linear probing
&lt;ul&gt;
&lt;li&gt;只改最後一層線性層&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/vit-mae.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/ratio-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;optimal masking ratio 意外地高，相比 BERT 只有 15%&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/fine-tune-blocks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;討論和結論&#34;&gt;討論和結論&lt;/h1&gt;
&lt;p&gt;在 CV 實用的預訓練做法主流是監督式的，CV 中自監督的做法可能正跟著 NLP 的軌跡走。&lt;/p&gt;
&lt;p&gt;要仔細處理圖像和語言的區別，作者去除圖片中很可能不構成 semantic segment 的部分，而不是移除某個 object。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ViT 論文</title>
        <link>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 12 Feb 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.11929&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;在 CV 領域 transformer 表現有限，目前 attention 常常是和卷積神經網路一起用，或是用來把一些卷積層換成 self-attention，但整體架構不變。這篇論文想展現一個純 Transformer 可以直接在影像分類上表現很好。如果用大量資料作預訓練，再遷移到中小型的資料集，可以和 SOTA 的 CNN 表現得一樣好，還需要較少的訓練資源作訓練。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;self-attention-based 架構，特別是 Transformer，已經是 NLP 的重要選擇。主流的作法是在大型文字資料集上作訓練，再針對小型任務資料集作 fine-tune。由於 Transformer 的計算效率高，還有可擴展性，可以 train 一些很大的 model，隨著 model 和資料集增大，目前還沒看出飽和的現象。&lt;/p&gt;
&lt;p&gt;然而在 CV，CNN 還是主流，一些工作嘗試用 self-attention 結合 CNN-like 的架構，比如把 feature map 當 transformer 的輸入，因為原始 pixel 太多，或甚至把卷積層全換成 self-attention，雖然後者理論上效率很高(原論文中有另外 cite 兩篇作法)，但因為他們做法特殊，在現代硬體上很難加速，所以無法很有效地擴展。在 large-scale 的影像識別上， ResNet-like 的架構還是 SOTA。&lt;/p&gt;
&lt;p&gt;該實驗直接把一個標準的 Transformer 作用於圖片上，只作最少的修改。把影像分成多個 patch，並把它們變成一系列的 linear embedding，當作 NLP 中的 tokens(words) 來處理。&lt;/p&gt;
&lt;p&gt;當在中型大小的資料集(e.g. ImageNet)上訓練，如果沒有 strong regularization，ViT 會略輸同等大小的 ResNets&lt;/p&gt;
&lt;p&gt;這篇論文在更大的資料集(14M-300M 的影像)上訓練，就打敗了 inductive bias。在大量資料上作預訓練就很讚。&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;大型的 Transformer-based 模型常常是先在大資料集上預訓練然後根據任務 fine-tune，比如 BERT 和 GPT。&lt;/p&gt;
&lt;p&gt;要把 self-attention 用在 CV 上，最簡單的做法就是把每個 Pixel 當一個元素，但 self-attention 是平方複雜度，在現實的圖片很難應用。一個應用 Transformer 的做法是只把 self-attention 用在 local neighborhood，另外一個是用 Sparse Transformer，還有一堆特殊的方法，雖然表現不錯，但要用硬體加速起來不容易。&lt;/p&gt;
&lt;p&gt;另一個有關的模型是 iGPT，在 reduce image resolution 和 color space 後把 transformer 應用在 image pixels 上。它用非監督式訓練後，再 fine-tune 或做 linear probing(只更新最後的 linear layer) 分類任務，表現很好。&lt;/p&gt;
&lt;p&gt;已經有類似的工作了，抽取 patches of size 2 * 2，最後再接 full self-attention，基本上和 ViT 非常像，這篇論文進一步證明了作大規模的預訓練可以讓 Transformer 和 SOTA 的 CNN 相比，而且 ViT 因為 patch 比較大，可以處理 medium-resolution 的圖片。這問題是可預期的，因為 Transformer 缺少了一些 inductive biases。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inductive biases
&lt;ul&gt;
&lt;li&gt;一些假設&lt;/li&gt;
&lt;li&gt;比如 CNN 常有四個假設
&lt;ul&gt;
&lt;li&gt;locality&lt;/li&gt;
&lt;li&gt;translation invariance with pooling layers
&lt;ul&gt;
&lt;li&gt;平移不變性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation equivariance
&lt;ul&gt;
&lt;li&gt;f(g(x)) = g(f(x))&lt;/li&gt;
&lt;li&gt;卷積和平移的先後順序沒差&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;模型盡可能類似原始 Transformer，這樣可以把一些 NLP 上成功的 Transformer 架構拿來用，還可以用一些很有效率的 implementation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-process.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;embedding 維度是 768 = 16 * 16 * 3
position embedding 的做法是 standard learnable 1D positional embeddings，就是 BERT 的做法，簡單來說就是生出一張可以訓練的表，(序列長度, embedding size)，作者也有嘗試其他方法，但發現成效差不多，比如 2D positional embedding，概念就是從生出(序列長度, embedding size)變成生出 2 個(sqrt(序列長度), embedding size)。&lt;/p&gt;
&lt;p&gt;[class] 的概念是 NLP 出來的，ResNet-like 的架構常見的做法也有通過 globally average-pooling (GAP)來生出向量，再接上分類器做預測。實驗發現直接在 transformer 的輸出做 GAP 和 [class] 都可以達到不錯的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-gap.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-acc.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;拿標準的 Transformer 來作 Image recognition，和以往用 self-attention 在 CV 的方法不一樣，除了一開始的 initial patch extraction，沒有引入其他影像特有的 inductive biases。直接把圖片當成是一系列的 patch，然後直接用 Transformer encoder 當一般 NLP 任務處理。在很多影像分類訓練集上表現得更好還在 pre-train 上相對便宜。&lt;/p&gt;
&lt;p&gt;還有一些值得挑戰的地方，比如把 ViT 應用在其他 CV 任務，比如 detection 和 segmentation。另一個挑戰是探索自監督預訓練的方法。這篇論文其實有實驗自監督，表現 OK，但和監督式還是有很大的落差。擴大 ViT 可能有更好的結果。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
