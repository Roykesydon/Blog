[{"content":"paper: RoBERTa: A Robustly Optimized BERT Pretraining Approach\nAbstract 發現 BERT 訓練不足，並且作者的模型在 4/9 的 GLUE 任務, RACE 和 SQuAD 取得 SOTA。\nIntroduction 自監督的訓練方法帶來了顯著的性能提升，但要確定這一堆方法中的哪些方面貢獻最大，具備挑戰性。\n訓練的計算量是昂貴的，使 fine-tune 受限，而且通常都是用不同大小的 private training data，使評估模型更加困難。\n作者提出了對 BERT 預訓練的 replication study，包括對超參數的調整，以及對訓練集大小的仔細評估。\n作者發現 BERT 訓練不足，並提出了一種改進方法，稱為 RoBERTa，可以達到或超過所有 post-BERT 的方法。\n修改如下:\n訓練模型的時間更長，batch 更大，用更多 data 移除 next sentence prediction objective 訓練更長的序列 動態地改變用於訓練資料的 masking pattern 貢獻:\n提出一組重要的 BERT 設計選擇和訓練策略 使用了新的 dataset，叫做 CCNEWS，並證明用更多的資料來預訓練，可以提高下游任務的表現 訓練表明，在正確的設計選擇下，pretrained masked language model 和其他最近的方法比，具有競爭力 Background 對 BERT 做回顧\nArchitecture L layers A self-attention heads H hidden dimension Training Objectives 預訓練的時候，BERT 有兩個目標: masked language modeling 和 next sentence prediction\nMasked Language Model (MLM) BERT 隨機選擇 15% 的 token 進行可能的替換\n80% 換成 [MASK]，10% 保持不變，10% 被選為一個隨便的 vocabulary token\nNext Sentence Prediction (NSP) 分類第二句是不是下一句，是二元分類。\n正例由提取連續的句子產生，負例由不同的片段配對產生。\n正例和負例以相等機率產生。\nOptimization Adam $\\beta_1$ = 0.9, $\\beta_2$ = 0.999, $\\epsilon$ = 1e-6 $L_2$ weight decay of 0.01 Learning rate 前 10,000 step warm up 到 1e-4，然後 linear decay 全部的 layer 和 attention weight 都 dropout 0.1 GELU 激活函數 1,000,000 次 update，batch size 256，序列長度 512 Data BERT 在 BookCorpus 和 English Wikipedia 混和的資料集上訓練，共有 16GB 的未壓縮文本\nExperimental Setup 描述對於 BERT 的 replication study 的實驗設置\nImplementation 作者用 FAIRSEQ 重新實現了 BERT。\n主要遵循 [Background-Optimization] 中的 BERT 原始超參數，但 peak learning rate 和 warmup step 除外，他們針對每個設置單獨調整。\n作者發現訓練對 Adam epsilon 非常敏感。\n作者發現設置 $\\beta_2$ = 0.98，在大 batch size 的情況下，可以提高訓練時的穩定性。\n用最多 512 個 token 預訓練。\n作者不會隨機注入短序列，也不會為前 90% 的更新縮短輸入的長度。\n作者只訓練 full-length 的 sequences。\nData BERT-style 的預訓練仰賴大量文本。\n已有研究證明增加數據量可以提高 end-task 的性能。\n已有一些研究，用比原始 BERT 更多樣更大的數據集，但不是所有的數據集都有公開。\n本研究用了五個不同大小和領域的英文文本，共有超過 160 GB 的未壓縮文本。\n使用以下數據集:\nBookCorpus + English Wikipedia BERT 原本使用的。 16 GB CC-News 作者從 CommonCrawl News dataset 的英文部分中蒐集，包含了 2016 年 9 月到 2019 年 2 月的 6300 萬篇英文新聞。 過濾後有 76 GB OpenWebText WebText 的開源重建版，從 Reddit 上至少有 3 個 upvotes 的 shared URLs 提取出的 Web 內容。 38 GB Stories 包含 CommonCrawl data 的一個子集合，經過過濾，以匹配 story-like style of Winograd schemas 31 GB Evaluation 使用以下三個 benchmarks 評估預訓練模型\nGLUE The General Language Understanding Evaluation\n用於評估自然語言理解的 9 個數據集的集合，任務被定義為 single-sentence 分類或 sentence-pair 分類任務。\nfinetune 的流程遵循原始 BERT paper\nSQuAD The Stanford Question Answering Dataset\n提供一段 context 以及一個問題\n具有兩個版本 V1.1 和 V2.0\nV1.1 context 總是包含一個答案 評估 V1.1 的時候，作者採用和 BERT 相同的 span prediction method V2.0 一些問題在提供的 context 中沒有回答，使任務更有挑戰性 評估 V2.0 的時候，作者會用一個額外的二元分類器預測問題是否可以回答，在評估的時候，只預測被分類為可回答的 RACE The ReAding Comprehension from Examinations 大型閱讀理解數據集，有超過 28,000 篇文章 以及將近 100,000 個問題 從中國的英文考試蒐集的，這些考試是為國中生和高中生設計的 每篇文章都與多個問題相關聯 對每個問題，要從四個選項中選出一個對的 context 比起其他閱讀理解的數據集要長，而且要推理的問題比例很大 Training Procedure Analysis 探討哪些選擇對成功預訓練 BERT 很重要。\n作者把架構固定，也就是訓練和$BERT_{BASE}$ (L=12, H=768, A=12, 110M params)一樣架構的 BERT models\n","date":"2023-03-22T01:08:46+08:00","permalink":"https://roykesydon.github.io/Blog/p/roberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/","title":"RoBERTa 論文閱讀"},{"content":"開發環境 IDE SW4STM32 支援 STM32 GCC C/C++ compiler GDB-based debugger 板子 STM32 Bucleo Board Cortex-M4 ST-LINK debugger Memories 1MB Flash 128KB SRAM Debug Interface JTAG Joint Test Action Group standard ASICs hardware debug interface SWD Serial Wire Debug 只從 JTAG 用 5 wires Bootup Code Reset\nBoot Loader\n0x00000000 的程式 把 CPU 重置 Reset handler\nStstem initialization C startup code\nApplication(main)\nMemory map 見官網 memory map\n只用到 SRAM 的 128KB(SRAM)，還有 Code 的 1MB(Flash)\nSections .data 儲存資料 .text 儲存程式碼 同 section 會放在一塊是為了設定 read-only 方便，比如 .text 的要靠硬體實現 read-only\n重要的額外文件 Linker Script 定義了不同 section 該存放的地方，以及 memory 相關定義 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 MEMORY { RAM (xrw)\t: ORIGIN = 0x20000000, LENGTH = 96K ROM (rx)\t: ORIGIN = 0x8000000, LENGTH = 1024K } SECTIONS { /* The program code and other data into ROM memory */ .text : { . = ALIGN(8); *(.text) /* .text sections (code) */ *(.text*) /* .text* sections (code) */ *(.glue_7) /* glue arm to thumb code */ *(.glue_7t) /* glue thumb to arm code */ *(.eh_frame) KEEP (*(.init)) KEEP (*(.fini)) . = ALIGN(8); _etext = .; /* define a global symbols at end of code */ } \u0026gt;ROM .data : { . = ALIGN(8); _sdata = .; /* create a global symbol at data start */ *(.data) /* .data sections */ *(.data*) /* .data* sections */ . = ALIGN(8); _edata = .; /* define a global symbol at data end */ } \u0026gt;RAM AT\u0026gt; ROM } Make File 描述如何編譯和連接的規則 把 startup 的 .s檔加進去 startup_stm32.s 編譯好後擺在 binary 頭的地方\nvector table\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /****************************************************************************** * * The STM32L476RGTx vector table. Note that the proper constructs * must be placed on this to ensure that it ends up at physical address * 0x0000.0000. * ******************************************************************************/ .section .isr_vector,\u0026#34;a\u0026#34;,%progbits .type g_pfnVectors, %object .size g_pfnVectors, .-g_pfnVectors g_pfnVectors: .word _estack .word Reset_Handler .word NMI_Handler .word HardFault_Handler .word\tMemManage_Handler .word\tBusFault_Handler .word\tUsageFault_Handler .word\t0 .word\t0 .word\t0 .word\t0 .word\tSVC_Handler Reset_Handler 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Reset_Handler: ldr r0, =_estack mov sp, r0 /* set stack pointer */ /* Copy the data segment initializers from flash to SRAM */ ldr r0, =_sdata ldr r1, =_edata ldr r2, =_sidata movs r3, #0 b LoopCopyDataInit LoopCopyDataInit: adds r4, r0, r3 cmp r4, r1 bcc CopyDataInit /* Zero fill the bss segment. */ ldr r2, =_sbss ldr r4, =_ebss movs r3, #0 b LoopFillZerobss LoopFillZerobss: cmp r2, r4 bcc FillZerobss /* Call the clock system intitialization function.*/ bl SystemInit /* Call static constructors */ bl __libc_init_array /* Call the application\u0026#39;s entry point.*/ bl main ARM Register ARM 的可存取暫存器為 R0-R15\nr13: Stack Pointer r14: Link Register r15: Program Counter r0~r7 是 low register r8~r15 是 high register 狀態暫存器\nCPSR (Current Processor Status Register) 用來儲存各種狀態，包含 condition flag，比如 negative, zero, carry, overflow carry: 無符號加法操作是否溢出 overflow: 有符號加法操作是否溢出 當兩個都為 1 或都為 0 代表運算沒問題 有多種模式，有些模式有自己獨立的 r 暫存器，並有 SPSR，用來在中斷發生時，把 CPSR 的資訊 copy 過去\nSpecial-purpose registers\nAPSR, IPSR, EPSR Assembly syntax UAL: Unified Assembler Language 自己去翻 instruction set Instructions class Branch instructions B, BL, BX,\u0026hellip; Data-processing instructions MOV, ADD, SUB, MUL,\u0026hellip; Load and store instructions LDR, STR,\u0026hellip; Status register access instructions MSR, MRS,\u0026hellip; Miscellaneous instructions Memory Barrier instructions Exception-Related instructions Pseudo instructions examples MOVS R0, #0x12\nR0=0x12 MOVS R1, #`A` R1=A(ASCII) NVIC_IRQ_SETEN EQU 0xE000E100\n宣告常數 NVIC_IRQ_SETEN，賦值 0xE000E100 LDR R0,=NVIC_IRQ_SETEN\n放 0xE000E100 進 R0 這不能改成 MOVS R0, #0xE000E100 ，因為每個 instruction 只有 32 個 bits，這勢必塞不下，必須從記憶體 load 進來 NVIC_IRQ0_ENABLE EQU 0x1\n宣告常數 NVIC_IRQ0_ENABLE，賦值 0x1 MOVS R1, #NVIC_IRQ0_ENABLE\nR1=0x1 STR R1, [R0]\n把 0x1 存到 0xE000E100，這裡可以 enable external interrupt IRQ#0 LDR rn [pc, #offset to literal pool]\nload register n with one word from the address [pc + offset] 最後的形式 Operand2 共有 12 bits 設計成 4 bits for rotate, 8 bits for Immediate ARM instrcution formats ADD vs ADDS 有 S 代表會去更新 status cond 根據之前的執行情況，判斷指令要不要執行 suffix Reverse Ordering Operations REV (Byte-Reverse Word) 把 4 個 Byte 全數反轉，用在一個是 Little-Endian 一個是 Big-Endian 的情況 Load and Store Instructions examples LDR r0, [r1] r0 = [r1] LDM r0, {r1, r2} r1 = [r0] r2 = [r0+4] STM r0, {r1, r2} [r0] = r1 [r0+4] = r2 Status Register Access Instructions 一般來說不太會用到，因為用 suffix 就可以看條件 MRS: Register = Status Register MRS r0, IPSR MSR: Status Register = Register MSR APSR, r0 If-Then-Else 用 CMP 和 conditional branches Example 1 2 CMP R0, #10 ;compare r0 to 10 BLE incr_counter ; if less or equal, then branch to incr_counter Branch Instructinos 能跳的距離受限於 operand 長度\nB-Branch 能跳 PC 的 +/- 2046 bytes BL-Branch and Link 能跳 PC 的 +/- 254 bytes Branch to subroutine 的時候，會把下一行指令放到 Link register 沒有 push 到 stack，所以要特別小心，register 是共用的， 可能要視情況自己放到 stack 比如要進兩層 function，可以用 push {r4-r6, LR} 和 POP {R4-R6, PC} 這種做法來保留參數 BX-Branch and exchange return Stack memory access PUSH\nSP = SP - N*4 POP\nSP = SP + N*4 Ascending/Descending\nstack 往哪個方向長 Empty/Full\nstack 指向下一個空的位置，還是最後一個 item 預設且常見的是 fully descending\nSTM 和 LDM 可以透過 suffix 來存到 stack\nexample STMFD r13!, {r4-r7} 把 r4 到 r7 push 到 stack Memory Barrier Instructions DMB, SDB, ISB 在下個指令前 sync memory data Function Call and Parameter Passing caller 和 callee 誰負責 backup 和 restore caller 負責 不管 callee 怎樣亂搞都行 但不知道 callee 要用哪些參數，全 backup 可能多此一舉 怎麼傳遞參數給 callee 常放在 stack，但這樣要透過 memory，相較 register 慢 怎麼 return value 給 caller 和上個問題差不多 ARM Procedure Call Standard 又稱 APCS，講不同的 register 的一種使用規範\nr0-r3 用來當參數和回傳 r4-r11 用來 local variable，callee 使用前可以先 backup r12-r15 特殊用途，沒事別亂動 ","date":"2023-03-21T01:32:54+08:00","permalink":"https://roykesydon.github.io/Blog/p/arm-%E7%B5%84%E5%90%88%E8%AA%9E%E8%A8%80%E4%BB%8B%E7%B4%B9/","title":"ARM 組合語言介紹"},{"content":"paper: PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT\nAbstract 本研究提供了一個計算 patent-to-patent (p2p) technological similarity 的有效方法。\n並提出一個 hybrid framework，用於把 p2p 相似性的結果應用於 semantic search 和 automated patent classification。\n把 Sentence-BERT (SBERT) 用在 claims 上來作 embeddings。\n為了進一步提升 embedding 的品質，使用基於 SBERT 和 RoBERT 的 transformer model，然後再用 augmented approach 在 in-domain supervised patent claims data(相對於 out-domain) 來 fine-tune SBERT。\n用 KNN(Nearest Neighbors) 來根據 p2p similarity 分類模型。\nIntroduction 傳統上的 p2p 相似度是基於關鍵字、技術類別等 metadata 決定的，但近期 semantic-based 的方法也越來越受歡迎。\n目前遇到的問題 BERT 用來計算 p2p 相似性的成本很高 基於 generic text 的 pre-trained model 在遇到特定領域的專業術語時可能會遇到侷限。 在專利做 multi-label classification (MLC) 是個挑戰 貢獻 提供一個快速高效的框架，利用 Transformer 架構計算 p2p 相似度 透過 augmented SBERT，將 transformer model fine-tune 到 domain-specific language 提出一個基於 Transformer 和 傳統 ML 模型的混和架構，可以打敗 multi-label 和 multi-class 的專利分類 SOTA 模型 用簡單的 KNN 進行專利分類，提供了一種簡單的方法來檢查、理解和解釋模型的預測結果 Data Dataset Description 本研究使用 PatentsView dataset，PatentsView 平台建立在一個定期更新的 database 上。\ndataset 已用於之前類似的研究，比如 DeepPatent、PatentBERT。\n本研究使用了 2013-2017 的所有專利，這些專利至少要在 BigQuery 上有一條 claim。\n本研究的 record 有 1,492,294 項專利，並用 8% 作為測試集。\n此外，本研究刪除了有重複專利 ID 和 claim text 的 record。\nTextual Data: Patent Claims 本研究使用 claim 作為輸入。\nclaim 被認為是準備專利文件的初始框架，其他文件都是根據 claim 準備的， 因此，claim 比其他文件包含更全面和準確的訊息。\nclaim 具有層次結構，first claim 被視為該架構的主幹。\n本研究僅使用 first claim，但在以後的研究中，希望根據 tree structure 組合所有 claim，並計算 semantic similarity，並做多標籤分類。\n在研究樣本中， claim 平均有 17 個。\nclaim 的平均長度是 162，本研究中，BERT 的 max_seq_length 是 510。\nPatent Classification: CPC Classes CPC系統和IPC（國際專利分類）系統是最常用的兩種分類系統，CPC 是 IPC 系統的更具體和詳細的版本。\nCPC 具有用於分類的層次結構，包括 Section、Class、Subclass 和 Group， 在子類級別，CPC 有 667 個標籤。\n在數據集中我們有 663 個標籤，其中 159 個在數據集中的樣本少於 350 個，這種標籤分佈導致了 KNN 不好處理，一般來說，隨著 instance 數量的增加，我們可以提高模型的準確性。\nMethod and experimental setup Pretrained Language Models (LMs) 在 NLP 中變得十分流行。\n在 pairwise sentence semantic similarity，SBERT 和 BERT 是兩種具有顯著不同效果的方法。\nBERT 通常可以取得更好的性能，但在實際應用上來說太慢了。\nSBERT 在實際應用上表現還行，但需要 in-domain training data 並且 finetune。\n上圖是 Augmented SBERT In-domain approach。\nin-domain sentence pairs 透過 cross-encoder 來標記，假設有 n 個 in-domain sentences，會有 $C_2^n$ 組可能的組合。\n使用所有可能的組合並不會提高性能，所以要有正確的採樣策略，才可提升性能的同時也減少計算開銷。\n上圖那種結合 cross-encoder 和 bi-encoder 的作法被稱為 Augmented SBERT (AugSBERT)， 涉及以下三個步驟:\n用資料集 Fine-tune RoBERTa 以生出 cross-encoder 用 cross-encoder 來把未標記的資料標記，同時基於某種特定的採樣策略，從 652,653 種可能的組合中挑選 3432 組 把資料集 + 額外的 3432 組資料一起拿來訓練 SBERT Results P2P similarity and semantic search Patent Semantic Search (PSS) 是專利分析的基礎部分。\nTransformer 模型等語義相似性的解法是一種新解法，可以用來解決基於關鍵字的搜尋方法中， query terms 和專利內容不匹配的問題。\n為了評估模型的準確性，未來的研究中，作者希望通過 Mean Reciprocal Rank (MRR) 來評估分類結果。\nCPC Prediction Top-N 準確度等於 GT 與預測有最高概率的任何 N 個預測匹配的頻率， 所以 Top-5 就是最高的五個分類中一個就有中。\nConclusion 本文使用 augmented SBERT 獲得 SOTA 的專利文本 embedding。\n介紹了一種 augmented 的方法，把 SBERT 微調到適合 patent claims 的 domain。\nSBERT 的一個主要優點是可以有效率地獲得 embedding distance，使我們能夠為大的專利資料集建構 p2p similarity。\n雖然基於文本的 p2p similarity 的有用性已經在各種應用方面得到證明，但本文進一步證明作者的 transformer-based p2p similarity 可以被用在 SOTA 的專利分類。\n而且使用簡單的 KNN 方法，檢查他們可以使模型決策具備 understandable 和 explainable。\nLimitations \u0026amp; Future Research 未來希望用 Annoy(Approximate Nearest Neighbor Oh Yeah!) 來測試更大樣本的模型並比較結果。\nAnnoy(Approximate Nearest Neighbor Oh Yeah!) 是想尋找近似相似而不是精確相似的句子。\n","date":"2023-03-15T15:08:46+08:00","permalink":"https://roykesydon.github.io/Blog/p/patentsberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/","title":"PatentSBERTa 論文閱讀"},{"content":"Introduction 結合 policy-based 和 value-based\nA3C Actor-Critic 最知名的方法 Advantage Actor-Critic 是 A2C Advantage Actor-Critic Review: Policy gradient\n$\\triangledown \\overline{R_{\\theta}}\\approx \\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{t=1}^{T_n}(\\displaystyle\\sum_{t^{\u0026rsquo;}=t}^{T_n}\\gamma^{t^{\u0026rsquo;}-t}r_{t^{\u0026rsquo;}}^n-b)\\triangledown log p_{\\theta}(a_t^n|s_t^n)$ $G_t^n=\\displaystyle\\sum_{t^{\u0026rsquo;}=t}^{T_n}\\gamma^{t^{\u0026rsquo;}-t}r_{t^{\u0026rsquo;}}^n-b$ G very unstable，因為給同樣的 state 作同樣的 action 不一定會得到同樣的結果，G 是個 random variable 想要改獲得期望值，取代掉 sample 的值(G 的部分)，可以用 Q-Learning\n$E[G_t^n]=Q^{\\pi_\\theta}(s_t^n,a_t^n)$ Q function 這樣定義 所以我們可以把 G 的部分改用 Q 替換掉，就可以把 Actor 和 Critic 結合起來 baseline 的部分也可以用 value function 替換掉 但用 $Q^{\\pi}(s_t^n,a_t^n)-V^{\\pi}(s_t^n)$ 要一次 estimate 兩個 network\n可以把 Q 以 V 來表示，那只需要估測 V $Q^{\\pi}(s_t^n,a_t^n)=E[r_t^n+V^{\\pi}(s_{t+1}^n)]$ 雖然有隨機性(獲得的 reward 和跳到什麼 state 不一定)，但先不管期望值 $Q^{\\pi}(s_t^n,a_t^n)=r_t^n+V^{\\pi}(s_{t+1}^n)$ 現在雖然多個一個 r，有一些 variance，但也比 G 好 $\\triangledown \\overline{R_{\\theta}}\\approx \\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{t=1}^{T_n}(r_t^n+V^{\\pi}(s_{t+1}^n)-V^{\\pi}(s_t^n))\\triangledown log p_{\\theta}(a_t^n|s_t^n)$\nTips actor $\\pi(s)$ 和 critic $V^{\\pi}(s)$ 的權重可以共享\n前面幾個 layer 可以 share 對 $\\pi$ 的 output 下 constrain，讓他的 entropy 不要太小，達到 exploration 的效果\nAsynchronous Advantage Actor-Critic 一開始有個 global network，開一堆 worker，每次工作前，把 global network 的參數 copy 過去 個別去和環境作互動，更新的梯度施加在 global network 上 Pathwise Derivative Policy Gradient 可以當作是 Q-Learning 解 continuous action 的一種方法 訓練一個 actor，目標是生出的 a 餵給 Q 後，可以讓 Q function 的輸出越大越好 只會調 actor 的參數，會 fix Q 的 就是個 GAN 在每個 episode\n對於每個 time step t ⚠️給 state $s_t$，根據 $\\pi$ 執行 action $a_t$ (epsilon greedy)⚠️ 獲得 reward $r_t$，到達 $s_{t+1}$ 把 {$s_t,a_t,r_t,s_{t+1}$} 存到 buffer 從 buffer sample {$s_t,a_t,r_t,s_{t+1}$}(通常是一個 batch) ⚠️Target $y=r_i+\\hat{Q}(s_{i+1},\\hat{\\pi}(s_{i+1}))$⚠️ Update Q 的參數，好讓 $Q(s_i,a_i)$ 更接近 y(regression) ⚠️Update $\\pi$ 的參數，讓 $Q(s_i,\\pi(s_i))$ 最大化⚠️ 每 C 步 reset $\\hat{Q}=Q$ ⚠️每 C 步 reset $\\hat{\\pi}=\\pi$⚠️ ⚠️ 是和 Q-Learning 不一樣的地方\n","date":"2023-03-14T16:21:23+08:00","permalink":"https://roykesydon.github.io/Blog/p/actor-critic/","title":"Actor-Critic"},{"content":"Normalization 目的 避免 redundent information 更容易 understand、enhance、extend 避免 anomalies 隨著 1NF ~ 5NF，有更多的 safety guarantee\n1NF 違反條件 用 row order 傳達資訊 mixing data types in single column 但 relational database 不會讓你這樣做 存在沒有 primary key 的 table repeating groups 同一個 column 有多個數值，或是在同一個 row 存多個同類型的數值。 ex : player item roy 1 item_1, 4 item_2 star 4 item_4 player item_type1 quantity1 item_type2 quantity2 roy item1 1 item2 4 star item_4 4 2NF 所有的 non-key attribute 都要 depend on 整個 PK 非正式定義，有點細微差異 functional dependency ex: {player_id, item_type} -\u0026gt; {item_Quantity} 3NF transitive dependency {A} -\u0026gt; {B} -\u0026gt; {C} 所有 non-key attribute 都要 depend on the whole key，不能 depend on 其他 non-key attribute Boyce-Codd Normal Form 所有 attribute 都要 depend on the whole key，不能 depend on 其他 non-key attribute 4NF multivalued dependency 不像 functional dependency，箭頭後方的那項可以有多個 value {Model} $\\twoheadrightarrow$ {Color} 一個 table 中的所有 multivalued dependency 必須依賴於 key 5NF 沒有 Join Dependency table 不能表示成其他 table join 起來的結果 ","date":"2023-03-14T10:26:17+08:00","permalink":"https://roykesydon.github.io/Blog/p/database-normalization/","title":"Database Normalization"},{"content":"paper: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\nAbstract BERT 和 RoBERTa 在 semantic textual similarity (STS) 上太花時間，因為他需要將兩個句子都輸入網路，並且兩兩比對。\nSentence-BERT(SBERT) 對預訓練的 BERT 作了一些修改，透過 siamese 和 triplet network 的結構來生出有意義的 embeddings，使其最後可以透過 cosine-similarity 比較相似度。\nIntroduction SBERT 使 BERT 可以用於某些迄今為止不適用於 BERT 的任務，比如 large-scale semantic similarity comparison、clustering 還有 information retrieval via semantic search。\n以往的相關研究是把單個句子輸入 BERT，最後 average BERT output layer，或是使用第一個 output，但這樣會產生糟糕的 sentence embeddings。\nSentEval 是一個 evaluation toolkit for sentence embeddings\nRelated Work BERT 透過輸入兩個句子，以 [SEP] 隔開，可以在 STS 取得 SOTA。\n但這樣無法計算獨立的 sentence embedding，所以過往的研究人員把單個句子輸入 BERT，最後 average BERT output layer，或是使用第一個 output。\nModel SBERT 在 BERT / RoBERTa 的輸出中添加了 pooling，作者嘗試了三種策略，CLS-token 的輸出、所以輸出向量的平均、max-over-time of the output vectors，默認是 MEAN。\n實驗以下結構和目標函數:\nClassification Objective Function\nRegression Objective Function\n用 mean squared-error loss\nTriplet Objective Function\nTraining Details Dataset SNLI 結合 Multi-Genre NLI SNLI: 570,000 個 句子 pair，有三類，contradiction, eintailment, and neutral MultiNLI: 430,000 個句子 pair 3-way softmax Classification Objective Function 1-epoch batch-size: 16 Adam lr: 2e-5 warm-up: 超過 10% of the training data 默認 pooling 策略: MEAN Evaluation 學習一個複雜的回歸函數分析 STS 常是 SOTA，但是由於他是 pair-wise，遇到 combinatorial explosion，不好拓展。\n本文用 cosine-similarity 比較兩個 embeddings 的相似度，也用 negative Manhatten 和 negative Euclidean distances，但得到差不多的結果。\nConclusion 用 BERT 生出的 embeddings 不適合常見的相似度測量方法，比如 cosine-similarity。\n本文提出 SBERT 改進，在 siamese / triplet 網路架構中微調 BERT。\n用 RoBERTa 替換掉 BERT 並沒有什麼顯著改進。\n","date":"2023-03-12T10:08:46+08:00","permalink":"https://roykesydon.github.io/Blog/p/sentence-bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/","title":"Sentence-BERT 論文閱讀"},{"content":"UML 類別圖 Relationship Dependency \u0026ldquo;uses-a\u0026rdquo; Association \u0026ldquo;knows-a\u0026rdquo; Composition \u0026ldquo;has-a\u0026rdquo; child 的存在依賴於 parent，若刪除 parent，child 也會隨之刪除 Aggregation \u0026ldquo;has-a\u0026rdquo; child 的存在獨立於 parent，若刪除 parent，child 不會隨之刪除 Inheritance \u0026ldquo;is-a\u0026rdquo; Implementation \u0026ldquo;can-do\u0026rdquo; 實現 interface other features Navigation 當兩個 class 都可以看到對方，就用沒箭頭的關聯線，否則有箭頭 Role Name 類別中的 Attribute Multiplicity 關聯端點上可以寫數量，代表物件個數 Self-Association 同個類別的物件彼此有關係 軟體設計原則 Encapsulate What Varies 把經常改變的程式碼封裝起來，使日後修改時不會影響其他區塊的程式碼 實際使用的情境，可以把常改變的東西放在 interface 後，使日後改變實作時不影響呼叫該 interface 的程式碼 Favor Composition over Inheritance Composition(組合)在很多情境可以取代掉 Inheritance(繼承)，可藉由 Polymorphism(多型)來達成 只有當 is-a 的情境出現，才用繼承比較好 Composition 使用起來更有彈性 SOLID 設計原則 Single Responsibility Principle, SRP 單一職責原則 A class should have only one reason to change. 可以把一個複雜的 module 拆成多個 Open-Close Principle, OCP 開放封閉原則 You should be able to extend the behavior of a system without having to modify that system. 要可以擴充，同時不修改到原系統 LiskovSubstitution Principle, LSP 里氏替換原則 父類別有的功能，子類別必須遵從，父類別的部分要可以直接替換成子類別 Interface Segregation Principle, ISP 介面隔離原則 No client should be forced to depend on methods it does not use 以 interface 來說，不該讓 module 實現它不需要的功能，可以把 interface 拆小 Dependency Inversion Principle, DIP 反向依賴原則 高階模組不應該依賴低階模組，兩者都應依賴抽象層 ","date":"2023-03-08T14:26:17+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E8%BB%9F%E9%AB%94%E8%A8%AD%E8%A8%88%E5%8E%9F%E5%89%87/","title":"軟體設計原則"},{"content":"Assembly language Control flow\njumps, branches, calls jmp, jne, je, bne, be, call branch 就是根據 status flag 而 jump je 0x33 代表「如果 zero flag 是 1 就跳到 0x33」 call 是 unconditional GOTO，但會把下一個 address 放到 stack 那 RET instruction 可以晚點把它取出，並回到那邊 status flag\n在 register 上的多數 operation，比如加減法，會對 status flags 產生影響 只有很少的 status flags，而且他們通常存在在一個 special register 上 範例 zero flag 上個運算的結果是不是 0，在一些條件判斷很常用該 flag Register 從組合語言的角度來看，是一些有固定 size 的全域變數 special register Program Counter 記錄下個要執行的 instruction 別名 PC Intel x86 叫 Instruction Pointer 根據 16/32/64 bits，稱為 IP/EIP/RIP Stack pointer SP, ESP, RSP Base Pointer BP, EBP, RBP 繞過字串比對 該程式吃一個參數，比對是不是密碼\n我們要透過 gdb 從執行檔觸發 printf(\u0026ldquo;Access Granted!\\n\u0026rdquo;);\n程式碼: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(int argc, char *argv[]) { if(argc==2) { printf(\u0026#34;Checking License: %s\\n\u0026#34;, argv[1]); if(strcmp(argv[1], \u0026#34;AAAA-Z10N-42-OK\u0026#34;)==0) { printf(\u0026#34;Access Granted!\\n\u0026#34;); } else { printf(\u0026#34;WRONG!\\n\u0026#34;); } } else { printf(\u0026#34;Usage: \u0026lt;key\u0026gt;\\n\u0026#34;); } return 0; } 來源: https://github.com/LiveOverflow/liveoverflow_youtube/blob/master/0x05_simple_crackme_intro_assembler/license_1.c gcc ./test.c -o ./test -Wall -Wall 可以顯示所有警告 輸入 gdb ./test\n可以透過 disassemble main 來顯示 main function 的 assembler instruction\n可以用 set disassembly-flavor intel 來換個風格\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 Dump of assembler code for function main: 0x0000000000001189 \u0026lt;+0\u0026gt;: endbr64 0x000000000000118d \u0026lt;+4\u0026gt;: push rbp 0x000000000000118e \u0026lt;+5\u0026gt;: mov rbp,rsp 0x0000000000001191 \u0026lt;+8\u0026gt;: sub rsp,0x10 0x0000000000001195 \u0026lt;+12\u0026gt;: mov DWORD PTR [rbp-0x4],edi 0x0000000000001198 \u0026lt;+15\u0026gt;: mov QWORD PTR [rbp-0x10],rsi 0x000000000000119c \u0026lt;+19\u0026gt;: cmp DWORD PTR [rbp-0x4],0x2 0x00000000000011a0 \u0026lt;+23\u0026gt;: jne 0x11fb \u0026lt;main+114\u0026gt; 0x00000000000011a2 \u0026lt;+25\u0026gt;: mov rax,QWORD PTR [rbp-0x10] 0x00000000000011a6 \u0026lt;+29\u0026gt;: add rax,0x8 0x00000000000011aa \u0026lt;+33\u0026gt;: mov rax,QWORD PTR [rax] 0x00000000000011ad \u0026lt;+36\u0026gt;: mov rsi,rax 0x00000000000011b0 \u0026lt;+39\u0026gt;: lea rdi,[rip+0xe4d] # 0x2004 0x00000000000011b7 \u0026lt;+46\u0026gt;: mov eax,0x0 0x00000000000011bc \u0026lt;+51\u0026gt;: call 0x1080 \u0026lt;printf@plt\u0026gt; 0x00000000000011c1 \u0026lt;+56\u0026gt;: mov rax,QWORD PTR [rbp-0x10] 0x00000000000011c5 \u0026lt;+60\u0026gt;: add rax,0x8 0x00000000000011c9 \u0026lt;+64\u0026gt;: mov rax,QWORD PTR [rax] 0x00000000000011cc \u0026lt;+67\u0026gt;: lea rsi,[rip+0xe47] # 0x201a 0x00000000000011d3 \u0026lt;+74\u0026gt;: mov rdi,rax 0x00000000000011d6 \u0026lt;+77\u0026gt;: call 0x1090 \u0026lt;strcmp@plt\u0026gt; 0x00000000000011db \u0026lt;+82\u0026gt;: test eax,eax 0x00000000000011dd \u0026lt;+84\u0026gt;: jne 0x11ed \u0026lt;main+100\u0026gt; 0x00000000000011df \u0026lt;+86\u0026gt;: lea rdi,[rip+0xe44] # 0x202a 0x00000000000011e6 \u0026lt;+93\u0026gt;: call 0x1070 \u0026lt;puts@plt\u0026gt; 0x00000000000011eb \u0026lt;+98\u0026gt;: jmp 0x1207 \u0026lt;main+126\u0026gt; 0x00000000000011ed \u0026lt;+100\u0026gt;: lea rdi,[rip+0xe46] # 0x203a 0x00000000000011f4 \u0026lt;+107\u0026gt;: call 0x1070 \u0026lt;puts@plt\u0026gt; 0x00000000000011f9 \u0026lt;+112\u0026gt;: jmp 0x1207 \u0026lt;main+126\u0026gt; 0x00000000000011fb \u0026lt;+114\u0026gt;: lea rdi,[rip+0xe3f] # 0x2041 0x0000000000001202 \u0026lt;+121\u0026gt;: call 0x1070 \u0026lt;puts@plt\u0026gt; 0x0000000000001207 \u0026lt;+126\u0026gt;: mov eax,0x0 0x000000000000120c \u0026lt;+131\u0026gt;: leave 0x000000000000120d \u0026lt;+132\u0026gt;: ret End of assembler dump. 可以先畫 control flow graph，分析每塊的運作情形，或找軟體生，比如在 MacOS 可以用 Hopper\n指令 break *main\n在 main 的第一行設置 breakpoint 也可以直接把 main 換成某個 address，比如break *0x00000000000011fb run \u0026lt;參數，比如這邊就是 key\u0026gt;\n跑程式 info registers\n執行的過程中，可以隨時查看 register 情形， e registers are the low 32 bits of the r registers 程式執行到下一步\n可用 si、ni、continue si 和 ni 會跑到下一行程式，差別是 si 會進去 function calls 接著可以一直按 enter，不用一直重打 x/s \u0026lt;address\u0026gt;\n印出 ASCII 字串，可以用在 .rodata section 最後會發現關鍵在\n1 2 3 0x00000000000011d6 \u0026lt;+77\u0026gt;: call 0x1090 \u0026lt;strcmp@plt\u0026gt; 0x00000000000011db \u0026lt;+82\u0026gt;: test eax,eax 0x00000000000011dd \u0026lt;+84\u0026gt;: jne 0x11ed \u0026lt;main+100\u0026gt; 我們執行 \u0026lt;+82\u0026gt; 前，把 eax 改為 0 即可，test 用來確認一個值是否為 0，目的是為了後面的 jne，我們的目標是讓 zero flag 設成 1，而 eax 是 rax 的 first 32 bits。\n本來是 rax 0x23 35，下完set $eax=0，變成rax 0x0 0\n此時我們接著 ni 下去，得到 \u0026ldquo;Access Granted!\u0026rdquo;\nTools hexdump\n可以把二進制文件印出來 -C 可以額外顯示 ASCII strings\n把高於某種長度的 printable character sequence 印出來 objdump\n-d 可以用來 disassemble -x 可以查看 header section .text 存放程式碼的地方 15 .text 000001e5 00000000000010a0 00000000000010a0 000010a0 2**4 長度 0x1e5 bytes address 在 0x10a0 .rodata read only data strace\ntrace system calls and signal radare2\nUNIX-like reverse engineering framework and command-line toolset\nr2 ./test\naaa\n深度分析 afl\n列出所有 function ?\n\u0026ndash;help a? a 的詳細說明 s main\nseek address pdf\n印出當前函式的 disassembly VV\nVisual mode(graph) p 切換顯示方法(包含顯示 address) ? \u0026ndash;help V!\n可以看到大量資訊的方法 r2 -d ./test\n像 gdb 一樣 debug db \u0026lt;address\u0026gt; 設置 breakpoint :dc 按 \u0026ldquo;:\u0026rdquo; 跑 command mode，這樣就可以在 VV 看 dc 會 run 程式碼 s 和 si 一樣，可以用 S，那就是 ni dr register info dr rip=0x00123123123 可以這樣設置位址 ood [args]\nreopen in debug mode (with args) afvn \u0026lt;var1\u0026gt; \u0026lt;var2\u0026gt;\nrename variable Uncrackable? 密碼直接放在 binary，連 vim 都可以輕鬆看到，接下來會挑戰更難一點的情形，先從不把 key 放在裡面開始，雖然上次直接改 register 的做法根本不用管 key 是什麼\n不顯示 string 密碼\n這次的做法改成，不再是 check string 一不一樣，而是先算 license key 的所有 char 轉成 int 後的總合，並且刪除掉 license key，之後對輸入的密碼算總和看一不一樣 1 2 3 0x00001212 72c2 jb 0x11d6 0x00001214 817de8940300. cmp dword [var_18h], 0x394 0x0000121b 750e jne 0x122b 程式碼的這塊是看總合的，這時我們在 jne 設 breakpoint 後，可以直接把 rip 改到改成下一行的 address，直接繞過 關鍵在於只要可以找到是哪一行 cmp，就可以繞過它 parser differential\n讓 linux 可執行，但讓 debugger 不能跑 每個 Parser 的演算法略有不同 fuzzing 隨機改一個 bytes，有可能執行結果和原本一樣，但是 parser 有問題 ","date":"2023-03-07T14:26:17+08:00","permalink":"https://roykesydon.github.io/Blog/p/crack-simple-elf/","title":"Crack simple ELF"},{"content":"paper: PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model\nAbstract 把 fine-tune BERT 應用在專利分類上，當應用於超過 200 萬件專利的資料集時，該方法超越了結合 word-embedding 的 CNN 的 SOTA 作法。\n貢獻: 一個用預訓練的 BERT 去 fine-tune 的 SOTA 方法 一個叫做 USPTO-3M 的大型資料集，屬於 CPC subclass level，並提供 SQL 語句讓後續的研究者使用 與傳統觀念相反，只需要 claim 就足以完成分類任務 Introduction 專利分類是一個 multi-label 的分類任務。\n由於標籤的數量可能很大，所以是個具有挑戰性的任務。\n作者準備了一個基於 CPC 的新資料集，有超過三百萬項美國專利。\nCPC Cooperative Patent Classification 是 IPC 更具體和詳細的版本 可預見將取代 IPC 成為新的標準 只是由於 CLEP-IP 競賽，大部分論文都基於 IPC 資料集包含 1978 到 2009 提交的專利 IPC International Patent Classification 此外，作者的 dataset 基於 patent claims\npatent claims 重要性在過往被低估 在起草專利申請時，專利業者會先起草 patent claims 專利文件的其餘部分由 claim 做延伸 在專利法中，claims 定義了專利發明的界線，確定了專利權範圍 為使模型更簡單，只關注 patent claims，並且僅用第一項 claim。\n相關工作 過往有些研究只顯示了 precision，但沒有 F1 value 或 recall，難以公平比較。\n以 DeepPatent\nData 過往資料基於 CLEF-IP 或 patent offices。\n作者發現在 BigQuery 用 Google Patents Public Datasets 更容易。\n而且可用 SQL statements，作者認為比共享傳統資料集更好，原因如下:\nSeperation of concerns 如果資料包含前處理或後處理，其他研究人員需要不同操作時會很頭痛。 Clarity and flexibility SQL statement 精確且容易根據不同條件進行修改。 在和 DeepPatent 比較的時候，可以的話，會用 USPTO2M 進行測試，如果不行，才會合併來自 USPTO-3M 的資料，比如 USPTO-2M 沒有 claims 的情況。\n為了比較 claim 如何影響性能，將合併兩個資料集。\nMethod \u0026amp; Experimental Setup 用 BERT-Base 就可以打敗 DeepPatent。\n遵循 BERT Project 中給的 fine-tune 範例。\n為了 multilabel，用 sigmoid cross entropy with logits function 而不是用 softmax。\nConclusion 專利分類作為具有挑戰性的任務，幾十年來一直沒有令人滿意的表現。\n本文提出一個基於 fine-tune BERT 的方法，性能優於 DeepPatent。\n並且結果表明只用 patent claim 就可以完成分類任務。\n","date":"2023-03-02T16:08:46+08:00","permalink":"https://roykesydon.github.io/Blog/p/patentbert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/","title":"PatentBERT 論文閱讀"},{"content":"linear equation $a_1x_1+a_2x_2+\u0026hellip;+a_nx_n = b$ $a$ 是 coefficient $x$ 是 variables $b$ 是 constant term Systems of linear equations m equations, n variables\n$a_{11}x_1+a_{12}x_2+\u0026hellip;+a_{1n}x_n = b_1\\\\ a_{21}x_1+a_{22}x_2+\u0026hellip;+a_{2n}x_n = b_2\\\\ \u0026hellip;\\\\ a_{m1}x_1+a_{m2}x_2+\u0026hellip;+a_{mn}x_n = b_m$\nsolution\n$[s_1~s_2~\u0026hellip;~s_n]^T$ 是一組解，代換到 $x_1$~$x_n$ 後滿足所有 equation 的向量 所有 Systems of linear equations 都有\nno solution exactly one solution infinitely many solutions consistent/inconsistent\n如果有一組以上的解就是 consistent 無解就是 inconsistent equivalent\n如果兩組 Systems of linear equations 的 solution set 一樣，稱為 equivalent elementary row operations\n不會影響 solution set types Interchange 兩 row 互換 Scaling 某 row 乘某個 nonzero scalar Row addition 把某 row 乘某個 scalar 後加到某 row property 所有 elementary row operations 都是 reversible 用來求解 coefficient matrix\n$a_{11}x_1+a_{12}x_2+\u0026hellip;+a_{1n}x_n = b_1\\\\ a_{21}x_1+a_{22}x_2+\u0026hellip;+a_{2n}x_n = b_2\\\\ \u0026hellip;\\\\ a_{m1}x_1+a_{m2}x_2+\u0026hellip;+a_{mn}x_n = b_m$ 可以拆為 $Ax=b$\n$A=\\begin{bmatrix} a_{11}\u0026amp; a_{12}\u0026amp;\u0026hellip;\u0026amp; a_{1n} \\\\ a_{21}\u0026amp; a_{22}\u0026amp;\u0026hellip;\u0026amp; a_{2n} \\\\ \u0026hellip;\u0026amp; \u0026hellip;\u0026amp;\u0026hellip;\u0026amp; \u0026hellip; \\\\ a_{m1}\u0026amp; a_{m2}\u0026amp;\u0026hellip;\u0026amp; a_{mn} \\end{bmatrix}$\nA 就是 coefficient matrix $x=\\begin{bmatrix} x_1\\\\ x_2\\\\ \u0026hellip;\\\\ x_n \\end{bmatrix}$\n$x$ 是 variable vector $[A|b]=\\begin{bmatrix} a_{11}\u0026amp; a_{12}\u0026amp;\u0026hellip;\u0026amp; a_{1n} \u0026amp; b_1 \\\\ a_{21}\u0026amp; a_{22}\u0026amp;\u0026hellip;\u0026amp; a_{2n} \u0026amp; b_2\\\\ \u0026hellip;\u0026amp; \u0026hellip;\u0026amp;\u0026hellip;\u0026amp; \u0026hellip; \u0026amp; \u0026hellip;\\\\ a_{m1}\u0026amp; a_{m2}\u0026amp;\u0026hellip;\u0026amp; a_{mn} \u0026amp; b_m \\end{bmatrix}$\n叫做 augmented matrix ","date":"2023-02-21T15:42:47+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-ii/","title":"線性代數 - II"},{"content":"matrix a rectangular array of scalars\nsize\nm by n 叫做 square if m = n equal\n兩個矩陣的 size 和每個 entry 都一樣 submatrix\n從一個大矩陣刪掉 rows 或 columns addition\n兩個大小相同的矩陣，每個對應位置的 entry 兩兩相加 scalar multiplication\n一個矩陣的所有 entry 乘以某個 scalar zero matrix\n所有 entry 都是 0，該矩陣常以 $O_{n \\times m}$ 來表示 性質 $A = O + A$ $0 \\cdot A = O $ subtraction\n$A-B=A+(-B)$ transpose\n$A^T$ 的 $(i,j)$-entry 是 $A$ 的 $(j,i)$-entry Properties $(A+B)^T=A^T+B^T$ $(sA)^T=sA^T$ $(A^T)^T=A$ vectors type\nrow vector 只有 1 row 的 matrix column vector 只有 1 column 的 matrix components\nthe entries of a vector 用 the $i$ th component 代表 $v_i$ addition, scalar multiplication\n和 matrix 一樣 矩陣表示\n一個矩陣常被表示為 a stack of row vectors a cross list of column vectors linear combination $c_1u_1+c_2u_2+\u0026hellip;+c_ku_k$\nscalars\n$c_1,c_2,\u0026hellip;,c_k$ 又被稱作 linear combination 的 coefficients vectors\n$u_1,u_2,\u0026hellip;,u_k$ 如果 $u,v$ 非平行二維向量，則二維空間中所有向量皆是 $u,v$ 的 linear combination，且是 unique 的\nstandard vectors $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \u0026hellip; \\\\ 0 \\end{bmatrix} ,e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \u0026hellip; \\\\ 0 \\end{bmatrix},\u0026hellip;, e_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ \u0026hellip; \\\\ 1 \\end{bmatrix}$\n$R^n$ 的任何一個向量都可以被 standard vectors 表示成 uniquely linearly combined\n矩陣向量乘法 $Av=v_1a_1+v_2a_2+\u0026hellip;+v_na_n$ Identity Matrix 對整數 n，$n \\times n$ identity matrix $I_n$ 每個 columns 是 standard vectors $e_1, e_2, \u0026hellip;, e_n$ in $R^n$ Stochastic Matrix 對整數 n，$n \\times n$ stochastic matrix 所有 entry 都必須非負 每個 column 的 entry 總和必須是 unity (相加為 1) ","date":"2023-02-21T14:42:47+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-i/","title":"線性代數 - I"},{"content":"Process Scheduling 可能時機\nrunning -\u0026gt; waiting running -\u0026gt; ready waiting -\u0026gt; ready running -\u0026gt; terminate Process Scheduler\nPreemptive scheduler (Time slice) 可以被搶占 Non-Preemptive scheduler 又稱 cooperative scheduling 只可能出現在時機 1 或 4 Classification fo Processes(related to scheduling)\nInteractive Processes (50 - 150 ms) Batch Processes Real time Processes Hard Soft Classification of Processes(related to CPU usage)\nCPU Bound I/O Bound Standard Scheduling Algorithm FCFS SJF SRTF Priority Based Highest Response Ratio Next Round Robin Virtual RR Multi-Level Queue Scheduler Multi-Level Feed Back Queue Scheduler Rotating Staircase Deadline Scheduler UNIX SVR3 Scheduler 有 32 個 runqueue，每個 runqueue 負責 4 個 priority values\n128 Priority values\n0-49: Kernel 50-127: User $Priority_j=Base_j+CPU_j(i)+nice_j$\nBase: 0-127 $CPU_j(i) = DR * CPU_j(i-1)$ DR = $\\frac{1}{2}$ nice: -20 ~ +19 可以用 nice 和 renice 改 process nice value Schedtool Query \u0026amp; set per process scheduling parameters\nScheduling Policy Real time SCHED_RR SCHED_FIFO Conventional SCHED_NORMAL (default) SCHED_BATCH (CPU intensive) SCHED_ISO (unused) SCHED_IDLEPRIO (low pri jobs) Nice Value (-20 to +19) Static Priority (1-99) CPU affinity process 想運行在某個指定的 CPU 上，不被轉移到其他 CPU，才不會降低指定 CPU 的 cache 命中率 soft CPU affinity hard CPU affinity cpus_allowed 一個用來指定 CPU 的 mask 1 schedtool \u0026lt;PID\u0026gt; ","date":"2023-02-20T21:12:52+08:00","permalink":"https://roykesydon.github.io/Blog/p/process-scheduling/","title":"Process Scheduling"},{"content":"RL 方法 Policy-based learn 做事的 actor Value-based 不直接 learn policy，而是 Learn critic，負責批評 Q-learning 屬於這種 Critic 不直接決定 action 給予 actor $\\pi$，評估 actor $\\pi$ 有多好 critic 的 output 依賴於 actor 的表現 State Value Function State value function $V^{\\pi}(s)$ 用 actor $\\pi$，看到 s 後玩到結束，cumulated reward expectation 是多少 評估方法 Monte-Carlo(MC) based approach\ncritic 看 $\\pi$ 玩遊戲 訓練一個 network，看到不同的 state ，輸出 cumulated reward(直到遊戲結束，以下稱為 $G_a$)，解 regression 問題 Temporal-difference(TD) approach\nMC 的方法至少要玩到遊戲結束才可以 update network，但有些遊戲超長 TD 只需要 {$s_t,a_t,r_t,s_{t+1}$} $V^{\\pi}(s_t)=V^{\\pi}(s_{t+1})+r_t$ MS v.s. TD\nMC Larger variance 每次的輸出差異很大 TD smaller variance 相較 $G_a$ 較小，因為這邊的 random variable 是 r，但 $G_a$ 是由很多 r 組合而成 V 可能估得不準確 那 learn 出來的結果自然也不准 較常見 Another Critic State-action value function $Q^\\pi(s,a)$\n又叫 Q function 當用 actor $\\pi$ 時，在 state s 採取 a 這個 action 後的 cumulated reward expectation 有一個要注意的地方是，actor 看到 s 不一定會採取 a 只要有 Q function，就可以找到\u0026quot;更好的\u0026quot; policy，再替換掉原本的 policy \u0026ldquo;更好的\u0026quot;定義 $V^{\\pi^{\u0026rsquo;}} \\ge V^{\\pi}(s), \\text{for all state s}$ $\\pi^{\u0026rsquo;}(s)=arg \\underset{a}{max}Q^{\\pi}(s,a)$ $\\pi^{\u0026rsquo;}$ 沒有多餘的參數，就單純靠 Q function 推出來 這邊如果 a 是 continuous 的會有問題，等等解決 這樣就可以達到\u0026quot;更好的\u0026quot;policy，不過就不列證明了 Basic Tip Target network 在 training 的時候，把其中一個 Q 固定住，不然要學的 target 是不固定的，會不好 train Exploration policy 完全 depend on Q function 如果 action 總是固定，這不是好的 data collection 方法，要在 s 採取 a 過，才比較好估計 Q(s, a)，如果 Q function 是 table 就根本不可能估出來，network 也會有一樣的問題，只是沒那麼嚴重。 解法 Epsilon Greedy $a=\\begin{cases} arg \\underset{a}{max}Q(s,a), \u0026amp; \\text{with probability } 1-\\varepsilon \\\\ random, \u0026amp; otherwise \\end{cases}$ 通常 $\\varepsilon$ 會隨時間遞減，因為你一開始 train 的時候不知道怎麼比較好 Boltzmann Exploration $P(a|s)=\\frac{exp(Q(s,a))}{\\sum_a exp(Q(s,a))}$ Replay Buffer 把一堆的 {$s_t,a_t,r_t,s_{t+1}$} 存放在一個 buffer {$s_t,a_t,r_t,s_{t+1}$} 簡稱為 exp 裡面的 exp 可能來自於不同的 policy 在 buffer 裝滿的時候才把舊的資料丟掉 每次從 buffer 隨機挑一個 batch 出來，update Q function 好處 跟環境作互動很花時間，這樣可以減少跟環境作互動的次數 本來就希望 batch 裡的 data 越 diverse 越好，不會希望 batch 裡的 data 都是同性質的 issue 我們要觀察 $\\pi$ 的 value，混雜了一些不是 $\\pi$ 的 exp 到底有沒有關係? 理論上沒問題，但李老師沒解釋 Typical Q-learning 演算法 初始化 Q-fucntion Q，target Q-function $\\hat{Q}=Q$ 在每個 episode 對於每個 time step t 給 state $s_t$，根據 Q 執行 action $a_t$ (epsilon greedy) 獲得 reward $r_t$，到達 $s_{t+1}$ 把 {$s_t,a_t,r_t,s_{t+1}$} 存到 buffer 從 buffer sample {$s_t,a_t,r_t,s_{t+1}$}(通常是一個 batch) Target $y=r_i+\\underset{a}{max}\\hat{Q}(s_{i+1},a)$ Update Q 的參數，好讓 $Q(s_i,a_i)$ 更接近 y(regression) 每 C 步 reset $\\hat{Q}=Q$ Adveanced Tip Double DQN Q Value 往往被高估 我們的目的是要讓 $Q(s_t, a_t)$ 和 $r_t+\\underset{a}{max}Q(s_{t+1},a)$ 越接近越好(後者就是 target) target 常常不小心設太高，因為如果有 action 被高估了，就會選那個當 target Double DQN: 兩個函式 $Q$ 和 $Q^{\u0026rsquo;}$ 把 target 換成 $r_t+Q^{\u0026rsquo;}(s_{t+1},arg \\underset{a}{max}Q(s_{t+1},a))$ 選 action 交給 $Q$，實際算交給 $Q^{\u0026rsquo;}$ 如果 $Q$ 選了高估的 action，$Q^{\u0026rsquo;}$ 有可能修正回來 如果 $Q^{\u0026rsquo;}$ 高估，$Q$ 不一定會選到 $Q^{\u0026rsquo;}$ 是 target network(固定不動) Dueling DQN 改變 network 架構 分成兩條 path 第一條算 scalar 第二條算 vector，每個 action 都有個 value 把 scalar 加到每一個維度 只更改到 V(s) 的時候，會全部的 action 都改到，可能會是一個比較有效率的方式，不用 sample 所有的 action 但有可能模型不管 V(s)，直接設 0，只改 A 所以會對 A 下 constrain，讓 network 傾向於改 V 比如同個 state 下的所有 action 要生出 A(s,a) 總和為 0 在 A 的輸出加個 normalization 即可辦到，這個 normalization 就是把每個維度都減掉平均 Prioritized Replay 原本是 uniform 的從 buffer sample data 改讓 「有更大的 TD error」的 data 有更高的機率被 sample TD error 就是 $Q(s_t, a_t)$ 和 target 的差距 實際在做的時候有額外的細節，不會只改 sampling 的 process，還要改 update 參數的方法 Multi-step Balance between MC 和 TD TD 只需要存 {$s_t,a_t,r_t,s_{t+1}$} 改存 {$s_t,a_t,r_t,\u0026hellip;,s_{t+N},a_{t+N},r_{t+N}, s_{t+N+1}$} 我們的目的是要讓 $Q(s_t, a_t)$ 和 $\\displaystyle\\sum_{t^{\u0026rsquo;}=t}^{t+N} r_{t^{\u0026rsquo;}}+\\hat{Q}(s_{t+N+1},a_{t+N+1})$ 越接近越好(後者就是 target) $a_{t+N+1}=arg\\underset{a}{max}\\hat{Q}(s_{t+N+1},a)$ 同時有 MC 和 TD 的好處和壞處 估測的影響比較輕微 r 比較多項，variance 比較大 Noisy Net improve exploration Noise on Action Epsilon Greedy(之前的回顧) $f_X(x) = \\begin{cases} arg \\underset{a}{max}Q(s,a), \u0026amp; \\text{with probability }1-\\varepsilon \\\\ random, \u0026amp; ,otherwise \\end{cases}$ 給同樣的 state，採取的 action 不一定一樣 沒有真實的 policy 會這樣運作 Noise on Parameters $a = arg \\underset{a}{max}\\tilde{Q}(s,a)$\n在每個 episode 剛開始的時候，在 Q-function 的參數上面加上 gaussian noise 給同樣的 state，採取同樣的 action\n叫做 state-dependent exploration explore in a consistent way\nDistributional Q-function Q-function 生出的東西是 cumulated reward 的期望值 所以我們是在對 distribution 取 mean，但不同的 distribution 也可能有同樣的 mean 想做的事情是 model distribution 如果有做這個，就比較不會有 over estimate reward 的結果，反而容易 under estimate，使 double 比較沒用 output 的 range 不可能無限寬，超過邊界的 reward 會被丟掉 Rainbow 綜合一堆方法 Continuous actions Q learning 不容易處理 continuous action Solution sample n 個可能的 a，都丟 Q function 看誰最大\ngradient descent\n把 a 當作 parameter，要找一組 a 去 maximize Q function 運算量大，要 iterative 的 update a 不一定可以找到 global 的最佳解 特別設計 Q network，讓解 optimization 的問題變容易\n範例 Q network 輸出 $\\mu(s)$、$\\Sigma(s)$、$V(s)$，個別是 vector、matrix、scalar a 是 continuous 的 Action，是一個 vector，每個維度都是實數 $\\Sigma(s)$ 是 positive definite 的，實作的時候會把 $\\Sigma$ 和它的 transpose 相乘 $Q(s,a)=-(a-\\mu(s))^T\\Sigma(s)(a-\\mu(s))+V(s)$ $(a-\\mu(s))^T\\Sigma(s)(a-\\mu(s))$ 這項必為正，所以 $a=\\mu(s)$ 的時候就是最佳解 不要用 Q-learning\n","date":"2023-02-20T16:21:23+08:00","permalink":"https://roykesydon.github.io/Blog/p/q-learning/","title":"Q-learning"},{"content":"On/Off-policy On-policy 學習的 agent 和與環境互動的 agent 是同一個 Off-policy 學習的 agent 和與環境互動的 agent 是不同個 想從 On-policy 轉 Off-policy On-policy 每次都要重新蒐集資料，很花時間 由另一個 $\\pi_{\\theta^{\u0026rsquo;}}$ 去 train $\\theta$，$\\theta^{\u0026rsquo;}$是固定的，所以我們可以 re-use sample data Importance Sampling 是一個 general 的想法，不限於 RL\n$E_{x \\text{\\textasciitilde} p}[f(x)]\\approx \\frac{1}{N}\\displaystyle\\sum_{i=1}^N f(x^i)$\n$x^i$ is sampled from p(x) 我們遇到的問題是沒辦法從 p 來 sample data，只能透過 q(x) 去 sample $x^i$\n可以把上式改寫成 $E_{x \\text{\\textasciitilde} p}[f(x)]=E_{x \\text{\\textasciitilde} q}[f(x)\\frac{p(x)}{q(x)}]$\nIssue 雖然理論上 q 可以任意選，只要不要 q(x) 是 0 的時候 p(x) 不是 0，實作上 p 和 q 不能差太多，不然會有問題\n這兩項的 Variance 不一樣，如果 p 除以 q 差距很大，右邊的 Variance 會很大，如果 sample 不夠多次就會有問題 轉換 原本\n$\\triangledown \\overline{R_{\\theta}}=E_{\\tau \\text{\\textasciitilde}p_{\\theta}(\\tau)}[R(\\tau)\\triangledown log p_{\\theta} (\\tau)]$ 改為\n$\\triangledown \\overline{R_{\\theta}}=E_{\\tau \\text{\\textasciitilde}p_{\\theta^{\u0026rsquo;}}(\\tau)}[\\frac{p_{\\theta}(\\tau)}{p_{\\theta^{\u0026rsquo;}}(\\tau)}R(\\tau)\\triangledown log p_{\\theta} (\\tau)]$ 從 $\\theta^{\u0026rsquo;}$ sample 資料 更新 $\\theta$ 多次 Advantage function 原本\n$E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta}}[A^{\\theta}(s_t,a_t)\\triangledown log p_\\theta(a_t^n|s_t^n)]$ 改為\n$E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta^{\u0026rsquo;}}}[\\frac{P_\\theta(s_t,a_t)}{P_{\\theta^{\u0026rsquo;}}(s_t,a_t)}A^{\\theta^{\u0026rsquo;}}(s_t,a_t)\\triangledown log p_\\theta(a_t^n|s_t^n)]$ 要注意 Advantage 的結果要由 $\\theta^{\u0026rsquo;}$ 得出，是 $\\theta^{\u0026rsquo;}$在和環境互動 新的 objective function\n$J^{\\theta^{\u0026rsquo;}}(\\theta)=E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta^{\u0026rsquo;}}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{\u0026rsquo;}}(a_t|s_t)}A^{\\theta^{\u0026rsquo;}}(s_t,a_t)]$ PPO 確保 $\\theta$ 和 $\\theta^{\u0026rsquo;}$ 不會差太多 $J_{PPO}^{\\theta^{\u0026rsquo;}}(\\theta)=J^{\\theta^{\u0026rsquo;}}(\\theta)-\\beta KL(\\theta, \\theta^{\u0026rsquo;})$ 前身 TRPO Trust Region Policy Optimization $J_{TRPO}^{\\theta^{\u0026rsquo;}}(\\theta)=E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta^{\u0026rsquo;}}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{\u0026rsquo;}}(a_t|s_t)}A^{\\theta^{\u0026rsquo;}}(s_t,a_t)], KL(\\theta, \\theta^{\u0026rsquo;})\u0026lt;\\delta$ constrain 很難處理 KL divergence 這邊不是 $\\theta$ 和 $\\theta^{\u0026rsquo;}$ 參數上的距離，而是 behavior 的距離 參數上的距離是指這兩個參數有多像 是給同樣的 state 生出 action 的 distribution 要像 algorithm 初始參數 $\\theta^0$ 每個 iteration 用 $\\theta^k$ 和環境互動，蒐集{$s_t,a_t$}，並計算 advantage $A^{\\theta^k}(s_t,a_t)$\n找出 theta 最佳化 $J_{PPO}(\\theta)$\n$J_{PPO}^{\\theta^{k}}(\\theta)=J^{\\theta^{k}}(\\theta)-\\beta KL(\\theta, \\theta^{k})$ 可以更新很多次 動態調整 $\\beta$\nAdaptive KL Penalty 設可接受的 KL 數值範圍 if $KL(\\theta,\\theta^k)\u0026gt;KL_{max},\\text{increase} \\beta$ if $KL(\\theta,\\theta^k)\u0026lt;KL_{min},\\text{decrease} \\beta$ PPO2 PPO\n$J_{PPO}^{\\theta^{k}}(\\theta)=J^{\\theta^{k}}(\\theta)-\\beta KL(\\theta, \\theta^{k})$ PPO2\n$J_{PPO2}^{\\theta^{k}}(\\theta)\\approx \\displaystyle\\sum_{(s_t,a_t)}min(\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta^k}(a_t|s_t)}A^{\\theta^k}(s_t,a_t), \\\\ clip(\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta^k}(a_t|s_t)}, 1-\\varepsilon, 1+\\varepsilon)A^{\\theta^k}(s_t,a_t))$ ","date":"2023-02-20T12:35:56+08:00","permalink":"https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/","title":"Proximal Policy Optimization(PPO)"},{"content":"Basic Components Actor Policy $\\pi$ is a network with parameter $\\theta$ Env Reward Function Trajectory 在一場遊戲，把 env 輸出的 s 和 actor 輸出的 a 串起來，是一個 Trajectory Trajectory $\\tau$ = {$s_1,a_1,s_2,a_2,\u0026hellip;,s_T,a_T$} $p_{\\theta}(\\tau)=p(s_1)\\displaystyle\\prod_{t=1}^Tp_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$ Update $\\theta \\leftarrow \\theta + \\eta \\triangledown \\overline{R}_{\\theta}$\n$\\triangledown \\overline{R_{\\theta}} = \\displaystyle\\sum_{\\tau} R(\\tau) \\triangledown p_{\\theta} (\\tau) \\\\ =\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{t=1}^{T_n}R(\\tau^n)\\triangledown log p_{\\theta} (a_t^n|s_t^n)$\n實作 常見公式 $\\triangledown f(x)=f(x)\\triangledown logf(x)$ 用當前模型蒐集一堆 Trajectory 更新模型 回到第一步 細節 做一個分類問題，把 state 當作分類器的 Input，把 action 當作分類器的 ground truth 作訓練 在實作分類問題的時候，objective function 都會寫成 minimize cross entropy，就是 maximize log likelihood RL 和一般分類的區別是，要記得在 loss 前面乘上 $R(\\tau^n)$ Tip Add a Baseline $R(\\tau^n)$ 有可能永遠都為正 此時等於告訴 Model 說，今天不管是什麼 action，都要提高它的機率。不一定會有問題，因為雖然都是正的，但正的量有大有小，可能某些 action 上升的幅度會更大。因為我們是在做 sampling，不一定會 sample 到某些 action，本來想的情況是所有的 trajectory 都會出現才沒問題。 解法: 希望 reward 不要總是正的 $\\triangledown \\overline{R_{\\theta}}\\approx \\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{t=1}^{T_n}(R(\\tau^n)-b)\\triangledown log p_{\\theta}(a_t^n|s_t^n)$ $b \\approx E[R(\\tau)]$ Assign Suitable Credit 原本整場遊戲的所有 action 都會乘上 $R(\\tau)$，但這不太公平，因為就算結果是好的，不代表所有 action 都是對的，反之亦然。在理想的情況下，如果 sample 夠多，就可以解決這問題。 解法 只計算從這個 action 後的 reward 總和 因為前面的 reward 和你做了什麼沒關係 接續解法 1，把比較未來的 reward 做 discount 乘某個小於 1 的 $\\gamma^{t^{\u0026rsquo;}-t}$ Advantage function base 可以是 state-dependent，可以根據 network 得出，以後再說 $(Reward-b)$ 可以合起來看做 Advantage function $A^{\\theta}(s_t,a_t)$ 這邊 Reward 不管你是什麼形式，有沒有 discount。 它的意義是，這個 action 相較於其他的 action 有多好，而不是絕對好 這個 A 通常可以由某個類神經網路估計，那個類神經網路叫做 critic，以後講 Actor-Critic 的時候再說 ","date":"2023-02-19T17:16:14+08:00","permalink":"https://roykesydon.github.io/Blog/p/policy-gradient/","title":"Policy Gradient"},{"content":"paper: Masked Autoencoders Are Scalable Vision Learners\nAbstract 這篇論文顯示出 MAE 是 CV 中的 scalable self-supervised learners。\nMAE 的方法很簡單\n隨機蓋住輸入影像的一些 patch 重建 missing pixels 具備兩個核心設計\n非對稱的 encoder-decoder 架構，encoder 只作用於可見的 patch 子集合(沒有 mask tokens)，lightweight decoder 則根據 latent representation 和 make tokens 來重建圖片。 當遮住高比例(比如 75%)的影像時，會得到一個 nontrivial 和 meaningful 的 self-supervisory task 結合這兩點設計，可以有效地訓練大模型。 以 ViT-Huge 用 ImageNet-1K 訓練(訓練集一百多萬張照片)可達到 87.8% 的準確度。\nIntroduction 在 CV 中，常需要大量 labeled images。 NLP 中，自監督預訓練處理了需要大量標註資料的問題。 masked autoencoders 是一種更 general 的 denoising autoencoders 的形式。 BERT 非常成功，autoencoding methods 在 CV 的研究卻落後 NLP，作者思考是什麼讓 masked autoencoding 在 CV 和 NLP 產生不同。 有以下觀點\n直到前陣子，CV 中的 CNN 是主流，但卷積層不好引入 mask tokens 或 positional embedding 這些 indicator。但這些可以透過 ViT 來解決，不應成為問題。 語言和視覺的 Information density 不同，語言是 highly semantic 和 information-dense，使填字本身不是很簡單的事情，但影像含有大量冗餘的訊息，缺失的部分比較好從相鄰的 patch 重建，比如直接插值，所以作者用一種簡單的策略，隨機 mask 很大一部分的 patch，創造一個具有挑戰性的自監督任務，強迫模型關注 global 的資訊。 關於 decoder，CV 還原 pixel，pixel 屬於 lower semantic level，NLP 還原 word，word 的 semantic information 較高。作者發現，雖然在 BERT 中，可以用簡單的 decoder 還原(一個 MLP)，但 CV 中 decoder 的設計就很重要。 基於以上觀點，作者提出 MAE，隨機遮住大量的 patch，並在 pixel space 重建失去的 patch。而且是非對稱 encoder-decoder 架構，encoder 只會看到可見的 patch，但 docoder 除了 latent representation，還會看到 mask tokens。這種設計在非常高的掩蓋率(比如 75%)下不但可以提高準確度，還可以讓 encoder 只處理較少比例(比如 25%)的 patch，將訓練時間減少 3 倍或更多，使 MAE 可以輕鬆擴展成更大的模型。\n在這樣的架構下，用 MAE 的 pre-training，可以訓練非常吃 data 的模型，比如 ViT-Large/-Huge，而只使用 ImageNet-1K。\n用 ImageNet-1K 在 vanilla ViT-Huge 上 fine-tune 可達到 87.8% 準確度，比以往只使用 ImageNet-1K 的結果都高。\n在 obejct detection、instance segmentation、semantic segmentation 上做 transfer learning 都達到不錯的效果，可以打敗用監督式預訓練模型的對手。\n相關工作 Autoencoding MAE 是一種 denoising autoencoding 的形式，但和 DAE 還是差別很大。 Masked image encoding iGPT、ViT、BEiT Approach Masking\n和 ViT 一樣，把圖片切成多個 patch，對於 patch 均勻隨機地採樣保留，剩下地遮住 MAE encoder\nViT 也有 positional embedding MAE decoder\nTransformer block 輸入 encoded visible patches mask tokens shared, learned vector 都會加入 positional embedding 用相較 encoder 輕量的解碼器，所有的 patch 由這個輕量的 decoder 處理，減少預訓練時間 Reconstruction target\ndecoder 的最後一層是 linear projection，之後再 reshape 成你要的 patch loss function mean squared error(MSE) 只算 masked patched 的 MSE，像 BERT Simple implementation\n先取得一系列 token(patch 做 linear projection + positional embedding) randomly shuffle，根據比例移除尾端一部份 encoding 後，尾端接上 mask tokens，並且 unshuffle 加上 positional embedding 後，給 decoder ImageNet Experiments 在 ImageNet-1K 上做自監督的預訓練，然後做\nend-to-end fine-tuning 所有參數都可改 linear probing 只改最後一層線性層 optimal masking ratio 意外地高，相比 BERT 只有 15%\n討論和結論 在 CV 實用的預訓練做法主流是監督式的，CV 中自監督的做法可能正跟著 NLP 的軌跡走。\n要仔細處理圖像和語言的區別，作者去除圖片中很可能不構成 semantic segment 的部分，而不是移除某個 object。\n","date":"2023-02-15T16:08:46+08:00","permalink":"https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/","title":"MAE 論文"},{"content":"paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAbstract 在 CV 領域 transformer 表現有限，目前 attention 常常是和卷積神經網路一起用，或是用來把一些卷積層換成 self-attention，但整體架構不變。這篇論文想展現一個純 Transformer 可以直接在影像分類上表現很好。如果用大量資料作預訓練，再遷移到中小型的資料集，可以和 SOTA 的 CNN 表現得一樣好，還需要較少的訓練資源作訓練。\nIntroduction self-attention-based 架構，特別是 Transformer，已經是 NLP 的重要選擇。主流的作法是在大型文字資料集上作訓練，再針對小型任務資料集作 fine-tune。由於 Transformer 的計算效率高，還有可擴展性，可以 train 一些很大的 model，隨著 model 和資料集增大，目前還沒看出飽和的現象。\n然而在 CV，CNN 還是主流，一些工作嘗試用 self-attention 結合 CNN-like 的架構，比如把 feature map 當 transformer 的輸入，因為原始 pixel 太多，或甚至把卷積層全換成 self-attention，雖然後者理論上效率很高(原論文中有另外 cite 兩篇作法)，但因為他們做法特殊，在現代硬體上很難加速，所以無法很有效地擴展。在 large-scale 的影像識別上， ResNet-like 的架構還是 SOTA。\n該實驗直接把一個標準的 Transformer 作用於圖片上，只作最少的修改。把影像分成多個 patch，並把它們變成一系列的 linear embedding，當作 NLP 中的 tokens(words) 來處理。\n當在中型大小的資料集(e.g. ImageNet)上訓練，如果沒有 strong regularization，ViT 會略輸同等大小的 ResNets\n這篇論文在更大的資料集(14M-300M 的影像)上訓練，就打敗了 inductive bias。在大量資料上作預訓練就很讚。\nRelated Work 大型的 Transformer-based 模型常常是先在大資料集上預訓練然後根據任務 fine-tune，比如 BERT 和 GPT。\n要把 self-attention 用在 CV 上，最簡單的做法就是把每個 Pixel 當一個元素，但 self-attention 是平方複雜度，在現實的圖片很難應用。一個應用 Transformer 的做法是只把 self-attention 用在 local neighborhood，另外一個是用 Sparse Transformer，還有一堆特殊的方法，雖然表現不錯，但要用硬體加速起來不容易。\n另一個有關的模型是 iGPT，在 reduce image resolution 和 color space 後把 transformer 應用在 image pixels 上。它用非監督式訓練後，再 fine-tune 或做 linear probing(只更新最後的 linear layer) 分類任務，表現很好。\n已經有類似的工作了，抽取 patches of size 2 * 2，最後再接 full self-attention，基本上和 ViT 非常像，這篇論文進一步證明了作大規模的預訓練可以讓 Transformer 和 SOTA 的 CNN 相比，而且 ViT 因為 patch 比較大，可以處理 medium-resolution 的圖片。這問題是可預期的，因為 Transformer 缺少了一些 inductive biases。\ninductive biases 一些假設 比如 CNN 常有四個假設 locality translation invariance with pooling layers 平移不變性 translation equivariance f(g(x)) = g(f(x)) 卷積和平移的先後順序沒差 Method 模型盡可能類似原始 Transformer，這樣可以把一些 NLP 上成功的 Transformer 架構拿來用，還可以用一些很有效率的 implementation\nembedding 維度是 768 = 16 * 16 * 3 position embedding 的做法是 standard learnable 1D positional embeddings，就是 BERT 的做法，簡單來說就是生出一張可以訓練的表，(序列長度, embedding size)，作者也有嘗試其他方法，但發現成效差不多，比如 2D positional embedding，概念就是從生出(序列長度, embedding size)變成生出 2 個(sqrt(序列長度), embedding size)。\n[class] 的概念是 NLP 出來的，ResNet-like 的架構常見的做法也有通過 globally average-pooling (GAP)來生出向量，再接上分類器做預測。實驗發現直接在 transformer 的輸出做 GAP 和 [class] 都可以達到不錯的效果。\nConclusion 拿標準的 Transformer 來作 Image recognition，和以往用 self-attention 在 CV 的方法不一樣，除了一開始的 initial patch extraction，沒有引入其他影像特有的 inductive biases。直接把圖片當成是一系列的 patch，然後直接用 Transformer encoder 當一般 NLP 任務處理。在很多影像分類訓練集上表現得更好還在 pre-train 上相對便宜。\n還有一些值得挑戰的地方，比如把 ViT 應用在其他 CV 任務，比如 detection 和 segmentation。另一個挑戰是探索自監督預訓練的方法。這篇論文其實有實驗自監督，表現 OK，但和監督式還是有很大的落差。擴大 ViT 可能有更好的結果。\n","date":"2023-02-12T00:27:55+08:00","permalink":"https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/","title":"ViT 論文"},{"content":"隨機變數之和 Z=X+Y\n$p_Z(z)=\\displaystyle\\sum_{x=-\\infty}^{\\infty}p_{X,Y}(x,z-x)\\\\ =\\displaystyle\\sum_{y=-\\infty}^{\\infty}p_{X,Y}(z-y,y)$\n$f_Z(z)=\\int_{-\\infty}^{\\infty}f_{X,Y}(x,z-x)dx\\\\ =\\int_{-\\infty}^{\\infty}f_{X,Y}(z-y,y)dy$\n如果 X, Y 獨立\n離散\n$p_Z(z)=\\displaystyle\\sum_{x=-\\infty}^{\\infty}p_{X}(x)\\cdot p_Y(z-x)\\\\ =\\displaystyle\\sum_{y=-\\infty}^{\\infty}p_{X}(z-y)\\cdot p_Y(y)$ 這兩個等式是 discrete convolution $=p_X(z) * p_Y(z)$ 連續\n$f_Z(z)=\\int_{-\\infty}^{\\infty}f_{X}(x) f_Y(z-x) dx\\\\ =\\int_{-\\infty}^{\\infty}f_{X}(z-y) f_Y(y) dy$ 這兩個等式是 continuous convolution $=f_X(z) * f_Y(z)$ 如果有 n 個獨立隨機變數\n$X=X_1+X_2+\u0026hellip;+X_n$ 如果 $X_1,\u0026hellip;,X_n$ 獨立 $p_X(x)=p_{X_1}(x) * p_{X_2}(x) * p_{X_3}(x) * \u0026hellip; * p_{X_n}(x)$ 連續做 convolution $f_X(x)=f_{X_1}(x) * f_{X_2}(x) * f_{X_3}(x) * \u0026hellip; * f_{X_n}(x)$ 連續做 convolution MGF moment generating function\nconvolution 很難算\n流程\n如果有多個連續 convolution 也適用下面流程，全部一次一起相乘 給定 $p_{X_1}(x), p_{X_2}(x)$，目標是求 $p_{X_1}(x) * p{X_2}(x)$\n轉換到 MGF\n$\\phi_{X_1}(s)=E \\lbrack e^{sX_1} \\rbrack\\\\ = \\displaystyle\\sum_{x=-\\infty}^{\\infty}e^{sx}\\cdot p_{X_1}(x)$\n$\\phi_{X_2}(s)=E \\lbrack e^{sX_2} \\rbrack$\n相乘 $\\phi_{X_1}(s) \\cdot \\phi_{X_2}(s)$\n逆轉換\n查表 $\\phi_X(s)$ 定義\n$\\phi_X(s)=E \\lbrack e^{sX} \\rbrack = \\begin{cases} \\displaystyle\\sum_{x=-\\infty}^{\\infty} e^{sx} \\cdot p_{X}(x) \u0026amp; 離散, \\\\ \\int_{-\\infty}^{\\infty} e^{sx} \\cdot f_{X}(x)dx \u0026amp; 連續 \\end{cases}$ 性質\nY = aX + b $\\phi_Y(s) = e^{sb} \\cdot \\phi_X(as) $ 常見離散機率分佈的 MGF\n$X$~$Bernoulli(p)$ $\\phi_X(s)=1-p+pe^s$ $X$~$BIN(n, p)$ 作 n 次實驗成功次數等於個實驗室成功次數的總和 $X = X_1 + X_2 + \u0026hellip; + X_n, X_i 獨立, Xi$~$Bernoulli(p)$ $\\phi_{X_i}(s)=1-p+pe^s$ $\\phi_{X}(s)=\\lbrack 1-p+pe^s \\rbrack ^n$ $X$~$Geometric(p)$ 自行推導 $X$~$Pascal(k,p)$ 看到第 k 次成功，花的總實驗室次數等於第 1 號成功花多少次 + 第 2 號 +\u0026hellip;+ 第 k 號 $X = X_1 + X_2 + \u0026hellip; + X_n, X_i 獨立, Xi$~$Gemetric(p)$ $X$~$Exponential(\\lambda)$ 自行推導 $X$~$Erlang(n,\\lambda)$ $X = X_1 + X_2 + \u0026hellip; + X_n, X_i 獨立, Xi$~$Exponential(\\lambda)$ 多個隨機變數之和 獨立隨機變數之和 $X_1, X_2, \u0026hellip;$獨立，且各自有一模一樣的機率分佈 { $X_i$ } $I.I.D.$\nIndependently and Identically Distributed $X = X_1+X_2+\u0026hellip;+X_n$，n 為常數，請問 X 的機率分佈\n$p_X(x)=p_{X_1}(x) * p_{X_1}(x) * p_{X_1}(x) * \u0026hellip; * p_{X_1}(x)$ $f_X(x)=f_{X_1}(x) * f_{X_1}(x) * f_{X_1}(x) * \u0026hellip; * f_{X_1}(x)$ 因為他們機率分佈一模一樣，所以底下都是 $X_1$ $\\phi_X(s)=\\lbrack \\phi_{X_1}(s) \\rbrack ^n$ e.g. 假設壽司理想重量是 13g，抓飯量是常態分佈，期望值是 14，標準差是 3，每天要作 100 個，每天飯量的機率分佈是?\n$X_i$ : 第 i 個壽司的飯量，{ $X_i$ } I.I.D. $X_i$~$N(14,9)\\\\ \\Rightarrow \\phi_{X_i}(s)=\\phi_{X_1}(s)\\\\ =e^{\\mu S + \\frac{\\sigma^2}{2}s^2} = e^{14 s + \\frac{9}{2}s^2}$ $X=X_1+X_2+\u0026hellip;+X_{100}$ $\\phi_X(s)=\\lbrack \\phi_{X_1}(s) \\rbrack^{100}\\\\ =e^{1400 s + \\frac{900}{2}s^2}$ 這個東西是 $X$~$N(1400,900)$ 的 MGF，所以可以逆推回來機率分佈 隨機變數之獨立隨機變數和 $X_1,X_2,\u0026hellip;I.I.D.$\n$X = X_1 + X_2 + \u0026hellip; + X_N$\nN 本身也是隨機變數，其機率分佈已知\n$\\phi_X(s)=\\phi_N(ln(\\phi_{X_1}(s)))$\n中央極限定理 central limit theorem(CLT)\n若 $X_1,X_2,\u0026hellip;,X_n$ 為 $I.I.D.$，當 n 趨近於無窮大時\n$X=X_1+X_2+\u0026hellip;+X_n$~$N(\\mu_{X_1+X_2\u0026hellip;+X_n}, \\sigma^2_{X_1+X_2+\u0026hellip;+X_n})$ $\\mu_{X_1+X_2+\u0026hellip;+X_n}=\\mu_{X_1}+\\mu_{X_2}+\u0026hellip;+\\mu_{X_n}=n\\mu_{X_1}$ $\\sigma^2_{X_1+X_2+\u0026hellip;+X_n}=\\sigma^2_{X_1}+\\sigma^2_{X_2}+\u0026hellip;+\\sigma^2_{X_n}=n\\sigma^2_{X_1}$ 應用\n要處理多個獨立的隨機變數的和時，可以用 CLT 將其機率分佈近似為常態分佈後計算機率 比如雜訊常當作常態分佈 如果某機率分佈等於多個獨立隨機變數的和，此機率分佈可以用常態分佈近似，再算機率 e.g. $X$~$BIN(100,0.3)$ $X=X_1+X_2+\u0026hellip;+X_100$ {$X_i$} $I.I.D., X_i$~$Bernoulli(0.3)$ 範例\n天團粉絲有 0.2 的機率買 CD，共有100萬個粉絲，發售 CD 超過 200800 張的機率為何 $X$~$BIN(1000000,0.2)$ $P(X\u0026gt;200800)=\\displaystyle\\sum_{x=200801}^{10^6}(\\overset{1000000}{x})0.2^x0.8^{10^6-x}$ $(\\overset{1000000}{x})=\\frac{1000000!}{200801!799199!}$ 算不出來 $X=X_1+X_2+\u0026hellip;+X_{1000000}, X_i$~$Bernoulli(0.2)\\\\ \\Rightarrow \\mu_{X_1}=0.2, \\sigma_{X_1}^2=0.16$ By CLT $\\Rightarrow X$~$N(200000,160000)$ $P(X\u0026gt;200800)\\\\ =P(\\frac{X-200000}{400} \u0026gt; \\frac{200800-200000}{400})\\\\ =P(Z\u0026gt;2) =Q(2) \\approx0.023$ De Moivre - Laplace Formula 如果是離散的隨機變數和，可以算的更精確 $P(k_1 \\le X \\le k_2) \\approx \\Phi(\\frac{k_2+0.5-n\\mu_{X_1}}{\\sqrt{n}\\sigma_{X_1}}) - \\Phi(\\frac{k_1-0.5-n\\mu_{X_1}}{\\sqrt{n}\\sigma_{X_1}})$ ","date":"2023-02-05T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iv/","title":"機率論 - IV"},{"content":"隨機變數的函數 隨機變數 X 的任意函數 g(x) 也是一個隨機變數，常被稱為 Derived Random Variable 求 g(x) 的機率分佈 X 是離散 直接推 g(X) 的 PMF X 是離散隨機變數，Y = g(X) 也是離散隨機變數 $p_{g(X)}(y) = \\displaystyle\\sum_{會讓g(x)=y 的所有x}p_X(x)$ X 是連續 先推 g(x) 的 CDF，再微分得 PDF\n先算 g(X) 的 CDF $F_{g(X)}(y)=P\\lbrack g(X) \\le y \\rbrack$ 若 g(X) 可以微分，再對 y 微分得 PDF $f_{g(X)}(y)=\\frac{d}{dy}F_{g(X)}(y)$ e.g. 若 Y=3X+2，請問 Y 的 PDF 與 $f_X(x) 的關係?$\n$F_Y(y)=P(Y \\le y)\\\\ =P(3X+2 \\le y)\\\\ =P(X \\le \\frac{y-2}{3})\\\\ =F_X(\\frac{y-2}{3})$ $f_Y(y)=\\frac{d}{dy}F_Y(y)\\\\ =\\frac{d}{dy}F_X(\\frac{y-2}{3})\\\\ =\\frac{dF_X(\\frac{y-2}{3})}{d(\\frac{y-2}{3})} \\cdot \\frac{d \\frac{y-2}{3}}{dy}\\\\ =f_X(\\frac{y-2}{3}) \\cdot \\frac{1}{3}$ 若 Y=aX+b\n$f_Y(y)=\\frac{1}{|a|}f_X(\\frac{y-b}{a})$ 條件機率分佈 若 X 是離散隨機變數，PMF 是 $p_X(x)$，某事件 B 已發生 PMF: $p_{X|B}(x)= x = \\begin{cases} x \\in B: \u0026amp; \\frac{p_X(x)}{p(B)}, \\ x \\notin B: \u0026amp; 0 \\end{cases}$ CDF: $F_{X|B}(x)\\\\ =\\displaystyle\\sum_{u \\le x}p_{X|B}(u)\\\\ =\\displaystyle\\sum_{u \\le x, u \\in B} \\frac{p_X(u)}{P(B)}$ 若 X 是連續隨機變數，某事件 B 已發生 PDF: $f_{X|B}(x)\\\\ =\\begin{cases} x \\in B: \u0026amp; \\frac{f_X(x)}{P(B)}, \\ x \\notin B: \u0026amp; 0 \\end{cases}$ CDF: $F_{X|B}(x)\\\\ =\\int_{-\\infty \\le u \\le x, u \\in B} \\frac{f_X(u)}{P(B)} du$ 條件期望值 Conditional Excpectation $E \\lbrack X|B \\rbrack\\\\ =\\begin{cases} \\displaystyle\\sum_{x=-\\infty}^{\\infty} x \\cdot p_{X|B}(x) \u0026amp; 離散, \\\\ \\int_{-\\infty}^{\\infty} x \\cdot f_{X|B}(x)dx \u0026amp; 連續 \\end{cases}$\n$E \\lbrack g(X)|B \\rbrack\\\\ =\\begin{cases} \\displaystyle\\sum_{x=-\\infty}^{\\infty} g(x) \\cdot p_{X|B}(x) \u0026amp; 離散, \\\\ \\int_{-\\infty}^{\\infty} g(x) \\cdot f_{X|B}(x)dx \u0026amp; 連續 \\end{cases}$\n$Var(X|B) = E\\lbrack X^2 | B \\rbrack - (\\mu_{X|B})^2$\n失憶性 Memoryless Geometric 和 Exponential 機率分佈都有失憶性 不管事情已經進行多久，對於事情之後的進行一點影響都沒有 聯合機率分佈 joint probability distribution 同時考慮多個隨機變數的機率分佈 Joint PMF X, Y 皆為離散，聯合PMF\n$p_{X,Y}(x,y)=P(X=x, Y=y)$ 性質\n$0 \\le p_{X,Y}(x,y) \\le 1$ $\\Sigma^{\\infty}{x=-\\infty}\\Sigma^{\\infty}{y=-\\infty} p_{X,Y}(x,y)=1$ X, Y 獨立 $P_{X,Y}(x,y)\\\\ =P(X=x,Y=y)\\\\ =P_X(x)P_Y(y)$ 對任何事件 B $P(B)=\\Sigma_{(x,y)\\in B}P_{X,Y}(x,y)$ Joint CDF $F_{X,Y}(x,y)=P(X \\le x, Y \\le y)$\n性質\n$0 \\le F_{X,Y}(x, y) \\le 1$ 若 $x_1 \\le x_2$ 且 $y_1 \\le y_2$，則 $F_{X,Y}(x_1,y_1) \\le F_{X,Y} (x_2, y_2)$ $F_{X,Y}(x, \\infty) = F_X(x)$ $F_{X,Y}(\\infty, y) = F_Y(y)$ $F_{X,Y}(\\infty, \\infty) = 1$ $F_{X,Y}(x, -\\infty)\\\\ = P(X \\le x, Y \\le -\\infty)\\\\ \\le P(Y \\le -\\infty) \\\\ = 0$ $F_{X,Y}(-\\infty, y) = 0$ $P(x_1 \u0026lt; X \\le x_2, y_1 \u0026lt; Y \\le y_2)\\\\ =F_{X,Y}(x_2,y_2)-F_{X,Y}(x_2,y_1)-F_{X,Y}(x_1,y_2)+F_{X,Y}(x_1,y_1)$ Joint PDF $f_{X,Y}(x,y)= \\frac{\\partial^2F_{X,Y}(x,y)}{\\partial x \\partial y}$\n$F_{X,Y}(x,y) = \\int_{-\\infty}^{x} \\int_{-\\infty}^{y} f_{X,Y}(u,v)dv du$\n性質\n$f_{X,Y}(x,y) \\ge 0$ $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dxdy=1$ 如果 X,Y 獨立 $f_{X,Y}(x,y)=f_X(x) \\cdot f_Y(y)$ 對任何事件 B $P(B)=\\int\\int_{(x,y)\\in B}f_{X,Y}(x,y)dxdy$ 邊際 PMF Marginal PMF 已知聯合 PMF : $p_{X,Y}(x,y)$，求 $p_X(x), p_Y(y)$，稱為邊際 PMF $p_X(x)=\\displaystyle\\sum_{y=-\\infty}^{\\infty}P_{X,Y}(x,y)$ $p_Y(y)=\\displaystyle\\sum_{x=-\\infty}^{\\infty}P_{X,Y}(x,y)$ 已知聯合 PDF : $p_{X,Y}(x,y)$，求 $f_X(x), f_Y(y)$，稱為邊際 PDF $f_X(x)=\\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dy$ $f_Y(y)=\\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dx$ 雙變數期望值 聯合 PMF 下的期望值\n$E\\lbrack h(X,Y) \\rbrack = \\displaystyle\\sum_{x=-\\infty}^{\\infty}\\displaystyle\\sum_{y=-\\infty}^{\\infty}h(x,y)\\cdot p_{X,Y}(x,y)$ h(X,Y) 也可以只和 X 有關，比如它可以是 $x^2$ 聯合 PDF 下的期望值\n$E\\lbrack h(X,Y) \\rbrack = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}h(x,y)\\cdot f_{X,Y}(x,y) dxdy$ e.g. 已知 $f_{X,Y}(x,y)=\\begin{cases} 0.5, \u0026amp; \\text{if } 0 \\le y \\le x \\le 2, \\\\ 0, \u0026amp; otherwise \\end{cases}$ $E \\lbrack X + Y \\rbrack \\\\ = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x+y)\\cdot f_{X,Y}(x,y) dxdy\\\\ = \\int_{0}^{2}\\int_{y}^{2}(x+y)\\cdot 0.5 dxdy$ 期望值性質\n$E\\lbrack \\alpha h_1(X,Y)+ \\beta h_2(X,Y) \\rbrack\\\\ =\\alpha E\\lbrack h_1(X,Y)\\rbrack + \\beta E\\lbrack h_2(X,Y) \\rbrack$ 若 X,Y 獨立 $E\\lbrack g(X)h(Y) \\rbrack = E \\lbrack g(X) \\rbrack \\cdot E \\lbrack h(Y) \\rbrack$ Variance 性質\n$Var(X+Y)=Var(X)+Var(Y)+2 \\cdot Cov(X,Y)$ $Cov(X,Y)=E\\lbrack (X-\\mu_X)(Y -\\mu_Y) \\rbrack$ 如果 X, Y 獨立 $2E\\lbrack (X-\\mu_X)(Y -\\mu_Y) \\rbrack \\\\ = 2E\\lbrack (X-\\mu_X) \\rbrack E\\lbrack (Y -\\mu_Y) \\rbrack \\\\ = 0$ ","date":"2023-02-02T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iii/","title":"機率論 - III"},{"content":"機率密度函數 PDF probability density function PMF 在 連續R.V. 上，假如 $X\\text{\\textasciitilde}[0,1)$，$p_X(0.7)$ = 0，因為有無窮多個數字 公式 $f_X(x)=\\lim\\limits_{\\Delta x \\rightarrow 0} \\frac{P(x \\le X \\le x + \\Delta x)}{\\Delta x} \\\\ = \\lim\\limits_{\\Delta x \\rightarrow 0} \\frac{F_X(x+\\Delta x) - F_X(x)}{\\Delta x} \\\\ = F^{\\prime}_X(x) $ 和 CDF 的關係 $CDF: F_X(x) = PDF: f_X(x)$ $\\int^x_{-\\infty}$ 可以從 PDF 轉到 CDF\n$\\frac{d}{dx} 可以從 CDF 轉到 PDF$\n跟機率的關係 $P(a \u0026lt; X \\le b) = F_X(b) - F_X(a) \\\\ = \\int^b_{-\\infty} f_X(x)dx - \\int^a_{-\\infty} f_X(x)dx \\\\ = \\int^a_b f_X(x)dx$ $f_X(x)=\\lim\\limits_{\\Delta x \\rightarrow 0} \\frac{P(x \\le X \\le x + \\Delta x)}{\\Delta x}$ 當 $\\Delta x$ 很小時 $P(x \\le X \\le x + \\Delta x) \\approx f_X(x) \\cdot \\Delta x$ 性質 $f_X(x) = F^{\\prime}_X(x)$ $F_X(x)=\\int^x_{-\\infty}f_X(u)du$ $P(a \\le X \\le b)=\\int^b_a f_X(x) dx$ $\\int^{\\infty}_{-\\infty}f_X(x)dx=1$ $f_X(x) \\ge 0$ $f_X(x)$ 可以比 1 大 連續機率分佈 Uniform 機率分佈 $X \\text{\\textasciitilde}UNIF(a,b)$ PDF $f_X(x) = \\begin{cases} \\frac{1}{b-a} \u0026amp; ,a \\le x \\le b \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 0 \u0026amp; ,x \\le a \\\\ \\frac{x-a}{b-a} \u0026amp; ,a \u0026lt; x \\le b\\\\ 1 \u0026amp; ,x \u0026gt; b \\end{cases}$ Exponential 機率分佈 有失憶性(memoryless)，常被用來 model 有這種性質的事情 $X \\text{\\textasciitilde}Exponential(\\lambda)$ PDF $f_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} \u0026amp; ,x \\ge 0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = 1-e^{-\\lambda x}$ Erlang 機率分佈 Gamma Distribution $X \\text{\\textasciitilde}Erlang(n,\\lambda)$ PDF $f_X(x) = \\begin{cases} \\frac{1}{(n-1)!}\\lambda^n x^{n-1} e^{-\\lambda x} \u0026amp; ,x \\ge 0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ $f_X(x)=(\\lambda e^{-\\lambda x}) * (\\lambda e^{-\\lambda x}) * \u0026hellip; * (\\lambda e^{-\\lambda x})$ 自己和自己做 n 次 convolution CDF $F_X(x) = \\begin{cases} 1 - \\Sigma^{n-1}_{k=0}\\frac{(\\lambda x)^k}{k!}e^{-\\lambda x} \u0026amp; ,x \\ge 0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ 常見用法 用來 model 一件有多個關卡事情的總時間，而每個關卡所需時間是隨機的 關卡數: n 每關卡所需時間之機率分佈 $Exponential(\\lambda)$ e.g. 打電動過三關所需時間 $Erlang(3, \\lambda)$ Normal 機率分佈 (常態分佈) 在自然界常出現\n常被用做「很多隨機量的總和」的機率模型\n又稱 Gaussian 機率分佈\n$X \\text{\\textasciitilde}Gaussian(\\mu,\\sigma)$\nPDF\n$f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ 也常用 $X \\text{\\textasciitilde}N(\\mu,\\sigma^2)$\n注意 $\\sigma$ 不一樣 CDF\n太難算，積不出來\n針對某組特別的 $\\mu, \\sigma$ 的 CDF 建表，把其他常態分佈的 CDF 和這組產生關聯 標準常態分佈\n$Z \\text{\\textasciitilde}N(0,1)$ $f_Z(z)=\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{z^2}{2}}$ CDF 表示為 $\\Phi(z)$ $\\Phi(z)=\\int^z_{-\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}}du$\n積不出來，以數值方法近似出來後建表給人家查 查 standard normal table e.g. $F_Z(1.325)=?$\n查表 $F_Z(1.32)=0.9066$，$F_Z(1.33)=0.9082$ 用內插約略得 0.9074 性質\n$\\Phi(-z) = 1 - \\Phi(z)$ 任意 $\\mu, \\sigma$ 的 CDF\n對任何 $X \\text{\\textasciitilde}N(\\mu,\\sigma^2)$ $\\frac{X-\\mu}{\\sigma}\\text{\\textasciitilde}N(0,1)$ $F_X(x)=\\Phi(\\frac{x-\\mu}{\\sigma})$ 期望值 Expectation 大數法則 $P(A)=\\lim\\limits_{N \\rightarrow \\infty}\\frac{N_A}{N}$ 基本上期望值是利用大數法則算的 mean 值，雖然平均值是 R.V.，但當實驗無窮多次時，會收斂到常數，因此以這為估算值 Mean 值又稱做期望值 離散隨機變數 $E\\lbrack X \\rbrack=\\mu_X=\\displaystyle\\sum^{\\infty}_{x=-\\infty}x \\cdot P_X(x)$ 離散隨機變數的函數的期望值 對離散隨機變數 X 而言，其任意函數 g(x) 也是一隨機變數，也有期望值 $g(X)$ 的期望值定義為 $E \\lbrack g(X) \\rbrack=\\displaystyle\\sum^{\\infty}_{x=-\\infty}g(x)\\cdot P_X(x)$ 性質 $E\\lbrack \\alpha g(X) \\rbrack = \\alpha \\cdot E \\lbrack g(X) \\rbrack$ $E\\lbrack \\alpha g(X) + \\beta h(X) \\rbrack \\\\ =\\alpha \\cdot E \\lbrack g(X) \\rbrack + \\beta \\cdot E \\lbrack h(X) \\rbrack$ $E\\lbrack \\alpha \\rbrack = \\alpha$ 常見隨機變數函數的期望值 $X$ 的 $n^{th} moment$ $E \\lbrack X^n \\rbrack = \\displaystyle\\sum^{\\infty}_{x=-\\infty}x^n \\cdot P_X(x)$ X 的變異數(variance) $E \\lbrack (X-\\mu_X)^2 \\rbrack = \\displaystyle\\sum^{\\infty}_{x=-\\infty} (x-\\mu_X)^2 \\cdot P_X(x)$ 變異數 Variance Variance 通常符號表示為 $\\sigma^2_X=E \\lbrack (X-\\mu_X)^2 \\rbrack$ 隱含隨機變數 X 多「亂」的資訊 variance 大的話，X 不見得接近 $\\mu_X$ 變異數開根號是標準差(standard deviation) $\\sigma_X = \\sqrt{Variance} \\ge 0$ 算法 $\\sigma^2_X=E \\lbrack X^2 \\rbrack - \\mu^2_X\\\\ \\Rightarrow E \\lbrack X^2 \\rbrack = \\sigma^2_X + \\mu^2_X$\n常見離散分佈的期望值 / 變異數 $X\\text{\\textasciitilde}Bernouli(p)$\n$\\mu_X=1 \\cdot p + 0 \\cdot (1-p) \\\\ = p$\n$\\sigma^2_X = E \\lbrack X^2 \\rbrack - \\mu^2_X \\\\ = \\displaystyle\\sum^1_{x=0}x^2\\cdot p_X(x)-\\mu_X^2 \\\\ =1^2 \\cdot p + 0^2 \\cdot (1-p) - p^2\\\\ =p(1-p)$\n$X$~$BIN(n,p)$\n$\\mu_X = np$\n$\\sigma^2_X = np(1-p)$\n$X$~$GEO(p)$\n$\\mu_X = \\frac{1}{p}$\n$\\sigma^2_X = \\frac{(1-p)}{p^2}$\n$X$~$PASKAL(k,p)$\n$\\mu_X = \\frac{k}{p}$\n$\\sigma^2_X = \\frac{k(1-p)}{p^2}$\n$X$~$POI(\\alpha)$\n$\\mu_X = \\alpha$\n$\\sigma^2_X = \\alpha$\n$X$~$UNIF(a,b)$\n$\\mu_X = \\frac{a+b}{2}$\n$\\sigma^2_X = \\frac{1}{12}(b-a)(b-a+2)$\n連續隨機變數 對連續的隨機變數 X 而言，將 X 的值以 $\\Delta$ 為單位無條件捨去來近似，以隨機變數 Y 表示(當 $\\Delta \\rightarrow$ 0 時，$X \\approx Y$)，然後再當做 PMF 處理。\n$E \\lbrack X \\rbrack = \\int^{\\infty}_{-\\infty}xf_X(x)dx$ 連續隨機變數的函數的期望值 對連續隨機變數 X 而言，其任意函數 g(x) 也是一隨機變數，也有期望值 $g(X)$ 的期望值定義為 $E \\lbrack g(X) \\rbrack=\\int^{\\infty}_{-\\infty}g(x)\\cdot f_X(x)dx$ 性質 $E\\lbrack \\alpha g(X) \\rbrack = \\alpha \\cdot E \\lbrack g(X) \\rbrack$ $E\\lbrack \\alpha g(X) + \\beta h(X) \\rbrack \\\\ =\\alpha \\cdot E \\lbrack g(X) \\rbrack + \\beta \\cdot E \\lbrack h(X) \\rbrack$ $E\\lbrack \\alpha \\rbrack = \\alpha$ 常見隨機變數函數的期望值 $X$ 的 $n^{th} moment$ $E \\lbrack X^n \\rbrack = \\int^{\\infty}_{-\\infty}x^n \\cdot f_X(x)dx$ X 的變異數(variance) $E \\lbrack (X-\\mu_X)^2 \\rbrack = \\int^{\\infty}_{-\\infty} (x-\\mu_X)^2 \\cdot f_X(x)dx$ 變異數 Variance 和離散隨機變數的資訊一樣 常見連續分佈之期望值/變異數 $X$~$Exponential(\\lambda)$\n$\\mu_X = \\frac{1}{\\lambda}$\n$\\sigma^2_X = \\frac{1}{\\lambda^2}$\n$X$~$Erlang(n, \\lambda)$\n$\\mu_X = \\frac{n}{\\lambda}$\n$\\sigma^2_X = \\frac{n}{\\lambda^2}$\n$X$~$Gaussian(\\mu,\\sigma)$\n$\\mu_X = \\mu$\n$\\sigma^2_X = \\sigma^2$\n$X$~$UNIF(a,b)$\n$\\mu_X = \\frac{a+b}{2}$\n$\\sigma^2_X = \\frac{1}{12}(b-a)^2$\n","date":"2023-02-01T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-ii/","title":"機率論 - II"},{"content":"集合論 名詞 子集合(Subset) B 是 C 的子集(B 不能等於 C) $B \\subset C$ 補集(Complement) C 是 A 的補集 $C=A^C$ 不相交(Disjoint) $X \\cap Y = \\{\\}$ 互斥(Mutually Exclusive) 一群集合 $X_1, X_2, \u0026hellip;, X_n$ 中任選兩個集合 $X_i, X_j$ 都不相交，則 $X_1, X_2, \u0026hellip;, X_n$ 這群集合互斥 公式 De Morgan\u0026rsquo;s Law ${(A \\cup B)}^C=A^C \\cap B^C$ 機率名詞 Outcome (結果) 實驗中可能的結果 Sample Space (樣本空間) 機率實驗所有可能的結果的集合，常以 $S$ 表示 Event (事件) 對於實驗結果的某種敘述 事件可以看做是 outcome 的集合，也是 sample space 的子集 機率是一個函數，其自變數是 event，故可看做是一個映射 公理 Axioms 對任何事件 $A$ 而言, $P(A) \\geq 0$\n$P(S) = 1$\n事件 $A_1, A_2, \u0026hellip;$ 互斥 $\\Rightarrow$ $P(A_1 \\cup A_2 \\cup A_3 \\cup \u0026hellip;)$\n$=P(A_1)+P(A_2)+P(A_3)+\u0026hellip;$\n衍生公式 Boole\u0026rsquo;s 不等式\n對任意 $n$ 個事件 $A_1, A_2, \u0026hellip;, A_n$ 而言 $P(\\cup^n_{i=1}A_i \\leq \\Sigma^n_{i=1}P(A_i))$ Bonferroni\u0026rsquo;s 不等式\n對任意 $n$ 個事件 $A_1, A_2, \u0026hellip;, A_n$ 而言 $P(\\cap^n_{i=1} A_i) \\geq 1 - \\Sigma^n_{i=1} P(A^C_i)$ 條件機率 公式 $P(X|Y) = \\frac{P(X \\cap Y)}{P(Y)}$ $P(X \\cap Y) = P(X|Y) * {P(Y)} = P(Y|X) * P(X)$ 性質 $P(X|Y) \\geq 0$ $P(Y|Y) = 1$ $A, B$ 互斥 $\\Rightarrow P(A \\cup B |Y) = \\frac{P(A)}{P(Y)} + \\frac{P(B)}{P(Y)} = P(A|Y)+P(B|Y)$ 定理 Total Probability 定理 若 $C_1, C_2, \u0026hellip;, C_n$ 互斥且 $C_1 \\cup C_2 \\cup \u0026hellip; \\cup C_n = S$，則對任意事件 $A$ $P(A) = P(A|C_1)P(C_1) + P(A|C_2)P(C_2) + \u0026hellip; + P(A|C_n)P(C_n)$ Bayes\u0026rsquo; Rule 貝式定理 若 $C_1, C_2, \u0026hellip;, C_n$ 互斥且 $C_1 \\cup C_2 \\cup \u0026hellip; \\cup C_n = S$，則對任意事件 $A$ $P(C_j|A)=\\frac{P(A|C_j) * P(C_j)}{\\Sigma^n_{i=1}P(A|C_i)*P(C_i)}$\n$= \\frac{P(C_j \\cap A)}{P(A)}$\n獨立性 Independence 若兩事件 $A, B$ 之機率滿足\n$P(A \\cap B) = P(A) * P(B)$ 或以 $P(A|B) = P(A)$ 表示 則 $A, B$ 兩事件稱為機率上的獨立事件\n若事件 $A_1, A_2, \u0026hellip; A_n$ 滿足下列條件，則稱此 $n$ 事件獨立 $(n\u0026gt;2)$\n從中任選 $m$ 事件 $A_{i_1}, A_{i_2}, \u0026hellip; A_{i_m}$ 均滿足 $P(A_{i_1} \\cap A_{i_2} \\cap \u0026hellip; \\cap A_{i_m}) = P(A_{i_1})P(A_{i_2})\u0026hellip;P(A_{i_m}) , m=2, 3, \u0026hellip;, n$ 排列組合 二項式係數(binomial coefficient) $(^n_k)$ 有 $n$ 個異物，從中取出 $k$ 個 多項式係數(multinomial coefficient) $\\frac{n!}{n_1!n_2!\u0026hellip;n_m!}$ 有 m 種異物，每次選物從中選一後放回，依序選 n 次，共有 $m^n$ 種 outcome，在所有實驗結果中，第一種出現 $n_1$ 次，以此類推，這樣的實驗結果有多少種 隨機變數 Random Variable, R.V. 用來把 outcome 數字化的表示方式 通常用大寫英文字母 是將 outcome 轉成對應數字的函數 $X: S \\rightarrow R$ 從樣本空間映射到實數 隨機變數的函數，也是一個隨機變數 種類 離散隨機變數 (Discrete R.V.)\n值是有限個，或是「可數的」無窮多個 連續隨機變數 (Continuous R.V.)\n值有無窮多個，而且「不可數」 可數、不可數 可數 包含的東西可一個個被數，總有一天會被數到 e.g. 正偶數集合 不可數的 不管怎麼數，裡面一定有個東西會沒數到 e.g. 0~1 之間的所有數字 累積分佈函數 CDF cumulative distribution function\n對任一個隨機變數 $X$，定義 CDF 為\n$F_X(x) \\overset{def}{=}P(X \\leq x)$ 永遠用 $F$ 表示 常見用途\n算 X 落在某範圍的機率 $P(A \u0026lt; X \\le b) = F_X(b)-F_X(a)$ $P(A \\le X \\le b) = F_X(b)-F_X(a)+P(X=a)$ $P(A \u0026lt; X \u0026lt; b) = P(A \u0026lt; X \\le b^-)$ 性質 離散隨機變數的 CDF $F_X(x^+)=F_X(x)$ $F_X(x^-)=F_X(x)-P(X=x)$ 連續隨機變數的 CDF $F_X(x^-)=F_X(x)=F_X(x^+)$ 共同 $F_X(- \\infty)=P(X \\le - \\infty)=0$ $F_X(\\infty)=P(X \\le \\infty) = 1$ $0 \\le F_X(x) \\le 1$ 機率質量函數 PMF probability mass function 對任一個「離散」隨機變數 $X$，其 PMF 為 $p_X(x) \\overset{def}{=}P(X=x)$ PMF 和 CDF 的關係 對任何 $x$ $F_X(x) = \\displaystyle\\sum^{\\lfloor x \\rfloor}_{n=-\\infty}p_X(n)$ $P_X(x)=F_X(x^+)-F_X(x^-)$ 機率分佈(Probability Distribution) PMF 和 PDF 都是一種機率分佈 將總和為 1 的機率分佈在點上 離散機率分佈 Bernoulli 機率分佈 1 次實驗，2 種結果，在意某結果發生與否 $X \\text{\\textasciitilde}Bernoulli(p)$ PMF $p_X(x) = \\begin{cases} p \u0026amp; ,x=1 \\\\ 1-p \u0026amp; x=0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 0 \u0026amp; ,x\u0026lt;0 \\\\ 1-p \u0026amp; 0 \\leq x \u0026lt;1 \\\\ 1 \u0026amp; ,x \\geq 1 \\end{cases}$ Binomial 機率分佈 實驗成功機率為 p，做 n 次實驗，X 表成功次數 $X \\text{\\textasciitilde}BIN(p)$ PMF $p_X(x) = (^n_x)p^x(1-p)^{n-x}$ 成功 $x$ 次 CDF $F_X(x) = \\displaystyle\\sum^{\\lfloor x \\rfloor}_{m=-\\infty} (^n_m)\\cdot p^m \\cdot (1-p)^{n-m}$ Uniform 機率分佈 1 次實驗，n 種結果，各結果機率均等，在意某結果發生否 $X \\text{\\textasciitilde}UNIF(a,b)$ PMF $p_X(x) = \\begin{cases} \\frac{1}{b-a+1} \u0026amp; ,x=a,a+1,\u0026hellip;,b \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 0 \u0026amp; ,x\u0026lt;a \\\\ \\frac{\\lfloor x \\rfloor - a + 1}{b-a+1} \u0026amp; ,a \\leq x\u0026lt; b\\\\ 1 \u0026amp; ,x \\geq b \\end{cases}$ Geometric 機率分佈 若實驗成功機率為 p，到成功為止，做了 X 次嘗試 有失憶性 $X \\text{\\textasciitilde}Geometric(p)$ PMF $p_X(x) = \\begin{cases} (1-p)^{x-1} \\cdot p \u0026amp; ,x=1, 2, 3, \u0026hellip; \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 1-(1-p)^{\\lfloor x \\rfloor} \u0026amp; ,x \\ge 1 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ Pascal 機率分佈 若實驗成功機率為 p，到第 k 次成功為止，共做了 X 次嘗試 $X \\text{\\textasciitilde}Pascal(k, p)$ PMF $p_X(x) = \\begin{cases} \\binom{x-1}{k-1}(1-p)^{x-k} p^k \u0026amp; ,x=k, k+1, \u0026hellip; \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = P(X \\le x) \\\\ = P(在 x 次實驗中 \\ge k 次成功)\\\\ = P(Y \\ge k), Y~BIN (x,p) \\\\ $ 故 Pascal 又稱 Negative Binomial Poisson 機率分佈 已知某事發生速率為每單位時間 $\\lambda$ 次，觀察時間為 $T$ 時間單位，$X$ 為該觀察時間內發生該事的總次數。 $X \\text{\\textasciitilde}POI(\\lambda T)$ 有時候也會以 $\\mu$ 來表示 $\\lambda T$ PMF $p_X(x) = e^{-\\lambda T} \\cdot \\frac{(\\lambda T)^x}{x!}$ CDF $F_X(x) = \\begin{cases} \\displaystyle\\sum^{\\lfloor x \\rfloor}_{n=-\\infty}e^{-\\lambda T} \\cdot \\frac{(\\lambda T)^n}{n!} \u0026amp; ,x = 0,1,2,\u0026hellip; \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ ","date":"2023-01-31T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-i/","title":"機率論 - I"},{"content":"Share information between processes 透過硬碟上的文件溝通 超慢 透過 kernel buffer 滿快的，但這樣要一直在 user mode 和 kernel mode 來回切換，因為kernel buffer 在 kernel space 透過 shared memory region shared memory region 在 user space Mechanisms Signals\nCommunication\nData transfer Byte Stream Pipes FIFOs(Named Pipes) stream sockets Message Passing SystemV MsgQ POSIX MsgQ datagram sockets Shared Memory SystemV S.M POSIX S.M Memory Mapping anonymous memory mapping memory mapped file Synchronization\nPipes Related processes parent-child sibling Executing on same machine 用法 cmd1 | cmd2 cmd1 不是輸出到 stdout，而是由 kernel 維護的 buffer，也就是 pipe cmd 不是從 stdin 獲取輸入，而是從 pipe 獲取 cmd1 | cmd2 | \u0026hellip; | cmdn Named Pipes / FIFOs Related / Unrelated processes\nExecuting on same machine\ncreat a FIFO\ncommands mkfifo mknod 嘗試寫入或讀取 FIFO 時，會被 redirect 到 pipe\nSignal Handling Signal Used by OS to notify running process some event has occured without the process needing to pull for that event process 收到 signal 後會先停止執行並執行 signal handler A process did something SIGSEGV(11), SIGFPE(8), SIGILL(4), SIGPIPE(13)\u0026hellip; A process wants to tell another process something SIGCHILD(17) child process terminated User sends sig to foreground processes Ctrl + C SIGINT(2) Ctrl + \\ SIGQUIT(3) Ctrl + Z SIGTSTP(20) disposition 決定 process 遇到 signal 時該怎麼處理\nTerm teminate process Ign ignore Core terminate the process and dump core Stop stop the process Cont continue the process if it is stopped Signal can\u0026rsquo;t not be caught SIGKILL(9) SIGSTOP(19) Commands trap\n可以 handle signal\nkill kill - L 可以看到 standard signal 和 real-time signal\nstandard signal 開頭是 SIG，realt-time signal 是 SIGRT\n","date":"2023-01-28T15:31:50+08:00","permalink":"https://roykesydon.github.io/Blog/p/ipc--inter-process-communication/","title":"IPC -- Inter-Process Communication"},{"content":"paper: Training language models to follow instructions with human feedback\nAbstract 把語言模型變大不代表他們會更好地遵循用戶的意圖。\n大的語言模型有可能會生成 untruthful, toxic, not helpful 的答案。\n該論文透過 fine-tuning with human feedback 來解決這問題。\n一開始準備一系列人工標註的 prompts，然後用這 dataset 對 GPT-3 做 fine-tune。\n接下來再蒐集一個 dataset，存放 rankings of model outputs，由人工判斷輸出好壞，再用 RL 把剛剛 fine-tune 過的 model 繼續 fine-tune。\n最後有 1.3B 參數的 InstructGPT 表現的結果比 175B 參數的 GPT-3 還好。\nIntroduction Large language models(LMs) 可以透過 \u0026ldquo;prompt\u0026rdquo; 來執行各種 NLP 任務。\n但這些模型也常有一些非目的性的行為，諸如捏造事實等等。\n原因是出在目標函數上，多數 LMs 的目標函數是根據網路上的文本生出下一個字詞。\n這和「根據使用者指令生出安全且有幫助的答案不同」。\n上述的差異使語言模型的目標是 misaligned。\n作者的目標是生出 helpful、 honest(沒有誤導性資訊)、harmless 的 model。\n具體作法，使用 reinforcement learning from human feedback(RLHF)。\n訓練步驟 結果 Labelers 明顯偏好 InstructGPT 的答案，勝過 GPT-3 的答案\nInstructGPT 的答案在 truthfulness 勝過 GPT-3 的答案\nInstructGPT 的答案在 toxicity 上小勝 GPT-3 的答案，但在 bias 上沒有\nMethods Dataset 標註人員寫很多 prompts\nPlain: 隨便寫任意任務 Few-shot: 想個 instruction，並寫 multiple query/response pairs for that instruction User-based: 根據一些申請使用 OpenAI API 的用戶，提出有關的 prompts 然後根據這個訓練初步模型，並把這個初步模型放到他們的 Playground 給用戶使用。\n再把用戶問的問題蒐集回來，並做篩選。\n訓練 SFT 的模型用 13k training prompts\n訓練 RM 的模型用 33k training prompts\n訓練 PPO 的模型用 31k training prompts\nModel Supervised fine-tuning(SFT)\n拿 GPT-3 去訓練 16 個 epochs 跑一個 epoch 就發現 overfitting，但發現訓練更多 epoches 對後面的 RM 有用，而且這個 model 也只是過渡產品 Reward modeling(RM)\n把 SFT 後面的 unembedding layer 去除掉，接上線性層，最後輸出一個 scalar reward\n用 6B RMs\n這模型會吃 prompt 和 response\n人工標記的是排序，不是分數\n對每個 prompt 生出 9 個答案\n原本是 4 個，但排 9 個花的時間可能不會到 4 個的兩倍，因為主要心力會花在讀 prompt。但標註訊息會多很多，因為都是兩兩比較。 而且在 loss 中最多只要丟入 RM 9 次，因為可以重用 Pairwise Ranking Loss\n對一個 prompt(假設是 x)，取出一對回覆(假設是 $y_w$ 和 $y_l$)，算出 RM(x, $y_w$) 和 RM(x, $y_l$)，假設 $y_w$ 比 $y_l$ 排序高，讓 RM(x, $y_w$) - RM(x, $y_l$) 的數值越大越好 Reinforcement learning(RL)\nPPO\n$\\beta$ 那項是 KL divergence $\\gamma$ 那項是不想要讓這 model 太專注在微調的任務，而失去原本在其他 NLP 任務也表現很好的功能。 $D_{pretrain}$ 是 pretraining distribution 如果 $\\gamma$ 為 0，在該實驗中叫做 PPO，否則，稱為 PPO-ptx Result ","date":"2023-01-27T17:39:12+08:00","permalink":"https://roykesydon.github.io/Blog/p/instructgpt/","title":"InstructGPT"},{"content":"介紹 一種用於自動化找超參數的方法，用在採樣昂貴而且是黑盒子的情況\n流程 取樣一些資料點 生出一個 Surrogate Model(可採用 Gaussian Process) 反覆做以下事情 用 Acquisition Function 挑選下一個要採樣的點 重新評估 Surrogate Model Gaussian Process 最終的 prediction 是一個 distribution 而不是單一個數字 生成方法需借助 kernel function，常用 RBF(Radial Basis Function)\n$K(x, x^{\u0026rsquo;}|\\tau)=\\sigma^2exp(-\\frac{1}{2}(\\frac{x-x^{\u0026rsquo;}}{l})^2)$\n$\\sigma$ 和 $l$ 是兩個可以調整的超參數\nAcquisition Function 可用超參數來調節 exploitation 和 exploitation\nUCB(Upper confidence bound) PI(probability of improvement) EI(Expected improvement) ","date":"2023-01-26T01:36:53+08:00","permalink":"https://roykesydon.github.io/Blog/p/bayesian-optimization/","title":"Bayesian Optimization"},{"content":"PPFDT per process file descriptor table 每個 process 都有 存放 file descriptors file descriptors 是一個唯一的整數，用來識別作業系統上的 open file 0, 1, 2 是 Standard input / ouput / error 大小受限於 OPEN_MAX，亦即能同時間能開的最多檔案數 Redirection Input redirection $ wc \u0026lt; /etc/passwd 把 wc 的 PPFDT 的 stdin 改成 /etc/passwd 如果是 $ wc /etc/passwd，則是在 PPFDT 追加 /etc/passwd Ouput redirection $ wc \u0026gt; f1 把 wc 的 PPFDT 的 stdout 改成 f1 Input \u0026amp; output redirection 兩個可以同時用\n$ cat \u0026lt; f1 \u0026gt; f2 \u0026gt;\u0026gt; 可以 append $ \u0026lt; f1 cat \u0026gt; f2 可以亂換位置 Error redirection $ find / -name f1 2\u0026gt; error 1\u0026gt; outputs 這樣就會把那些 Permission denied 的給到 errors，成功的給到 outputs 2\u0026gt;/dev/null /dev/null 會把丟進來的東西都丟棄 Copy Descripter 這兩者等價 $ cat f1 1\u0026gt;op_err 2\u0026gt;op_err $ cat f1 1\u0026gt;op_err 2\u0026gt;\u0026amp;1 make 2 a copy of 1 ","date":"2023-01-21T02:20:43+08:00","permalink":"https://roykesydon.github.io/Blog/p/io-redirection/","title":"IO Redirection"},{"content":"Compile C 4-steps pre-processing compilation assembly linking Types of Object Files Executable object file Relocatable object file Shared object file Core file Formats of Object Files a.out initial version of UNIX COFF SVR3 UNIX PE Win. NT ELF SVR4 Linux ELF format of a program ELF Header Program Header Table .text .rodata .data .bss .symtab .rel.text .rel.data .debug .line .strtab Section Header Table 可參考: http://ccckmit.wikidot.com/lk:elf\nProcess Instance of a program running on a computer\nProcess Control Block task_struct\nProcess Identification PID, PPID, SID, UID, EUID.. Process State Information Process Control Information ","date":"2023-01-21T00:08:25+08:00","permalink":"https://roykesydon.github.io/Blog/p/process-management/","title":"Process Management"},{"content":"Features Process control Variables Flow control Functions File \u0026amp; cmd name completions Cmd line editng Cmd history Command Mode Interactive Non- Interactive Command Type internal / Builtin command\n指令的程式碼是 shell 的一部分 e.g., cd, exit 不會產生 child process 有些 internal command，比如 echo, pwd，會 internal 和 external 都有實作 external command\n指令的程式碼在硬碟上的某個 binary file e.g., clear, ls 會產生 child process Common Commands 比較實用或常用的\ngrep\n找字詞\ngrep \u0026lt;string/pattern\u0026gt; -i 大小寫不敏感 -v 不包含關鍵字的 cut 找 column\n-f 找哪些 column -d 分隔符是什麼 比較兩個檔案\ncomm\n顯示 file1 獨有的列、 file2 獨有的列、file1 和 file2 共有的列\ncmp, diff\n回傳不一樣的列資訊\nunset\n把指定的變數移除掉\ntee\n吃 stdin 輸出到 stdout 和其他檔案\nless\n讀檔案用\nExpansions White space Control Operators ; 讓指令接著執行 \u0026amp; 放在結尾，讓指令在背景執行 \u0026amp;\u0026amp; logical AND || logical OR\n前面失敗才會跑後面\n# 註解用 \\ escape special characters 放結尾好換行繼續輸入 $? 一個特別的變數，有上個指令的 exit code Shell variables User defined Env var Shell history File Globing *, ?, [], -, ! ","date":"2023-01-19T23:00:02+08:00","permalink":"https://roykesydon.github.io/Blog/p/shell/","title":"Shell"},{"content":"GPT 本質上就是 Transformer 的 decoder\nGPT-1 paper: Improving Language Understanding by Generative Pre-Training\n用 semi-supervised，後來被歸為 self-supervised\nUnsupervised pre-training $L_1(U)=\\sum_i logP(u_i|u_{i-k},\u0026hellip;,u_{i-1};\\theta)$\n$U= \\{ u_1,\u0026hellip;,u_n \\}$\n$U$ 是一系列未標記的文本 token\n$k$ 是窗口大小\n模型大致架構 $h_0=UW_e+W_p$\n$h_1=transformer \\_ block(h_{i-1})\\forall i \\in[1,n]$\n$P(u)=softmax(h_nW^T_e)$\n$U=\\{u_{-k},\u0026hellip;,u_{-1}\\}$\nSupervised fine-tuning $P(y|x^1,\u0026hellip;,x^m)=softmax(h^m_lW_y)$\n$L2(C)=\\sum_{(x,y)}log P(y|x^1,\u0026hellip;,x^m)$\n$L_3(C)=L_2(C)+\\lambda*L_1(C)$\n$C$ 是 labeled 的資料集，微調基本上就是在後面加上線性層\n作者最大化 likelihood 的時候是用 $L_3$ 而非單純的 $L_2$\n微調應用範例 資料集 用 BooksCorpus 訓練出來的\n有超過 7000 本未出版的書\n模型結構 12 層 transformer 的 decoder 768 維 word embedding 12 個 attention heads 和 BERT BASE 比較 BERT 論文比較晚出，但 BASE 的模型架構和 GPT 有相似之處，\nBASE 是 12 層的 decoder，word embedding 和 attention head 的維度或數量和 GPT-1 相同\nGPT-2 paper: Language Models are Unsupervised Multitask Learner\nGPT-2 除了用更大的的模型和更大的資料集，把重點放在 zero-shot 上，雖然在 GPT-1 的論文就有提過 zero-shot\n資料集 這次做了一個叫做 WebText 的資料集，有百萬級別的網頁\nCommon Crawl 大型爬蟲專案，有大量網頁資料，但充斥了垃圾訊息\nWebText WebText 的資料來源是 reddit 上的外部連結，只要有至少三個 karma，就會被採納，由此取得品質較好的網頁資料。透過這種方法，取得了 4500 萬個連結。並用Dragnet (Peters \u0026amp; Lecocq, 2013) and Newspaper content extractors 把文字訊息從 HTML 中抓出來\n架構 和原本差不多，變成有 1.5B 參數的 Transformer decoder\nzero-shot 不需要下游任務的標記資料\n改把任務輸入進模型\n目前問題 現在的模型泛化能力不太好 Multitask learning 在 NLP 上不太常用，NLP 現在主流還是在預訓練模型上做微調以應對下游任務 對每個下游任務都得重新訓練模型 得蒐集 labeled 資料 結果 GPT-3 paper: Language Models are Few-Shot Learners\n摘要 有 175B 的參數，由於模型極大，要在子任務微調會成本很大，所以不做任何梯度更新 在很多 NLP 任務有傑出的成果 可以生出人類難以區分的新聞文章 目前有的問題 要在子任務微調，需要資料集 微調後在有些子任務上表現好不代表你預訓練模型一定泛化能力高 人類不需要大量 labeled 資料去完成小任務 評估方式 分為三種，few / one / zero-shot learning 架構 基本上 GPT-3 和 GPT-2 架構一樣\n相同 modified initialization pre-normalization reversible tokenization described therein 不同 把 Sparse Transformer 的一些修改拿過來用 GPT-3 Small 是 GPT-1 的大小 GPT-3 Medium 是 BERT Large 的大小 GPT-3 XL 和 GPT-2 相近，比較淺也比較寬\nBatch Size 大小 模型小的時候需要小一點，透過這種額外的 noise 來避免 overfitting(不確定是不是猜想)\n資料集 Common Crawl 架構比 GPT-2 大很多，所以回頭考慮這個資料集\n三步驟 先過濾，透過 reddit 那個高品質的資料集，來訓練一個模型分類高品質和低品質的網頁。 透過 LSH 演算法把相似的文本過濾掉 把一些已知高品質的資料集也加進來 這是一個 Batch 裡有 60% 來自 Common Crawl(filtered) 的意思 Wikipedia 雖然總量比較少，但也有 3% 的採樣率\n結果 計算量指數增長，loss 卻是線性的往下降\npaper 裡有很多任務的實驗結果，這邊就不附上了\nLimitations 在文本生成上還是比較弱，生很長的東西，可能會重複自己說過的話、失去連貫性、自相矛盾等等\n在有些雙向性的任務上可能表現更差\n影響 可能被用來散布不實消息、垃圾郵件等等 偏見 結論 在很多 NLP 任務可以做到接近 SOTA 微調模型的成果\n","date":"2023-01-19T01:50:07+08:00","permalink":"https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/","title":"GPT 三部曲"},{"content":"VM A software implementation of a machine\nSystem VM 提供可以執行 GuestOS 的 complete system platform Process VM 像一個一般的 app 一樣在 hostOS 跑，支援單一個 process Hypervisor 又稱虛擬機器監視器（英語：virtual machine monitor，縮寫為VMM） 用來管理 VM\n允許多個 GuestOS 跑在 host computer\nType-1\nbare-metal hypervisors 直接在硬體上執行 Type-2\nhosted hypervisors 在 hostOS 上執行 directories Binary\ne.g., bin, sbin, lib, opt bin: 有關 user 的指令 sbin: 管理員會用的指令 opt: optional software，多數機器中這是空的 Configuration\ne.g., boot, etc, Data\ne.g., home, root, srv, media, mnt, temp In memory 字面上的意思，不在 hard disk，在 memory\ne.g., dev, proc, sys System Resources\ne.g., usr Variable Data\ne.g., var ","date":"2023-01-19T01:50:07+08:00","permalink":"https://roykesydon.github.io/Blog/p/linux-%E7%91%A3%E4%BA%8B/","title":"Linux 瑣事"}]