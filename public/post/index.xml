<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/post/</link>
        <description>Recent content in Posts on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 21 Feb 2023 15:42:47 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>線性代數 - II</title>
        <link>https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-ii/</link>
        <pubDate>Tue, 21 Feb 2023 15:42:47 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-ii/</guid>
        <description>&lt;h1 id=&#34;linear-equation&#34;&gt;linear equation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;$a_1x_1+a_2x_2+&amp;hellip;+a_nx_n = b$
&lt;ul&gt;
&lt;li&gt;$a$ 是 coefficient&lt;/li&gt;
&lt;li&gt;$x$ 是 variables&lt;/li&gt;
&lt;li&gt;$b$ 是 constant term&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;systems-of-linear-equations&#34;&gt;Systems of linear equations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;m equations, n variables&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$a_{11}x_1+a_{12}x_2+&amp;hellip;+a_{1n}x_n = b_1\\
a_{21}x_1+a_{22}x_2+&amp;hellip;+a_{2n}x_n = b_2\\
&amp;hellip;\\
a_{m1}x_1+a_{m2}x_2+&amp;hellip;+a_{mn}x_n = b_m$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;solution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$[s_1~s_2~&amp;hellip;~s_n]^T$ 是一組解，代換到 $x_1$~$x_n$ 後滿足所有 equation 的向量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所有 Systems of linear equations 都有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;no solution&lt;/li&gt;
&lt;li&gt;exactly one solution&lt;/li&gt;
&lt;li&gt;infinitely many solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;consistent/inconsistent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果有一組以上的解就是 consistent&lt;/li&gt;
&lt;li&gt;無解就是 inconsistent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equivalent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果兩組 Systems of linear equations 的 solution set 一樣，稱為 equivalent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;elementary row operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不會影響 solution set&lt;/li&gt;
&lt;li&gt;types
&lt;ul&gt;
&lt;li&gt;Interchange
&lt;ul&gt;
&lt;li&gt;兩 row 互換&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scaling
&lt;ul&gt;
&lt;li&gt;某 row 乘某個 nonzero scalar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Row addition
&lt;ul&gt;
&lt;li&gt;把某 row 乘某個 scalar 後加到某 row&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;property
&lt;ul&gt;
&lt;li&gt;所有 elementary row operations 都是 reversible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;用來求解&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;coefficient matrix&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_{11}x_1+a_{12}x_2+&amp;hellip;+a_{1n}x_n = b_1\\
a_{21}x_1+a_{22}x_2+&amp;hellip;+a_{2n}x_n = b_2\\
&amp;hellip;\\
a_{m1}x_1+a_{m2}x_2+&amp;hellip;+a_{mn}x_n = b_m$
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;可以拆為 $Ax=b$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A=\begin{bmatrix}
a_{11}&amp;amp; a_{12}&amp;amp;&amp;hellip;&amp;amp; a_{1n} \\
a_{21}&amp;amp; a_{22}&amp;amp;&amp;hellip;&amp;amp; a_{2n} \\
&amp;hellip;&amp;amp; &amp;hellip;&amp;amp;&amp;hellip;&amp;amp; &amp;hellip; \\
a_{m1}&amp;amp; a_{m2}&amp;amp;&amp;hellip;&amp;amp; a_{mn}
\end{bmatrix}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A 就是 coefficient matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$x=\begin{bmatrix}
x_1\\
x_2\\
&amp;hellip;\\
x_n
\end{bmatrix}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$ 是 variable vector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$[A|b]=\begin{bmatrix}
a_{11}&amp;amp; a_{12}&amp;amp;&amp;hellip;&amp;amp; a_{1n} &amp;amp; b_1 \\
a_{21}&amp;amp; a_{22}&amp;amp;&amp;hellip;&amp;amp; a_{2n} &amp;amp; b_2\\
&amp;hellip;&amp;amp; &amp;hellip;&amp;amp;&amp;hellip;&amp;amp; &amp;hellip; &amp;amp; &amp;hellip;\\
a_{m1}&amp;amp; a_{m2}&amp;amp;&amp;hellip;&amp;amp; a_{mn} &amp;amp; b_m
\end{bmatrix}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;叫做 augmented matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>線性代數 - I</title>
        <link>https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-i/</link>
        <pubDate>Tue, 21 Feb 2023 14:42:47 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-i/</guid>
        <description>&lt;h1 id=&#34;matrix&#34;&gt;matrix&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a rectangular array of scalars&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;size&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;m by n&lt;/li&gt;
&lt;li&gt;叫做 square if m = n&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;兩個矩陣的 size 和每個 entry 都一樣&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;submatrix&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;從一個大矩陣刪掉 rows 或 columns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;addition&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;兩個大小相同的矩陣，每個對應位置的 entry 兩兩相加&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scalar multiplication&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一個矩陣的所有 entry 乘以某個 scalar&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;zero matrix&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有 entry 都是 0，該矩陣常以 $O_{n \times m}$ 來表示&lt;/li&gt;
&lt;li&gt;性質
&lt;ul&gt;
&lt;li&gt;$A = O + A$&lt;/li&gt;
&lt;li&gt;$0 \cdot A = O $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;subtraction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A-B=A+(-B)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;transpose&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$A^T$ 的 $(i,j)$-entry 是 $A$ 的 $(j,i)$-entry&lt;/li&gt;
&lt;li&gt;Properties
&lt;ul&gt;
&lt;li&gt;$(A+B)^T=A^T+B^T$&lt;/li&gt;
&lt;li&gt;$(sA)^T=sA^T$&lt;/li&gt;
&lt;li&gt;$(A^T)^T=A$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;vectors&#34;&gt;vectors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;type&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;row vector
&lt;ul&gt;
&lt;li&gt;只有 1 row 的 matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;column vector
&lt;ul&gt;
&lt;li&gt;只有 1 column 的 matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;components&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the entries of a vector&lt;/li&gt;
&lt;li&gt;用 the $i$ th component 代表 $v_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;addition, scalar multiplication&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;和 matrix 一樣&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;矩陣表示&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一個矩陣常被表示為
&lt;ul&gt;
&lt;li&gt;a stack of row vectors&lt;/li&gt;
&lt;li&gt;a cross list of column vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;linear-combination&#34;&gt;linear combination&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$c_1u_1+c_2u_2+&amp;hellip;+c_ku_k$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scalars&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$c_1,c_2,&amp;hellip;,c_k$&lt;/li&gt;
&lt;li&gt;又被稱作 linear combination 的 coefficients&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;vectors&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u_1,u_2,&amp;hellip;,u_k$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果 $u,v$ 非平行二維向量，則二維空間中所有向量皆是 $u,v$ 的 linear combination，且是 unique 的&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;standard-vectors&#34;&gt;standard vectors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$e_1 = \begin{bmatrix}
1 \\
0 \\
&amp;hellip; \\
0
\end{bmatrix}
,e_2 = \begin{bmatrix}
0 \\
1 \\
&amp;hellip; \\
0
\end{bmatrix},&amp;hellip;,
e_n = \begin{bmatrix}
0 \\
0 \\
&amp;hellip; \\
1
\end{bmatrix}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$R^n$ 的任何一個向量都可以被 standard vectors 表示成 uniquely linearly combined&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;矩陣向量乘法&#34;&gt;矩陣向量乘法&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;$Av=v_1a_1+v_2a_2+&amp;hellip;+v_na_n$&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;identity-matrix&#34;&gt;Identity Matrix&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;對整數 n，$n \times n$ identity matrix
&lt;ul&gt;
&lt;li&gt;$I_n$&lt;/li&gt;
&lt;li&gt;每個 columns 是 standard vectors $e_1, e_2, &amp;hellip;, e_n$ in $R^n$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;stochastic-matrix&#34;&gt;Stochastic Matrix&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;對整數 n，$n \times n$ stochastic matrix&lt;/li&gt;
&lt;li&gt;所有 entry 都必須非負&lt;/li&gt;
&lt;li&gt;每個 column 的 entry 總和必須是 unity (相加為 1)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Process Scheduling</title>
        <link>https://roykesydon.github.io/Blog/p/process-scheduling/</link>
        <pubDate>Mon, 20 Feb 2023 21:12:52 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/process-scheduling/</guid>
        <description>&lt;h1 id=&#34;process-scheduling&#34;&gt;Process Scheduling&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;可能時機&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;running -&amp;gt; waiting&lt;/li&gt;
&lt;li&gt;running -&amp;gt; ready&lt;/li&gt;
&lt;li&gt;waiting -&amp;gt; ready&lt;/li&gt;
&lt;li&gt;running -&amp;gt; terminate&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Process Scheduler&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preemptive scheduler (Time slice)
&lt;ul&gt;
&lt;li&gt;可以被搶占&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Non-Preemptive scheduler
&lt;ul&gt;
&lt;li&gt;又稱 cooperative scheduling&lt;/li&gt;
&lt;li&gt;只可能出現在時機 1 或 4&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Classification fo Processes(related to scheduling)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Interactive Processes (50 - 150 ms)&lt;/li&gt;
&lt;li&gt;Batch Processes&lt;/li&gt;
&lt;li&gt;Real time Processes
&lt;ul&gt;
&lt;li&gt;Hard&lt;/li&gt;
&lt;li&gt;Soft&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Classification of Processes(related to CPU usage)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CPU Bound&lt;/li&gt;
&lt;li&gt;I/O Bound&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;standard-scheduling-algorithm&#34;&gt;Standard Scheduling Algorithm&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;FCFS&lt;/li&gt;
&lt;li&gt;SJF&lt;/li&gt;
&lt;li&gt;SRTF&lt;/li&gt;
&lt;li&gt;Priority Based&lt;/li&gt;
&lt;li&gt;Highest Response Ratio Next&lt;/li&gt;
&lt;li&gt;Round Robin&lt;/li&gt;
&lt;li&gt;Virtual RR&lt;/li&gt;
&lt;li&gt;Multi-Level Queue Scheduler&lt;/li&gt;
&lt;li&gt;Multi-Level Feed Back Queue Scheduler&lt;/li&gt;
&lt;li&gt;Rotating Staircase Deadline Scheduler&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;unix-svr3-scheduler&#34;&gt;UNIX SVR3 Scheduler&lt;/h1&gt;
&lt;p&gt;有 32 個 runqueue，每個 runqueue 負責 4 個 priority values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;128 Priority values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0-49: Kernel&lt;/li&gt;
&lt;li&gt;50-127: User&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Priority_j=Base_j+CPU_j(i)+nice_j$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base: 0-127&lt;/li&gt;
&lt;li&gt;$CPU_j(i) = DR * CPU_j(i-1)$
&lt;ul&gt;
&lt;li&gt;DR = $\frac{1}{2}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nice: -20 ~ +19
&lt;ul&gt;
&lt;li&gt;可以用 nice 和 renice 改 process nice value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;schedtool&#34;&gt;Schedtool&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Query &amp;amp; set per process scheduling parameters&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scheduling Policy
&lt;ul&gt;
&lt;li&gt;Real time
&lt;ol&gt;
&lt;li&gt;SCHED_RR&lt;/li&gt;
&lt;li&gt;SCHED_FIFO&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Conventional
&lt;ol&gt;
&lt;li&gt;SCHED_NORMAL (default)&lt;/li&gt;
&lt;li&gt;SCHED_BATCH (CPU intensive)&lt;/li&gt;
&lt;li&gt;SCHED_ISO (unused)&lt;/li&gt;
&lt;li&gt;SCHED_IDLEPRIO (low pri jobs)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Nice Value (-20 to +19)&lt;/li&gt;
&lt;li&gt;Static Priority (1-99)&lt;/li&gt;
&lt;li&gt;CPU affinity
&lt;ul&gt;
&lt;li&gt;process 想運行在某個指定的 CPU 上，不被轉移到其他 CPU，才不會降低指定 CPU 的 cache 命中率
&lt;ul&gt;
&lt;li&gt;soft CPU affinity&lt;/li&gt;
&lt;li&gt;hard CPU affinity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cpus_allowed
&lt;ul&gt;
&lt;li&gt;一個用來指定 CPU 的 mask&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  schedtool &amp;lt;PID&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Q-learning</title>
        <link>https://roykesydon.github.io/Blog/p/q-learning/</link>
        <pubDate>Mon, 20 Feb 2023 16:21:23 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/q-learning/</guid>
        <description>&lt;h1 id=&#34;rl-方法&#34;&gt;RL 方法&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Policy-based
&lt;ul&gt;
&lt;li&gt;learn 做事的 actor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Value-based
&lt;ul&gt;
&lt;li&gt;不直接 learn policy，而是 Learn critic，負責批評&lt;/li&gt;
&lt;li&gt;Q-learning 屬於這種&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;critic&#34;&gt;Critic&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;不直接決定 action&lt;/li&gt;
&lt;li&gt;給予 actor $\pi$，評估 actor $\pi$ 有多好&lt;/li&gt;
&lt;li&gt;critic 的 output 依賴於 actor 的表現&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;state-value-function&#34;&gt;State Value Function&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;State value function $V^{\pi}(s)$
&lt;ul&gt;
&lt;li&gt;用 actor $\pi$，看到 s 後玩到結束，cumulated reward expectation 是多少&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;評估方法&#34;&gt;評估方法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Monte-Carlo(MC) based approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;critic 看 $\pi$ 玩遊戲&lt;/li&gt;
&lt;li&gt;訓練一個 network，看到不同的 state ，輸出 cumulated reward(直到遊戲結束，以下稱為 $G_a$)，解 regression 問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal-difference(TD) approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC 的方法至少要玩到遊戲結束才可以 update network，但有些遊戲超長
&lt;ul&gt;
&lt;li&gt;TD 只需要 {$s_t,a_t,r_t,s_{t+1}$}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$V^{\pi}(s_t)=V^{\pi}(s_{t+1})+r_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MS v.s. TD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC
&lt;ul&gt;
&lt;li&gt;Larger variance
&lt;ul&gt;
&lt;li&gt;每次的輸出差異很大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD
&lt;ul&gt;
&lt;li&gt;smaller variance
&lt;ul&gt;
&lt;li&gt;相較 $G_a$ 較小，因為這邊的 random variable 是 r，但 $G_a$ 是由很多 r 組合而成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V 可能估得不準確
&lt;ul&gt;
&lt;li&gt;那 learn 出來的結果自然也不准&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;較常見&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;another-critic&#34;&gt;Another Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;State-action value function $Q^\pi(s,a)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;又叫 Q function&lt;/li&gt;
&lt;li&gt;當用 actor $\pi$ 時，在 state s 採取 a 這個 action 後的 cumulated reward expectation
&lt;ul&gt;
&lt;li&gt;有一個要注意的地方是，actor 看到 s 不一定會採取 a&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/q-function.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/how-to-use-q.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只要有 Q function，就可以找到&amp;quot;更好的&amp;quot; policy，再替換掉原本的 policy
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;更好的&amp;quot;定義
&lt;ul&gt;
&lt;li&gt;$V^{\pi^{&amp;rsquo;}} \ge V^{\pi}(s), \text{for all state s}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\pi^{&amp;rsquo;}(s)=arg \underset{a}{max}Q^{\pi}(s,a)$
&lt;ul&gt;
&lt;li&gt;$\pi^{&amp;rsquo;}$ 沒有多餘的參數，就單純靠 Q function 推出來&lt;/li&gt;
&lt;li&gt;這邊如果 a 是 continuous 的會有問題，等等解決&lt;/li&gt;
&lt;li&gt;這樣就可以達到&amp;quot;更好的&amp;quot;policy，不過就不列證明了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;basic-tip&#34;&gt;Basic Tip&lt;/h1&gt;
&lt;h2 id=&#34;target-network&#34;&gt;Target network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;在 training 的時候，把其中一個 Q 固定住，不然要學的 target 是不固定的，會不好 train&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/target-network.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;exploration&#34;&gt;Exploration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;policy 完全 depend on Q function&lt;/li&gt;
&lt;li&gt;如果 action 總是固定，這不是好的 data collection 方法，要在 s 採取 a 過，才比較好估計 Q(s, a)，如果 Q function 是 table 就根本不可能估出來，network 也會有一樣的問題，只是沒那麼嚴重。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;解法&#34;&gt;解法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Epsilon Greedy
&lt;ul&gt;
&lt;li&gt;$a=\begin{cases}
arg \underset{a}{max}Q(s,a), &amp;amp; \text{with probability } 1-\varepsilon \\
random, &amp;amp; otherwise
\end{cases}$&lt;/li&gt;
&lt;li&gt;通常 $\varepsilon$ 會隨時間遞減，因為你一開始 train 的時候不知道怎麼比較好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Boltzmann Exploration
&lt;ul&gt;
&lt;li&gt;$P(a|s)=\frac{exp(Q(s,a))}{\sum_a exp(Q(s,a))}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;replay-buffer&#34;&gt;Replay Buffer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;把一堆的 {$s_t,a_t,r_t,s_{t+1}$} 存放在一個 buffer&lt;/li&gt;
&lt;li&gt;{$s_t,a_t,r_t,s_{t+1}$} 簡稱為 exp&lt;/li&gt;
&lt;li&gt;裡面的 exp 可能來自於不同的 policy&lt;/li&gt;
&lt;li&gt;在 buffer 裝滿的時候才把舊的資料丟掉&lt;/li&gt;
&lt;li&gt;每次從 buffer 隨機挑一個 batch 出來，update Q function&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;好處&#34;&gt;好處&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;跟環境作互動很花時間，這樣可以減少跟環境作互動的次數&lt;/li&gt;
&lt;li&gt;本來就希望 batch 裡的 data 越 diverse 越好，不會希望 batch 裡的 data 都是同性質的&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;issue&#34;&gt;issue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;我們要觀察 $\pi$ 的 value，混雜了一些不是 $\pi$ 的 exp 到底有沒有關係?
&lt;ul&gt;
&lt;li&gt;理論上沒問題，但李老師沒解釋&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;typical-q-learning-演算法&#34;&gt;Typical Q-learning 演算法&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;初始化 Q-fucntion Q，target Q-function $\hat{Q}=Q$&lt;/li&gt;
&lt;li&gt;在每個 episode
&lt;ul&gt;
&lt;li&gt;對於每個 time step t
&lt;ul&gt;
&lt;li&gt;給 state $s_t$，根據 Q 執行 action $a_t$ (epsilon greedy)&lt;/li&gt;
&lt;li&gt;獲得 reward $r_t$，到達 $s_{t+1}$&lt;/li&gt;
&lt;li&gt;把 {$s_t,a_t,r_t,s_{t+1}$} 存到 buffer&lt;/li&gt;
&lt;li&gt;從 buffer sample {$s_t,a_t,r_t,s_{t+1}$}(通常是一個 batch)&lt;/li&gt;
&lt;li&gt;Target $y=r_i+\underset{a}{max}\hat{Q}(s_{i+1},a)$&lt;/li&gt;
&lt;li&gt;Update Q 的參數，好讓 $Q(s_i,a_i)$ 更接近 y(regression)&lt;/li&gt;
&lt;li&gt;每 C 步 reset $\hat{Q}=Q$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Proximal Policy Optimization(PPO)</title>
        <link>https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/</link>
        <pubDate>Mon, 20 Feb 2023 12:35:56 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/</guid>
        <description>&lt;h1 id=&#34;onoff-policy&#34;&gt;On/Off-policy&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;On-policy
&lt;ul&gt;
&lt;li&gt;學習的 agent 和與環境互動的 agent 是同一個&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Off-policy
&lt;ul&gt;
&lt;li&gt;學習的 agent 和與環境互動的 agent 是不同個&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;想從-on-policy-轉-off-policy&#34;&gt;想從 On-policy 轉 Off-policy&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;On-policy 每次都要重新蒐集資料，很花時間&lt;/li&gt;
&lt;li&gt;由另一個 $\pi_{\theta^{&amp;rsquo;}}$ 去 train $\theta$，$\theta^{&amp;rsquo;}$是固定的，所以我們可以 re-use sample data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;是一個 general 的想法，不限於 RL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$E_{x \text{\textasciitilde} p}[f(x)]\approx \frac{1}{N}\displaystyle\sum_{i=1}^N f(x^i)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x^i$ is sampled from p(x)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我們遇到的問題是沒辦法從 p 來 sample data，只能透過 q(x) 去 sample $x^i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以把上式改寫成 $E_{x \text{\textasciitilde} p}[f(x)]=E_{x \text{\textasciitilde} q}[f(x)\frac{p(x)}{q(x)}]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;雖然理論上 q 可以任意選，只要不要 q(x) 是 0 的時候 p(x) 不是 0，實作上 p 和 q 不能差太多，不然會有問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;這兩項的 Variance 不一樣，如果 p 除以 q 差距很大，右邊的 Variance 會很大，如果 sample 不夠多次就會有問題&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/ppo/importance-sample-issue.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;轉換&#34;&gt;轉換&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}=E_{\tau \text{\textasciitilde}p_{\theta}(\tau)}[R(\tau)\triangledown log p_{\theta} (\tau)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;改為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}=E_{\tau \text{\textasciitilde}p_{\theta^{&amp;rsquo;}}(\tau)}[\frac{p_{\theta}(\tau)}{p_{\theta^{&amp;rsquo;}}(\tau)}R(\tau)\triangledown log p_{\theta} (\tau)]$&lt;/li&gt;
&lt;li&gt;從 $\theta^{&amp;rsquo;}$ sample 資料&lt;/li&gt;
&lt;li&gt;更新 $\theta$ 多次&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantage-function&#34;&gt;Advantage function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta}}[A^{\theta}(s_t,a_t)\triangledown log p_\theta(a_t^n|s_t^n)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;改為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{P_\theta(s_t,a_t)}{P_{\theta^{&amp;rsquo;}}(s_t,a_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)\triangledown log p_\theta(a_t^n|s_t^n)]$&lt;/li&gt;
&lt;li&gt;要注意 Advantage 的結果要由 $\theta^{&amp;rsquo;}$ 得出，是 $\theta^{&amp;rsquo;}$在和環境互動&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;新的 objective function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J^{\theta^{&amp;rsquo;}}(\theta)=E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{&amp;rsquo;}}(a_t|s_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ppo&#34;&gt;PPO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;確保 $\theta$ 和 $\theta^{&amp;rsquo;}$ 不會差太多&lt;/li&gt;
&lt;li&gt;$J_{PPO}^{\theta^{&amp;rsquo;}}(\theta)=J^{\theta^{&amp;rsquo;}}(\theta)-\beta KL(\theta, \theta^{&amp;rsquo;})$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;前身-trpo&#34;&gt;前身 TRPO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trust Region Policy Optimization&lt;/li&gt;
&lt;li&gt;$J_{TRPO}^{\theta^{&amp;rsquo;}}(\theta)=E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{&amp;rsquo;}}(a_t|s_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)], KL(\theta, \theta^{&amp;rsquo;})&amp;lt;\delta$&lt;/li&gt;
&lt;li&gt;constrain 很難處理&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kl-divergence&#34;&gt;KL divergence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;這邊不是 $\theta$ 和 $\theta^{&amp;rsquo;}$ 參數上的距離，而是 behavior 的距離
&lt;ul&gt;
&lt;li&gt;參數上的距離是指這兩個參數有多像&lt;/li&gt;
&lt;li&gt;是給同樣的 state 生出 action 的 distribution 要像&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;初始參數 $\theta^0$&lt;/li&gt;
&lt;li&gt;每個 iteration
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用 $\theta^k$ 和環境互動，蒐集{$s_t,a_t$}，並計算 advantage $A^{\theta^k}(s_t,a_t)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找出 theta 最佳化 $J_{PPO}(\theta)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta KL(\theta, \theta^{k})$&lt;/li&gt;
&lt;li&gt;可以更新很多次&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;動態調整 $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adaptive KL Penalty&lt;/li&gt;
&lt;li&gt;設可接受的 KL 數值範圍&lt;/li&gt;
&lt;li&gt;if $KL(\theta,\theta^k)&amp;gt;KL_{max},\text{increase} \beta$&lt;/li&gt;
&lt;li&gt;if $KL(\theta,\theta^k)&amp;lt;KL_{min},\text{decrease} \beta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ppo2&#34;&gt;PPO2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta KL(\theta, \theta^{k})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PPO2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO2}^{\theta^{k}}(\theta)\approx \displaystyle\sum_{(s_t,a_t)}min(\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}A^{\theta^k}(s_t,a_t), \\
clip(\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}, 1-\varepsilon, 1+\varepsilon)A^{\theta^k}(s_t,a_t))$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/ppo/ppo2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Policy Gradient</title>
        <link>https://roykesydon.github.io/Blog/p/policy-gradient/</link>
        <pubDate>Sun, 19 Feb 2023 17:16:14 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/policy-gradient/</guid>
        <description>&lt;h1 id=&#34;basic-components&#34;&gt;Basic Components&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Actor
&lt;ul&gt;
&lt;li&gt;Policy $\pi$ is a network with parameter $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Env&lt;/li&gt;
&lt;li&gt;Reward Function&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;trajectory&#34;&gt;Trajectory&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/policy-gradient/aer.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在一場遊戲，把 env 輸出的 s 和 actor 輸出的 a 串起來，是一個 Trajectory&lt;/li&gt;
&lt;li&gt;Trajectory $\tau$ = {$s_1,a_1,s_2,a_2,&amp;hellip;,s_T,a_T$}&lt;/li&gt;
&lt;li&gt;$p_{\theta}(\tau)=p(s_1)\displaystyle\prod_{t=1}^Tp_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;update&#34;&gt;Update&lt;/h1&gt;
&lt;p&gt;$\theta \leftarrow \theta + \eta \triangledown \overline{R}_{\theta}$&lt;/p&gt;
&lt;p&gt;$\triangledown \overline{R_{\theta}} = \displaystyle\sum_{\tau} R(\tau) \triangledown p_{\theta} (\tau) \\
=\frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}R(\tau^n)\triangledown log p_{\theta} (a_t^n|s_t^n)$&lt;/p&gt;
&lt;h1 id=&#34;實作&#34;&gt;實作&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;常見公式
&lt;ul&gt;
&lt;li&gt;$\triangledown f(x)=f(x)\triangledown logf(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;用當前模型蒐集一堆 Trajectory&lt;/li&gt;
&lt;li&gt;更新模型&lt;/li&gt;
&lt;li&gt;回到第一步&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;細節
&lt;ul&gt;
&lt;li&gt;做一個分類問題，把 state 當作分類器的 Input，把 action 當作分類器的 ground truth 作訓練&lt;/li&gt;
&lt;li&gt;在實作分類問題的時候，objective function 都會寫成 minimize cross entropy，就是 maximize log likelihood&lt;/li&gt;
&lt;li&gt;RL 和一般分類的區別是，要記得在 loss 前面乘上 $R(\tau^n)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;tip&#34;&gt;Tip&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Add a Baseline
&lt;ul&gt;
&lt;li&gt;$R(\tau^n)$ 有可能永遠都為正
&lt;ul&gt;
&lt;li&gt;此時等於告訴 Model 說，今天不管是什麼 action，都要提高它的機率。不一定會有問題，因為雖然都是正的，但正的量有大有小，可能某些 action 上升的幅度會更大。因為我們是在做 sampling，不一定會 sample 到某些 action，本來想的情況是所有的 trajectory 都會出現才沒問題。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解法: 希望 reward 不要總是正的
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(R(\tau^n)-b)\triangledown log p_{\theta}(a_t^n|s_t^n)$&lt;/li&gt;
&lt;li&gt;$b \approx E[R(\tau)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Assign Suitable Credit
&lt;ul&gt;
&lt;li&gt;原本整場遊戲的所有 action 都會乘上 $R(\tau)$，但這不太公平，因為就算結果是好的，不代表所有 action 都是對的，反之亦然。在理想的情況下，如果 sample 夠多，就可以解決這問題。&lt;/li&gt;
&lt;li&gt;解法
&lt;ol&gt;
&lt;li&gt;只計算從這個 action 後的 reward 總和
&lt;ul&gt;
&lt;li&gt;因為前面的 reward 和你做了什麼沒關係&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;接續解法 1，把比較未來的 reward 做 discount
&lt;ul&gt;
&lt;li&gt;乘某個小於 1 的 $\gamma^{t^{&amp;rsquo;}-t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;advantage-function&#34;&gt;Advantage function&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;base 可以是 state-dependent，可以根據 network 得出，以後再說&lt;/li&gt;
&lt;li&gt;$(Reward-b)$ 可以合起來看做 Advantage function $A^{\theta}(s_t,a_t)$
&lt;ul&gt;
&lt;li&gt;這邊 Reward 不管你是什麼形式，有沒有 discount。&lt;/li&gt;
&lt;li&gt;它的意義是，這個 action 相較於其他的 action 有多好，而不是絕對好&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;這個 A 通常可以由某個類神經網路估計，那個類神經網路叫做 critic，以後講 Actor-Critic 的時候再說&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>MAE 論文</title>
        <link>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</link>
        <pubDate>Wed, 15 Feb 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.06377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;這篇論文顯示出 MAE 是 CV 中的 scalable self-supervised learners。&lt;/p&gt;
&lt;p&gt;MAE 的方法很簡單&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隨機蓋住輸入影像的一些 patch&lt;/li&gt;
&lt;li&gt;重建 missing pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具備兩個核心設計&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;非對稱的 encoder-decoder 架構，encoder 只作用於可見的 patch 子集合(沒有 mask tokens)，lightweight decoder 則根據 latent representation 和 make tokens 來重建圖片。&lt;/li&gt;
&lt;li&gt;當遮住高比例(比如 75%)的影像時，會得到一個 nontrivial 和 meaningful 的 self-supervisory task&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;結合這兩點設計，可以有效地訓練大模型。
以 ViT-Huge 用 ImageNet-1K 訓練(訓練集一百多萬張照片)可達到 87.8% 的準確度。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/intro.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;在 CV 中，常需要大量 labeled images。
NLP 中，自監督預訓練處理了需要大量標註資料的問題。
masked autoencoders 是一種更 general 的 denoising autoencoders 的形式。
BERT 非常成功，autoencoding methods 在 CV 的研究卻落後 NLP，作者思考是什麼讓 masked autoencoding 在 CV 和 NLP 產生不同。
有以下觀點&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;直到前陣子，CV 中的 CNN 是主流，但卷積層不好引入 mask tokens 或 positional embedding 這些 indicator。但這些可以透過 ViT 來解決，不應成為問題。&lt;/li&gt;
&lt;li&gt;語言和視覺的 Information density 不同，語言是 highly semantic 和 information-dense，使填字本身不是很簡單的事情，但影像含有大量冗餘的訊息，缺失的部分比較好從相鄰的 patch 重建，比如直接插值，所以作者用一種簡單的策略，隨機 mask 很大一部分的 patch，創造一個具有挑戰性的自監督任務，強迫模型關注 global 的資訊。&lt;/li&gt;
&lt;li&gt;關於 decoder，CV 還原 pixel，pixel 屬於 lower semantic level，NLP 還原 word，word 的 semantic information 較高。作者發現，雖然在 BERT 中，可以用簡單的 decoder 還原(一個 MLP)，但 CV 中 decoder 的設計就很重要。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基於以上觀點，作者提出 MAE，隨機遮住大量的 patch，並在 pixel space 重建失去的 patch。而且是非對稱 encoder-decoder 架構，encoder 只會看到可見的 patch，但 docoder 除了 latent representation，還會看到 mask tokens。這種設計在非常高的掩蓋率(比如 75%)下不但可以提高準確度，還可以讓 encoder 只處理較少比例(比如 25%)的 patch，將訓練時間減少 3 倍或更多，使 MAE 可以輕鬆擴展成更大的模型。&lt;/p&gt;
&lt;p&gt;在這樣的架構下，用 MAE 的 pre-training，可以訓練非常吃 data 的模型，比如 ViT-Large/-Huge，而只使用 ImageNet-1K。&lt;/p&gt;
&lt;p&gt;用 ImageNet-1K 在 vanilla ViT-Huge 上 fine-tune 可達到 87.8% 準確度，比以往只使用 ImageNet-1K 的結果都高。&lt;/p&gt;
&lt;p&gt;在 obejct detection、instance segmentation、semantic segmentation 上做 transfer learning 都達到不錯的效果，可以打敗用監督式預訓練模型的對手。&lt;/p&gt;
&lt;h1 id=&#34;相關工作&#34;&gt;相關工作&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoding
&lt;ul&gt;
&lt;li&gt;MAE 是一種 denoising autoencoding 的形式，但和 DAE 還是差別很大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masked image encoding
&lt;ul&gt;
&lt;li&gt;iGPT、ViT、BEiT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Masking&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;和 ViT 一樣，把圖片切成多個 patch，對於 patch 均勻隨機地採樣保留，剩下地遮住&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViT&lt;/li&gt;
&lt;li&gt;也有 positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer block&lt;/li&gt;
&lt;li&gt;輸入
&lt;ul&gt;
&lt;li&gt;encoded visible patches&lt;/li&gt;
&lt;li&gt;mask tokens
&lt;ul&gt;
&lt;li&gt;shared, learned vector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;都會加入 positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;用相較 encoder 輕量的解碼器，所有的 patch 由這個輕量的 decoder 處理，減少預訓練時間&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconstruction target&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decoder 的最後一層是 linear projection，之後再 reshape 成你要的  patch&lt;/li&gt;
&lt;li&gt;loss function
&lt;ul&gt;
&lt;li&gt;mean squared error(MSE)&lt;/li&gt;
&lt;li&gt;只算 masked patched 的 MSE，像 BERT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple implementation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先取得一系列 token(patch 做 linear projection + positional embedding)&lt;/li&gt;
&lt;li&gt;randomly shuffle，根據比例移除尾端一部份&lt;/li&gt;
&lt;li&gt;encoding 後，尾端接上 mask tokens，並且 unshuffle&lt;/li&gt;
&lt;li&gt;加上 positional embedding 後，給 decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;imagenet-experiments&#34;&gt;ImageNet Experiments&lt;/h1&gt;
&lt;p&gt;在 ImageNet-1K 上做自監督的預訓練，然後做&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;end-to-end fine-tuning
&lt;ul&gt;
&lt;li&gt;所有參數都可改&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;linear probing
&lt;ul&gt;
&lt;li&gt;只改最後一層線性層&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/vit-mae.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/ratio-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;optimal masking ratio 意外地高，相比 BERT 只有 15%&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/fine-tune-blocks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;討論和結論&#34;&gt;討論和結論&lt;/h1&gt;
&lt;p&gt;在 CV 實用的預訓練做法主流是監督式的，CV 中自監督的做法可能正跟著 NLP 的軌跡走。&lt;/p&gt;
&lt;p&gt;要仔細處理圖像和語言的區別，作者去除圖片中很可能不構成 semantic segment 的部分，而不是移除某個 object。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ViT 論文</title>
        <link>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 12 Feb 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.11929&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;在 CV 領域 transformer 表現有限，目前 attention 常常是和卷積神經網路一起用，或是用來把一些卷積層換成 self-attention，但整體架構不變。這篇論文想展現一個純 Transformer 可以直接在影像分類上表現很好。如果用大量資料作預訓練，再遷移到中小型的資料集，可以和 SOTA 的 CNN 表現得一樣好，還需要較少的訓練資源作訓練。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;self-attention-based 架構，特別是 Transformer，已經是 NLP 的重要選擇。主流的作法是在大型文字資料集上作訓練，再針對小型任務資料集作 fine-tune。由於 Transformer 的計算效率高，還有可擴展性，可以 train 一些很大的 model，隨著 model 和資料集增大，目前還沒看出飽和的現象。&lt;/p&gt;
&lt;p&gt;然而在 CV，CNN 還是主流，一些工作嘗試用 self-attention 結合 CNN-like 的架構，比如把 feature map 當 transformer 的輸入，因為原始 pixel 太多，或甚至把卷積層全換成 self-attention，雖然後者理論上效率很高(原論文中有另外 cite 兩篇作法)，但因為他們做法特殊，在現代硬體上很難加速，所以無法很有效地擴展。在 large-scale 的影像識別上， ResNet-like 的架構還是 SOTA。&lt;/p&gt;
&lt;p&gt;該實驗直接把一個標準的 Transformer 作用於圖片上，只作最少的修改。把影像分成多個 patch，並把它們變成一系列的 linear embedding，當作 NLP 中的 tokens(words) 來處理。&lt;/p&gt;
&lt;p&gt;當在中型大小的資料集(e.g. ImageNet)上訓練，如果沒有 strong regularization，ViT 會略輸同等大小的 ResNets&lt;/p&gt;
&lt;p&gt;這篇論文在更大的資料集(14M-300M 的影像)上訓練，就打敗了 inductive bias。在大量資料上作預訓練就很讚。&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;大型的 Transformer-based 模型常常是先在大資料集上預訓練然後根據任務 fine-tune，比如 BERT 和 GPT。&lt;/p&gt;
&lt;p&gt;要把 self-attention 用在 CV 上，最簡單的做法就是把每個 Pixel 當一個元素，但 self-attention 是平方複雜度，在現實的圖片很難應用。一個應用 Transformer 的做法是只把 self-attention 用在 local neighborhood，另外一個是用 Sparse Transformer，還有一堆特殊的方法，雖然表現不錯，但要用硬體加速起來不容易。&lt;/p&gt;
&lt;p&gt;另一個有關的模型是 iGPT，在 reduce image resolution 和 color space 後把 transformer 應用在 image pixels 上。它用非監督式訓練後，再 fine-tune 或做 linear probing(只更新最後的 linear layer) 分類任務，表現很好。&lt;/p&gt;
&lt;p&gt;已經有類似的工作了，抽取 patches of size 2 * 2，最後再接 full self-attention，基本上和 ViT 非常像，這篇論文進一步證明了作大規模的預訓練可以讓 Transformer 和 SOTA 的 CNN 相比，而且 ViT 因為 patch 比較大，可以處理 medium-resolution 的圖片。這問題是可預期的，因為 Transformer 缺少了一些 inductive biases。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inductive biases
&lt;ul&gt;
&lt;li&gt;一些假設&lt;/li&gt;
&lt;li&gt;比如 CNN 常有四個假設
&lt;ul&gt;
&lt;li&gt;locality&lt;/li&gt;
&lt;li&gt;translation invariance with pooling layers
&lt;ul&gt;
&lt;li&gt;平移不變性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation equivariance
&lt;ul&gt;
&lt;li&gt;f(g(x)) = g(f(x))&lt;/li&gt;
&lt;li&gt;卷積和平移的先後順序沒差&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;模型盡可能類似原始 Transformer，這樣可以把一些 NLP 上成功的 Transformer 架構拿來用，還可以用一些很有效率的 implementation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-process.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;embedding 維度是 768 = 16 * 16 * 3
position embedding 的做法是 standard learnable 1D positional embeddings，就是 BERT 的做法，簡單來說就是生出一張可以訓練的表，(序列長度, embedding size)，作者也有嘗試其他方法，但發現成效差不多，比如 2D positional embedding，概念就是從生出(序列長度, embedding size)變成生出 2 個(sqrt(序列長度), embedding size)。&lt;/p&gt;
&lt;p&gt;[class] 的概念是 NLP 出來的，ResNet-like 的架構常見的做法也有通過 globally average-pooling (GAP)來生出向量，再接上分類器做預測。實驗發現直接在 transformer 的輸出做 GAP 和 [class] 都可以達到不錯的效果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-gap.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-acc.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;拿標準的 Transformer 來作 Image recognition，和以往用 self-attention 在 CV 的方法不一樣，除了一開始的 initial patch extraction，沒有引入其他影像特有的 inductive biases。直接把圖片當成是一系列的 patch，然後直接用 Transformer encoder 當一般 NLP 任務處理。在很多影像分類訓練集上表現得更好還在 pre-train 上相對便宜。&lt;/p&gt;
&lt;p&gt;還有一些值得挑戰的地方，比如把 ViT 應用在其他 CV 任務，比如 detection 和 segmentation。另一個挑戰是探索自監督預訓練的方法。這篇論文其實有實驗自監督，表現 OK，但和監督式還是有很大的落差。擴大 ViT 可能有更好的結果。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>機率論 - IV</title>
        <link>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iv/</link>
        <pubDate>Sun, 05 Feb 2023 15:18:41 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iv/</guid>
        <description>&lt;h1 id=&#34;隨機變數之和&#34;&gt;隨機變數之和&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Z=X+Y&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$p_Z(z)=\displaystyle\sum_{x=-\infty}^{\infty}p_{X,Y}(x,z-x)\\
=\displaystyle\sum_{y=-\infty}^{\infty}p_{X,Y}(z-y,y)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f_Z(z)=\int_{-\infty}^{\infty}f_{X,Y}(x,z-x)dx\\
=\int_{-\infty}^{\infty}f_{X,Y}(z-y,y)dy$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果 X, Y 獨立&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;離散&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_Z(z)=\displaystyle\sum_{x=-\infty}^{\infty}p_{X}(x)\cdot p_Y(z-x)\\
=\displaystyle\sum_{y=-\infty}^{\infty}p_{X}(z-y)\cdot p_Y(y)$
&lt;ul&gt;
&lt;li&gt;這兩個等式是 discrete convolution&lt;/li&gt;
&lt;li&gt;$=p_X(z) * p_Y(z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;連續&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_Z(z)=\int_{-\infty}^{\infty}f_{X}(x) f_Y(z-x) dx\\
=\int_{-\infty}^{\infty}f_{X}(z-y) f_Y(y) dy$
&lt;ul&gt;
&lt;li&gt;這兩個等式是 continuous convolution&lt;/li&gt;
&lt;li&gt;$=f_X(z) * f_Y(z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果有 n 個獨立隨機變數&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X=X_1+X_2+&amp;hellip;+X_n$
&lt;ul&gt;
&lt;li&gt;如果 $X_1,&amp;hellip;,X_n$ 獨立
&lt;ul&gt;
&lt;li&gt;$p_X(x)=p_{X_1}(x) * p_{X_2}(x) * p_{X_3}(x) * &amp;hellip; * p_{X_n}(x)$
&lt;ul&gt;
&lt;li&gt;連續做 convolution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$f_X(x)=f_{X_1}(x) * f_{X_2}(x) * f_{X_3}(x) * &amp;hellip; * f_{X_n}(x)$
&lt;ul&gt;
&lt;li&gt;連續做 convolution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;mgf&#34;&gt;MGF&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;moment generating function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;convolution 很難算&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;流程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果有多個連續 convolution 也適用下面流程，全部一次一起相乘&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;給定 $p_{X_1}(x), p_{X_2}(x)$，目標是求 $p_{X_1}(x) * p{X_2}(x)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;轉換到 MGF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\phi_{X_1}(s)=E \lbrack e^{sX_1} \rbrack\\
= \displaystyle\sum_{x=-\infty}^{\infty}e^{sx}\cdot p_{X_1}(x)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\phi_{X_2}(s)=E \lbrack e^{sX_2} \rbrack$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;相乘
$\phi_{X_1}(s) \cdot \phi_{X_2}(s)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;逆轉換&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查表&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\phi_X(s)$ 定義&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\phi_X(s)=E \lbrack e^{sX} \rbrack = \begin{cases}
\displaystyle\sum_{x=-\infty}^{\infty} e^{sx} \cdot p_{X}(x) &amp;amp; 離散, \\
\int_{-\infty}^{\infty} e^{sx} \cdot f_{X}(x)dx &amp;amp; 連續
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Y = aX + b
&lt;ul&gt;
&lt;li&gt;$\phi_Y(s) = e^{sb} \cdot \phi_X(as) $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;常見離散機率分佈的 MGF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X$~$Bernoulli(p)$
&lt;ul&gt;
&lt;li&gt;$\phi_X(s)=1-p+pe^s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$X$~$BIN(n, p)$
&lt;ul&gt;
&lt;li&gt;作 n 次實驗成功次數等於個實驗室成功次數的總和&lt;/li&gt;
&lt;li&gt;$X = X_1 + X_2 + &amp;hellip; + X_n, X_i 獨立, Xi$~$Bernoulli(p)$&lt;/li&gt;
&lt;li&gt;$\phi_{X_i}(s)=1-p+pe^s$&lt;/li&gt;
&lt;li&gt;$\phi_{X}(s)=\lbrack 1-p+pe^s \rbrack ^n$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$X$~$Geometric(p)$
&lt;ul&gt;
&lt;li&gt;自行推導&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$X$~$Pascal(k,p)$
&lt;ul&gt;
&lt;li&gt;看到第 k 次成功，花的總實驗室次數等於第 1 號成功花多少次 + 第 2 號 +&amp;hellip;+ 第 k 號&lt;/li&gt;
&lt;li&gt;$X = X_1 + X_2 + &amp;hellip; + X_n, X_i 獨立, Xi$~$Gemetric(p)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$X$~$Exponential(\lambda)$
&lt;ul&gt;
&lt;li&gt;自行推導&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$X$~$Erlang(n,\lambda)$
&lt;ul&gt;
&lt;li&gt;$X = X_1 + X_2 + &amp;hellip; + X_n, X_i 獨立, Xi$~$Exponential(\lambda)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;多個隨機變數之和&#34;&gt;多個隨機變數之和&lt;/h1&gt;
&lt;h2 id=&#34;獨立隨機變數之和&#34;&gt;獨立隨機變數之和&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$X_1, X_2, &amp;hellip;$獨立，且各自有一模一樣的機率分佈
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;{ $X_i$ } $I.I.D.$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Independently and Identically Distributed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X = X_1+X_2+&amp;hellip;+X_n$，n 為常數，請問 X 的機率分佈&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_X(x)=p_{X_1}(x) * p_{X_1}(x) * p_{X_1}(x) * &amp;hellip; * p_{X_1}(x)$&lt;/li&gt;
&lt;li&gt;$f_X(x)=f_{X_1}(x) * f_{X_1}(x) * f_{X_1}(x) * &amp;hellip; * f_{X_1}(x)$
&lt;ul&gt;
&lt;li&gt;因為他們機率分佈一模一樣，所以底下都是 $X_1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\phi_X(s)=\lbrack \phi_{X_1}(s) \rbrack ^n$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;e.g. 假設壽司理想重量是 13g，抓飯量是常態分佈，期望值是 14，標準差是 3，每天要作 100 個，每天飯量的機率分佈是?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_i$ : 第 i 個壽司的飯量，{ $X_i$ } I.I.D.&lt;/li&gt;
&lt;li&gt;$X_i$~$N(14,9)\\
\Rightarrow \phi_{X_i}(s)=\phi_{X_1}(s)\\
=e^{\mu S + \frac{\sigma^2}{2}s^2} = e^{14 s + \frac{9}{2}s^2}$&lt;/li&gt;
&lt;li&gt;$X=X_1+X_2+&amp;hellip;+X_{100}$&lt;/li&gt;
&lt;li&gt;$\phi_X(s)=\lbrack \phi_{X_1}(s) \rbrack^{100}\\
=e^{1400 s + \frac{900}{2}s^2}$
&lt;ul&gt;
&lt;li&gt;這個東西是 $X$~$N(1400,900)$ 的 MGF，所以可以逆推回來機率分佈&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;隨機變數之獨立隨機變數和&#34;&gt;隨機變數之獨立隨機變數和&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$X_1,X_2,&amp;hellip;I.I.D.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X = X_1 + X_2 + &amp;hellip; + X_N$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;N 本身也是隨機變數，其機率分佈已知&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\phi_X(s)=\phi_N(ln(\phi_{X_1}(s)))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;中央極限定理&#34;&gt;中央極限定理&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;central limit theorem(CLT)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 $X_1,X_2,&amp;hellip;,X_n$ 為 $I.I.D.$，當 n 趨近於無窮大時&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X=X_1+X_2+&amp;hellip;+X_n$~$N(\mu_{X_1+X_2&amp;hellip;+X_n}, \sigma^2_{X_1+X_2+&amp;hellip;+X_n})$&lt;/li&gt;
&lt;li&gt;$\mu_{X_1+X_2+&amp;hellip;+X_n}=\mu_{X_1}+\mu_{X_2}+&amp;hellip;+\mu_{X_n}=n\mu_{X_1}$&lt;/li&gt;
&lt;li&gt;$\sigma^2_{X_1+X_2+&amp;hellip;+X_n}=\sigma^2_{X_1}+\sigma^2_{X_2}+&amp;hellip;+\sigma^2_{X_n}=n\sigma^2_{X_1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;應用&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;要處理多個獨立的隨機變數的和時，可以用 CLT 將其機率分佈近似為常態分佈後計算機率
&lt;ul&gt;
&lt;li&gt;比如雜訊常當作常態分佈&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果某機率分佈等於多個獨立隨機變數的和，此機率分佈可以用常態分佈近似，再算機率
&lt;ul&gt;
&lt;li&gt;e.g. $X$~$BIN(100,0.3)$
&lt;ul&gt;
&lt;li&gt;$X=X_1+X_2+&amp;hellip;+X_100$&lt;/li&gt;
&lt;li&gt;{$X_i$} $I.I.D., X_i$~$Bernoulli(0.3)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;範例&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;天團粉絲有 0.2 的機率買 CD，共有100萬個粉絲，發售 CD 超過 200800 張的機率為何
&lt;ul&gt;
&lt;li&gt;$X$~$BIN(1000000,0.2)$&lt;/li&gt;
&lt;li&gt;$P(X&amp;gt;200800)=\displaystyle\sum_{x=200801}^{10^6}(\overset{1000000}{x})0.2^x0.8^{10^6-x}$
&lt;ul&gt;
&lt;li&gt;$(\overset{1000000}{x})=\frac{1000000!}{200801!799199!}$&lt;/li&gt;
&lt;li&gt;算不出來&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$X=X_1+X_2+&amp;hellip;+X_{1000000}, X_i$~$Bernoulli(0.2)\\
\Rightarrow \mu_{X_1}=0.2, \sigma_{X_1}^2=0.16$&lt;/li&gt;
&lt;li&gt;By CLT $\Rightarrow X$~$N(200000,160000)$
&lt;ul&gt;
&lt;li&gt;$P(X&amp;gt;200800)\\
=P(\frac{X-200000}{400} &amp;gt; \frac{200800-200000}{400})\\
=P(Z&amp;gt;2)
=Q(2)
\approx0.023$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;de-moivre---laplace-formula&#34;&gt;De Moivre - Laplace Formula&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果是離散的隨機變數和，可以算的更精確&lt;/li&gt;
&lt;li&gt;$P(k_1 \le X \le k_2) \approx \Phi(\frac{k_2+0.5-n\mu_{X_1}}{\sqrt{n}\sigma_{X_1}}) - \Phi(\frac{k_1-0.5-n\mu_{X_1}}{\sqrt{n}\sigma_{X_1}})$&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>機率論 - III</title>
        <link>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iii/</link>
        <pubDate>Thu, 02 Feb 2023 15:18:41 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iii/</guid>
        <description>&lt;h1 id=&#34;隨機變數的函數&#34;&gt;隨機變數的函數&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;隨機變數 X 的任意函數 g(x) 也是一個隨機變數，常被稱為 Derived Random Variable&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;求-gx-的機率分佈&#34;&gt;求 g(x) 的機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;X 是離散
&lt;ul&gt;
&lt;li&gt;直接推 g(X) 的 PMF
&lt;ul&gt;
&lt;li&gt;X 是離散隨機變數，Y = g(X) 也是離散隨機變數&lt;/li&gt;
&lt;li&gt;$p_{g(X)}(y) = \displaystyle\sum_{會讓g(x)=y 的所有x}p_X(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;X 是連續
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;先推 g(x) 的 CDF，再微分得 PDF&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先算 g(X) 的 CDF
&lt;ul&gt;
&lt;li&gt;$F_{g(X)}(y)=P\lbrack g(X) \le y \rbrack$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;若 g(X) 可以微分，再對 y 微分得 PDF
&lt;ul&gt;
&lt;li&gt;$f_{g(X)}(y)=\frac{d}{dy}F_{g(X)}(y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;e.g. 若 Y=3X+2，請問 Y 的 PDF 與 $f_X(x) 的關係?$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$F_Y(y)=P(Y \le y)\\
=P(3X+2 \le y)\\
=P(X \le \frac{y-2}{3})\\
=F_X(\frac{y-2}{3})$&lt;/li&gt;
&lt;li&gt;$f_Y(y)=\frac{d}{dy}F_Y(y)\\
=\frac{d}{dy}F_X(\frac{y-2}{3})\\
=\frac{dF_X(\frac{y-2}{3})}{d(\frac{y-2}{3})} \cdot \frac{d \frac{y-2}{3}}{dy}\\
=f_X(\frac{y-2}{3}) \cdot \frac{1}{3}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若 Y=aX+b&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_Y(y)=\frac{1}{|a|}f_X(\frac{y-b}{a})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;條件機率分佈&#34;&gt;條件機率分佈&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;若 X 是離散隨機變數，PMF 是 $p_X(x)$，某事件 B 已發生
&lt;ul&gt;
&lt;li&gt;PMF: $p_{X|B}(x)= x = \begin{cases}
x \in B: &amp;amp; \frac{p_X(x)}{p(B)}, \
x \notin B: &amp;amp; 0
\end{cases}$&lt;/li&gt;
&lt;li&gt;CDF: $F_{X|B}(x)\\
=\displaystyle\sum_{u \le x}p_{X|B}(u)\\
=\displaystyle\sum_{u \le x, u \in B} \frac{p_X(u)}{P(B)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;若 X 是連續隨機變數，某事件 B 已發生
&lt;ul&gt;
&lt;li&gt;PDF: $f_{X|B}(x)\\
=\begin{cases}
x \in B: &amp;amp; \frac{f_X(x)}{P(B)}, \
x \notin B: &amp;amp; 0
\end{cases}$&lt;/li&gt;
&lt;li&gt;CDF: $F_{X|B}(x)\\
=\int_{-\infty \le u \le x, u \in B} \frac{f_X(u)}{P(B)} du$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;條件期望值-conditional-excpectation&#34;&gt;條件期望值 Conditional Excpectation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$E \lbrack X|B \rbrack\\
=\begin{cases}
\displaystyle\sum_{x=-\infty}^{\infty} x \cdot p_{X|B}(x) &amp;amp; 離散, \\
\int_{-\infty}^{\infty} x \cdot f_{X|B}(x)dx &amp;amp; 連續
\end{cases}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$E \lbrack g(X)|B \rbrack\\
=\begin{cases}
\displaystyle\sum_{x=-\infty}^{\infty} g(x) \cdot p_{X|B}(x) &amp;amp; 離散, \\
\int_{-\infty}^{\infty} g(x) \cdot f_{X|B}(x)dx &amp;amp; 連續
\end{cases}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Var(X|B) = E\lbrack X^2 | B \rbrack - (\mu_{X|B})^2$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;失憶性-memoryless&#34;&gt;失憶性 Memoryless&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Geometric 和 Exponential 機率分佈都有失憶性&lt;/li&gt;
&lt;li&gt;不管事情已經進行多久，對於事情之後的進行一點影響都沒有&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;聯合機率分佈&#34;&gt;聯合機率分佈&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;joint probability distribution&lt;/li&gt;
&lt;li&gt;同時考慮多個隨機變數的機率分佈&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;joint-pmf&#34;&gt;Joint PMF&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;X, Y 皆為離散，聯合PMF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_{X,Y}(x,y)=P(X=x, Y=y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$0 \le p_{X,Y}(x,y) \le 1$&lt;/li&gt;
&lt;li&gt;$\Sigma^{\infty}&lt;em&gt;{x=-\infty}\Sigma^{\infty}&lt;/em&gt;{y=-\infty}
p_{X,Y}(x,y)=1$&lt;/li&gt;
&lt;li&gt;X, Y 獨立
&lt;ul&gt;
&lt;li&gt;$P_{X,Y}(x,y)\\
=P(X=x,Y=y)\\
=P_X(x)P_Y(y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;對任何事件 B
&lt;ul&gt;
&lt;li&gt;$P(B)=\Sigma_{(x,y)\in B}P_{X,Y}(x,y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;joint-cdf&#34;&gt;Joint CDF&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$F_{X,Y}(x,y)=P(X \le x, Y \le y)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$0 \le F_{X,Y}(x, y) \le 1$&lt;/li&gt;
&lt;li&gt;若 $x_1 \le x_2$ 且 $y_1 \le y_2$，則 $F_{X,Y}(x_1,y_1) \le F_{X,Y} (x_2, y_2)$&lt;/li&gt;
&lt;li&gt;$F_{X,Y}(x, \infty) = F_X(x)$&lt;/li&gt;
&lt;li&gt;$F_{X,Y}(\infty, y) = F_Y(y)$&lt;/li&gt;
&lt;li&gt;$F_{X,Y}(\infty, \infty) = 1$&lt;/li&gt;
&lt;li&gt;$F_{X,Y}(x, -\infty)\\
= P(X \le x, Y \le -\infty)\\
\le P(Y \le -\infty) \\
= 0$&lt;/li&gt;
&lt;li&gt;$F_{X,Y}(-\infty, y) = 0$&lt;/li&gt;
&lt;li&gt;$P(x_1 &amp;lt; X \le x_2, y_1 &amp;lt; Y \le y_2)\\
=F_{X,Y}(x_2,y_2)-F_{X,Y}(x_2,y_1)-F_{X,Y}(x_1,y_2)+F_{X,Y}(x_1,y_1)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;joint-pdf&#34;&gt;Joint PDF&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$f_{X,Y}(x,y)= \frac{\partial^2F_{X,Y}(x,y)}{\partial x \partial y}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$F_{X,Y}(x,y) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X,Y}(u,v)dv du$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_{X,Y}(x,y) \ge 0$&lt;/li&gt;
&lt;li&gt;$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f_{X,Y}(x,y)dxdy=1$&lt;/li&gt;
&lt;li&gt;如果 X,Y 獨立
&lt;ul&gt;
&lt;li&gt;$f_{X,Y}(x,y)=f_X(x) \cdot f_Y(y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;對任何事件 B
&lt;ul&gt;
&lt;li&gt;$P(B)=\int\int_{(x,y)\in B}f_{X,Y}(x,y)dxdy$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;邊際-pmf&#34;&gt;邊際 PMF&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Marginal PMF&lt;/li&gt;
&lt;li&gt;已知聯合 PMF : $p_{X,Y}(x,y)$，求 $p_X(x), p_Y(y)$，稱為邊際 PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x)=\displaystyle\sum_{y=-\infty}^{\infty}P_{X,Y}(x,y)$&lt;/li&gt;
&lt;li&gt;$p_Y(y)=\displaystyle\sum_{x=-\infty}^{\infty}P_{X,Y}(x,y)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;已知聯合 PDF : $p_{X,Y}(x,y)$，求 $f_X(x), f_Y(y)$，稱為邊際 PDF
&lt;ul&gt;
&lt;li&gt;$f_X(x)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)dy$&lt;/li&gt;
&lt;li&gt;$f_Y(y)=\int_{-\infty}^{\infty}f_{X,Y}(x,y)dx$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;雙變數期望值&#34;&gt;雙變數期望值&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;聯合 PMF 下的期望值&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E\lbrack h(X,Y) \rbrack = \displaystyle\sum_{x=-\infty}^{\infty}\displaystyle\sum_{y=-\infty}^{\infty}h(x,y)\cdot p_{X,Y}(x,y)$
&lt;ul&gt;
&lt;li&gt;h(X,Y) 也可以只和 X 有關，比如它可以是 $x^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;聯合 PDF 下的期望值&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E\lbrack h(X,Y) \rbrack = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h(x,y)\cdot f_{X,Y}(x,y) dxdy$&lt;/li&gt;
&lt;li&gt;e.g. 已知 $f_{X,Y}(x,y)=\begin{cases}
0.5, &amp;amp; \text{if } 0 \le y \le x \le 2, \\
0, &amp;amp; otherwise
\end{cases}$
&lt;ul&gt;
&lt;li&gt;$E \lbrack X + Y \rbrack \\
= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x+y)\cdot f_{X,Y}(x,y) dxdy\\
= \int_{0}^{2}\int_{y}^{2}(x+y)\cdot 0.5 dxdy$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;期望值性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E\lbrack \alpha h_1(X,Y)+ \beta h_2(X,Y) \rbrack\\
=\alpha E\lbrack  h_1(X,Y)\rbrack + \beta E\lbrack  h_2(X,Y) \rbrack$&lt;/li&gt;
&lt;li&gt;若 X,Y 獨立
&lt;ul&gt;
&lt;li&gt;$E\lbrack g(X)h(Y) \rbrack = E \lbrack g(X) \rbrack \cdot E \lbrack h(Y) \rbrack$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variance 性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Var(X+Y)=Var(X)+Var(Y)+2 \cdot Cov(X,Y)$
&lt;ul&gt;
&lt;li&gt;$Cov(X,Y)=E\lbrack (X-\mu_X)(Y -\mu_Y) \rbrack$
&lt;ul&gt;
&lt;li&gt;如果 X, Y 獨立
&lt;ul&gt;
&lt;li&gt;$2E\lbrack (X-\mu_X)(Y -\mu_Y) \rbrack \\
= 2E\lbrack (X-\mu_X) \rbrack  E\lbrack (Y -\mu_Y) \rbrack \\
= 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>機率論 - II</title>
        <link>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-ii/</link>
        <pubDate>Wed, 01 Feb 2023 15:18:41 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-ii/</guid>
        <description>&lt;h1 id=&#34;機率密度函數-pdf&#34;&gt;機率密度函數 PDF&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;probability density function&lt;/li&gt;
&lt;li&gt;PMF 在 連續R.V. 上，假如 $X\text{\textasciitilde}[0,1)$，$p_X(0.7)$ = 0，因為有無窮多個數字&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;公式&#34;&gt;公式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$f_X(x)=\lim\limits_{\Delta x \rightarrow 0} \frac{P(x \le X \le x + \Delta x)}{\Delta x} \\
= \lim\limits_{\Delta x \rightarrow 0} \frac{F_X(x+\Delta x) - F_X(x)}{\Delta x} \\
= F^{\prime}_X(x)
$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;和-cdf-的關係&#34;&gt;和 CDF 的關係&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$CDF: F_X(x) = PDF: f_X(x)$
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\int^x_{-\infty}$ 可以從 PDF 轉到 CDF&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\frac{d}{dx} 可以從 CDF 轉到 PDF$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;跟機率的關係&#34;&gt;跟機率的關係&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$P(a &amp;lt; X \le b) = F_X(b) - F_X(a) \\
= \int^b_{-\infty} f_X(x)dx - \int^a_{-\infty} f_X(x)dx \\
= \int^a_b f_X(x)dx$&lt;/li&gt;
&lt;li&gt;$f_X(x)=\lim\limits_{\Delta x \rightarrow 0} \frac{P(x \le X \le x + \Delta x)}{\Delta x}$
&lt;ul&gt;
&lt;li&gt;當 $\Delta x$ 很小時
&lt;ul&gt;
&lt;li&gt;$P(x \le X \le x + \Delta x) \approx f_X(x) \cdot \Delta x$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;性質&#34;&gt;性質&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$f_X(x) = F^{\prime}_X(x)$&lt;/li&gt;
&lt;li&gt;$F_X(x)=\int^x_{-\infty}f_X(u)du$&lt;/li&gt;
&lt;li&gt;$P(a \le X \le b)=\int^b_a f_X(x) dx$&lt;/li&gt;
&lt;li&gt;$\int^{\infty}_{-\infty}f_X(x)dx=1$&lt;/li&gt;
&lt;li&gt;$f_X(x) \ge 0$&lt;/li&gt;
&lt;li&gt;$f_X(x)$ 可以比 1 大&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;連續機率分佈&#34;&gt;連續機率分佈&lt;/h1&gt;
&lt;h2 id=&#34;uniform-機率分佈&#34;&gt;Uniform 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$X \text{\textasciitilde}UNIF(a,b)$&lt;/li&gt;
&lt;li&gt;PDF
&lt;ul&gt;
&lt;li&gt;$f_X(x) = \begin{cases}
\frac{1}{b-a} &amp;amp; ,a \le x \le b \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \begin{cases}
0 &amp;amp; ,x \le a \\
\frac{x-a}{b-a} &amp;amp; ,a &amp;lt; x \le b\\
1 &amp;amp; ,x &amp;gt; b
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exponential-機率分佈&#34;&gt;Exponential 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有失憶性(memoryless)，常被用來 model 有這種性質的事情&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}Exponential(\lambda)$&lt;/li&gt;
&lt;li&gt;PDF
&lt;ul&gt;
&lt;li&gt;$f_X(x) = \begin{cases}
\lambda e^{-\lambda x} &amp;amp; ,x \ge 0 \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = 1-e^{-\lambda x}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;erlang-機率分佈&#34;&gt;Erlang 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Gamma Distribution&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}Erlang(n,\lambda)$&lt;/li&gt;
&lt;li&gt;PDF
&lt;ul&gt;
&lt;li&gt;$f_X(x) = \begin{cases}
\frac{1}{(n-1)!}\lambda^n x^{n-1} e^{-\lambda x} &amp;amp; ,x \ge 0 \\
0 &amp;amp; ,otherwise
\end{cases}$
&lt;ul&gt;
&lt;li&gt;$f_X(x)=(\lambda e^{-\lambda x}) * (\lambda e^{-\lambda x}) * &amp;hellip; * (\lambda e^{-\lambda x})$
&lt;ul&gt;
&lt;li&gt;自己和自己做 n 次 convolution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \begin{cases}
1 - \Sigma^{n-1}_{k=0}\frac{(\lambda x)^k}{k!}e^{-\lambda x} &amp;amp; ,x \ge 0 \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;常見用法&#34;&gt;常見用法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用來 model 一件有多個關卡事情的總時間，而每個關卡所需時間是隨機的
&lt;ul&gt;
&lt;li&gt;關卡數: n&lt;/li&gt;
&lt;li&gt;每關卡所需時間之機率分佈 $Exponential(\lambda)$&lt;/li&gt;
&lt;li&gt;e.g. 打電動過三關所需時間
&lt;ul&gt;
&lt;li&gt;$Erlang(3, \lambda)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;normal-機率分佈-常態分佈&#34;&gt;Normal 機率分佈 (常態分佈)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在自然界常出現&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;常被用做「很多隨機量的總和」的機率模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;又稱 Gaussian 機率分佈&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X \text{\textasciitilde}Gaussian(\mu,\sigma)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PDF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;也常用 $X \text{\textasciitilde}N(\mu,\sigma^2)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注意 $\sigma$ 不一樣&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CDF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;太難算，積不出來&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;針對某組特別的 $\mu, \sigma$ 的 CDF 建表，把其他常態分佈的 CDF 和這組產生關聯&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;標準常態分佈&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Z \text{\textasciitilde}N(0,1)$
&lt;ul&gt;
&lt;li&gt;$f_Z(z)=\frac{1}{\sqrt{2 \pi}}e^{-\frac{z^2}{2}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF 表示為 $\Phi(z)$
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\Phi(z)=\int^z_{-\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}}du$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;積不出來，以數值方法近似出來後建表給人家查
&lt;ul&gt;
&lt;li&gt;查 standard normal table&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;e.g. $F_Z(1.325)=?$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查表 $F_Z(1.32)=0.9066$，$F_Z(1.33)=0.9082$
&lt;ul&gt;
&lt;li&gt;用內插約略得 0.9074&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性質&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Phi(-z) = 1 - \Phi(z)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;任意  $\mu, \sigma$ 的 CDF&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對任何 $X \text{\textasciitilde}N(\mu,\sigma^2)$
&lt;ul&gt;
&lt;li&gt;$\frac{X-\mu}{\sigma}\text{\textasciitilde}N(0,1)$&lt;/li&gt;
&lt;li&gt;$F_X(x)=\Phi(\frac{x-\mu}{\sigma})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;期望值&#34;&gt;期望值&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Expectation&lt;/li&gt;
&lt;li&gt;大數法則
&lt;ul&gt;
&lt;li&gt;$P(A)=\lim\limits_{N \rightarrow \infty}\frac{N_A}{N}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基本上期望值是利用大數法則算的 mean 值，雖然平均值是 R.V.，但當實驗無窮多次時，會收斂到常數，因此以這為估算值&lt;/li&gt;
&lt;li&gt;Mean 值又稱做期望值&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;離散隨機變數&#34;&gt;離散隨機變數&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$E\lbrack X \rbrack=\mu_X=\displaystyle\sum^{\infty}_{x=-\infty}x \cdot P_X(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;離散隨機變數的函數的期望值&#34;&gt;離散隨機變數的函數的期望值&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;對離散隨機變數 X 而言，其任意函數 g(x) 也是一隨機變數，也有期望值
&lt;ul&gt;
&lt;li&gt;$g(X)$ 的期望值定義為
&lt;ul&gt;
&lt;li&gt;$E \lbrack g(X) \rbrack=\displaystyle\sum^{\infty}_{x=-\infty}g(x)\cdot P_X(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;性質-1&#34;&gt;性質&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$E\lbrack \alpha g(X) \rbrack = \alpha \cdot E \lbrack g(X) \rbrack$&lt;/li&gt;
&lt;li&gt;$E\lbrack \alpha g(X) + \beta h(X) \rbrack \\
=\alpha \cdot E \lbrack g(X) \rbrack + \beta \cdot E \lbrack h(X) \rbrack$&lt;/li&gt;
&lt;li&gt;$E\lbrack \alpha \rbrack = \alpha$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;常見隨機變數函數的期望值&#34;&gt;常見隨機變數函數的期望值&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$X$ 的 $n^{th} moment$
&lt;ul&gt;
&lt;li&gt;$E \lbrack X^n \rbrack = \displaystyle\sum^{\infty}_{x=-\infty}x^n \cdot P_X(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;X 的變異數(variance)
&lt;ul&gt;
&lt;li&gt;$E \lbrack (X-\mu_X)^2 \rbrack = \displaystyle\sum^{\infty}_{x=-\infty} (x-\mu_X)^2 \cdot P_X(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;變異數-variance&#34;&gt;變異數 Variance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Variance 通常符號表示為 $\sigma^2_X=E \lbrack (X-\mu_X)^2 \rbrack$&lt;/li&gt;
&lt;li&gt;隱含隨機變數 X 多「亂」的資訊
&lt;ul&gt;
&lt;li&gt;variance 大的話，X 不見得接近 $\mu_X$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;變異數開根號是標準差(standard deviation)
&lt;ul&gt;
&lt;li&gt;$\sigma_X = \sqrt{Variance} \ge 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;算法&#34;&gt;算法&lt;/h4&gt;
&lt;p&gt;$\sigma^2_X=E \lbrack X^2 \rbrack - \mu^2_X\\
\Rightarrow E \lbrack X^2 \rbrack = \sigma^2_X + \mu^2_X$&lt;/p&gt;
&lt;h3 id=&#34;常見離散分佈的期望值--變異數&#34;&gt;常見離散分佈的期望值 / 變異數&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$X\text{\textasciitilde}Bernouli(p)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X=1 \cdot p + 0 \cdot (1-p) \\
= p$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = E \lbrack X^2 \rbrack - \mu^2_X \\
= \displaystyle\sum^1_{x=0}x^2\cdot p_X(x)-\mu_X^2 \\
=1^2 \cdot p + 0^2 \cdot (1-p) - p^2\\
=p(1-p)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$BIN(n,p)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = np$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = np(1-p)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$GEO(p)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \frac{1}{p}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \frac{(1-p)}{p^2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$PASKAL(k,p)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \frac{k}{p}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \frac{k(1-p)}{p^2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$POI(\alpha)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \alpha$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \alpha$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$UNIF(a,b)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \frac{a+b}{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \frac{1}{12}(b-a)(b-a+2)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;連續隨機變數&#34;&gt;連續隨機變數&lt;/h2&gt;
&lt;p&gt;對連續的隨機變數 X 而言，將 X 的值以 $\Delta$ 為單位無條件捨去來近似，以隨機變數 Y 表示(當 $\Delta \rightarrow$ 0 時，$X \approx Y$)，然後再當做 PMF 處理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E \lbrack X \rbrack = \int^{\infty}_{-\infty}xf_X(x)dx$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;連續隨機變數的函數的期望值&#34;&gt;連續隨機變數的函數的期望值&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;對連續隨機變數 X 而言，其任意函數 g(x) 也是一隨機變數，也有期望值
&lt;ul&gt;
&lt;li&gt;$g(X)$ 的期望值定義為
&lt;ul&gt;
&lt;li&gt;$E \lbrack g(X) \rbrack=\int^{\infty}_{-\infty}g(x)\cdot f_X(x)dx$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;性質-2&#34;&gt;性質&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$E\lbrack \alpha g(X) \rbrack = \alpha \cdot E \lbrack g(X) \rbrack$&lt;/li&gt;
&lt;li&gt;$E\lbrack \alpha g(X) + \beta h(X) \rbrack \\
=\alpha \cdot E \lbrack g(X) \rbrack + \beta \cdot E \lbrack h(X) \rbrack$&lt;/li&gt;
&lt;li&gt;$E\lbrack \alpha \rbrack = \alpha$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;常見隨機變數函數的期望值-1&#34;&gt;常見隨機變數函數的期望值&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$X$ 的 $n^{th} moment$
&lt;ul&gt;
&lt;li&gt;$E \lbrack X^n \rbrack = \int^{\infty}_{-\infty}x^n \cdot f_X(x)dx$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;X 的變異數(variance)
&lt;ul&gt;
&lt;li&gt;$E \lbrack (X-\mu_X)^2 \rbrack = \int^{\infty}_{-\infty} (x-\mu_X)^2 \cdot f_X(x)dx$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;變異數-variance-1&#34;&gt;變異數 Variance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;和離散隨機變數的資訊一樣&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;常見連續分佈之期望值變異數&#34;&gt;常見連續分佈之期望值/變異數&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$X$~$Exponential(\lambda)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \frac{1}{\lambda}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \frac{1}{\lambda^2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$Erlang(n, \lambda)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \frac{n}{\lambda}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \frac{n}{\lambda^2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$Gaussian(\mu,\sigma)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \mu$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \sigma^2$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$~$UNIF(a,b)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mu_X = \frac{a+b}{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\sigma^2_X = \frac{1}{12}(b-a)^2$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>機率論 - I</title>
        <link>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-i/</link>
        <pubDate>Tue, 31 Jan 2023 15:18:41 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-i/</guid>
        <description>&lt;h1 id=&#34;集合論&#34;&gt;集合論&lt;/h1&gt;
&lt;h2 id=&#34;名詞&#34;&gt;名詞&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;子集合(Subset)
&lt;ul&gt;
&lt;li&gt;B 是 C 的子集(B 不能等於 C)
&lt;ul&gt;
&lt;li&gt;$B \subset C$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;補集(Complement)
&lt;ul&gt;
&lt;li&gt;C 是 A 的補集
&lt;ul&gt;
&lt;li&gt;$C=A^C$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不相交(Disjoint)
&lt;ul&gt;
&lt;li&gt;$X \cap Y = \{\}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;互斥(Mutually Exclusive)
&lt;ul&gt;
&lt;li&gt;一群集合 $X_1, X_2, &amp;hellip;, X_n$ 中任選兩個集合 $X_i, X_j$ 都不相交，則 $X_1, X_2, &amp;hellip;, X_n$ 這群集合互斥&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;公式&#34;&gt;公式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;De Morgan&amp;rsquo;s Law
&lt;ul&gt;
&lt;li&gt;${(A \cup B)}^C=A^C \cap B^C$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;機率名詞&#34;&gt;機率名詞&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Outcome (結果)
&lt;ul&gt;
&lt;li&gt;實驗中可能的結果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample Space (樣本空間)
&lt;ul&gt;
&lt;li&gt;機率實驗所有可能的結果的集合，常以 $S$ 表示&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Event (事件)
&lt;ul&gt;
&lt;li&gt;對於實驗結果的某種敘述&lt;/li&gt;
&lt;li&gt;事件可以看做是 outcome 的集合，也是 sample space 的子集&lt;/li&gt;
&lt;li&gt;機率是一個函數，其自變數是 event，故可看做是一個映射&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;公理-axioms&#34;&gt;公理 Axioms&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;對任何事件 $A$ 而言, $P(A) \geq 0$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$P(S) = 1$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;事件 $A_1, A_2, &amp;hellip;$ 互斥 $\Rightarrow$ $P(A_1 \cup A_2 \cup A_3 \cup &amp;hellip;)$&lt;/p&gt;
&lt;p&gt;$=P(A_1)+P(A_2)+P(A_3)+&amp;hellip;$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;衍生公式&#34;&gt;衍生公式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Boole&amp;rsquo;s 不等式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對任意 $n$ 個事件 $A_1, A_2, &amp;hellip;, A_n$ 而言
&lt;ul&gt;
&lt;li&gt;$P(\cup^n_{i=1}A_i \leq \Sigma^n_{i=1}P(A_i))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bonferroni&amp;rsquo;s 不等式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對任意 $n$ 個事件 $A_1, A_2, &amp;hellip;, A_n$ 而言
&lt;ul&gt;
&lt;li&gt;$P(\cap^n_{i=1} A_i) \geq 1 - \Sigma^n_{i=1} P(A^C_i)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;條件機率&#34;&gt;條件機率&lt;/h1&gt;
&lt;h2 id=&#34;公式-1&#34;&gt;公式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$P(X|Y) = \frac{P(X \cap Y)}{P(Y)}$
&lt;ul&gt;
&lt;li&gt;$P(X \cap Y) = P(X|Y) * {P(Y)} = P(Y|X) * P(X)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;性質&#34;&gt;性質&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;$P(X|Y) \geq 0$&lt;/li&gt;
&lt;li&gt;$P(Y|Y) = 1$&lt;/li&gt;
&lt;li&gt;$A, B$ 互斥 $\Rightarrow P(A \cup B |Y) = \frac{P(A)}{P(Y)} + \frac{P(B)}{P(Y)} = P(A|Y)+P(B|Y)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;定理&#34;&gt;定理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Total Probability 定理
&lt;ul&gt;
&lt;li&gt;若 $C_1, C_2, &amp;hellip;, C_n$ 互斥且 $C_1 \cup C_2 \cup &amp;hellip; \cup C_n = S$，則對任意事件 $A$
&lt;ul&gt;
&lt;li&gt;$P(A) = P(A|C_1)P(C_1) +  P(A|C_2)P(C_2) + &amp;hellip; + P(A|C_n)P(C_n)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bayes&amp;rsquo; Rule 貝式定理
&lt;ul&gt;
&lt;li&gt;若 $C_1, C_2, &amp;hellip;, C_n$ 互斥且 $C_1 \cup C_2 \cup &amp;hellip; \cup C_n = S$，則對任意事件 $A$
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$P(C_j|A)=\frac{P(A|C_j) * P(C_j)}{\Sigma^n_{i=1}P(A|C_i)*P(C_i)}$&lt;/p&gt;
&lt;p&gt;$= \frac{P(C_j \cap A)}{P(A)}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;獨立性-independence&#34;&gt;獨立性 Independence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;若兩事件 $A, B$ 之機率滿足&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P(A \cap B) = P(A) * P(B)$&lt;/li&gt;
&lt;li&gt;或以 $P(A|B) = P(A)$ 表示&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;則 $A, B$ 兩事件稱為機率上的獨立事件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;若事件 $A_1, A_2, &amp;hellip; A_n$ 滿足下列條件，則稱此 $n$ 事件獨立 $(n&amp;gt;2)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;從中任選 $m$ 事件 $A_{i_1}, A_{i_2}, &amp;hellip; A_{i_m}$ 均滿足
&lt;ul&gt;
&lt;li&gt;$P(A_{i_1} \cap A_{i_2} \cap &amp;hellip; \cap A_{i_m}) = P(A_{i_1})P(A_{i_2})&amp;hellip;P(A_{i_m}) , m=2, 3, &amp;hellip;, n$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;排列組合&#34;&gt;排列組合&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;二項式係數(binomial coefficient)
&lt;ul&gt;
&lt;li&gt;$(^n_k)$
&lt;ul&gt;
&lt;li&gt;有 $n$ 個異物，從中取出 $k$ 個&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多項式係數(multinomial coefficient)
&lt;ul&gt;
&lt;li&gt;$\frac{n!}{n_1!n_2!&amp;hellip;n_m!}$
&lt;ul&gt;
&lt;li&gt;有 m 種異物，每次選物從中選一後放回，依序選 n 次，共有 $m^n$ 種 outcome，在所有實驗結果中，第一種出現 $n_1$ 次，以此類推，這樣的實驗結果有多少種&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;隨機變數-random-variable-rv&#34;&gt;隨機變數 Random Variable, R.V.&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;用來把 outcome 數字化的表示方式&lt;/li&gt;
&lt;li&gt;通常用大寫英文字母&lt;/li&gt;
&lt;li&gt;是將 outcome 轉成對應數字的函數
&lt;ul&gt;
&lt;li&gt;$X: S \rightarrow R$
&lt;ul&gt;
&lt;li&gt;從樣本空間映射到實數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;隨機變數的函數，也是一個隨機變數&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;種類&#34;&gt;種類&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;離散隨機變數 (Discrete R.V.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;值是有限個，或是「可數的」無窮多個&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;連續隨機變數 (Continuous R.V.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;值有無窮多個，而且「不可數」&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;可數不可數&#34;&gt;可數、不可數&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;可數
&lt;ul&gt;
&lt;li&gt;包含的東西可一個個被數，總有一天會被數到
&lt;ul&gt;
&lt;li&gt;e.g. 正偶數集合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不可數的
&lt;ul&gt;
&lt;li&gt;不管怎麼數，裡面一定有個東西會沒數到
&lt;ul&gt;
&lt;li&gt;e.g. 0~1 之間的所有數字&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;累積分佈函數-cdf&#34;&gt;累積分佈函數 CDF&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;cumulative distribution function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對任一個隨機變數 $X$，定義 CDF 為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$F_X(x) \overset{def}{=}P(X \leq x)$
&lt;ul&gt;
&lt;li&gt;永遠用 $F$ 表示&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;常見用途&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;算 X 落在某範圍的機率&lt;/li&gt;
&lt;li&gt;$P(A &amp;lt; X \le b) = F_X(b)-F_X(a)$
&lt;ul&gt;
&lt;li&gt;$P(A \le X \le b) = F_X(b)-F_X(a)+P(X=a)$&lt;/li&gt;
&lt;li&gt;$P(A &amp;lt; X &amp;lt; b) = P(A &amp;lt; X \le b^-)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;性質-1&#34;&gt;性質&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;離散隨機變數的 CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x^+)=F_X(x)$&lt;/li&gt;
&lt;li&gt;$F_X(x^-)=F_X(x)-P(X=x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;連續隨機變數的 CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x^-)=F_X(x)=F_X(x^+)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;共同
&lt;ul&gt;
&lt;li&gt;$F_X(- \infty)=P(X \le - \infty)=0$&lt;/li&gt;
&lt;li&gt;$F_X(\infty)=P(X \le \infty) = 1$&lt;/li&gt;
&lt;li&gt;$0 \le F_X(x) \le 1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;機率質量函數-pmf&#34;&gt;機率質量函數 PMF&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;probability mass function&lt;/li&gt;
&lt;li&gt;對任一個「離散」隨機變數 $X$，其 PMF 為
&lt;ul&gt;
&lt;li&gt;$p_X(x) \overset{def}{=}P(X=x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pmf-和-cdf-的關係&#34;&gt;PMF 和 CDF 的關係&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;對任何 $x$
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \displaystyle\sum^{\lfloor x \rfloor}_{n=-\infty}p_X(n)$
&lt;ul&gt;
&lt;li&gt;$P_X(x)=F_X(x^+)-F_X(x^-)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;機率分佈probability-distribution&#34;&gt;機率分佈(Probability Distribution)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;PMF 和 PDF 都是一種機率分佈
&lt;ul&gt;
&lt;li&gt;將總和為 1 的機率分佈在點上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;離散機率分佈&#34;&gt;離散機率分佈&lt;/h1&gt;
&lt;h2 id=&#34;bernoulli-機率分佈&#34;&gt;Bernoulli 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1 次實驗，2 種結果，在意某結果發生與否&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}Bernoulli(p)$&lt;/li&gt;
&lt;li&gt;PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x) = \begin{cases}
p &amp;amp; ,x=1 \\
1-p &amp;amp; x=0 \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \begin{cases}
0 &amp;amp; ,x&amp;lt;0 \\
1-p &amp;amp; 0 \leq x &amp;lt;1 \\
1 &amp;amp; ,x \geq 1
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;binomial-機率分佈&#34;&gt;Binomial 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;實驗成功機率為 p，做 n 次實驗，X 表成功次數&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}BIN(p)$&lt;/li&gt;
&lt;li&gt;PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x) = (^n_x)p^x(1-p)^{n-x}$
&lt;ul&gt;
&lt;li&gt;成功 $x$ 次&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \displaystyle\sum^{\lfloor x \rfloor}_{m=-\infty} (^n_m)\cdot p^m \cdot (1-p)^{n-m}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;uniform-機率分佈&#34;&gt;Uniform 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1 次實驗，n 種結果，各結果機率均等，在意某結果發生否&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}UNIF(a,b)$&lt;/li&gt;
&lt;li&gt;PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x) = \begin{cases}
\frac{1}{b-a+1} &amp;amp; ,x=a,a+1,&amp;hellip;,b \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \begin{cases}
0 &amp;amp; ,x&amp;lt;a \\
\frac{\lfloor x \rfloor - a + 1}{b-a+1} &amp;amp; ,a \leq x&amp;lt; b\\
1 &amp;amp; ,x \geq b
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;geometric-機率分佈&#34;&gt;Geometric 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;若實驗成功機率為 p，到成功為止，做了 X 次嘗試&lt;/li&gt;
&lt;li&gt;有失憶性&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}Geometric(p)$&lt;/li&gt;
&lt;li&gt;PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x) = \begin{cases}
(1-p)^{x-1} \cdot p &amp;amp; ,x=1, 2, 3, &amp;hellip; \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \begin{cases}
1-(1-p)^{\lfloor x \rfloor} &amp;amp; ,x \ge 1 \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pascal-機率分佈&#34;&gt;Pascal 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;若實驗成功機率為 p，到第 k 次成功為止，共做了 X 次嘗試&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}Pascal(k, p)$&lt;/li&gt;
&lt;li&gt;PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x) = \begin{cases}
\binom{x-1}{k-1}(1-p)^{x-k} p^k &amp;amp; ,x=k, k+1, &amp;hellip; \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = P(X \le x) \\
= P(在 x 次實驗中 \ge k 次成功)\\
= P(Y \ge k), Y~BIN (x,p) \\
$
&lt;ul&gt;
&lt;li&gt;故 Pascal 又稱 Negative Binomial&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;poisson-機率分佈&#34;&gt;Poisson 機率分佈&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;已知某事發生速率為每單位時間 $\lambda$ 次，觀察時間為 $T$ 時間單位，$X$ 為該觀察時間內發生該事的總次數。&lt;/li&gt;
&lt;li&gt;$X \text{\textasciitilde}POI(\lambda T)$
&lt;ul&gt;
&lt;li&gt;有時候也會以 $\mu$ 來表示 $\lambda T$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PMF
&lt;ul&gt;
&lt;li&gt;$p_X(x) = e^{-\lambda T} \cdot \frac{(\lambda T)^x}{x!}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CDF
&lt;ul&gt;
&lt;li&gt;$F_X(x) = \begin{cases}
\displaystyle\sum^{\lfloor x \rfloor}_{n=-\infty}e^{-\lambda T} \cdot \frac{(\lambda T)^n}{n!} &amp;amp; ,x = 0,1,2,&amp;hellip; \\
0 &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>IPC -- Inter-Process Communication</title>
        <link>https://roykesydon.github.io/Blog/p/ipc--inter-process-communication/</link>
        <pubDate>Sat, 28 Jan 2023 15:31:50 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/ipc--inter-process-communication/</guid>
        <description>&lt;h1 id=&#34;share-information-between-processes&#34;&gt;Share information between processes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;透過硬碟上的文件溝通
&lt;ul&gt;
&lt;li&gt;超慢&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;透過 kernel buffer
&lt;ul&gt;
&lt;li&gt;滿快的，但這樣要一直在 user mode 和 kernel mode 來回切換，因為kernel buffer 在 kernel space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;透過 shared memory region
&lt;ul&gt;
&lt;li&gt;shared memory region 在 user space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;mechanisms&#34;&gt;Mechanisms&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Signals&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Communication&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data transfer
&lt;ul&gt;
&lt;li&gt;Byte Stream
&lt;ul&gt;
&lt;li&gt;Pipes&lt;/li&gt;
&lt;li&gt;FIFOs(Named Pipes)&lt;/li&gt;
&lt;li&gt;stream sockets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Message Passing
&lt;ul&gt;
&lt;li&gt;SystemV MsgQ&lt;/li&gt;
&lt;li&gt;POSIX MsgQ&lt;/li&gt;
&lt;li&gt;datagram sockets&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shared Memory
&lt;ul&gt;
&lt;li&gt;SystemV S.M&lt;/li&gt;
&lt;li&gt;POSIX S.M&lt;/li&gt;
&lt;li&gt;Memory Mapping
&lt;ul&gt;
&lt;li&gt;anonymous memory mapping&lt;/li&gt;
&lt;li&gt;memory mapped file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Synchronization&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;pipes&#34;&gt;Pipes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Related processes
&lt;ul&gt;
&lt;li&gt;parent-child&lt;/li&gt;
&lt;li&gt;sibling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Executing on same machine&lt;/li&gt;
&lt;li&gt;用法
&lt;ul&gt;
&lt;li&gt;cmd1 | cmd2
&lt;ul&gt;
&lt;li&gt;cmd1 不是輸出到 stdout，而是由 kernel 維護的 buffer，也就是 pipe&lt;/li&gt;
&lt;li&gt;cmd 不是從 stdin 獲取輸入，而是從 pipe 獲取&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cmd1 | cmd2 | &amp;hellip; | cmdn&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;named-pipes--fifos&#34;&gt;Named Pipes / FIFOs&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Related / Unrelated processes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Executing on same machine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;creat a FIFO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;commands
&lt;ul&gt;
&lt;li&gt;mkfifo&lt;/li&gt;
&lt;li&gt;mknod&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;嘗試寫入或讀取 FIFO 時，會被 redirect 到 pipe&lt;/p&gt;
&lt;h1 id=&#34;signal-handling&#34;&gt;Signal Handling&lt;/h1&gt;
&lt;h2 id=&#34;signal&#34;&gt;Signal&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Used by OS to notify running process some event has occured without the process needing to pull for that event&lt;/li&gt;
&lt;li&gt;process 收到 signal 後會先停止執行並執行 signal handler&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;A process did something
&lt;ul&gt;
&lt;li&gt;SIGSEGV(11), SIGFPE(8), SIGILL(4), SIGPIPE(13)&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A process wants to tell another process something
&lt;ul&gt;
&lt;li&gt;SIGCHILD(17)
&lt;ul&gt;
&lt;li&gt;child process terminated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User sends sig to foreground processes
&lt;ul&gt;
&lt;li&gt;Ctrl + C SIGINT(2)&lt;/li&gt;
&lt;li&gt;Ctrl + \ SIGQUIT(3)&lt;/li&gt;
&lt;li&gt;Ctrl + Z SIGTSTP(20)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;disposition&#34;&gt;disposition&lt;/h3&gt;
&lt;p&gt;決定 process 遇到 signal 時該怎麼處理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Term
&lt;ul&gt;
&lt;li&gt;teminate process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ign
&lt;ul&gt;
&lt;li&gt;ignore&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Core
&lt;ul&gt;
&lt;li&gt;terminate the process and dump core&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stop
&lt;ul&gt;
&lt;li&gt;stop the process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cont
&lt;ul&gt;
&lt;li&gt;continue the process if it is stopped&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;signal-cant-not-be-caught&#34;&gt;Signal can&amp;rsquo;t not be caught&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SIGKILL(9)&lt;/li&gt;
&lt;li&gt;SIGSTOP(19)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;commands&#34;&gt;Commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;trap&lt;/p&gt;
&lt;p&gt;可以 handle signal&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kill&#34;&gt;kill&lt;/h2&gt;
&lt;p&gt;kill - L 可以看到 standard signal 和 real-time signal&lt;/p&gt;
&lt;p&gt;standard signal 開頭是 SIG，realt-time signal 是 SIGRT&lt;/p&gt;
</description>
        </item>
        <item>
        <title>InstructGPT</title>
        <link>https://roykesydon.github.io/Blog/p/instructgpt/</link>
        <pubDate>Fri, 27 Jan 2023 17:39:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/instructgpt/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;把語言模型變大不代表他們會更好地遵循用戶的意圖。&lt;/p&gt;
&lt;p&gt;大的語言模型有可能會生成 untruthful, toxic, not helpful 的答案。&lt;/p&gt;
&lt;p&gt;該論文透過 fine-tuning with human feedback 來解決這問題。&lt;/p&gt;
&lt;p&gt;一開始準備一系列人工標註的 prompts，然後用這 dataset 對 GPT-3 做 fine-tune。&lt;/p&gt;
&lt;p&gt;接下來再蒐集一個 dataset，存放 rankings of model outputs，由人工判斷輸出好壞，再用 RL 把剛剛 fine-tune 過的 model 繼續 fine-tune。&lt;/p&gt;
&lt;p&gt;最後有 1.3B 參數的 InstructGPT 表現的結果比 175B 參數的 GPT-3 還好。&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Large language models(LMs) 可以透過 &amp;ldquo;prompt&amp;rdquo; 來執行各種 NLP 任務。&lt;/p&gt;
&lt;p&gt;但這些模型也常有一些非目的性的行為，諸如捏造事實等等。&lt;/p&gt;
&lt;p&gt;原因是出在目標函數上，多數 LMs 的目標函數是根據網路上的文本生出下一個字詞。&lt;/p&gt;
&lt;p&gt;這和「根據使用者指令生出安全且有幫助的答案不同」。&lt;/p&gt;
&lt;p&gt;上述的差異使語言模型的目標是 misaligned。&lt;/p&gt;
&lt;p&gt;作者的目標是生出 helpful、 honest(沒有誤導性資訊)、harmless 的 model。&lt;/p&gt;
&lt;p&gt;具體作法，使用 reinforcement learning from human feedback(RLHF)。&lt;/p&gt;
&lt;h2 id=&#34;訓練步驟&#34;&gt;訓練步驟&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-train-step.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labelers 明顯偏好 InstructGPT 的答案，勝過 GPT-3 的答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT 的答案在 truthfulness 勝過 GPT-3 的答案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT 的答案在 toxicity 上小勝 GPT-3 的答案，但在 bias 上沒有&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;methods&#34;&gt;Methods&lt;/h1&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;標註人員寫很多 prompts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plain:
&lt;ul&gt;
&lt;li&gt;隨便寫任意任務&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Few-shot:
&lt;ul&gt;
&lt;li&gt;想個 instruction，並寫 multiple query/response pairs for that instruction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User-based:
&lt;ul&gt;
&lt;li&gt;根據一些申請使用 OpenAI API 的用戶，提出有關的 prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然後根據這個訓練初步模型，並把這個初步模型放到他們的 Playground 給用戶使用。&lt;/p&gt;
&lt;p&gt;再把用戶問的問題蒐集回來，並做篩選。&lt;/p&gt;
&lt;p&gt;訓練 SFT 的模型用 13k training prompts&lt;/p&gt;
&lt;p&gt;訓練 RM 的模型用 33k training prompts&lt;/p&gt;
&lt;p&gt;訓練 PPO 的模型用 31k training prompts&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Supervised fine-tuning(SFT)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拿 GPT-3 去訓練 16 個 epochs&lt;/li&gt;
&lt;li&gt;跑一個 epoch 就發現 overfitting，但發現訓練更多 epoches 對後面的 RM 有用，而且這個 model 也只是過渡產品&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward modeling(RM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;把 SFT 後面的 unembedding layer 去除掉，接上線性層，最後輸出一個 scalar reward&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用 6B RMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;這模型會吃 prompt 和 response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;人工標記的是排序，不是分數&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對每個 prompt 生出 9 個答案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原本是 4 個，但排 9 個花的時間可能不會到 4 個的兩倍，因為主要心力會花在讀 prompt。但標註訊息會多很多，因為都是兩兩比較。&lt;/li&gt;
&lt;li&gt;而且在 loss 中最多只要丟入 RM 9 次，因為可以重用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pairwise Ranking Loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對一個 prompt(假設是 x)，取出一對回覆(假設是 $y_w$ 和 $y_l$)，算出 RM(x, $y_w$) 和 RM(x, $y_l$)，假設 $y_w$ 比 $y_l$ 排序高，讓 RM(x, $y_w$) - RM(x, $y_l$) 的數值越大越好&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-reward-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement learning(RL)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-rl-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\beta$ 那項是 KL divergence&lt;/li&gt;
&lt;li&gt;$\gamma$ 那項是不想要讓這 model 太專注在微調的任務，而失去原本在其他 NLP 任務也表現很好的功能。
&lt;ul&gt;
&lt;li&gt;$D_{pretrain}$ 是 pretraining distribution&lt;/li&gt;
&lt;li&gt;如果 $\gamma$ 為 0，在該實驗中叫做 PPO，否則，稱為 PPO-ptx&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Bayesian Optimization</title>
        <link>https://roykesydon.github.io/Blog/p/bayesian-optimization/</link>
        <pubDate>Thu, 26 Jan 2023 01:36:53 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/bayesian-optimization/</guid>
        <description>&lt;h1 id=&#34;介紹&#34;&gt;介紹&lt;/h1&gt;
&lt;p&gt;一種用於自動化找超參數的方法，用在採樣昂貴而且是黑盒子的情況&lt;/p&gt;
&lt;h1 id=&#34;流程&#34;&gt;流程&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;取樣一些資料點&lt;/li&gt;
&lt;li&gt;生出一個 Surrogate Model(可採用 Gaussian Process)&lt;/li&gt;
&lt;li&gt;反覆做以下事情
&lt;ul&gt;
&lt;li&gt;用 Acquisition Function 挑選下一個要採樣的點&lt;/li&gt;
&lt;li&gt;重新評估 Surrogate Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;gaussian-process&#34;&gt;Gaussian Process&lt;/h2&gt;
&lt;p&gt;最終的 prediction 是一個 distribution 而不是單一個數字
生成方法需借助 kernel function，常用 RBF(Radial Basis Function)&lt;/p&gt;
&lt;p&gt;$K(x, x^{&amp;rsquo;}|\tau)=\sigma^2exp(-\frac{1}{2}(\frac{x-x^{&amp;rsquo;}}{l})^2)$&lt;/p&gt;
&lt;p&gt;$\sigma$ 和 $l$ 是兩個可以調整的超參數&lt;/p&gt;
&lt;h2 id=&#34;acquisition-function&#34;&gt;Acquisition Function&lt;/h2&gt;
&lt;p&gt;可用超參數來調節 exploitation 和 exploitation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UCB(Upper confidence bound)&lt;/li&gt;
&lt;li&gt;PI(probability of improvement)&lt;/li&gt;
&lt;li&gt;EI(Expected improvement)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>IO Redirection</title>
        <link>https://roykesydon.github.io/Blog/p/io-redirection/</link>
        <pubDate>Sat, 21 Jan 2023 02:20:43 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/io-redirection/</guid>
        <description>&lt;h1 id=&#34;ppfdt&#34;&gt;PPFDT&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;per process file descriptor table&lt;/li&gt;
&lt;li&gt;每個 process 都有&lt;/li&gt;
&lt;li&gt;存放 file descriptors
&lt;ul&gt;
&lt;li&gt;file descriptors 是一個唯一的整數，用來識別作業系統上的 open file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;0, 1, 2 是 Standard input / ouput / error&lt;/li&gt;
&lt;li&gt;大小受限於 OPEN_MAX，亦即能同時間能開的最多檔案數&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;redirection&#34;&gt;Redirection&lt;/h1&gt;
&lt;h2 id=&#34;input-redirection&#34;&gt;Input redirection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$ wc &amp;lt; /etc/passwd
&lt;ul&gt;
&lt;li&gt;把 wc 的 PPFDT 的 stdin 改成 /etc/passwd&lt;/li&gt;
&lt;li&gt;如果是 $ wc /etc/passwd，則是在 PPFDT 追加 /etc/passwd&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ouput-redirection&#34;&gt;Ouput redirection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$ wc &amp;gt; f1
&lt;ul&gt;
&lt;li&gt;把 wc 的 PPFDT 的 stdout 改成 f1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;input--output-redirection&#34;&gt;Input &amp;amp; output redirection&lt;/h2&gt;
&lt;p&gt;兩個可以同時用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ cat &amp;lt; f1 &amp;gt; f2&lt;/li&gt;
&lt;li&gt;&amp;gt;&amp;gt; 可以 append&lt;/li&gt;
&lt;li&gt;$ &amp;lt; f1 cat &amp;gt; f2
&lt;ul&gt;
&lt;li&gt;可以亂換位置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;error-redirection&#34;&gt;Error redirection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;$ find / -name f1 2&amp;gt; error 1&amp;gt; outputs
&lt;ul&gt;
&lt;li&gt;這樣就會把那些 Permission denied 的給到 errors，成功的給到 outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2&amp;gt;/dev/null
&lt;ul&gt;
&lt;li&gt;/dev/null 會把丟進來的東西都丟棄&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copy-descripter&#34;&gt;Copy Descripter&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;這兩者等價
&lt;ul&gt;
&lt;li&gt;$ cat f1 1&amp;gt;op_err 2&amp;gt;op_err&lt;/li&gt;
&lt;li&gt;$ cat f1 1&amp;gt;op_err 2&amp;gt;&amp;amp;1
&lt;ul&gt;
&lt;li&gt;make 2 a copy of 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Process Management</title>
        <link>https://roykesydon.github.io/Blog/p/process-management/</link>
        <pubDate>Sat, 21 Jan 2023 00:08:25 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/process-management/</guid>
        <description>&lt;h1 id=&#34;compile-c&#34;&gt;Compile C&lt;/h1&gt;
&lt;h2 id=&#34;4-steps&#34;&gt;4-steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;pre-processing&lt;/li&gt;
&lt;li&gt;compilation&lt;/li&gt;
&lt;li&gt;assembly&lt;/li&gt;
&lt;li&gt;linking&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;types-of-object-files&#34;&gt;Types of Object Files&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Executable object file&lt;/li&gt;
&lt;li&gt;Relocatable object file&lt;/li&gt;
&lt;li&gt;Shared object file&lt;/li&gt;
&lt;li&gt;Core file&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;formats-of-object-files&#34;&gt;Formats of Object Files&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;a.out
&lt;ul&gt;
&lt;li&gt;initial version of UNIX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;COFF
&lt;ul&gt;
&lt;li&gt;SVR3 UNIX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PE
&lt;ul&gt;
&lt;li&gt;Win. NT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ELF
&lt;ul&gt;
&lt;li&gt;SVR4 Linux&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;elf-format-of-a-program&#34;&gt;ELF format of a program&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ELF Header&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Program Header Table&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.rodata&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.bss&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.symtab&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.rel.text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.rel.data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.debug&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.line&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;.strtab&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Section Header Table&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;可參考: &lt;a class=&#34;link&#34; href=&#34;http://ccckmit.wikidot.com/lk:elf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://ccckmit.wikidot.com/lk:elf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;process&#34;&gt;Process&lt;/h1&gt;
&lt;p&gt;Instance of a program running on a computer&lt;/p&gt;
&lt;h2 id=&#34;process-control-block&#34;&gt;Process Control Block&lt;/h2&gt;
&lt;p&gt;task_struct&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Process Identification
&lt;ul&gt;
&lt;li&gt;PID, PPID, SID, UID, EUID..&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Process State Information&lt;/li&gt;
&lt;li&gt;Process Control Information&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Shell</title>
        <link>https://roykesydon.github.io/Blog/p/shell/</link>
        <pubDate>Thu, 19 Jan 2023 23:00:02 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/shell/</guid>
        <description>&lt;h1 id=&#34;features&#34;&gt;Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Process control&lt;/li&gt;
&lt;li&gt;Variables&lt;/li&gt;
&lt;li&gt;Flow control&lt;/li&gt;
&lt;li&gt;Functions&lt;/li&gt;
&lt;li&gt;File &amp;amp; cmd name completions&lt;/li&gt;
&lt;li&gt;Cmd line editng&lt;/li&gt;
&lt;li&gt;Cmd history&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;command-mode&#34;&gt;Command Mode&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Interactive&lt;/li&gt;
&lt;li&gt;Non- Interactive&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;command-type&#34;&gt;Command Type&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;internal / Builtin command&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指令的程式碼是 shell 的一部分
&lt;ul&gt;
&lt;li&gt;e.g., cd, exit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不會產生 child process&lt;/li&gt;
&lt;li&gt;有些 internal command，比如 echo, pwd，會 internal 和 external 都有實作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;external command&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指令的程式碼在硬碟上的某個 binary file
&lt;ul&gt;
&lt;li&gt;e.g., clear, ls&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;會產生 child process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;common-commands&#34;&gt;Common Commands&lt;/h1&gt;
&lt;p&gt;比較實用或常用的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;grep&lt;/p&gt;
&lt;p&gt;找字詞&lt;/p&gt;
&lt;p&gt;grep &amp;lt;string/pattern&amp;gt; &lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-i 大小寫不敏感&lt;/li&gt;
&lt;li&gt;-v 不包含關鍵字的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cut
找 column&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;-f 找哪些 column&lt;/li&gt;
&lt;li&gt;-d 分隔符是什麼&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;比較兩個檔案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;comm&lt;/p&gt;
&lt;p&gt;顯示 file1 獨有的列、 file2 獨有的列、file1 和 file2 共有的列&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cmp, diff&lt;/p&gt;
&lt;p&gt;回傳不一樣的列資訊&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;unset&lt;/p&gt;
&lt;p&gt;把指定的變數移除掉&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tee&lt;/p&gt;
&lt;p&gt;吃 stdin 輸出到 stdout 和其他檔案&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;less&lt;/p&gt;
&lt;p&gt;讀檔案用&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;expansions&#34;&gt;Expansions&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;White space&lt;/li&gt;
&lt;li&gt;Control Operators
&lt;ul&gt;
&lt;li&gt;;
&lt;ul&gt;
&lt;li&gt;讓指令接著執行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;amp;
&lt;ul&gt;
&lt;li&gt;放在結尾，讓指令在背景執行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&amp;amp;&amp;amp;
&lt;ul&gt;
&lt;li&gt;logical AND&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;||
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;logical OR&lt;/p&gt;
&lt;p&gt;前面失敗才會跑後面&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;#
&lt;ul&gt;
&lt;li&gt;註解用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;\
&lt;ul&gt;
&lt;li&gt;escape special characters&lt;/li&gt;
&lt;li&gt;放結尾好換行繼續輸入&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$?
&lt;ul&gt;
&lt;li&gt;一個特別的變數，有上個指令的 exit code&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shell variables
&lt;ul&gt;
&lt;li&gt;User defined&lt;/li&gt;
&lt;li&gt;Env var&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shell history&lt;/li&gt;
&lt;li&gt;File Globing
&lt;ul&gt;
&lt;li&gt;*, ?, [], -, !&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>GPT 三部曲</title>
        <link>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</link>
        <pubDate>Thu, 19 Jan 2023 01:50:07 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</guid>
        <description>&lt;p&gt;GPT 本質上就是 Transformer 的 decoder&lt;/p&gt;
&lt;h1 id=&#34;gpt-1&#34;&gt;GPT-1&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;用 semi-supervised，後來被歸為 self-supervised&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h2&gt;
&lt;p&gt;$L_1(U)=\sum_i logP(u_i|u_{i-k},&amp;hellip;,u_{i-1};\theta)$&lt;/p&gt;
&lt;p&gt;$U= \{ u_1,&amp;hellip;,u_n \}$&lt;/p&gt;
&lt;p&gt;$U$ 是一系列未標記的文本 token&lt;/p&gt;
&lt;p&gt;$k$ 是窗口大小&lt;/p&gt;
&lt;h3 id=&#34;模型大致架構&#34;&gt;模型大致架構&lt;/h3&gt;
&lt;p&gt;$h_0=UW_e+W_p$&lt;/p&gt;
&lt;p&gt;$h_1=transformer \_ block(h_{i-1})\forall i \in[1,n]$&lt;/p&gt;
&lt;p&gt;$P(u)=softmax(h_nW^T_e)$&lt;/p&gt;
&lt;p&gt;$U=\{u_{-k},&amp;hellip;,u_{-1}\}$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h2&gt;
&lt;p&gt;$P(y|x^1,&amp;hellip;,x^m)=softmax(h^m_lW_y)$&lt;/p&gt;
&lt;p&gt;$L2(C)=\sum_{(x,y)}log P(y|x^1,&amp;hellip;,x^m)$&lt;/p&gt;
&lt;p&gt;$L_3(C)=L_2(C)+\lambda*L_1(C)$&lt;/p&gt;
&lt;p&gt;$C$ 是 labeled 的資料集，微調基本上就是在後面加上線性層&lt;/p&gt;
&lt;p&gt;作者最大化 likelihood 的時候是用 $L_3$ 而非單純的 $L_2$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;微調應用範例&#34;&gt;微調應用範例&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-1-tasks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;資料集&#34;&gt;資料集&lt;/h2&gt;
&lt;p&gt;用 BooksCorpus 訓練出來的&lt;/p&gt;
&lt;p&gt;有超過 7000 本未出版的書&lt;/p&gt;
&lt;h2 id=&#34;模型結構&#34;&gt;模型結構&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;12 層 transformer 的 decoder&lt;/li&gt;
&lt;li&gt;768 維 word embedding&lt;/li&gt;
&lt;li&gt;12 個 attention heads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;和-bert-base-比較&#34;&gt;和 BERT BASE 比較&lt;/h2&gt;
&lt;p&gt;BERT 論文比較晚出，但 BASE 的模型架構和 GPT 有相似之處，&lt;/p&gt;
&lt;p&gt;BASE 是 12 層的 decoder，word embedding 和 attention head 的維度或數量和 GPT-1 相同&lt;/p&gt;
&lt;h1 id=&#34;gpt-2&#34;&gt;GPT-2&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/paper/language-models-are-unsupervised-multitask&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Unsupervised Multitask Learner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GPT-2 除了用更大的的模型和更大的資料集，把重點放在 zero-shot 上，雖然在 GPT-1 的論文就有提過 zero-shot&lt;/p&gt;
&lt;h2 id=&#34;資料集-1&#34;&gt;資料集&lt;/h2&gt;
&lt;p&gt;這次做了一個叫做 WebText 的資料集，有百萬級別的網頁&lt;/p&gt;
&lt;h3 id=&#34;common-crawl&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;大型爬蟲專案，有大量網頁資料，但充斥了垃圾訊息&lt;/p&gt;
&lt;h3 id=&#34;webtext&#34;&gt;WebText&lt;/h3&gt;
&lt;p&gt;WebText 的資料來源是 reddit 上的外部連結，只要有至少三個 karma，就會被採納，由此取得品質較好的網頁資料。透過這種方法，取得了 4500 萬個連結。並用Dragnet (Peters &amp;amp; Lecocq, 2013) and Newspaper content extractors 把文字訊息從 HTML 中抓出來&lt;/p&gt;
&lt;h2 id=&#34;架構&#34;&gt;架構&lt;/h2&gt;
&lt;p&gt;和原本差不多，變成有 1.5B 參數的 Transformer decoder&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h2&gt;
&lt;p&gt;不需要下游任務的標記資料&lt;/p&gt;
&lt;p&gt;改把任務輸入進模型&lt;/p&gt;
&lt;h3 id=&#34;目前問題&#34;&gt;目前問題&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;現在的模型泛化能力不太好&lt;/li&gt;
&lt;li&gt;Multitask learning
在 NLP 上不太常用，NLP 現在主流還是在預訓練模型上做微調以應對下游任務
&lt;ul&gt;
&lt;li&gt;對每個下游任務都得重新訓練模型&lt;/li&gt;
&lt;li&gt;得蒐集 labeled 資料&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;gpt-3&#34;&gt;GPT-3&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;有 175B 的參數，由於模型極大，要在子任務微調會成本很大，所以不做任何梯度更新&lt;/li&gt;
&lt;li&gt;在很多 NLP 任務有傑出的成果&lt;/li&gt;
&lt;li&gt;可以生出人類難以區分的新聞文章&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;目前有的問題&#34;&gt;目前有的問題&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;要在子任務微調，需要資料集&lt;/li&gt;
&lt;li&gt;微調後在有些子任務上表現好不代表你預訓練模型一定泛化能力高&lt;/li&gt;
&lt;li&gt;人類不需要大量 labeled 資料去完成小任務&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;評估方式&#34;&gt;評估方式&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;分為三種，few / one / zero-shot learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;架構-1&#34;&gt;架構&lt;/h2&gt;
&lt;p&gt;基本上 GPT-3 和 GPT-2 架構一樣&lt;/p&gt;
&lt;h3 id=&#34;相同&#34;&gt;相同&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;modified initialization&lt;/li&gt;
&lt;li&gt;pre-normalization&lt;/li&gt;
&lt;li&gt;reversible tokenization described therein&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;不同&#34;&gt;不同&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;把 Sparse Transformer 的一些修改拿過來用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;GPT-3 Small 是 GPT-1 的大小
GPT-3 Medium 是 BERT Large 的大小
GPT-3 XL 和 GPT-2 相近，比較淺也比較寬&lt;/p&gt;
&lt;h4 id=&#34;batch-size-大小&#34;&gt;Batch Size 大小&lt;/h4&gt;
&lt;p&gt;模型小的時候需要小一點，透過這種額外的 noise 來避免 overfitting(不確定是不是猜想)&lt;/p&gt;
&lt;h2 id=&#34;資料集-2&#34;&gt;資料集&lt;/h2&gt;
&lt;h3 id=&#34;common-crawl-1&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;架構比 GPT-2 大很多，所以回頭考慮這個資料集&lt;/p&gt;
&lt;h4 id=&#34;三步驟&#34;&gt;三步驟&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;先過濾，透過 reddit 那個高品質的資料集，來訓練一個模型分類高品質和低品質的網頁。&lt;/li&gt;
&lt;li&gt;透過 LSH 演算法把相似的文本過濾掉&lt;/li&gt;
&lt;li&gt;把一些已知高品質的資料集也加進來&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;這是一個 Batch 裡有 60% 來自 Common Crawl(filtered) 的意思
Wikipedia 雖然總量比較少，但也有 3% 的採樣率&lt;/p&gt;
&lt;h2 id=&#34;結果-1&#34;&gt;結果&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;計算量指數增長，loss 卻是線性的往下降&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;paper 裡有很多任務的實驗結果，這邊就不附上了&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;在文本生成上還是比較弱，生很長的東西，可能會重複自己說過的話、失去連貫性、自相矛盾等等&lt;/p&gt;
&lt;p&gt;在有些雙向性的任務上可能表現更差&lt;/p&gt;
&lt;h2 id=&#34;影響&#34;&gt;影響&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;可能被用來散布不實消息、垃圾郵件等等&lt;/li&gt;
&lt;li&gt;偏見&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;結論&#34;&gt;結論&lt;/h2&gt;
&lt;p&gt;在很多 NLP 任務可以做到接近 SOTA 微調模型的成果&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Linux 瑣事</title>
        <link>https://roykesydon.github.io/Blog/p/linux-%E7%91%A3%E4%BA%8B/</link>
        <pubDate>Thu, 19 Jan 2023 01:50:07 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/linux-%E7%91%A3%E4%BA%8B/</guid>
        <description>&lt;h1 id=&#34;vm&#34;&gt;VM&lt;/h1&gt;
&lt;p&gt;A software implementation of a machine&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;System VM
&lt;ul&gt;
&lt;li&gt;提供可以執行 GuestOS 的 complete system platform&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Process VM
&lt;ul&gt;
&lt;li&gt;像一個一般的 app 一樣在 hostOS 跑，支援單一個 process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;hypervisor&#34;&gt;Hypervisor&lt;/h2&gt;
&lt;p&gt;又稱虛擬機器監視器（英語：virtual machine monitor，縮寫為VMM）
用來管理 VM&lt;/p&gt;
&lt;p&gt;允許多個 GuestOS 跑在 host computer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Type-1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bare-metal hypervisors&lt;/li&gt;
&lt;li&gt;直接在硬體上執行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Type-2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hosted hypervisors&lt;/li&gt;
&lt;li&gt;在 hostOS 上執行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;directories&#34;&gt;directories&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Binary&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g., bin, sbin, lib, opt
&lt;ul&gt;
&lt;li&gt;bin: 有關 user 的指令&lt;/li&gt;
&lt;li&gt;sbin: 管理員會用的指令&lt;/li&gt;
&lt;li&gt;opt: optional software，多數機器中這是空的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configuration&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g., boot, etc,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g., home, root, srv, media, mnt, temp&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In memory
字面上的意思，不在 hard disk，在 memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g., dev, proc, sys&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;System Resources&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g., usr&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variable Data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g., var&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
