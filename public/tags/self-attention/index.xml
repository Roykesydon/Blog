<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>self-attention on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/tags/self-attention/</link>
        <description>Recent content in self-attention on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 30 Apr 2024 00:00:55 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/tags/self-attention/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MemAE è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/memae-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Tue, 30 Apr 2024 00:00:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/memae-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1904.02639&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;autoencoder åœ¨æ”¶åˆ°ç•°å¸¸è¼¸å…¥çš„æ™‚ï¼Œé æœŸæœƒç”Ÿå‡ºè¼ƒé«˜çš„ reconstruction errorã€‚ä½†æ˜¯é€™å€‹å‡è¨­å¯¦éš›ä¸Šä¸¦ä¸ç¸½æ˜¯ç™¼ç”Ÿã€‚ä»–æœ‰å¯èƒ½ã€Œæ³›åŒ–ã€çš„å¾ˆå¥½ï¼Œå°è‡´ç•°å¸¸çš„ä¹Ÿå¯ä»¥æ­£å¸¸é‡å»ºã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç·©è§£é€™å€‹å•é¡Œï¼Œæœ¬æ–‡å¹« auto-encoder åŠ ä¸Šä¸€å€‹ memory çš„æ©Ÿåˆ¶ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´éšæ®µï¼Œmemory è¦å­¸ç¿’ç”Ÿæˆ normal data çš„ prototypical elementsã€‚&lt;/p&gt;
&lt;p&gt;æ¸¬è©¦éšæ®µï¼Œlearned memory æœƒè¢«å›ºå®šä½ï¼Œç„¶å¾Œè¦æ ¹æ“š few selected memory é€²è¡Œé‡å»ºã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ç•°å¸¸è¼¸å…¥ï¼Œé€™å€‹é‡å»ºå‡ºçš„æ±è¥¿å°±æœƒæ¯”è¼ƒåƒæ˜¯ normal sampleã€‚é€™æ¨£ reconstruction error å°±æœƒè¢«å¢å¼·ã€‚&lt;/p&gt;
&lt;p&gt;MemAE æ˜¯ free of assumptionï¼Œæ‰€ä»¥å¯ä»¥ç”¨åœ¨å„ç¨®ä¸åŒçš„ä»»å‹™ä¸Šã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;AE æœ‰å¯èƒ½ã€Œæ³›åŒ–ã€çš„å¾ˆå¥½ï¼Œå°è‡´å°æ–¼ç•°å¸¸è¼¸å…¥ä¹Ÿå¯ä»¥æ­£å¸¸é‡å»ºã€‚å¦‚æœ anomolies å’Œ normal pattern æœ‰å…±äº«æŸäº› composition patternï¼Œæˆ–æ˜¯ decoder å¼·åˆ°é€£ abnormal encoding éƒ½å¯ä»¥é‡å»ºï¼Œå°±å¯èƒ½ç™¼ç”Ÿé€™æ¨£çš„ç‹€æ³ã€‚&lt;/p&gt;
&lt;p&gt;MemAE å¤šäº†ä¸€å€‹ memory moduleï¼Œå¾ encoder å‡ºä¾†çš„æ±è¥¿æœƒè¢«è¦–ä½œ queryï¼Œç”¨ä¾†æŠŠ memory ä¸­æœ€ç›¸é—œçš„å…ƒç´ å–å‡ºï¼Œç„¶å¾Œä½œ aggregationã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„é€²ä¸€æ­¥æå‡ºäº†ä¸€å€‹ä¸åŒçš„ hard shrinkage operatorï¼Œå¯ä»¥ç”Ÿå‡º sparsity of  memory addressing weightã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨“ç·´éšæ®µï¼ŒæœƒæŠŠ memory content é€£åŒ encoder å’Œ decoder ä¸€èµ·è¨“ç·´ï¼Œé€é sparse addressing strategyï¼ŒMemAE å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„ memory slot ä¾†è£½é€ å‡º prototypical normal patternsï¼Œä¾†è£½é€ å‡ºå¤ ä½çš„ reconstruction errorã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ¸¬è©¦éšæ®µï¼Œmemory content æœƒè¢«å›ºå®šä½ï¼Œç„¶å¾Œæ ¹æ“š few selected memory ä¾†é‡å»ºã€‚å› ç‚º memory module ä¸¦ä¸æ˜¯åŸºæ–¼æŸç¨®ç‰¹å®šè³‡æ–™çš„å‡è¨­ï¼Œæ‰€ä»¥å¯ä»¥ç”¨åœ¨å„ç¨®ä¸åŒçš„ä»»å‹™ä¸Šã€‚&lt;/p&gt;
&lt;h2 id=&#34;memory-augmented-autoencoder&#34;&gt;Memory-augmented Autoencoder&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MemAE/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;memory-module-with-attention-based-sparse-addressing&#34;&gt;Memory Module with Attention-based Sparse Addressing&lt;/h3&gt;
&lt;h4 id=&#34;attention-for-memory-addressing&#34;&gt;Attention for Memory Addressing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;attention weight $w_i$
&lt;ul&gt;
&lt;li&gt;$w_i=\frac{exp(d(z,m_i))}{\sum^{N}_{j=1}exp(d(z,m_j))}$&lt;/li&gt;
&lt;li&gt;$d(\cdot, \cdot)$ æ˜¯ similarity measurementï¼Œé€™è£¡æ˜¯ cosine similarity
&lt;ul&gt;
&lt;li&gt;$d(z,m_i)=\frac{zm_i^T}{||z|| \text{ } ||m_i||}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hard-shrinkage-for-sparse-addressing&#34;&gt;Hard Shrinkage for Sparse Addressing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;hard shrinkage
&lt;ul&gt;
&lt;li&gt;$\hat{w}_i=h(w_i;\lambda)=\begin{cases} w_i &amp;amp; \text{if } w_i&amp;gt;\lambda \\ 0 &amp;amp; \text{otherwise} \end{cases}$
&lt;ul&gt;
&lt;li&gt;é€™æ±è¥¿ä¸å¥½åš backpropagation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ”¹è‰¯ç‰ˆ
&lt;ul&gt;
&lt;li&gt;$\hat{w}_i=\frac{max(w_i-\lambda,0)}{|w_i-\lambda|+\epsilon}$
&lt;ul&gt;
&lt;li&gt;$max(\cdot, 0)$ å°±æ˜¯ ReLU&lt;/li&gt;
&lt;li&gt;æ ¹æ“šå¯¦é©—ï¼ŒæŠŠ $\lambda$ è¨­åœ¨ [1/N, 3/N] æœƒå¾—åˆ°é‚„ä¸éŒ¯çš„çµæœ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;shrinkage å¾Œæœƒå† normalize
&lt;ul&gt;
&lt;li&gt;$\hat{w}_i=\hat{w}_i/||\hat{w}||_1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sparse addressing æœ‰åŠ©æ–¼é¼“å‹µæ¨¡å‹ç”¨æ›´å°‘ä½†ç›¸é—œçš„ memory item ä¾†è¡¨ç¤º queryã€‚&lt;/p&gt;
&lt;p&gt;é¼“å‹µ memory addressing çš„ sparsity æ˜¯æœ‰ç›Šçš„ï¼Œå› ç‚º memory M è¢«è¨“ç·´ä¾†é©æ‡‰ sparse wã€‚&lt;/p&gt;
&lt;p&gt;é¼“å‹µ sparsity ä¹Ÿå¯ä»¥ç·©è§£ç•°å¸¸æ¨£æœ¬å¯ä»¥è¢«å¾ˆå¥½çš„é‡å»ºçš„å•é¡Œã€‚&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;åˆ†æˆå…©å€‹ loss
&lt;ul&gt;
&lt;li&gt;Reconstruction error
&lt;ul&gt;
&lt;li&gt;$R(x^t, \hat{x}^t)=||x^t-\hat{x}^t||^2_2$
&lt;ul&gt;
&lt;li&gt;$l2-norm$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;entropy of $\hat{w}^t$
&lt;ul&gt;
&lt;li&gt;ç”¨ä¾†é€²ä¸€æ­¥æå‡ sparsity&lt;/li&gt;
&lt;li&gt;$E(\hat{w}^t)=\sum^{T}_{i=1}-\hat{w}_i \cdot log(\hat{w}_i)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Combine
&lt;ul&gt;
&lt;li&gt;$L(\theta_e, \theta_d, M)=\frac{1}{T}\sum^{T}_{t=1}(R(x^t, \hat{x}^t)+\alpha E(\hat{w}^t))$
&lt;ul&gt;
&lt;li&gt;æ ¹æ“šå¯¦é©—ï¼Œ$\alpha$ è¨­æˆ 0.0002&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>ReALM è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/realm-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 18 Apr 2024 00:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/realm-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2403.20329&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ReALM: Reference Resolution As Language Modeling&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Reference resolution æ˜¯ä¸€å€‹é‡è¦çš„å•é¡Œï¼Œå°æ–¼ç†è§£å’Œå……åˆ†è™•ç†ä¸åŒé¡å‹çš„ context å¾ˆé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;Context åŒ…å« previous turns å’Œ non-conversational entitiesï¼Œæ¯”å¦‚ä½¿ç”¨è€…è¢å¹•ä¸Šçš„ entityï¼Œæˆ–æ˜¯åœ¨èƒŒæ™¯åŸ·è¡Œçš„ entityã€‚&lt;/p&gt;
&lt;p&gt;LLM é›–ç„¶åœ¨å„ç¨®ä»»å‹™éƒ½å¾ˆå¼·å¤§ï¼Œä½†å°æ–¼ reference resolution çš„ä½¿ç”¨ï¼Œç‰¹åˆ¥æ˜¯ non-conversational entities æ–¹é¢ï¼Œä¾ç„¶æ²’æœ‰å……åˆ†åˆ©ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å°‡å±•ç¤ºå¦‚ä½•å¼•ç”¨ LLM å‰µå»ºä¸€å€‹ç³»çµ±ä¾†è™•ç†ä¸åŒé¡å‹çš„ referenceï¼Œæ–¹æ³•æ˜¯å±•ç¤ºå¦‚ä½•æŠŠ reference resolution è½‰æ›æˆ language modeling problemã€‚
ç›¡ç®¡è¢å¹•ä¸Šçš„ entity å‚³çµ±ä¸Šä¸åˆ©æ–¼ç°¡åŒ–æˆ text-only modalityã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ GPT-4ï¼Œæœ¬æ–‡æœ€å°çš„æ¨¡å‹å¯¦ç¾äº†å’Œ GPT-4 ç›¸ç•¶çš„æ€§èƒ½ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹å‰‡å¤§å¹…é ˜å…ˆã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;äººé¡çš„èªè¨€å¾ˆé•·åŒ…å« ambiguous referenceï¼Œæ¯”å¦‚ã€Œé€™å€‹ã€ã€ã€Œé‚£å€‹ã€ç­‰ç­‰ï¼Œé€™äº› reference éœ€è¦æ ¹æ“š context ä¾†è§£æ±ºã€‚&lt;/p&gt;
&lt;p&gt;èƒ½å¤ èƒ½å¤ ç†è§£ context å° conversational assistant ä¾†èªªå¾ˆé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;è®“ä½¿ç”¨è€…èƒ½å¤ å°è¢å¹•ä¸Šçœ‹åˆ°çš„å…§å®¹ç™¼å‡ºæŸ¥è©¢æ˜¯ç¢ºä¿èªéŸ³åŠ©ç†å¯ä»¥æä¾› hands-free é«”é©—çš„ç¬¬ä¸€æ­¥ã€‚&lt;/p&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Speaker&lt;/th&gt;
&lt;th&gt;Dialogue&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;User&lt;/td&gt;
&lt;td&gt;Show me pharmacies near me&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Agent&lt;/td&gt;
&lt;td&gt;Here is a list I found.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Agent&lt;/td&gt;
&lt;td&gt;&amp;hellip; (list presented)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;User&lt;/td&gt;
&lt;td&gt;Call the one on Rainbow Rd.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;User&lt;/td&gt;
&lt;td&gt;Call the bottom one.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;User&lt;/td&gt;
&lt;td&gt;Call this number (present onscreen)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 1:  Sample Interactions between a user and an
agent.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ä»¥ Table 1 ç‚ºä¾‹ï¼Œå¦‚æœæ²’æœ‰ç†è§£ context çš„èƒ½åŠ›ï¼Œagent ä¸å¯èƒ½å®ŒæˆæŸ¥è©¢ã€‚&lt;/p&gt;
&lt;p&gt;ç…§ç†ä¾†èªªï¼Œè™•ç†ç”¨æˆ¶çš„æŸ¥è©¢éœ€è¦å¤šç¨®é¡å‹çš„ contextï¼ŒConversational context å’Œ on-screen context å°±æ˜¯å…©å€‹ä¸»è¦çš„ä¾‹å­ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ LLM é€šå¸¸å¯ä»¥å¯¦ç¾ End-to-End çš„é«”é©—ï¼Œç”šè‡³å¯èƒ½å¯ä»¥æ¶ˆé™¤åŒ…å« reference resolution çš„å¤šéšæ®µ pieplineã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯åœ¨ä¸€äº›å¯¦éš›æ¡ˆä¾‹ä¸­ï¼Œpipeline æœ‰ä»–çš„åƒ¹å€¼ï¼Œè€ƒæ…®ä»¥ä¸‹æƒ…å¢ƒï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;åœ¨é‹ç®—èƒ½åŠ›æœ‰é™çš„è¨­å‚™ä¸Šé‹ä½œ (ä¾‹å¦‚æ‰‹æ©Ÿ)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç”±æ–¼åŠŸè€—å’Œå»¶é²éœ€æ±‚ï¼Œå–®ä¸€å¤§å‹ End-to-End æ¨¡å‹å¯èƒ½ä¸é©ç”¨ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;éœ€è¦èˆ‡ API æ•´åˆçš„æƒ…å¢ƒ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é›–ç„¶å­˜åœ¨å¯ä»¥ç”¨ LLM ç·¨å¯«å‘¼å« API çš„æ–¹æ³•ï¼Œä½†ä¾ç„¶éœ€è¦è¶…å¤§çš„æ¨¡å‹é‚„æœ‰å°ç¾æœ‰ pipeline çš„å¾¹åº•è™•ç†ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½¿ç”¨ focused model å¯ä»¥å…è¨±ç¾æœ‰çš„ reference resolution module æ›¿æ›æˆæ›´æ–°çš„ç‰ˆæœ¬ï¼Œè€Œä¸”ç”±æ–¼ç³»çµ±æ˜¯æ¨¡çµ„åŒ–çš„ï¼Œå¯ä»¥æ”¹é€²èƒ½åŠ›å’Œå¯è§£é‡‹æ€§ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å°æ–¼æœ¬æ–‡æ‰€è€ƒæ…®çš„ä»»å‹™ï¼Œreference resolution ä¸åªé™å®šæ–¼ conversational contextï¼Œé‚„åŒ…å« on-screen entitiesï¼Œé€™äº› entities æ˜¯ä½¿ç”¨è€…å¯ä»¥æ„ŸçŸ¥åˆ°çš„ä¸€éƒ¨ä»½ï¼Œä½†æ˜¯é‚„æœªå‡ºç¾åœ¨å°è©±æ­·å²ç´€éŒ„ä¸­ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å› æ­¤ï¼Œç›¡ç®¡ä¸€äº› LLM å¯ä»¥ implicit åœ°è™•ç† reference resolutionï¼Œä½†æ˜¯å‚³çµ±çš„ NLP ä»»å‹™ (æ¯”å¦‚ reference resolution) ä»ç„¶æœ‰åƒ¹å€¼ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤æœ¬æ–‡æå€¡ç”¨è¼ƒå°çš„èªè¨€æ¨¡å‹ï¼Œä½†é‡å° reference resolution é€²è¡Œäº†å°ˆé–€ä¸”æ˜ç¢ºçš„ fine-tuningã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™ç¨®èªéŸ³åŠ©ç†æœƒé‡åˆ°æœ€å¤§çš„æŒ‘æˆ°å°±æ˜¯è¦æ€éº¼è®“ LM ã€Œçœ‹ã€åˆ°è¢å¹•ã€‚è€Œä¸”é‚„è¦ä»¥æœ‰åˆ©æ–¼ LM è§£æçš„æ–¹å¼å°è¢å¹•ä¸Šçš„ entity é€²è¡Œç·¨ç¢¼ï¼Œç„¶å¾Œç·¨ç¢¼é‚„å¾—è¶³å¤ ä¸€è‡´ï¼Œè®“ LM å¯ä»¥æˆåŠŸåŸ·è¡Œ reference resolutionã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å»ºè­°ä½¿ç”¨ä½¿ç”¨ parsed entity åŠå…¶ä½ç½®ä¾†é‡ä»¶è¢å¹•ï¼Œä»¥ç”¢ç”Ÿè¢å¹•çš„ç´”æ–‡å­—è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œè¢å¹•ä¸Šå±¬æ–¼ entity çš„éƒ¨åˆ†æœƒè¢«æ¨™è¨˜ï¼Œä»¥ä¾¿ LM èƒ½å¤ äº†è§£å¯¦é«”å‡ºç¾çš„ä½ç½®åŠä»–å€‘å‘¨åœçš„æ–‡å­—ã€‚&lt;/p&gt;
&lt;p&gt;æ“šä½œè€…æ‰€çŸ¥ï¼Œé€™æ˜¯ç¬¬ä¸€å€‹ç›®æ¨™æ˜¯å¾è¢å¹• encode context çš„ LLM å·¥ä½œã€‚&lt;/p&gt;
&lt;h2 id=&#34;related-work-and-motivation&#34;&gt;Related Work and Motivation&lt;/h2&gt;
&lt;p&gt;è§£æè¢å¹•ä¸Šçš„ reference æ˜¯ä¸€å€‹ç›®å‰æ¢ç´¢æ¯”è¼ƒä¸è¶³çš„é ˜åŸŸã€‚&lt;/p&gt;
&lt;p&gt;è¢å¹•ä¸Šçš„æ±è¥¿é€šå¸¸æ›´åŠ çµæ§‹åŒ–å’Œé«˜åº¦æ–‡å­—åŒ–ï¼Œä½¿çš„å¯ä»¥åˆ©ç”¨æ›´è¼•çš„æ¨¡å‹ä¾†è½‰æ›æˆæ–‡å­—ã€‚ä½†é›–ç„¶å®¹æ˜“è§£æï¼Œåˆ†å¸ƒå’Œé‚£ç¨®å¤§å‹è‚²è¨“ç·´çš„åŸºæ–¼åœ–åƒçš„ç³»çµ±ä¸åŒï¼Œå› ç‚ºä»–å€‘å¤šåŠéƒ½ç”¨è‡ªç„¶çš„ç¾å¯¦åœ–ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–é‚£äº›æ¨¡å‹çš„è¨“ç·´æˆæœ¬é€šå¸¸éƒ½éå¸¸é«˜ï¼Œè€Œä¸”åœ¨é€™ç¨®å¤§é‡æ–‡å­—çš„æƒ…å¢ƒä¹Ÿè¡¨ç¾ä¸ä½³ã€‚
å¸¸ç”¨æ–¼æ–‡å­—ç†è§£çš„æ–¹æ³•åˆå¸¸å¸¸ä¾è³´å¤šå€‹æ¨¡çµ„ï¼Œæ¯”å¦‚ bounding box detection å’Œ OCRã€‚&lt;/p&gt;
&lt;p&gt;è¯åˆè¦–è¦º + æ–‡å­—çš„æ¨¡å‹åœ¨è¨ˆç®—æˆæœ¬ä¹Ÿæ›´åŠ æ˜‚è²´ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼è¢å¹•ä¸Šçš„åƒè€ƒæœ‰ä¸€äº›ç›¸é—œå·¥ä½œï¼Œè¢«ç”¨ä½œæœ¬æ–‡çš„ baselineã€‚&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œä»–å€‘æœ‰äº›ç¼ºé»ï¼Œåœ¨æœ¬æ–‡è§£æ±ºäº†é€™äº›ç¼ºé»ã€‚&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ä»–å€‘çš„æ–¹æ³•ä¾è³´å°ˆç”¨çš„ ã€ŒCategory Moduleã€ä¾†è™•ç† type-based referenceï¼Œæ¯æ¬¡å»ºç«‹æ–°é¡å‹ï¼Œéƒ½éœ€è¦æ‰‹å‹•åŠ å…¥ Entity&lt;/li&gt;
&lt;li&gt;æ­¤é¡ Module å°‡æ¯ç¨®é¡åˆ¥è¦–ç‚ºä¸åŒçš„ï¼Œå¿½ç•¥äº†ä»–å€‘çš„ç›¸ä¼¼æ€§&lt;/li&gt;
&lt;li&gt;ä¾è³´æ‰‹å·¥è£½ä½œçš„è¦å‰‡ï¼Œå¾€å¾€éœ€è¦å¤§é‡ç‰¹å¾µå·¥ç¨‹ï¼Œè€Œä¸”ä¸ robust&lt;/li&gt;
&lt;li&gt;ä¸è€ƒæ…®èªæ„ç›¸ä¼¼æ€§ï¼Œä¹Ÿæ²’æ³•å°ç¾å¯¦çš„ç†è§£å’Œå¸¸è­˜é€²è¡Œæ¨ç†&lt;/li&gt;
&lt;li&gt;é€™äº›æ–¹æ³•æœƒç¨ç«‹åœ°åˆ¤æ–·å¯¦é«”å’Œ Query çš„é—œè¯æ€§ï¼Œè€Œä¸è€ƒæ…®å…¶ä»–æ‰€æœ‰çš„å¯¦é«”ï¼ŒæŸ¥è©¢æ—¢ä¸è€ƒæ…®æ•´å€‹è¢å¹•ä¹Ÿä¸è€ƒæ…®å…¶ä»–å¯¦é«”&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;task&#34;&gt;Task&lt;/h2&gt;
&lt;p&gt;ä½œè€…æäº†ä¸‰ç¨®å’Œä½¿ç”¨è€…ç›¸é—œçš„ Entity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;On-screen Entities&lt;/li&gt;
&lt;li&gt;Conversational Entities
&lt;ul&gt;
&lt;li&gt;å¯èƒ½æ˜¯ä¾†è‡ªä¸Šä¸€è¼ªçš„å°è©±ï¼Œä¹Ÿå¯èƒ½æ˜¯ virtual assistant ç”¢ç”Ÿçš„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Background Entities
&lt;ul&gt;
&lt;li&gt;ä¾†è‡ªå¾Œå° process çš„ç›¸é—œå¯¦é«”ï¼Œä½¿ç”¨è€…ä¸ä¸€å®šåœ¨è¢å¹•ä¸Šçœ‹çš„åˆ°ï¼Œæ¯”å¦‚èƒŒæ™¯çš„éŸ³æ¨‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;æœ¬æ–‡å°‡ reference resolution ä½œç‚º LLM çš„ multiple choice taskï¼Œé æœŸè¼¸å‡ºæ˜¯ç”¨æˆ¶è¢å¹•ä¸Šé¡¯ç¤ºçš„å…¶ä¸­ä¸€å€‹ Entityã€‚&lt;/p&gt;
&lt;p&gt;ç­”æ¡ˆä¹Ÿå¯èƒ½æ˜¯ã€ŒNone of theseã€ã€‚&lt;/p&gt;
&lt;h2 id=&#34;datasets&#34;&gt;Datasets&lt;/h2&gt;
&lt;p&gt;æ¯ç­†è³‡æ–™åŒ…å«ä½¿ç”¨è€…æŸ¥è©¢å’Œ Entity listï¼Œé‚„æœ‰èˆ‡å°æ‡‰ä½¿ç”¨è€…æŸ¥è©¢ç›¸é—œçš„ ground truth entityã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹å¯¦é«”åˆåŒ…å«èˆ‡å…¶ç›¸é—œçš„è¨Šæ¯ï¼Œæ¯”å¦‚èˆ‡å¯¦é«”é—œè¯çš„åç¨±å’Œå…¶ä»–æ–‡å­—æè¿°è¨Šæ¯ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼å­˜åœ¨ on-screen context çš„è³‡æ–™ï¼Œcontext ä»¥ Entity çš„é‚Šç•Œæ¡†ï¼Œåœç¹å®ƒçš„ç‰©ä»¶æ¸…å–®é‚„æœ‰å‘¨åœç‰©ä»¶çš„å±¬æ€§ (æ¯”å¦‚é¡å‹ã€æ–‡å­—å…§å®¹ã€ä½ç½®) ä¾†æä¾›&lt;/p&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Train Set&lt;/th&gt;
&lt;th&gt;Test Set&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Conversational&lt;/td&gt;
&lt;td&gt;2.3k&lt;/td&gt;
&lt;td&gt;1.2k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Synthetic&lt;/td&gt;
&lt;td&gt;3.9k&lt;/td&gt;
&lt;td&gt;1.1k&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;On-screen&lt;/td&gt;
&lt;td&gt;10.1k&lt;/td&gt;
&lt;td&gt;1.9k&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 2: Dataset Sizes (Train Set and Test Set)&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;conversational-data&#34;&gt;Conversational Data&lt;/h3&gt;
&lt;p&gt;æ”¶é›†ä½¿ç”¨è€…å’Œ agent çš„ç›¸é—œè³‡æ–™ã€‚&lt;/p&gt;
&lt;p&gt;è©•åˆ†è€…æœƒçœ‹åˆ°å¸¶æœ‰æ‰€æä¾› Entity æ¸…å–®çš„æˆªåœ–ï¼Œä¸¦è¦æ±‚æä¾›å¼•ç”¨æ¸…å–®ä¸­ä»»æ„æŒ‘é¸çš„ Entity çš„ Queryã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼Œå¯èƒ½æœƒçµ¦ä½¿ç”¨è€…çœ‹ä¼æ¥­æ¸…å–®ï¼Œä½¿ç”¨è€…å¯èƒ½æœƒèªªã€ŒCall the one on Main Streetã€ã€‚&lt;/p&gt;
&lt;h3 id=&#34;synthetic-data&#34;&gt;Synthetic Data&lt;/h3&gt;
&lt;p&gt;é€é template å»åˆæˆè³‡æ–™ã€‚&lt;/p&gt;
&lt;p&gt;æœ‰å…©ç¨® template:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Base Template
&lt;ul&gt;
&lt;li&gt;åŒ…å« mentions, entities, possible slots&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language Template
&lt;ul&gt;
&lt;li&gt;é™¤äº† Base é‚„æœƒæ–°å¢ä¸åŒè®Šé«”çš„ query&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;on-screen-data&#34;&gt;On-screen Data&lt;/h3&gt;
&lt;p&gt;å¾å­˜åœ¨é›»è©±ã€é›»å­éƒµä»¶å’Œå¯¦éš›åœ°å€çš„å„ç¨®ç¶²é è’é›†çš„ã€‚&lt;/p&gt;
&lt;p&gt;é€²è¡Œå…©éšæ®µçš„ annotationï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æ ¹æ“šè¢å¹• extract query
&lt;ul&gt;
&lt;li&gt;è©•åˆ†è€…æœƒæ”¶åˆ°ä¸€å¼µå¸¶æœ‰ç¶ è‰²æ–¹æ¡†å’Œç´…è‰²æ–¹æ¡†çš„æˆªåœ–ï¼Œè¦æŠŠç¶ æ–¹æ¡†æ–¹é¡ç‚ºé›»è©±ã€é›»å­éƒµä»¶æˆ–åœ°å€ç­‰ç­‰&lt;/li&gt;
&lt;li&gt;ç„¶å¾Œè¦ç‚ºç¶ æ¡†æä¾›ä¸‰å€‹ç¨ç‰¹çš„ query&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ ¹æ“š query è­˜åˆ¥ entity å’Œ mention
&lt;ul&gt;
&lt;li&gt;å‰é¢è’é›†çš„ query æœƒè¢«ä¸€ä¸€å±•ç¤ºçµ¦è©•åˆ†è€…ï¼Œä¸¦å¸¶æœ‰ç›¸æ‡‰çš„æˆªåœ–ï¼Œä½†æ²’æœ‰ bounding boxï¼Œé‚„æœƒæä¾›æ‰€æœ‰ screen entity list&lt;/li&gt;
&lt;li&gt;è©•åˆ†è€…è¦è©•ä¼° query æ˜¯å¦åŒ…å«çµ¦å®šçš„ visual entityï¼Œé‚„æœ‰è½èµ·ä¾†è‡ªä¸è‡ªç„¶&lt;/li&gt;
&lt;li&gt;æ­¤å¤–ï¼Œè©•åˆ†è€…é‚„è¦å¾æ¸…å–®ä¸­é¸å‡º query æåŠçš„ entityï¼Œé‚„è¦æ¨™è¨˜å®ƒå€‘åœ¨ query çš„å“ªè£¡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;
&lt;h3 id=&#34;marrs&#34;&gt;MARRS&lt;/h3&gt;
&lt;p&gt;é€™å€‹æ˜¯ baselineï¼Œé LLMï¼Œå’Œå‰äººçš„æ–¹æ³•å»åšæ¯”è¼ƒã€‚&lt;/p&gt;
&lt;p&gt;å‰äººçš„æ–¹æ³•è‘—é‡æ–¼è¢å¹•ä¸Šçš„ entityï¼ŒMARRS ä¹Ÿå‚¾å‘æ–¼ conversational å’Œ background entitiesã€‚&lt;/p&gt;
&lt;p&gt;è¦æ³¨æ„çš„æ˜¯ï¼Œå’Œå…¶ä»–é€šç”¨çš„ LLM ç›¸æ¯”ï¼Œä½œè€…çš„ baseline æ˜¯å°ˆé–€ç‚ºäº† reference resolution è€Œè¨­è¨ˆçš„ã€‚&lt;/p&gt;
&lt;h3 id=&#34;chatgpt&#34;&gt;ChatGPT&lt;/h3&gt;
&lt;p&gt;ä½œè€…è€ƒæ…® GPT-3.5 å’Œ GPT-4 ä½œç‚ºå¦å¤–ä¸€å€‹ baselineã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ GPT-3.5ï¼Œè¼¸å…¥åªåŒ…å« promptã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ GPT-4ï¼Œå› ç‚ºä»–å¯ä»¥æ¥å—åœ–ç‰‡çš„ contextï¼Œæ‰€ä»¥ç‚ºç³»çµ±æä¾›äº†è¢å¹•æˆªåœ–ï¼Œç™¼ç¾å¯ä»¥æœ‰æ•ˆæé«˜å…¶æ•ˆèƒ½ã€‚&lt;/p&gt;
&lt;h3 id=&#34;our-approach&#34;&gt;Our Approach&lt;/h3&gt;
&lt;p&gt;ä½œè€…éµå¾ªä»¥ä¸‹ pipeline ä¾†å¾®èª¿æ¨¡å‹ (FLAN-T5)&lt;/p&gt;
&lt;p&gt;å‘æ¨¡å‹æä¾›è§£æå¾Œçš„è¼¸å…¥ï¼Œä¸¦ç”¨ä¾†å¾®èª¿ã€‚&lt;/p&gt;
&lt;p&gt;å’Œ Baseline ä¸åŒï¼Œä½œè€…æ²’æœ‰åœ¨ FLAN-T5 ä¸Šé€²è¡Œå¤§è¦æ¨¡çš„è¶…åƒæ•¸æœç´¢ï¼Œè€Œæ˜¯å …æŒä½¿ç”¨é è¨­çš„åƒæ•¸ã€‚&lt;/p&gt;
&lt;h4 id=&#34;conversational-references&#34;&gt;Conversational References&lt;/h4&gt;
&lt;p&gt;ä½œè€…å‡è¨­ conversational references æœ‰å…©ç¨®ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Type-based
&lt;ul&gt;
&lt;li&gt;é€™ç¨®éå¸¸ä¾è³´è¦å°‡ query å’Œ entity type çµåˆï¼Œå¥½é€²è¡Œè­˜åˆ¥&lt;/li&gt;
&lt;li&gt;æ¯”å¦‚ã€Œæ‰“çµ¦ä»–ã€ï¼Œå¯ä»¥çŸ¥é“è¦çš„æ˜¯é›»è©±è™Ÿç¢¼ï¼Œè€Œä¸æ˜¯åœ°å€&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Descriptive
&lt;ul&gt;
&lt;li&gt;å‚¾å‘æ–¼ç”¨å”¯ä¸€çš„å¯¦é«”å±¬æ€§ä¾†æ¨™è¨˜ä»–ï¼Œæ¯”å¦‚ã€Œæ™‚ä»£å»£å ´çš„é‚£å€‹ã€&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;referenece é€šå¸¸æœƒåŒæ™‚ä¾è³´ type å’Œæè¿°ï¼Œæ¯”å¦‚ã€Œplay the one from Abbey Roadã€å’Œã€Œdirections to the one on Abbey Roadã€ã€‚&lt;/p&gt;
&lt;h4 id=&#34;onscreen-references&#34;&gt;Onscreen References&lt;/h4&gt;
&lt;p&gt;ä½œè€…å‡è¨­å­˜åœ¨èƒ½å¤ è§£æè¢å¹•æ–‡å­—ä»¥æå– Entity çš„ä¸Šæ¸¸è³‡æ–™åµæ¸¬å™¨ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ä»¥åƒ…æ¶‰åŠæ–‡å­—çš„æ–¹å¼å°‡å…¶ç·¨ç¢¼åˆ° LM ä¸­ï¼Œä½¿ç”¨ Algorithm 2ã€‚&lt;/p&gt;
&lt;p&gt;ç›´è§€ä¸Šï¼Œå‡è¨­æ‰€æœ‰å¯¦é«”å’Œå‘¨åœç‰©ä»¶çš„ä½ç½®éƒ½å¯ä»¥ç”¨å®ƒå€‘å„è‡ªçš„é‚Šç•Œæ¡†çš„ä¸­å¿ƒä¾†è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œå¾ä¸Šåˆ°ä¸‹å°é€™äº›ä¸­å¿ƒé€²è¡Œæ’åºï¼Œä¸¦ç”¨ stable sort å¾å·¦åˆ°å³æ’åºã€‚&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œï¼Œæ‰€æœ‰åœ¨ä¸€å€‹ margin å…§çš„ç‰©ä»¶éƒ½æœƒè¢«è¦–ä½œåœ¨ same lineï¼Œä¸¦ç”¨ tab éš”é–‹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ margin ä¸‹æ–¹å¤–é¢çš„æœƒè¢«æ”¾åœ¨ä¸‹ä¸€è¡Œï¼Œç„¶å¾Œä¸æ–·é‡è¤‡ã€‚&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Conv&lt;/th&gt;
&lt;th&gt;Synth&lt;/th&gt;
&lt;th&gt;Screen&lt;/th&gt;
&lt;th&gt;Unseen&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MARRS&lt;/td&gt;
&lt;td&gt;92.1&lt;/td&gt;
&lt;td&gt;99.4&lt;/td&gt;
&lt;td&gt;83.5&lt;/td&gt;
&lt;td&gt;84.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-3.5&lt;/td&gt;
&lt;td&gt;84.1&lt;/td&gt;
&lt;td&gt;34.2&lt;/td&gt;
&lt;td&gt;74.1&lt;/td&gt;
&lt;td&gt;67.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-4&lt;/td&gt;
&lt;td&gt;97.0&lt;/td&gt;
&lt;td&gt;58.7&lt;/td&gt;
&lt;td&gt;90.1&lt;/td&gt;
&lt;td&gt;98.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ReALM-80M&lt;/td&gt;
&lt;td&gt;96.7&lt;/td&gt;
&lt;td&gt;99.5&lt;/td&gt;
&lt;td&gt;88.9&lt;/td&gt;
&lt;td&gt;99.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ReALM-250M&lt;/td&gt;
&lt;td&gt;97.8&lt;/td&gt;
&lt;td&gt;99.8&lt;/td&gt;
&lt;td&gt;90.6&lt;/td&gt;
&lt;td&gt;97.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ReALM-1B&lt;/td&gt;
&lt;td&gt;97.9&lt;/td&gt;
&lt;td&gt;99.7&lt;/td&gt;
&lt;td&gt;91.4&lt;/td&gt;
&lt;td&gt;94.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ReALM-3B&lt;/td&gt;
&lt;td&gt;97.9&lt;/td&gt;
&lt;td&gt;99.8&lt;/td&gt;
&lt;td&gt;93.0&lt;/td&gt;
&lt;td&gt;97.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table 3:&lt;/p&gt;
&lt;p&gt;é æ¸¬æ­£ç¢ºçš„æ¨™æº–æ˜¯è¦æ­£ç¢ºé æ¸¬æ‰€æœ‰ç›¸é—œå¯¦é«”ï¼Œä¸ç„¶å°±æ˜¯éŒ¯çš„&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾å®ƒå€‘çš„æ–¹æ³•è´ MARRS å’Œ GPT-3.5ã€‚
GPT-3.5 çš„é¤æ•¸é‡é‚„å¤šå‡ºäº†å¹¾å€‹æ•¸é‡ç´šã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡æ¨¡å‹ç›¸è¼ƒ GPT-4 è¼•çš„å¤šï¼Œä½†æ˜¯æ€§èƒ½å¤§è‡´ç›¸åŒã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”ä½œè€…æ¡ç”¨æ–‡å­—ç·¨ç¢¼çš„æ–¹æ³•èƒ½å¤ è®“æ¨¡å‹å’Œ GPT-4 å¹¾ä¹åŒç­‰æ•ˆèƒ½ï¼Œå¾Œè€…é‚„æä¾›äº†è¢å¹•æˆªåœ–ã€‚&lt;/p&gt;
&lt;p&gt;éš¨è‘—æ¨¡å‹åŠ å¤§ï¼Œæå‡ä¹Ÿå¾ˆæ˜é¡¯ï¼Œç‰¹åˆ¥æ˜¯ Screenï¼Œæš—ç¤ºä»»å‹™æœ¬è³ªä¸Šæ›´åŠ è¤‡é›œã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$GPT-4 \approx ReALM &amp;raquo; MARRS$ for new use-cases
&lt;ul&gt;
&lt;li&gt;æ¸¬è©¦äº† zero-shot learning çš„èƒ½åŠ›ï¼ŒTable 3 æœ€å¾Œä¸€å€‹ column å°±æ˜¯åœ¨æ¯”è¼ƒå¾æœªè¦‹éçš„è³‡æ–™é›†ã€‚&lt;/li&gt;
&lt;li&gt;ä½œè€…ç™¼ç¾å°æ–¼æ¸¬è©¦é›†ï¼Œæ‰€æœ‰ LLM-based çš„æ–¹æ³•éƒ½è´é Fine-tuned Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ReaLM &amp;gt; GPT-4 for domain-specific queries
&lt;ul&gt;
&lt;li&gt;ä½œè€…ç™¼ç¾ ReALM å› ç‚ºæœ‰å° user requests åš fine-tuningï¼Œæ‰€ä»¥æ›´èƒ½ç†è§£ domain-specific questionsï¼Œå¦‚ä¸‹è¡¨ (Table 4)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;User Request: Can you make it brighter?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Entities Shown to User:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1. Type: Settings
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2. Type: UserEntity | homeAutomationAccessoryName
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;GPT-4 Prediction: 1 Ground Truth: 1, 2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Table 4&lt;/p&gt;
&lt;hr&gt;
</description>
        </item>
        <item>
        <title>SimSID è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/simsid-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sat, 13 Apr 2024 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/simsid-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2403.08689&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;å€‹äººå‰è¨€&#34;&gt;å€‹äººå‰è¨€&lt;/h2&gt;
&lt;p&gt;é€™ç¯‡æ–‡ç« æå‡ºäº†è¨±å¤šåœ¨ SQUID è«–æ–‡æœ‰å‡ºç¾éçš„çš„å…§å®¹ï¼Œç‰¹åˆ¥æ˜¯æœ‰é—œæ–¼æ”¾å°„ç·šæˆåƒçš„æè¿°ç­‰ç­‰ï¼Œå»ºè­°å…ˆçœ‹é SQUIDï¼Œé‡è¤‡çš„è«–é»ä¸å†æåŠã€‚&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æå‡ºä¸€ç¨®ç°¡å–®çš„ Space-aware Memoryï¼Œç”¨æ–¼ä¿®å¾©æ”¾å°„ç·šæˆåƒçš„ç•°å¸¸ã€‚&lt;/p&gt;
&lt;p&gt;å°‡ç•°å¸¸æª¢å®šåˆ¶å®šç‚º image reconstruction taskï¼Œç”¨ space-aware memory matrix å’Œ in-painting block çµ„æˆã€‚&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;our-previous-work&#34;&gt;Our Previous Work&lt;/h3&gt;
&lt;p&gt;ç›¸å°æ–¼ SQUIDï¼Œæœ¬æ–‡æœ‰ä»¥ä¸‹å››é»æ”¹é€²ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å¼•å…¥äº†æ–°çš„ç¬¦è™Ÿã€å…¬å¼å’Œåœ–è¡¨ï¼Œä»¥åŠè©³ç´°çš„æ–¹æ³•èªªæ˜åŠå­¸ç¿’ç›®æ¨™&lt;/li&gt;
&lt;li&gt;åˆªé™¤ Memory Queue å’Œ Masked shortcutï¼Œç°¡åŒ–æ¡†æ¶ï¼Œé‚„æé«˜äº†æ•ˆèƒ½&lt;/li&gt;
&lt;li&gt;åœ¨ä¸‰ç¨®æ”¾å°„ç·šæˆåƒä»»å‹™å‹éå…¶ä»– 21 ç¨® SOTA æ–¹æ³•&lt;/li&gt;
&lt;li&gt;ç ”ç©¶äº† SimSID å°æ–¼ desease-free (è¨“ç·´é›†ä¸­å°ç•°å¸¸è³‡æ–™çš„å®¹å¿) çš„ç©©å¥æ€§&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;simsid&#34;&gt;SimSID&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SimSID/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;developing-space-aware-and-hierarchical-memory&#34;&gt;Developing Space-aware and Hierarchical Memory&lt;/h3&gt;
&lt;h4 id=&#34;space-aware-memory&#34;&gt;Space-aware memory&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;$\hat{z}_{i,j}$ (augmented feature)&lt;/p&gt;
&lt;p&gt;$=\displaystyle\sum^N_{k=1}G(s^{k})M^k_{i,j}$&lt;/p&gt;
&lt;p&gt;$s^k$ æ˜¯åšå…§ç©ç®—å‡ºä¾†çš„ç›¸ä¼¼åº¦&lt;/p&gt;
&lt;p&gt;$G(\cdot)$ æ˜¯ Gumbel-softmax (ç”¨ SQUID çš„ Gumbel Shrinkage)&lt;/p&gt;
&lt;p&gt;$Memory Matrix$ è¢«æ‹†æˆå¤šå€‹ blockï¼Œæ‰æœ‰ $M_{i,j}$&lt;/p&gt;
&lt;h4 id=&#34;hierarchical-memory&#34;&gt;Hierarchical memory&lt;/h4&gt;
&lt;p&gt;åœ¨ encoder æœ€æ·±è™•ä½¿ç”¨ä¸€å€‹ memory matrix (in-painting block) ä¸è¶³ä»¥é‡å»ºå…·æœ‰ç´°ç¯€çš„é«˜å“è³ªåœ–åƒã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ•æ‰ä¸åŒå°ºåº¦çš„ anatomic patternsï¼Œæå‡ºåœ¨ generator çš„å¤šå€‹ level æ”¾ç½® space-aware memoryã€‚&lt;/p&gt;
&lt;p&gt;ç ”ç©¶ç™¼ç¾ï¼Œå¤ªå¤šçš„ memory æœƒå°è‡´éåº¦çš„ information filteringï¼Œé‚„æœƒ degrade æ¨¡å‹ï¼Œå°è‡´ä»–åªæœƒä¿ç•™æœ€å…·ä»£è¡¨æ€§çš„ normal patternï¼Œè€Œä¸æ˜¯æ‰€æœ‰éœ€è¦çš„ patternã€‚&lt;/p&gt;
&lt;p&gt;é€™å€‹å•é¡Œå¯ä»¥é€éæ·»åŠ  skip connection ä¾†è§£æ±ºã€‚&lt;/p&gt;
&lt;p&gt;å¾ç¶“é©—ç™¼ç¾ä¸‰å€‹ memory matrix å°±å·²è¶³å¤ ã€‚&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SimSID/fig7.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-simsid-on-three-public-datasets&#34;&gt;Benchmarking SimSID on Three Public Datasets&lt;/h3&gt;
&lt;p&gt;å°æ–¼æ­£å¸¸çš„æƒ…æ³ï¼Œ SimSID å¯ä»¥åœ¨ memory ä¸­æ‰¾åˆ°ç›¸ä¼¼çš„åŒ¹é…ï¼Œç„¶å¾Œé †åˆ©é‡å»ºã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ç•°å¸¸ï¼Œå°‡å½é€ çš„æ­£å¸¸ç‰¹å¾µå¼·åŠ åˆ°ç•°å¸¸ç‰¹å¾µï¼Œå°±æœƒç”¢ç”ŸçŸ›ç›¾ã€‚&lt;/p&gt;
&lt;p&gt;åœ– 7 ç¹ªè£½ Discriminator çš„ heatmap ä¾†æŒ‡ç¤ºå‡ºé‡å»ºæ•ˆæœä¸ä½³çš„éƒ¨åˆ†ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ğŸ¦‘SQUIDğŸ¦‘ è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 31 Mar 2024 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.13495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Radiography imaging protocols (æ”¾å°„ç·šæˆåƒå”å®š) æœƒå°ˆæ³¨æ–¼ç‰¹å®šçš„èº«é«”å€åŸŸï¼Œå› æ­¤æœƒåœ¨æ‚£è€…é–“ç”¢ç”Ÿå¤§é‡ç›¸ä¼¼çš„ç…§ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†åˆ©ç”¨é€™ç¨® structed informationï¼Œä½œè€…æå‡ºäº† Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (SQUID)ï¼Œå®ƒå¯ä»¥æŠŠå›ºæœ‰çš„äººé«”çµæ§‹åˆ†é¡ç‚ºåè¦†å‡ºç¾çš„ patternã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ¨ç†ç‹€æ…‹ä¸‹ï¼Œå®ƒå¯ä»¥è­˜åˆ¥åœ–ç‰‡ä¸­çš„ç•°å¸¸æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”è¼ƒå…©å€‹ chest X-ray benchmarkï¼ŒSQUID åœ¨éç›£ç£ç•°å¸¸æª¢æ¸¬ä¸Šè¶…è¶Šäº† 13 ç¨® SOTA æ–¹æ³•è‡³å°‘ 5 å€‹ç™¾åˆ†é»ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„å‰µå»ºäº†ä¸€å€‹æ–°çš„è³‡æ–™é›† (DigitAnatomy)ï¼Œè©²è³‡æ–™é›†çµåˆäº†èƒ¸è…”è§£å‰–å­¸ä¸­çš„ spatial correlation å’Œ consistent shape é€™å…©å€‹ç‰¹æ€§ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;æ”¾å°„ç·šæˆåƒå’Œä¸€èˆ¬åœ–ç‰‡çš„å·®åˆ¥
&lt;ul&gt;
&lt;li&gt;ä¸€èˆ¬çš„ photographic imaging å’Œ radiography imaging æ˜¯ä¸åŒçš„ã€‚ä¸€èˆ¬çš„åœ–ç‰‡ç‰©é«”ï¼Œæˆ‘å€‘æœƒå‡è¨­ translation invariance (å¹³ç§»ä¸è®Šæ€§)ï¼Œç„¡è«–è²“åœ¨å·¦å³ï¼Œéƒ½æ˜¯è²“ã€‚ä½†æ˜¯åœ¨æ”¾å°„ç·šæˆåƒä¸­ï¼Œçµæ§‹çš„ç›¸å°ä½ç½®å’Œæ–¹å‘æ˜¯è¾¨åˆ¥æ­£å¸¸å’Œç•°å¸¸çš„é‡è¦ç‰¹å¾µã€‚&lt;/li&gt;
&lt;li&gt;è€Œä¸”ç”±æ–¼ radiography imaging protocols ä»¥ç›¸ç•¶ä¸€è‡´çš„æ–¹å‘è©•ä¼°æ‚£è€…ï¼Œæˆåƒåœ¨ä¸åŒçš„è¨­å‚™è£½é€ å•†ã€è¨­æ–½ä½ç½®é‚„æœ‰æ‚£è€…çš„æƒ…æ³ä¸‹ï¼Œéƒ½å…·æœ‰å¾ˆå¤§çš„ç›¸ä¼¼æ€§ã€‚åƒé€™æ¨£åè¦†å‡ºç¾ä¸”ä¸€è‡´çš„çµæ§‹ï¼Œæœ‰åŠ©æ–¼åˆ†æå•é¡Œï¼Œæ˜¯æ”¾å°„ç·šæˆåƒçš„å„ªå‹¢ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æœ‰å¤šé …ç ”ç©¶è­‰æ˜äº†è¨±å¤šå…ˆé©—çŸ¥è­˜åœ¨å¢å¼·æ·±åº¦å­¸ç¿’æ¨¡å‹æ€§èƒ½ä¸Šçš„å„ªå‹¢ï¼Œæ¯”å¦‚æ·»åŠ  location featuresã€ä¿®æ”¹ç›®æ¨™å‡½æ•¸é‚„æœ‰ç´„æŸç›¸å°æ–¼ç…§ç‰‡ä¸­ landmarks çš„ç›¸å°åº§æ¨™ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æƒ³è§£æ±ºçš„å•é¡Œ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¤šé” 80% çš„è‡¨åºŠéŒ¯èª¤æ˜¯ç”±æ–¼æ”¾å°„ç§‘é†«ç”Ÿæ¼æ‰ç•°å¸¸è€Œé€ æˆã€‚&lt;/li&gt;
&lt;li&gt;æœ¬æ–‡æƒ³å›ç­”ä¸€å€‹é—œéµå•é¡Œï¼šæœ‰æ²’æœ‰è¾¦æ³•åˆ©ç”¨ anatomical patterns çš„ consistency å’Œ spatial informationï¼Œåœ¨æ²’æœ‰æ‰‹å‹•æ¨™è¨»çš„æƒ…æ³ä¸‹ï¼ŒåŠ å¼·æ·±åº¦å­¸ç¿’æ¨¡å‹çš„ç•°å¸¸æª¢æ¸¬èƒ½åŠ›ï¼Ÿéç›£ç£çš„ç•°å¸¸æª¢æ¸¬åªç”¨å¥åº·çš„åœ–ç‰‡é€²è¡Œè¨“ç·´ï¼Œä¸ç”¨ç–¾ç—…è¨ºæ–·æˆ–ä»»ä½• labelã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SQUID è§£æ±ºè¾¦æ³•&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ¬æ–‡ä¸åƒå…ˆå‰çš„ç•°å¸¸æª¢æ¸¬æ–¹æ³•ï¼Œæœ¬æ–‡æŠŠ task åˆ¶å®šç‚º in-painting task (åœ–åƒä¿®å¾©)ï¼Œå¥½åˆ©ç”¨æ”¾å°„ç·šæˆåƒçš„å¤–è§€ã€ä½ç½®ã€å¸ƒå±€ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½œè€…æå‡ºäº† SQUIDï¼Œåœ¨è¨“ç·´éç¨‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥é€éç©ºé–“ä¸­ç¶“å¸¸å‡ºç¾çš„ anoatomical patterns ä¾†å‹•æ…‹ç¶­è­·ä¸€å€‹ visual pattern dictionaryã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”±æ–¼è§£å‰–å­¸çš„ consistencyï¼Œå¥åº·æˆåƒä¸­çš„èº«é«”å€åŸŸæœƒå‘ˆç¾é¡ä¼¼çš„ visual patternï¼Œä½¿ unique pattern çš„æ•¸é‡æ˜¯å¯æ§çš„ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨æ¨ç†éšæ®µï¼Œç”±æ–¼ dictionary ä¸å­˜åœ¨ anomaly patternï¼Œå› æ­¤å¦‚æœå­˜åœ¨ç•°å¸¸ï¼Œç”¢ç”Ÿçš„æ”¾å°„ç·šæˆåƒæœƒå’Œç¾å¯¦æœ‰æ‰€å·®è·ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯ä»¥é€éå€åˆ†ä¿®å¾©ä»»å‹™çš„å“è³ªä¾†è­˜åˆ¥ç•°å¸¸ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯¦é©—å‡è¨­&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç•°å¸¸æª¢æ¸¬çš„æˆåŠŸåŸºæ–¼å…©å€‹å‡è¨­
&lt;ul&gt;
&lt;li&gt;è³‡æ–™ä¸­å¾ˆå°‘ç•°å¸¸åœ–ç‰‡&lt;/li&gt;
&lt;li&gt;ç•°å¸¸å’Œæ­£å¸¸æœ‰é¡¯è‘—ä¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯¦é©—&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨å…©å€‹å¤§è¦æ¨¡ã€å…¬é–‹çš„æ”¾å°„ç·šæˆåƒè³‡æ–™é›†ä¸Šå¯¦é©—
&lt;ul&gt;
&lt;li&gt;ZhangLab
&lt;ul&gt;
&lt;li&gt;åœ¨éç›£ç£æ–¹é¢è´ SOTA è¶…é 5 å€‹ç™¾åˆ†é»&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;æ¯”æœ€è¿‘çš„ 13 ç¨®æ–¹æ³•æé«˜ 10 å€‹ç™¾åˆ†é»&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ–°è³‡æ–™é›†&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‰µå»ºäº† DigitAnatomy è³‡æ–™é›†ï¼Œé—¡æ˜èƒ¸è…”è§£å‰–çµæ§‹çš„ spatial correlation å’Œ consistent shapeã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è²¢ç»ç¸½çµ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨èƒ¸è…”æ”¾å°„ç·šæˆåƒçš„æ–°éç›£ç£ SOTA ç•°å¸¸æª¢æ¸¬æ–¹æ³•&lt;/li&gt;
&lt;li&gt;æ–°çš„ç¶œåˆè³‡æ–™é›†&lt;/li&gt;
&lt;li&gt;ç™¼æ˜æ–°æ–¹æ³•æ‰“æ•—ä¸»æµéç›£ç£ç•°å¸¸æª¢æ¸¬æ–¹æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Anomaly detection in natural imaging
&lt;ul&gt;
&lt;li&gt;è­˜åˆ¥åé›¢æ­£å¸¸è³‡æ–™åˆ†ä½ˆçš„ç½•è¦‹äº‹ä»¶&lt;/li&gt;
&lt;li&gt;ç”±æ–¼ç•°å¸¸æ¨£æœ¬çš„ç¼ºä¹ï¼Œå¾Œä¾†çš„å·¥ä½œéƒ½åˆ¶å®šç‚ºéç›£ç£å­¸ç¿’å•é¡Œ&lt;/li&gt;
&lt;li&gt;å¤§è‡´åˆ†ç‚ºå…©é¡
&lt;ul&gt;
&lt;li&gt;reconstruction-based
&lt;ul&gt;
&lt;li&gt;æ¢å¾©åŸå§‹è¼¸å…¥ä¸¦åˆ†æé‡å»ºèª¤å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;density-based
&lt;ul&gt;
&lt;li&gt;é€éä¼°è¨ˆæ­£å¸¸è³‡æ–™çš„åˆ†ä½ˆä¾†é æ¸¬ç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä¸éé€™äº›æ–¹æ³•éƒ½æ²’è¾¦æ³•è§£é‡‹å¯èƒ½çš„ç•°å¸¸ï¼Œæœ¬æ–‡é€éç¶­è­· visual pattern memory ä¾†è§£æ±ºé€™å€‹å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anomaly detection in medical imaging
&lt;ul&gt;
&lt;li&gt;åŸºæ–¼ç›£ç£å­¸ç¿’çš„æ–¹æ³•å¤šåŠç”¨æ–¼æª¢æ¸¬ç‰¹å®šç¨®é¡çš„ç•°å¸¸ï¼Œæ¯”å¦‚è…«ç˜¤&lt;/li&gt;
&lt;li&gt;æœ€è¿‘æå‡ºäº†ä¸€äº›ç„¡ç›£ç£æ–¹æ³•ä¾†æª¢æ¸¬ä¸€èˆ¬ç•°å¸¸ï¼Œå’Œ GAN æœ‰é—œï¼Œä½†æ˜¯é€™äº›æ–¹æ³•éœ€è¦æœ‰é—œæ–¼ç•°å¸¸ç¨®é¡çš„å¼·å¤§å…ˆé©—çŸ¥è­˜å’Œå‡è¨­æ‰èƒ½ä½¿å¢å¼·æœ‰æ•ˆ&lt;/li&gt;
&lt;li&gt;å’Œä¸€èˆ¬çš„ç…§ç‰‡ä¸åŒï¼ŒRadiography imaging protocols ç”Ÿæˆå…·ä¸€è‡´æ€§çš„åœ–ç‰‡ï¼Œç•°å¸¸çš„è®ŠåŒ–æ¯”è¼ƒå¾®å¦™ (subtle)ï¼Œæª¢æ¸¬èµ·ä¾†æ›´å…·æŒ‘æˆ°ï¼Œä½œè€…åˆ©ç”¨æ”¾å°„ç·šæˆåƒçš„ç‰¹æ€§ï¼Œå¤§å¤§æé«˜æª¢æ¸¬æ€§èƒ½ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory networks
&lt;ul&gt;
&lt;li&gt;éå¾€æœ‰ä¸€äº›æœ‰é—œæ–¼æŠŠ Memory modules ç´å…¥ç¥ç¶“ç¶²è·¯çš„ç ”ç©¶ï¼Œå…¶ä¸­æœ‰æ¡ç”¨åˆ° Memory Matrixã€‚æœ¬æ–‡å…‹æœäº† Memory matrix çš„ä¾·é™æ€§ï¼Œä¸¦æå‡ºä¸€ç¨®æœ‰æ•ˆä¸”é«˜æ•ˆç‡çš„çš„ memory queueã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;squid&#34;&gt;SQUID&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Feature extraction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠåœ–ç‰‡åˆ‡æˆ N x N å€‹ non-overlapping patchesï¼Œç„¶å¾Œé¤µå…¥ä¸€å€‹ encoder åšç‰¹å¾µæå–ï¼Œé€™è£¡æ˜¯ç”¨ CNN æå–ï¼Œä½†è¦ç”¨å…¶ä»– backbone ä¹Ÿå¯ä»¥&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image reconstruction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é€™è£¡æœƒç”¨ teacher å’Œ student generator
&lt;ul&gt;
&lt;li&gt;teacher
&lt;ul&gt;
&lt;li&gt;ç›´æ¥ç”¨ encoder çš„ feature é‡å»ºåœ–ç‰‡&lt;/li&gt;
&lt;li&gt;æœ¬è³ªä¸Šæ˜¯ auto-encoder&lt;/li&gt;
&lt;li&gt;ä½œç‚º regularizer ä¾†é¿å… student generator é‡è¤‡ç”Ÿæˆç›¸åŒçš„æ­£å¸¸åœ–ç‰‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;student
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨ in-painting block å¢å¼·å¾Œçš„ feature ä¾†é‡å»ºï¼Œæœ€å¾Œæœƒè¢«ç”¨åœ¨ discrimination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å…©å€‹ generator æœƒåœ¨æ¯å€‹ up-sampling level ç”¨ knowledge distillation paradigm ä¾†çµåˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anomaly discrimination&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨ adversarial learning å¾Œï¼Œä½¿ç”¨ discriminator ä¾†å€åˆ†æ­£å¸¸å’Œç•°å¸¸&lt;/li&gt;
&lt;li&gt;ç”¨ 2 å€‹ generator ä¾†ç”Ÿæˆåœ–ç‰‡ï¼Œå†ç”¨ discriminator ä¾†å€åˆ†ï¼Œåªæœ‰ student generator æœƒæ¥æ”¶ discriminator çš„æ¢¯åº¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inventing-memory-queue-as-dictionary&#34;&gt;Inventing Memory Queue as Dictionary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Motivation
&lt;ul&gt;
&lt;li&gt;Memory Matrix è¢«å»£æ³›æ¡ç”¨
&lt;ul&gt;
&lt;li&gt;Feature æœƒé€éåœ¨ Memory matrix åšåŠ æ¬Šå¹³å‡ä¾†å¼·åŒ–&lt;/li&gt;
&lt;li&gt;ç¼ºé»
&lt;ul&gt;
&lt;li&gt;é€™æ¨£çš„å¢å¼·æ–¹æ³•æ˜¯å°æ•´å¼µåœ–ç‰‡çš„æå‡ºçš„ç‰¹å¾µåšçš„ï¼Œä¸Ÿæ£„äº†åœ–ç‰‡ä¸­çš„ spatial informationã€‚å°è‡´ä»–ç„¡æ³•æ„ŸçŸ¥åˆ°æ”¾å°„ç·šæˆåƒä¸­çš„ä¸€è‡´æ€§çµæ§‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Space-aware memory
&lt;ul&gt;
&lt;li&gt;ç‚ºäº†åˆ©ç”¨ç©ºé–“è³‡è¨Šï¼Œä½œè€…åªå°‡ patch è€Œä¸æ˜¯æ•´å¼µåœ–ç‰‡å‚³éåˆ° modelï¼Œè®“ patch åªèƒ½å­˜å– Memory matrix ä¸­å°æ‡‰åˆ°çš„å€æ®µï¼Œä½œè€…æŠŠé€™ç¨®ç­–ç•¥ç¨±ç‚º Space-aware memoryï¼Œè€Œä¸”é‚„å¯ä»¥åŠ å¿«é€Ÿåº¦ï¼Œå› ç‚ºä¸ç”¨å­˜å–æ•´å€‹ Memory matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory queue
&lt;ul&gt;
&lt;li&gt;åœ¨ learning-based Memory matrix ä¸­ï¼Œnormal patterns æ˜¯ç”± matrix ä¸­çš„ learned basis çµ„åˆè€Œæˆï¼Œä½†çµ„åˆå‡ºä¾†çš„æ±è¥¿å’Œç¾å¯¦ç…§ç‰‡çš„ç‰¹å¾µç¸½æœƒæœ‰åˆ†ä½ˆå·®è·ï¼Œä½¿å¾ŒçºŒçš„å½±åƒç”Ÿæˆè®Šå¾—å›°é›£&lt;/li&gt;
&lt;li&gt;ä½œè€…æå‡º memory queueï¼Œç”¨ä¾†åœ¨è¨“ç·´æœŸé–“å„²å­˜çœŸå¯¦çš„å½±åƒ featureï¼Œå¾è€Œå‘ˆç¾å’Œå½±åƒç‰¹å¾µç›¸åŒçš„åˆ†ä½ˆã€‚å®ƒåœ¨è¨“ç·´æœŸé–“æœƒæŠŠå…ˆå‰çœ‹åˆ°çš„ç‰¹å¾µç›´æ¥è¤‡è£½åˆ° queue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gumbel shrinkage
&lt;ul&gt;
&lt;li&gt;æ§åˆ¶ memory matrix ä¸­ activated pattern çš„æ•¸é‡æ˜¯æœ‰åˆ©çš„ï¼Œä½†å¦‚æœç”¨ hard shrinkage threashold æœƒç„¡æ³•è™•ç†æ‰¾ä¸åˆ°åˆé© entry çš„æƒ…æ³ã€‚ä¸€ç¨®è‡ªç„¶çš„è§£æ³•æ˜¯è®“æ¢¯åº¦æµéå‰ k å€‹ç›¸ä¼¼çš„ entryï¼Œå…¶é¤˜çš„ä¸æ›´æ–°ã€‚ä½†é€™æ¨£åˆæœƒå°è‡´æœªå•Ÿå‹•çš„ entry ç„¡æ³•æ¥æ”¶ä»»ä½•æ¢¯åº¦ä¸¦æ›´æ–°ï¼Œå› æ­¤æå‡ºäº† Gumbel shrinkage schema
&lt;ul&gt;
&lt;li&gt;$w&amp;rsquo; = sg(hs(w,topk(w)) - \phi(w)) + \phi(w)$
&lt;ul&gt;
&lt;li&gt;$w$ ä»£è¡¨ feature å’Œ entry çš„ç›¸ä¼¼åº¦&lt;/li&gt;
&lt;li&gt;$sg(\cdot)$ ä»£è¡¨ stop-gradientï¼Œä¸è¨ˆç®—è¼¸å…¥çš„æ¢¯åº¦&lt;/li&gt;
&lt;li&gt;$hs(\cdot, t)$ ä»£è¡¨ hard shrinkageï¼Œæœ‰å€‹ threshold $t$&lt;/li&gt;
&lt;li&gt;$\phi(\cdot)$ ä»£è¡¨ softmax&lt;/li&gt;
&lt;li&gt;é€™æ¨£ä¿ç•™äº† top k ä½œç‚º w çš„æœ€çµ‚çµæœï¼Œåˆç”¨ softmax å°æ‰€æœ‰ entry é€²è¡Œæ›´æ–°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;formulating-anomaly-detection-as-in-painting&#34;&gt;Formulating Anomaly Detection as In-painting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image in-painting æœ€åˆæ˜¯ç”¨ä¾†æ¢å¾©å…·æœ‰ neighboring context çš„åœ–ç‰‡å€å¡Šï¼Œå› æ­¤æ ¹æ“šæ­¤ç›´è¦ºï¼Œæƒ³æŠŠç•°å¸¸åœ–ç‰‡ä¿®å¾©æˆæ­£å¸¸åœ–ç‰‡ä¾†å¯¦ç¾æª¢æ¸¬&lt;/li&gt;
&lt;li&gt;åœ¨ä¿®å¾©åƒç´ çš„æ™‚å€™ï¼Œç‰¹åˆ¥æ˜¯ç”¨æ·±åº¦ç¶²è·¯ï¼Œå®¹æ˜“æœ‰ boundary artifactsï¼Œåœ¨ pixel ç´šåˆ¥çš„ä¿®å¾©ä¸­ï¼Œé€™äº› boundary artifacts æœƒå°è‡´å¤§é‡èª¤å ±
&lt;ul&gt;
&lt;li&gt;artifact ä¸­ç¿»å¥½åƒæ˜¯ã€Œå½å½±ã€ï¼Œå°±æ˜¯é‡å»ºçš„æ™‚å€™æœƒå‘ˆç¾æœ‰é»åƒæ£‹ç›¤çš„æ•ˆæ‡‰&lt;/li&gt;
&lt;li&gt;ä½œè€…é¸æ“‡åœ¨ feature level é€²è¡Œ in-paintingï¼Œé¿é–‹é€™å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In-painting block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æœƒå…ˆæŠŠæ¯å€‹ patch $F_{1,1}$ ~ $F_{w,h}$ éƒ½å…ˆæ‰¾åˆ°æœ€æ¥è¿‘çš„ normal patterns $N_{1,1}$ ~ $N_{w,h}$&lt;/li&gt;
&lt;li&gt;å› ç‚º N æ˜¯ä¹‹å‰è¨“ç·´è³‡æ–™ä¸­æå–çš„ç‰¹å¾µçµ„æˆçš„ï¼Œä¸å—ç•¶å‰è¼¸å…¥å½±åƒçš„å½±éŸ¿ã€‚ç‚ºäº†å°å…¥è¼¸å…¥åœ–ç‰‡çš„ç‰¹å¾µï¼Œä½œè€…æŠŠ F å’Œ N ç”¨ transformer block ä¾†çµåˆ
&lt;ul&gt;
&lt;li&gt;å°æ–¼æ¯å€‹ patch $F_{i,j}$ï¼ŒæœƒæŠŠå…¶ç•¶ä½œä¸­å¿ƒï¼Œç”¨ç›¸é„°çš„ 8 å€‹ N patch ä¾†é‡æ–°å®šç¾© $F_{i,j}$ï¼ŒæŠŠé€™ 8 å€‹ N patch ä½œç‚º key å’Œ valueï¼Œ$F_{i,j}$ ä½œç‚º query&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æœ€å¾Œæœƒåœ¨ in-painting block çš„å‰å¾Œåš point-wise convolution (1x1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Masked shortcut&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯¦é©—çµæœè¡¨æ˜ï¼Œç›´æ¥åš residual connection æœƒé™ä½ä¿®å¾©çš„æ€§èƒ½ï¼Œä½œè€…æ¡ç”¨ random binary mask åœ¨ training æœŸé–“ gate shortcut feature
&lt;ul&gt;
&lt;li&gt;$F&amp;rsquo;=(1-\delta)\cdot F + \delta \cdot inpaint(F)$
&lt;ul&gt;
&lt;li&gt;$\delta$~$Bernoulli(\rho)$
&lt;ul&gt;
&lt;li&gt;$\rho$ gating probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç²å¾— F&amp;rsquo; å¾Œï¼ŒåŸå§‹çš„ F æœƒè¢«æ›´æ–°é€² memory&lt;/li&gt;
&lt;li&gt;åœ¨æ¨è«–éšæ®µï¼Œæœƒ disable shortcutï¼Œä½¿ $F&amp;rsquo;=inpaint(F)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anomaly-discrimination&#34;&gt;Anomaly Discrimination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discriminator è©•ä¼°åœ–ç‰‡ç¾ä¸ç¾å¯¦ï¼Œä¸ç¾å¯¦è¡¨ç¤ºç•°å¸¸&lt;/li&gt;
&lt;li&gt;å› ç‚º Generator åªåœ¨æ­£å¸¸åœ–ç‰‡è¨“ç·´ï¼Œæ‰€ä»¥ Memory Queue ä¹Ÿåªæœ‰ normal pattern&lt;/li&gt;
&lt;li&gt;ç¨å¾®ç¸½çµ
&lt;ul&gt;
&lt;li&gt;in-painting block æœƒæŠŠ patch å¼·åŒ–ç‚ºç›¸ä¼¼çš„ normal feature&lt;/li&gt;
&lt;li&gt;student generator æœƒæ ¹æ“š &amp;ldquo;normal&amp;rdquo; feature é‡å»ºå‡º &amp;ldquo;normal&amp;rdquo; image&lt;/li&gt;
&lt;li&gt;å¦‚æœæ²’æœ‰ç•°å¸¸çš„è©±ï¼Œé‚£ input å’Œé‡å»ºçš„ image åœ¨èªæ„ä¸Šæ‡‰è©²ç›¸å·®å¾ˆå°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç•°å¸¸åˆ†æ•¸ $A$ çš„ç®—æ³•
&lt;ul&gt;
&lt;li&gt;$A=\phi(\frac{D(G_s(E(I)))-\mu}{\sigma})$
&lt;ul&gt;
&lt;li&gt;$\phi(\cdot)$ æ˜¯ sigmoid function&lt;/li&gt;
&lt;li&gt;$\mu$ å’Œ $\sigma$ æ˜¯æ ¹æ“š training samples ç®—å‡ºçš„ç•°å¸¸åˆ†æ•¸çš„å¹³å‡å€¼å’Œæ¨™æº–å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_t = (I-G_t (E(I)))^2$&lt;/li&gt;
&lt;li&gt;$\mathcal L_s = (I-G_s (E(I)))^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge distillation
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dist} = \sum_{l}^{i=1} (F^i_t-F^i_s)^2$
&lt;ul&gt;
&lt;li&gt;$l$ æ˜¯ levels of features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial loss
&lt;ul&gt;
&lt;li&gt;é¡ä¼¼ DCGAN&lt;/li&gt;
&lt;li&gt;$\mathcal L_{gen} = log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discriminator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dis} = log(D(I)) + log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;li&gt;æŠŠ real image æ©Ÿç‡æ‹‰é«˜ï¼ŒæŠŠ fake image æ©Ÿç‡æ‹‰ä½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Total loss
&lt;ul&gt;
&lt;li&gt;minimize generative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_t \mathcal L_t + \lambda_s \mathcal L_s + \lambda_{dist} \mathcal L_{dist} + \lambda_{gen} \mathcal L_{gen}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;maximize discriminative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_{dis} \mathcal L_{dis}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;new-benchmark&#34;&gt;New Benchmark&lt;/h3&gt;
&lt;p&gt;æå‡ºä¸€å€‹æ–°è³‡æ–™é›† - DigitAnatomyã€‚ã€‚å¦‚æœåŒ…å«æ­£ç¢ºé †åºçš„é˜¿æ‹‰ä¼¯æ•¸å­— 1~9 å‰‡è¦–ç‚ºæ­£å¸¸ï¼Œç•°å¸¸åŒ…æ‹¬ç¼ºå¤±ã€äº‚åºã€ç¿»è½‰å’Œ zero digitã€‚&lt;/p&gt;
&lt;p&gt;è©²è³‡æ–™é›†å°æ–¼æ”¾å°„ç·šæˆåƒå°¤å…¶æœ‰åˆ©ï¼ŒåŸå› å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial correlation and consistent shape&lt;/li&gt;
&lt;li&gt;æ”¾å°„ç·šæˆåƒè¦æ¨™è¨˜éœ€è¦å°ˆæ¥­çŸ¥è­˜ï¼Œä½†æ•¸å­—å®¹æ˜“ debug&lt;/li&gt;
&lt;li&gt;è©²è³‡æ–™é›†å¾ˆå®¹æ˜“å°±å¯ä»¥ç²å¾—æ¨¡æ“¬ç•°å¸¸çš„ ground truth&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;public-benchmarks&#34;&gt;Public Benchmarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ZhangLab Chest X-ray
&lt;ul&gt;
&lt;li&gt;åŒ…å«å¥åº·å’Œè‚ºç‚çš„å½±åƒ&lt;/li&gt;
&lt;li&gt;è¨“ç·´é›†
&lt;ul&gt;
&lt;li&gt;1349 å¼µæ­£å¸¸&lt;/li&gt;
&lt;li&gt;3883 å¼µç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ¸¬è©¦é›†
&lt;ul&gt;
&lt;li&gt;234 å¼µæ­£å¸¸&lt;/li&gt;
&lt;li&gt;390 å¼µç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä½œè€…å¾è¨“ç·´é›†éš¨æ©ŸæŒ‘ 200 å¼µåšç‚ºèª¿æ•´è¶…åƒæ•¸çš„ validation set&lt;/li&gt;
&lt;li&gt;å½±åƒéƒ½èª¿æ•´ç‚º 128x128&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;å° front-view PA å½±åƒé€²è¡Œè©•ä¼°ï¼Œå…±æœ‰ 12 ç¨®ç•°å¸¸&lt;/li&gt;
&lt;li&gt;æœ‰ 5249 å¼µæ­£å¸¸å’Œ 23671 å¼µç•°å¸¸ç”¨ä½œè¨“ç·´
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨å’Œ ZhangLab ç›¸åŒçš„è¶…åƒæ•¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç”¨è¨“ç·´é›†çš„ 250 å¼µæ­£å¸¸å’Œ 250 å¼µç•°å¸¸é€²è¡Œæ¸¬è©¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baselines-and-metrics&#34;&gt;Baselines and Metrics&lt;/h3&gt;
&lt;p&gt;è€ƒæ…® 13 å€‹ä¸»è¦çš„ baseline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç¶“å…¸ UAD (unsupervised anomaly detection)
&lt;ul&gt;
&lt;li&gt;Auto-encoderã€VAE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;é†«å­¸å½±åƒçš„ SOTA
&lt;ul&gt;
&lt;li&gt;Ganomalyã€f-AnoGANã€IFã€SALAD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æœ€è¿‘çš„ UAD
&lt;ul&gt;
&lt;li&gt;MemAEã€CutPasteã€M-KDã€PANDAã€PaDiMã€IGD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;é™¤éæœ‰ç‰¹åˆ¥è¨»æ˜ï¼Œä¸ç„¶éƒ½æ˜¯å¾é ­ç¨ç«‹è¨“ç·´è‡³å°‘ä¸‰æ¬¡&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;interpreting-squid-on-digitanatomy&#34;&gt;Interpreting SQUID on DigitAnatomy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ DigitAnatomy çš„å¯¦é©—ä¸­ï¼Œæ•…æ„æ³¨å…¥ç•°å¸¸åˆ°æ­£å¸¸åœ–ç‰‡ä¸­ï¼Œæ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ä»¥é‡å»ºæ­£å¸¸åœ–ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;SQUID é‡å»ºå‡ºçš„åœ–ç‰‡æ¯”å…¶ä»– baseline æœ‰æ›´å¤šæœ‰æ„ç¾©çš„è¨Šæ¯ï¼Œä¸»è¦æ­¸åŠŸæ–¼ space-aware memoryï¼Œå…¶ç”¢ç”Ÿç¨ç‰¹çš„ patternï¼Œè€Œä¸”å’Œç©ºé–“è¨Šæ¯ç›¸é—œè¯ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€æ—¦å‡ºç¾ç•°å¸¸ï¼Œin-painting block æœƒå¾å­—å…¸ä¸­æ‰¾å‡ºå‰ k å€‹ç›¸è¿‘çš„ï¼ŒæŠŠç•°å¸¸ç‰¹å¾µå¢å¼·åˆ°å…¶å°æ‡‰çš„æ­£å¸¸ç‰¹å¾µï¼Œå…¶ä»–æ–¹æ³•ä¸å…·å‚™æ­¤èƒ½åŠ›ï¼Œæ‰€ä»¥ä»–å€‘é‡å»ºå‡ºæœ‰ç¼ºé™·çš„åœ–åƒã€‚&lt;/p&gt;
&lt;p&gt;GAN å‚¾å‘æ–¼é‡å»ºè¨“ç·´æ¨£æœ¬å¹³å‡å¾—åˆ°çš„å½±åƒã€‚
MemAE å—ç›Šæ–¼ Memory matrixï¼Œè¡¨ç¾è¼ƒå¥½ï¼Œä½†å°æ–¼ç¼ºå¤±æ•¸å­—çš„ç•°å¸¸æ•ˆæœä¸ä½³ã€‚&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-squid-on-chest-radiography&#34;&gt;Benchmarking SQUID on Chest Radiography&lt;/h3&gt;
&lt;h4 id=&#34;limitation&#34;&gt;Limitation&lt;/h4&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ç›®å‰çš„ SQUID æ²’è¾¦æ³•åœ¨åƒç´ å±¤ç´šç²¾ç¢ºå®šä½ç•°å¸¸ã€‚é€™å¯ä»¥ç†è§£ï¼Œå› ç‚º SQUID æ˜¯ä¸€ç¨®éç›£ç£æ–¹æ³•ï¼Œä¸éœ€è¦æ¨™è¨»ã€‚&lt;/p&gt;
&lt;p&gt;é‚£äº›åƒç´ ç´šåˆ¥çš„ç•°å¸¸æª¢æ¸¬æœƒé­é‡æ”¾å¤§é›œè¨Šçš„å½±éŸ¿ï¼Œä½†æ˜¯ç”±æ–¼ SQUID æ˜¯åœ¨ç‰¹å¾µå±¤ç´šé€²è¡Œçš„ï¼Œæ¯”åƒç´ ç´šåˆ¥æ›´åŠ  robustã€‚&lt;/p&gt;
&lt;h3 id=&#34;ablating-key-properties-in-squid&#34;&gt;Ablating Key Properties in SQUID&lt;/h3&gt;
&lt;h4 id=&#34;component-study&#34;&gt;Component study&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;hyper-parameter-robustness&#34;&gt;Hyper-parameter robustness&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig9.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;disease-free-training-requirement&#34;&gt;Disease-free training requirement?&lt;/h4&gt;
&lt;p&gt;ç”¨æ–¼é†«å­¸ç•°å¸¸æª¢æ¸¬çš„éç›£ç£æ–¹æ³•ä¸¦ä¸å¸¸è¦‹ï¼Œå› ç‚ºæ‰€è¬‚çš„ UAD æ–¹æ³•ä¸¦ä¸æ˜¯ã€Œéç›£ç£ã€çš„ï¼Œå› ç‚ºä»–å€‘å¿…é ˆåªåœ¨ç„¡ç–¾ç—…å½±åƒä¸Šä½œè¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å¯¦è¸ä¸­ï¼Œè¦ç²å¾—å¥åº·åœ–ç‰‡éœ€è¦ manual annotationã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨“ç·´é›†ä¸­è€ƒæ…® disease-free å¾ 100% - 50% çš„æƒ…æ³ï¼ŒæŠŠ SQUID çš„ robust å’Œå¦å¤–ä¸‰å€‹ baseline é€²è¡Œæ¯”è¼ƒã€‚&lt;/p&gt;
&lt;p&gt;SQUID çš„ memory queue å¯ä»¥è‡ªå‹•å¿½ç•¥å°‘æ•¸çš„ anatomical patternsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion å…¥é–€</title>
        <link>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</link>
        <pubDate>Mon, 23 Oct 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</guid>
        <description>&lt;h2 id=&#34;å¤§è‡´æ¦‚å¿µ&#34;&gt;å¤§è‡´æ¦‚å¿µ&lt;/h2&gt;
&lt;p&gt;å±¬æ–¼ç”Ÿæˆå¼ AIï¼Œä¸€é–‹å§‹ç”¨åœ¨ç”Ÿæˆåœ–ç‰‡ï¼Œå¾Œä¾†ä¹Ÿæœ‰æ‡‰ç”¨åˆ°è«¸å¦‚ NLP ç­‰é ˜åŸŸã€‚&lt;/p&gt;
&lt;p&gt;ä¸‹æ–‡ç¨±å‘¼åŸåœ–ç‚º spriteã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡ AutoEncoder æœ‰é»é¡ä¼¼ï¼Œå…ˆå–å¾—ä¸€å¼µ spriteï¼Œéš¨è‘—æ™‚é–“æ¨é€²ï¼Œæ¯æ¬¡éƒ½åœ¨åœ–ç‰‡ä¸ŠåŠ ä¸€å±¤é›œè¨Šï¼Œåè¦†ç–ŠåŠ ï¼Œè¿­ä»£å¤šæ¬¡å¾Œï¼Œå°±æœƒå¾—åˆ°ä¸€å¼µé›£ä»¥çœ‹å‡ºåŸåœ–çš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;å¾ sprite åˆ°åªèƒ½çœ‹å‡ºæ˜¯ä¸€åœ˜é›œè¨Šä¸¦éæ˜¯ä¸€æ­¥åˆ°ä½çš„éç¨‹ã€‚ä¸€é–‹å§‹æ²’æœ‰é›œè¨Šæ™‚å¯ä»¥çœ‹å‡ºåŸæœ¬çš„ spriteï¼Œä¸€å€‹è¿­ä»£å¾Œå¯èƒ½å¯ä»¥å‹‰å¼·çœ‹å‡ºåŸæœ¬çš„ spriteï¼Œå†å¹¾å€‹è¿­ä»£å¾Œå¯èƒ½ä¹Ÿé‚„èƒ½çœ‹å‡ºåŸæœ¬çš„ outlineï¼Œç¶“éè¨±å¤šæ¬¡å¾Œæ‰æœƒè®Šæˆå®Œå…¨è¾¨è­˜ä¸äº†çš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘æœŸæœ›æ¨¡å‹åšçš„äº‹æƒ…å‰‡æ˜¯å¾ gaussian noise é€æ­¥æ¨å› spriteï¼ŒåŒæ¨£ä¸æ˜¯ä¸€æ­¥åˆ°ä½ï¼Œè€Œæ˜¯è®“æ¨¡å‹é æ¸¬ä¸Šä¸€å€‹æ™‚é–“é»çš„é›œè¨Šï¼Œç›¸æ¸›å¾Œå†é€æ­¥æ¨å› spriteï¼Œé€™éç¨‹ç¨±ç‚º denoiseã€‚&lt;/p&gt;
&lt;h2 id=&#34;ddpm&#34;&gt;DDPM&lt;/h2&gt;
&lt;p&gt;å¯¦ç¾ Diffusion å¯èƒ½æœƒæœ‰é» confusingï¼Œå› ç‚ºä»–å¯¦ä½œä¸Šå’Œä¸Šé¢èªªçš„ä¸å¤ªç›¸åŒã€‚
åœ¨è¨“ç·´çš„æ™‚å€™ï¼Œæˆ‘å€‘æœƒæ¡æ¨£ä¸‰å€‹æ±è¥¿ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;è¨“ç·´åœ–ç‰‡ (sprite)&lt;/li&gt;
&lt;li&gt;é›œè¨Š&lt;/li&gt;
&lt;li&gt;æ™‚é–“é» (t)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è¨“ç·´éšæ®µçš„æ™‚å€™ï¼Œæˆ‘å€‘æœƒæŠŠã€ŒåŸå§‹ä¹¾æ·¨çš„åœ–ç‰‡ã€å’Œã€Œé›œè¨Šã€æ ¹æ“šæ™‚é–“é€²è¡Œä¸åŒæ¯”ä¾‹çš„ç›¸åŠ  (æ··åˆ)ï¼Œt è¶Šå¤§ï¼Œé›œè¨Šçš„æ¯”ä¾‹è¶Šå¤§ã€‚&lt;/p&gt;
&lt;p&gt;æ¨¡å‹é æ¸¬çš„ç›®æ¨™æ˜¯å‰é¢ sample å‡ºçš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;é€™èˆ‡å‰é¢èªªçš„æ¦‚å¿µç›¸æ‚–ã€‚æŒ‰ç…§å‰é¢çš„èªªæ³•ï¼Œå°æ–¼æ™‚é–“é» tï¼Œæ‡‰è©²æ˜¯ä»¥ä¸€å¼µåŠ äº† t-1 æ¬¡é›œè¨Šçš„ sprite ä½œç‚ºè¼¸å…¥ï¼Œå†åŠ ä¸Š t æ‰€ sample å‡ºçš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;ç¾åœ¨å¯¦ä½œå»æ˜¯åŸå§‹ä¹¾æ·¨çš„ sprite ç›´æ¥æ ¹æ“šæ™‚é–“é»æ··å’ŒæŸå€‹é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;é€™èƒŒå¾Œçš„æ•¸å­¸æ¨å°ååˆ†å†—é•·ï¼Œé€™è£¡ä¸æ•˜è¿°ï¼Œä½†éœ€çŸ¥é“å¯¦ä½œå·®ç•°ã€‚&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;åœ¨æ¨è«–éšæ®µçš„æ™‚å€™ï¼Œæ¯æ¬¡ denoise å¾Œéœ€è¦æŠŠåœ–ç‰‡å’Œé¡å¤– sample çš„ noise ç›¸åŠ ã€‚é€™å€‹ noise å’Œå‰é¢çš„ noise ä¸€æ¨£ï¼Œéƒ½æ˜¯å¾ mean=0, std=1 çš„ gaussian distribution ä¸­ sample å‡ºä¾†çš„ã€‚&lt;/p&gt;
&lt;p&gt;ä¸åŠ çš„è©±ä¼¼ä¹é‚„å®¹æ˜“æœ‰ Mode Collapse çš„ç¾è±¡ã€‚
çœ‹åˆ°ä¸€å€‹èªªæ³•æ˜¯ï¼Œæ¨¡å‹å–œæ­¡åƒåœ–ç‰‡åŠ ä¸Šé›œè¨Šçš„åœ–åƒä½œç‚ºåœ–ç‰‡ï¼Œåœ¨åœ–ç‰‡ä¸ŠåŠ ä¸Š noise ä¼¼ä¹æœƒæ›´ç¬¦åˆæ¨¡å‹é æœŸçš„è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;çœ‹äº†æå¼˜æ¯…çš„å½±ç‰‡ï¼Œä¹Ÿæœ‰åŸºæ–¼éš¨æ©Ÿæ€§çš„è§€é»ã€‚&lt;/p&gt;
&lt;p&gt;ç”Ÿæˆå¼ Model ç”Ÿæˆæ–‡ç« æ™‚æ°¸é å–æ©Ÿç‡æœ€å¤§çš„ï¼Œä¸è¦‹å¾—æœ‰æ›´å¥½çš„æ•ˆæœï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ‰ç ”ç©¶æ˜¯è®“ Model é¸æ©Ÿç‡æœ€å¤§çš„ï¼Œçµæœå®¹æ˜“ç”Ÿå‡ºåè¦†è·³é‡çš„æ–‡ç« ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¹Ÿæœ‰æŠŠäººé¡å¯«çš„æ–‡ç« å»é¤µçµ¦ Model çœ‹ï¼Œå¾ä»–çš„è§’åº¦çœ‹äººé¡å¯«çš„ä¸‹ä¸€å€‹å­—çš„æ©Ÿç‡æ˜¯å¤šå°‘ï¼Œç™¼ç¾äººé¡å¯«çš„æ–‡ç« å¾ˆå¸¸å‡ºç¾ä¸€ä¸‹æ©Ÿç‡é«˜ä¸€ä¸‹æ©Ÿç‡ä½çš„å­—ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æŸç¯‡èªéŸ³åˆæˆçš„æ–‡ç« éœ€è¦åœ¨æ¨è«–éšæ®µã€Œå•Ÿç”¨ã€dropout æ‰å¯ä»¥æœ‰å¥½çš„çµæœã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Diffusion ä¹Ÿæœ‰å¯èƒ½æˆåŠŸçš„é»æ˜¯åœ¨æ–¼ä¸¦éã€Œä¸€æ¬¡åˆ°ä½ã€è€Œæ˜¯ã€ŒN æ¬¡åˆ°ä½ã€ã€‚
å¾é€™æ¨£çš„è§’åº¦çœ‹ï¼ŒDiffusion æ˜¯ autoregressive æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;é¡ä¼¼çš„ä½œæ³•ä¹Ÿæœ‰ Mask-Predictï¼Œå¤§è‡´æ¦‚å¿µæ˜¯å¾åŸæœ¬éƒ½æ˜¯ Mask çš„æƒ…å¢ƒé–‹å§‹ï¼Œå°‡ä¸€äº›ä¿¡å¿ƒé«˜çš„é æ¸¬ç•™ä½ï¼Œä¿¡å¿ƒä½çš„ä¿æŒç‚º Maskï¼Œä¸€æ­¥æ­¥é æ¸¬å‡ºæ‰€æœ‰è³‡è¨Šã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DETR è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 10 Aug 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.12872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ä½œè€…æŠŠ object detection è¦–ä½œä¸€å€‹ set prediction å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ç°¡åŒ–äº† pipelineï¼Œæ¶ˆé™¤äº†è¨±å¤š hand-designed componentsï¼Œæ¯”å¦‚ non-maximum suppression å’Œ anchor generationï¼Œé€™äº› component ç”±æˆ‘å€‘å°æ–¼ä»»å‹™çš„å…ˆé©—çŸ¥è­˜æ§‹æˆã€‚&lt;/p&gt;
&lt;p&gt;æå‡ºäº†ä¸€å€‹æ–°çš„ç›®æ¨™å‡½æ•¸ï¼Œé€éäºŒåˆ†åŒ¹é…ï¼ˆbipartite matchingï¼‰é€²è¡Œé æ¸¬ï¼Œä¹Ÿç”¨ Transformer encoder-decoder æ¶æ§‹ã€‚&lt;/p&gt;
&lt;p&gt;çµ¦äºˆä¸€çµ„å›ºå®šçš„ learned object queryï¼ŒDETR å¯ä»¥æ¨ç† objects å’Œ globol image context çš„é—œä¿‚ï¼Œä¸¦ã€Œä¸¦è¡Œã€è¼¸å‡ºä¸€çµ„é æ¸¬é›†ã€‚&lt;/p&gt;
&lt;p&gt;DETR æ¦‚å¿µéå¸¸ç°¡å–®ã€‚&lt;/p&gt;
&lt;p&gt;DETR åœ¨ COCO ä¸Šå’Œ Faster RCNN baseline åœ¨æº–ç¢ºåº¦å’Œ performance ä¸Šç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;DETR å¯ä»¥å¾ˆç°¡å–®åœ°æ¨å»£åˆ° Panoptic Segmentationã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ç›®æ¨™æª¢æ¸¬çš„ç›®æ¨™å°±æ˜¯é›†åˆé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç›®å‰éƒ½ç”¨ä¸€äº›å¾ˆé–“æ¥çš„æ–¹å¼å»åšï¼Œåƒæ˜¯ç”¨ proposals, anchors æˆ– window centersã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯é€™äº›æ–¹æ³•æ€§èƒ½æ˜é¡¯å—é™æ–¼å¾Œè™•ç†æ­¥é©Ÿï¼Œæ¯”å¦‚ non-maximum suppressionï¼Œå› ç‚ºä»–å€‘æœƒç”¢ç”Ÿå¤§é‡å†—é¤˜çš„æ¡†ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç°¡åŒ– pipelineï¼Œä½œè€…æå‡ºäº†ä¸€ç¨® End-to-End çš„æ–¹æ³•ï¼Œä»¥å¾€ä¹Ÿæœ‰ä¸€äº›å˜—è©¦ï¼Œä½†ä»–å€‘è¦ä¸æ·»åŠ äº†å…¶ä»–çš„å…ˆé©—çŸ¥è­˜ï¼Œä¸ç„¶å°±æ˜¯åœ¨å…·æœ‰æŒ‘æˆ°æ€§çš„ benchmark ä¸Šè¡¨ç¾ä¸å¥½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ COCO ä¸Šå’Œ Faster R-CNN çš„æ€§èƒ½ç›¸ç•¶ï¼Œè¡¨ç¾å’Œé€Ÿåº¦éƒ½å·®ä¸å¤šã€‚&lt;/p&gt;
&lt;p&gt;DETR åœ¨å¤§ç‰©é«”è¡¨ç¾å¾ˆå¥½ï¼Œå¯èƒ½æ˜¯æ­¸åŠŸæ–¼ Transformer non-local çš„è¨ˆç®—èƒ½åŠ›ã€‚
é›–ç„¶ DETR åœ¨å°ç‰©é«”ä¸Šè¡¨ç¾å€’ä¸æ€éº¼æ¨£ã€‚&lt;/p&gt;
&lt;p&gt;DETR éœ€è¦è¶…é•·çš„è¨“ç·´æ™‚é–“ï¼Œä½† DETR çš„è¨­è¨ˆç†å¿µå¯ä»¥æ‹“å±•åˆ° Panoptic Segmentationã€‚&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;set-prediction&#34;&gt;Set Prediction&lt;/h3&gt;
&lt;p&gt;æ²’æœ‰è¦ç¯„çš„æ·±åº¦å­¸ç¿’æ¨¡å‹å¯ä»¥ç›´æ¥é æ¸¬é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›ä»»å‹™ä¸­çš„ä¸€å€‹å›°é›£é»æ˜¯é¿å… near-dulicatesï¼ˆç›¸è¿‘çš„é‡è¤‡æª¢æ¸¬æ¡†ï¼‰ ç•¶å‰å¤šæ•¸æª¢æ¸¬å™¨ç”¨ NMS ä¾†è§£æ±ºæ­¤å•é¡Œï¼Œå¦‚æœæ˜¯ direct set prediction å°±ä¸ç”¨å¾Œè™•ç†ã€‚&lt;/p&gt;
&lt;h3 id=&#34;transformers-and-parallel-decoding&#34;&gt;Transformers and Parallel Decoding&lt;/h3&gt;
&lt;p&gt;Transformer åœ¨å„ç¨®åœ°æ–¹è¡¨ç¾å‡ºè‰²ï¼Œä½†æ¨ç†æˆæœ¬ä»¤äººæœ›è€Œç”Ÿç•ã€‚&lt;/p&gt;
&lt;h3 id=&#34;object-detection&#34;&gt;Object detection&lt;/h3&gt;
&lt;p&gt;ç¾åœ¨å¤šæ•¸çš„ç›®æ¨™æª¢æ¸¬æ–¹æ³•æ˜¯åŸºæ–¼ä¸€äº›åˆå§‹çš„çŒœæ¸¬ï¼Œå†å»åšé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚å°æ–¼ two-stage çš„æ–¹æ³•ï¼Œå°±æ˜¯å°æ–¼ proposals å¾€ä¸‹åšé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ single-stageï¼Œåˆå§‹çŒœæ¸¬å°±æ˜¯ anchorsã€‚&lt;/p&gt;
&lt;h4 id=&#34;set-based-loss&#34;&gt;Set-based loss&lt;/h4&gt;
&lt;p&gt;ä»¥å‰çš„ä¸€äº›ä½œæ³•æ¯”å¦‚ Learnable NMS æˆ– relation networks éƒ½å¯ä»¥é€é attention ä¾†è™•ç†ä¸åŒé æ¸¬ä¹‹é–“çš„é—œä¿‚ã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ direct set lossesï¼Œä»–å€‘ä¸éœ€è¦ä»»ä½•å¾Œè™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯é€™äº›æ–¹æ³•å¾€å¾€ç”¨é¡å¤–çš„ hand-crafted context featureï¼Œæ¯”å¦‚ proposal box coordinatesã€‚ä½œè€…å°‹æ‰¾æ¸›å°‘æ¨¡å‹ä¸­å…ˆé©—çŸ¥è­˜çš„æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;h4 id=&#34;recurrent-detectors&#34;&gt;Recurrent detectors&lt;/h4&gt;
&lt;p&gt;ä»¥å¾€æœ‰é¡ä¼¼çš„å·¥ä½œï¼Œä½†ä»–å€‘æ˜¯ç”¨ RNNã€‚&lt;/p&gt;
&lt;h2 id=&#34;the-detr-model&#34;&gt;The DETR model&lt;/h2&gt;
&lt;h4 id=&#34;object-detection-set-prediction-loss&#34;&gt;Object detection set prediction loss&lt;/h4&gt;
&lt;p&gt;DETE æœƒçµ¦ N å€‹å›ºå®šå¤§å°çš„é›†åˆé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;è¦è§£äºŒåˆ†åœ–åŒ¹é…ï¼Œæœ¬æ–‡ç”¨ scipy çš„ linear_sum_assignment è™•ç†ï¼Œä»–èƒŒå¾Œæ˜¯åŒˆç‰™åˆ©æ¼”ç®—æ³•ã€‚&lt;/p&gt;
&lt;p&gt;å…¶å¯¦é€™ç¨®æ–¹æ³•å’Œ proposals å’Œ anchors æœ‰å·®ä¸å¤šçš„ä½œç”¨ï¼Œå·®åˆ¥åœ¨æ–¼é€™è£¡æœƒæ‰¾ä¸€å°ä¸€çš„åŒ¹é…ï¼Œè€Œä¸ç”¨é‡è¤‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›®æ¨™å‡½æ•¸ï¼š&lt;/p&gt;
&lt;p&gt;$L_{Hungarian}(y, \text{\^{y}}) = \displaystyle\sum^{N}_{i=1} [-log \text{\^{p}} $
$_{\^{\sigma}(i)}(c_i) + \text{1}$
$_\{$
$_{c_i \neq \text{\o}}$
$_\}$
$\mathcal{L}$
$_{\text{box}} (b_i, \text{\^{b}}$
$_{\^{\sigma}}(i))]$&lt;/p&gt;
&lt;p&gt;å‰é¢æ˜¯åˆ†é¡çš„ lossï¼Œå¾Œé¢æ˜¯ bounding box çš„ lossã€‚&lt;/p&gt;
&lt;p&gt;é€™é‚Šæœ‰å…©å€‹æ”¹å‹•ï¼Œç¬¬ä¸€å€‹æ˜¯åˆ†é¡é‚£é‚Šä¸ç”¨ logï¼Œä½¿å€¼å’Œ bounding box çš„ loss æ¯”è¼ƒæ¥è¿‘ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹æ˜¯ bounding box é‚£é‚Šä¸¦ä¸æ˜¯ç”¨æœ€å¸¸è¦‹çš„ L1ï¼Œå› ç‚º L1 å°æ–¼å¤§çš„ç›®æ¨™ loss æ¯”è¼ƒé«˜ï¼Œé€™è£¡é™¤äº† L1 é‚„é¸ç”¨ generalized IoU lossï¼Œå®ƒåœ¨å°ºåº¦ä¸Šèˆ‡ loss ç„¡é—œã€‚&lt;/p&gt;
&lt;h4 id=&#34;detr-architecture&#34;&gt;DETR architecture&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ç”¨ CNN å¾åœ–ç‰‡æŠ½ç‰¹å¾µï¼Œæ‹‰ç›´ï¼Œé¤µçµ¦ Transformer encoder-decoderï¼Œå¾—åˆ°ä¸€çµ„é æ¸¬é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;é€™è£¡ encoder æœ‰åŠ©æ–¼ç‰¹å¾µé–“å½¼æ­¤äº¤äº’ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´çš„æ™‚å€™ï¼Œé æ¸¬çš„æ¡†å’Œ GT åšåŒ¹é…ï¼Œæ²’åŒ¹é…åˆ°çš„å°±æ”¾åˆ° &amp;ldquo;no object&amp;rdquo; classã€‚&lt;/p&gt;
&lt;p&gt;decoder æœƒé¤µå…¥ object queriesï¼Œé€™äº›æ˜¯ learnable positional encodingsã€‚&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;h4 id=&#34;number-of-encoder-layers&#34;&gt;Number of encoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…é€éæ”¹è®Š Encoder layer çš„æ•¸é‡ä¾†è©•ä¼° global imagelevel self-attention çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æ¨è«– encoder å¯èƒ½å°æ–¼åˆ¤æ–·åˆ†é–‹å°è±¡å¾ˆé‡è¦ï¼Œåœ– 3 å¯è¦–åŒ–äº†æœ€å¾Œä¸€å€‹ encoder layer çš„ attention mapã€‚&lt;/p&gt;
&lt;p&gt;encoder çœ‹ä¼¼å·²ç¶“åˆ†é›¢äº† instanceï¼Œå¯èƒ½ç°¡åŒ–äº† decoder å°æ–¼ object extraction å’Œ localization çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h4 id=&#34;number-of-decoder-layers&#34;&gt;Number of decoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨åœ– 6 åšäº† decoder çš„æ³¨æ„åŠ›å¯è¦–åŒ–ï¼Œå¯ä»¥æ³¨æ„åˆ°è§€å¯Ÿçš„æ³¨æ„åŠ›ç›¸ç•¶å±€éƒ¨ã€‚&lt;/p&gt;
&lt;p&gt;æ¨è«–æ˜¯ encoder ä¸»è¦åˆ†é›¢å¯¦é«”ï¼Œdecoder åªéœ€è¦é—œæ³¨å››è‚¢å³å¯æå–å‡ºå°è±¡çš„é‚Šç•Œå’Œåˆ†é¡ã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig7.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ– 7 æŠŠ 100 å€‹é æ¸¬æ§½ä¸­çš„ 20 å€‹åšå¯è¦–åŒ–ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹é æ¸¬æ¡†ä»£è¡¨ä¸€é»ï¼Œå¯ä»¥æ³¨æ„åˆ°ä¸åŒçš„æ§½ä½æœƒå°ˆæ³¨åœ¨ä¸åŒå€åŸŸã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>BERT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sat, 05 Aug 2023 00:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.04805&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ç¾åœ¨å›é ­å¯« BERT è«–æ–‡ç­†è¨˜æ„Ÿè¦ºæœ‰é»æ€ªï¼Œä¹‹å‰å·²ç¶“å¯«éä»€éº¼ RoBERTa ä¹‹é¡çš„ã€‚&lt;/p&gt;
&lt;p&gt;ä¸éç¾åœ¨å› æ‡‰å¯¦é©—å®¤è®€æ›¸æœƒè¦æ±‚ï¼Œé‚„æ˜¯çœ‹ä¸€ä¸‹è«–æ–‡ä¹Ÿå¯«ä¸€ä¸‹ç­†è¨˜ã€‚&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº† BERTï¼Œä¸€ç¨®åŸºæ–¼ Transformer Bidirectional Encoder çš„èªè¨€è¡¨ç¤ºæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;BERT æ—¨åœ¨é€é unlabeled text é€²è¡Œ pretrainã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œåªéœ€è¦ä¸€å€‹é¡å¤–çš„è¼¸å‡ºå±¤å°±å¯ä»¥å°é è¨“ç·´çš„ BERT é€²è¡Œå¾®èª¿ï¼Œåœ¨å„ç¨®ä»»å‹™ä¸Šå–å¾— SOTAã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ã€Œèªè¨€æ¨¡å‹åšé è¨“ç·´ã€å·²è¢«è­‰æ˜å¯ä»¥æœ‰æ•ˆæ”¹å–„å¤šç¨® NLP ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å°‡é è¨“ç·´æ¨¡å‹æ‡‰ç”¨åœ¨ä¸‹æ¸¸ä»»å‹™ï¼Œæœ‰å…©ç¨®ç­–ç•¥ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feature-based
&lt;ul&gt;
&lt;li&gt;æŠŠ pretrained çš„ representations ä½œç‚ºé¡å¤–çš„ç‰¹å¾µ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tuning
&lt;ul&gt;
&lt;li&gt;æ ¹æ“šç‰¹å®šä»»å‹™å¼•å…¥é¡å¤–åƒæ•¸ï¼Œä¸¦ç°¡å–®åœ°å¾®èª¿æ‰€æœ‰åƒæ•¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é€™å…©ç¨®æ–¹æ³•åœ¨é è¨“ç·´æœŸé–“å…±ç”¨åŒå€‹ objective functionï¼Œä¸¦ç”¨å–®å‘èªè¨€æ¨¡å‹ä¾†å­¸ç¿’ representationã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…èªç‚ºç•¶å‰çš„æŠ€è¡“é™åˆ¶äº†é è¨“ç·´çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œç‰¹åˆ¥æ˜¯åœ¨ Fine-tuning æ–¹æ³•ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦çš„å•é¡Œåœ¨æ–¼èªè¨€æ¨¡å‹æ˜¯å–®å‘çš„ï¼Œé™åˆ¶äº†é è¨“ç·´æœŸé–“å¯ä»¥ä½¿ç”¨çš„æ¶æ§‹çš„é¸æ“‡ã€‚é€™ç¨®å–®å‘çš„æ¶æ§‹å¯èƒ½åœ¨ä¸€äº›ä»»å‹™æœ‰å®³ï¼Œç‰¹åˆ¥æ˜¯å°æ–¼é‚£äº›éœ€è¦å…©å€‹æ–¹å‘çš„ context çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºçš„ BERT æ”¹å–„äº†ç¾æœ‰çš„ Fine-tuning æ–¹æ³•ï¼Œç”¨ Transformer çš„ Bidirectional Encoder ä¾†è¨“ç·´èªè¨€æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;BERT é€éå—åˆ° Cloze taskï¼ˆå¡«ç©ºï¼‰å•Ÿç™¼çš„ masked language model(MLM)ï¼Œä½œç‚ºé è¨“ç·´ç›®æ¨™ã€‚MLM éš¨æ©Ÿåœ°é®è”½ä¸€äº›è¼¸å…¥çš„ä¸€äº› tokenï¼Œç›®æ¨™æ˜¯æ ¹æ“šä¸Šä¸‹æ–‡ä¾†å›æ¨åŸè©ï¼Œä½¿ representation å¯ä»¥èåˆå·¦å³å…©é‚Šçš„ contextã€‚&lt;/p&gt;
&lt;p&gt;é™¤äº† MLMï¼Œä½œè€…é‚„åˆ©ç”¨ next sentence predictionï¼ˆNSPï¼‰ä»»å‹™ä¾†è¨“ç·´ BERTã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è²¢ç»å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BERT è­‰æ˜äº†é›™å‘é è¨“ç·´å° representation çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT å±•ç¾å‡ºé è¨“ç·´çš„ representation æ¸›å°‘äº†è¨±å¤šé‡å° NLP ä»»å‹™ç²¾å¿ƒè¨­è¨ˆæ¶æ§‹çš„éœ€æ±‚ã€‚ BERT æ˜¯ç¬¬ä¸€å€‹åŸºæ–¼ Fine-tuningï¼Œåœ¨å¤§é‡ sentence-level å’Œ token-level ä»»å‹™ä¸Šå–å¾— SOTA çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT æ¨é€²äº† 11 å€‹ NLP ä»»å‹™çš„ SOTAã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;unsupervised-feature-based-approaches&#34;&gt;Unsupervised Feature-based Approaches&lt;/h3&gt;
&lt;p&gt;å­¸ç¿’å»£æ³›é©ç”¨çš„ representation of words ä¸€ç›´æ˜¯æ´»èºçš„ç ”ç©¶é ˜åŸŸï¼Œç”šè‡³åœ¨éç¥ç¶“ç¶²è·¯çš„é ˜åŸŸä¹Ÿæ˜¯ã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´çš„ word embeddings èˆ‡å¾é ­è¨“ç·´çš„ embedding ç›¸æ¯”ï¼Œæœ‰é¡¯è‘—æ”¹é€²ã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›æ–¹æ³•é è¢«æ¨å»£åˆ° coarser granularitiesï¼Œåƒæ˜¯ sentence embedding æˆ–æ˜¯ paragraph embeddingã€‚&lt;/p&gt;
&lt;p&gt;æœ‰ç ”ç©¶è­‰æ˜ cloze task æé«˜äº†ç”Ÿæˆæ¨¡å‹çš„ robustnessã€‚&lt;/p&gt;
&lt;h2 id=&#34;bert&#34;&gt;BERT&lt;/h2&gt;
&lt;p&gt;æ¡†æ¶æœ‰å…©æ­¥é©Ÿï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pre-training
&lt;ul&gt;
&lt;li&gt;åœ¨ä¸åŒçš„é è¨“ç·´ä»»å‹™ä¸­ï¼Œç”¨ unlabeled data ä¾† fine-tuneã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tuning
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨é è¨“ç·´çš„åƒæ•¸åˆå§‹åŒ–ï¼Œåœ¨åˆ©ç”¨ä¸‹æ¸¸ä»»å‹™çš„ labeled data å°æ‰€æœ‰åƒæ•¸å¾®èª¿ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;BERT çš„ä¸€å€‹ç‰¹é»æ˜¯ä»–å…·å‚™è·¨ä¸åŒä»»å‹™çš„çµ±ä¸€æ¶æ§‹ï¼Œé è¨“ç·´æ¶æ§‹å’Œä¸‹æ¸¸ä»»å‹™æœ€çµ‚æ¶æ§‹å·®ç•°ä¸å¤§ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model Architecture&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ¬æ–‡è¡¨ç¤ºæ–¹æ³•&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L: Transformer çš„å±¤æ•¸&lt;/li&gt;
&lt;li&gt;H: hidden size&lt;/li&gt;
&lt;li&gt;A: self-attention heads çš„æ•¸é‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;model size&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BASE: L=12, H=768, A=12, 110M parameters
&lt;ul&gt;
&lt;li&gt;å’Œ GPT ç›¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LARGE: L=24, H=1024, A=16, 340M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Input/Output Representations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Input representation å¯ä»¥åœ¨ token sequence ä¸­æ˜ç¢ºè¡¨ç¤ºå–®å€‹ sentence å’Œä¸€å° sentenceã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sentence å¯ä»¥æ˜¯é€£çºŒæ–‡æœ¬çš„ä»»æ„ç¯„åœï¼Œè€Œä¸æ˜¯å¯¦éš›çš„å¥å­ã€‚&lt;/li&gt;
&lt;li&gt;sequence æ˜¯è¼¸å…¥çš„ token sequenceï¼Œå¯ä»¥æ˜¯å–®å€‹ sentence æˆ–æ˜¯ä¸€å° sentenceã€‚&lt;/li&gt;
&lt;li&gt;æ¯å€‹ sequence çš„ç¬¬ä¸€å€‹ token å§‹çµ‚æ˜¯ç‰¹æ®Šçš„åˆ†é¡ token &amp;ndash; [CLS]&lt;/li&gt;
&lt;li&gt;å°æ–¼å…©å€‹å¥å­æ”¾åœ¨ä¸€å€‹åºåˆ—çš„æƒ…æ³ï¼Œç”¨ [SEP] éš”é–‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Token Embeddings&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä½œè€…ä½¿ç”¨ WordPiece embeddingsï¼Œæœ‰ 30000 å€‹è©å½™ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;learned embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å°æ¯å€‹ token æ·»åŠ é€™å€‹æ±è¥¿ï¼Œè¡¨ç¤ºå±¬æ–¼ sentence A é‚„ sentence B&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-training-bert&#34;&gt;Pre-training BERT&lt;/h3&gt;
&lt;h4 id=&#34;masked-lm&#34;&gt;Masked LM&lt;/h4&gt;
&lt;p&gt;ç›´è§€ä¸Šï¼Œæœ‰ç†ç”±ç›¸ä¿¡æ·±åº¦çš„é›™å‘æ¨¡å‹æœƒæ¯”å–®åƒä¸²é€£èµ·ä¾†çš„æ·ºå±¤æ¨¡å‹æ›´å¼·å¤§ã€‚&lt;/p&gt;
&lt;p&gt;ä¸å¹¸çš„æ˜¯ standard condition language model åªèƒ½å–®å‘è¨“ç·´ï¼Œå› ç‚ºé›™å‘æœƒå…è¨±æ¯å€‹å–®è©ã€Œé–“æ¥çœ‹åˆ°è‡ªå·±ã€ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è¨“ç·´ deep bidirectional representationsï¼Œæœ¬æ–‡éš¨æ©Ÿé®è”½äº†ä¸€å®šæ¯”ä¾‹çš„ tokensï¼Œä¸¦é æ¸¬é€™äº› tokenï¼Œé€™ç¨®æ–¹æ³•ç¨±ç‚º masked language modelï¼Œæˆ–å¸¸è¢«ç¨±ç‚º cloze taskã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æœƒç”¨ [MASK] åšé è¨“ç·´ï¼Œä½†æœ‰å€‹å•é¡Œæ˜¯ [MASK] åœ¨ fine-tuning æœŸé–“ä¸æœƒå‡ºç¾ï¼Œé€ æˆé è¨“ç·´å’Œå¾®èª¿ä¹‹é–“çš„ mismatchingï¼Œç‚ºäº†ç·©æ¸›é€™ç¨®æƒ…æ³ï¼Œä¸¦ä¸æœƒç¸½æ˜¯ç”¨ [MASK] æ›¿ä»£ masked tokenã€‚&lt;/p&gt;
&lt;p&gt;è¦æ›¿æ› token çš„æ™‚å€™ï¼Œæœ‰ 80% çš„æ™‚é–“æ˜¯ [MASK]ï¼Œ10% æ˜¯éš¨æ©Ÿ tokenï¼Œ10% æ˜¯åŸæœ¬çš„ tokenã€‚&lt;/p&gt;
&lt;h4 id=&#34;next-sentence-prediction-nsp&#34;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;
&lt;p&gt;è¨±å¤šé‡è¦ä¸‹æ¸¸ä»»å‹™ï¼Œæ¯”å¦‚ Question Answering (QA) å’Œ Natural Language Inference (NLI) æ˜¯åŸºæ–¼å…©å€‹å¥å­é–“çš„é—œä¿‚ã€‚&lt;/p&gt;
&lt;p&gt;NSP å°±æ˜¯ç‚ºäº†ç†è§£å¥å­é–“çš„é—œä¿‚è€Œç”¨çš„ã€‚&lt;/p&gt;
&lt;p&gt;æ¯æ¬¡æŒ‘å¥å­ A å’Œ B çš„æ™‚å€™ï¼Œæœ‰ 50% çš„æ©Ÿæœƒ B æ˜¯ A çš„ä¸‹ä¸€å€‹å¥å­ï¼Œæœ‰ 50% æ˜¯éš¨æ©Ÿçš„ã€‚&lt;/p&gt;
&lt;p&gt;é‡å° NSP çš„é è¨“ç·´å° QA å’Œ NLI éƒ½å¾ˆæœ‰ç”¨ã€‚&lt;/p&gt;
&lt;h4 id=&#34;é è¨“ç·´è³‡æ–™&#34;&gt;é è¨“ç·´è³‡æ–™&lt;/h4&gt;
&lt;p&gt;ç”¨ BookCorpus å’Œ English Wikipedia ä¾†è¨“ç·´ BERTã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ English Wikipediaï¼Œåªæå– text passagesï¼Œå¿½ç•¥ lists, tables, headersã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æå–é•·çš„é€£çºŒåºåˆ—ï¼Œç”¨ document-level çš„ corpus è€Œä¸æ˜¯æ‰“äº‚çš„ sentence-level corpus éå¸¸é‡è¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;ablation-studies&#34;&gt;Ablation Studies&lt;/h2&gt;
&lt;h3 id=&#34;effect-of-pre-training-tasks&#34;&gt;Effect of Pre-training Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;No NSP
&lt;ul&gt;
&lt;li&gt;åªæœ‰ MLM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LTR &amp;amp; No NSP
&lt;ul&gt;
&lt;li&gt;Left-to-Right&lt;/li&gt;
&lt;li&gt;åªçœ‹å·¦é‚Šçš„ context&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç™¼ç¾åˆªé™¤ NSP æœƒé¡¯è‘—å‚·å®³å° QNLI ç­‰è³‡æ–™é›†çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;LTR åœ¨æ‰€æœ‰ä»»å‹™ä¸Šéƒ½æ¯” MLM å·®ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶å¯ä»¥åƒ ELMo å–®ç¨è¨“ç·´ LTR å’Œ RTLï¼Œä¸¦ä¸”æŠŠä»–å€‘çµåˆèµ·ä¾†&lt;/p&gt;
&lt;p&gt;ä½†æœ‰ä»¥ä¸‹ç¼ºé»ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ¯”å–®å‘æ¨¡å‹è²´å…©å€&lt;/li&gt;
&lt;li&gt;å° QA ä»»å‹™ä¸ç›´è§€ï¼Œå› ç‚º RTL ç„¡æ³•æ ¹æ“šå•é¡Œçµ¦å‡ºç­”æ¡ˆ&lt;/li&gt;
&lt;li&gt;ä¸å¦‚æ·±åº¦é›™å‘æ¨¡å‹å¼·å¤§ï¼Œå› ç‚ºå…¶å¯ä»¥ç›´æ¥åœ¨æ¯ä¸€å±¤çœ‹åˆ°å·¦å³çš„ context&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-based-approach-with-bert&#34;&gt;Feature-based Approach with BERT&lt;/h3&gt;
&lt;p&gt;ä½œè€…ä¹Ÿç ”ç©¶äº†ç”¨ feature-based çš„æ•ˆæœï¼Œç™¼ç¾å…·å‚™ç«¶çˆ­åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ä»–çš„å¯¦é©—ä¸­ï¼Œç”¨é è¨“ç·´ Transformer çš„ top 4 éš±è—å±¤çš„ token ä¸²è¡—æ•ˆæœæœ€å¥½ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Self-Instruct è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 30 Apr 2023 00:00:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2212.10560&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;å¤§å‹ &amp;ldquo;instruction-tuned&amp;rdquo; èªè¨€æ¨¡å‹ (ç¶“éå¾®èª¿å¥½å›æ‡‰ instruction) å·²ç¶“å±•ç¾å‡ºåœ¨æ–°ä»»å‹™ä¸Š zero-shot çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œä»–å€‘åš´é‡ä¾è³´äººå·¥ç·¨å¯«çš„æŒ‡ä»¤ï¼Œåœ¨æ•¸é‡ã€å¤šæ¨£æ€§å’Œå‰µé€ åŠ›ä¸Šéƒ½å—åˆ°äº†é™åˆ¶ï¼Œé˜»ç¤™äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä»‹ç´¹äº† Self-Instruct é€™å€‹æ¡†æ¶ï¼Œå¯ä»¥é€éè‡ªå·±ç”Ÿæˆçš„æŒ‡ä»¤ï¼Œä¾†å¢å¼·é è¨“ç·´æ¨¡å‹éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;å°‡ä½œè€…çš„æ–¹æ³•æ‡‰ç”¨åœ¨ GPT3ï¼Œåœ¨ SuperNaturalInstructions ç²å¾—äº†æ¯”åŸå§‹æ¨¡å‹é«˜ 33% çš„æ”¹é€²ï¼Œèˆ‡ä½¿ç”¨ private user data å’Œ human annotations çš„ $InstructGPT_{001}$ æ€§èƒ½ç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥è©•ä¼°ï¼Œæˆ‘å€‘ç‚ºæ–°ä»»å‹™æ•´ç†ä¸€çµ„å°ˆå®¶ç·¨å¯«çš„æŒ‡ä»¤ï¼Œä¸¦é€šéäººå·¥è©•ä¼°ï¼Œé¡¯ç¤ºå‡ºä½¿ç”¨ Self-Instruction èª¿æ•´ GPT3 çš„æ€§èƒ½å¤§å¤§å„ªæ–¼ä½¿ç”¨ç¾æœ‰å…¬å…±æŒ‡ä»¤è³‡æ–™é›†ï¼Œåªæ¯” $InstructGPT_{001}$ è½å¾Œ 5% çš„å·®è·ã€‚&lt;/p&gt;
&lt;p&gt;Self-Instruct æä¾›ä¸€å€‹å¹¾ä¹ annotation-free çš„æ–¹æ³•ï¼Œalign é è¨“ç·´æ¨¡å‹å’Œ instructionsï¼Œè€Œä¸”ä½œè€…é‡‹å‡ºäº†ä»–å€‘çš„å¤§å‹åˆæˆè³‡æ–™é›†ï¼Œä»¥ä¿ƒé€²æœªä¾†å° instruction tuning çš„ç ”ç©¶ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;æœ€è¿‘çš„ NLP æ–‡ç»è¦‹è­‰äº†ã€Œå»ºæ§‹å¯ä»¥éµå¾ªè‡ªç„¶èªè¨€æŒ‡ä»¤çš„æ¨¡å‹æ–¹é¢ã€çš„å¤§é‡æ´»å‹•ã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›ç™¼å±•ç”±å…©å€‹é—œéµéƒ¨åˆ†çµ„æˆï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å¤§å‹é è¨“ç·´èªè¨€æ¨¡å‹ (LM)&lt;/li&gt;
&lt;li&gt;äººå·¥ç·¨å¯«çš„æŒ‡ä»¤è³‡æ–™&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PromptSource å’Œ SuperNaturalInstructions æ˜¯æœ€è¿‘å…©å€‹è‘—åçš„è³‡æ–™é›†ã€‚
ä»–å€‘é€éå¤§é‡æ‰‹å‹•è¨»é‡‹ä¾†æ”¶é›†æŒ‡ä»¤ï¼Œä»¥å»ºé€  T0 å’Œ T$k$-Instructã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™éç¨‹ä»£åƒ¹é«˜æ˜‚ï¼Œè€Œä¸”ç”±æ–¼å¤§å¤šæ•¸äººå¾€å¾€ç”Ÿæˆçš„éƒ½æ˜¯æµè¡Œçš„ NLP ä»»å‹™ï¼Œä½¿å…¶æœªèƒ½æ¶µè“‹çœŸæ­£å¤šæ¨£çš„ä»»å‹™ï¼Œä¹Ÿä¸èƒ½æ¶µè“‹å„ç¨®æè¿°ä»»å‹™çš„ä¸åŒæ–¹å¼ï¼Œå› æ­¤å¤šæ¨£æ€§å—ä¾·é™ã€‚&lt;/p&gt;
&lt;p&gt;é‘’æ–¼é€™äº›é™åˆ¶ï¼Œæƒ³è¦ç¹¼çºŒæå‡ instruction-tuned models çš„å“è³ªï¼Œéœ€è¦å¹« supervising instruction-tuned models ç™¼å±•æ›¿ä»£æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ä»‹ç´¹äº† Self-Instructï¼Œé€™æ˜¯ä¸€ç¨® semi-automated çš„éç¨‹ï¼Œç”¨æ¨¡å‹è‡ªèº«çš„ instructional signals å° pretrained LM é€²è¡Œ instruction-tuningã€‚&lt;/p&gt;
&lt;p&gt;æ•´å€‹æµç¨‹æ˜¯ä¸€ç¨® iterative bootstrapping algorithmï¼Œå¾æ‰‹å‹•ç·¨å¯«çš„ limited seed set å¼•å°ç”Ÿæˆã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ç¬¬ä¸€éšæ®µï¼Œæ¨¡å‹è¦å¹«æ–°ä»»å‹™ç”ŸæˆæŒ‡ä»¤ã€‚
åˆ©ç”¨ç¾æœ‰çš„æŒ‡ä»¤é›†åˆï¼Œå‰µå»ºæ›´å»£æ³›çš„æŒ‡ä»¤ï¼Œå¥½å®šç¾© (é€šå¸¸æ˜¯æ–°çš„) ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼æ–°ç”Ÿæˆçš„æŒ‡ä»¤é›†ï¼Œæ¡†æ¶ç‚ºä»–å€‘å‰µå»º input-output instancesï¼Œç¨å¾Œå¯ä»¥é€é supervising ç”¨æ–¼ instruction tuningã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œï¼Œé€éå„ç¨®æ‰‹æ®µï¼Œåœ¨ä½å“è³ªå’Œé‡è¤‡çš„æŒ‡ä»¤åŠ åˆ° task pool å‰ï¼ŒæŠŠä»–å€‘ä¿®å‰ªæ‰ã€‚&lt;/p&gt;
&lt;p&gt;å¯ä»¥é‡è¤‡é€™å€‹æµç¨‹éå¸¸å¤šæ¬¡ï¼Œç›´åˆ°ç²å¾—å¤§é‡ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;è©²æ¨¡å‹çš„è·Œä»£éç¨‹ä¸­ç”¢ç”Ÿäº†å¤§ç´„ 52K å€‹æŒ‡ä»¤ï¼Œèˆ‡å¤§ç´„ 85K å€‹ instance inputs å’Œ target outputs é…å° (æœ‰äº›ç›¸åŒçš„æŒ‡ä»¤æœƒå°æ‡‰å¤šç¨®è¼¸å…¥è¼¸å‡º)ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…è§€å¯Ÿåˆ°ç”Ÿæˆçš„è³‡æ–™æä¾›äº†å„ç¨®æœ‰å‰µæ„çš„ä»»å‹™ï¼Œå…¶ä¸­è¶…é 50% çš„ä»»å‹™å’Œ seed instructions çš„ ROUGE-L overlap å°æ–¼ 0.3ã€‚&lt;/p&gt;
&lt;p&gt;åŸºæ–¼ä¸Šè¿°çµæœï¼Œä½œè€…é€šéå¾®èª¿ GPT3 (å’Œç”ŸæˆæŒ‡ä»¤è³‡æ–™æ˜¯åŒå€‹æ¨¡å‹) å»ºæ§‹äº† $GPT3_{SELF-INST}$ã€‚&lt;/p&gt;
&lt;p&gt;SuperNI çš„çµæœè¡¨æ˜ï¼Œ$GPT3_{SELF-INST}$ æ€§èƒ½å¤§å¤§å„ªæ–¼ GPT3 (åŸå§‹æ¨¡å‹)ï¼Œé«˜äº† 33.1%ï¼Œå¹¾ä¹å’Œ $InstructGPT_{001}$ çš„æ€§èƒ½ç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…åœ¨æ–°å‰µå»ºçš„çš„æŒ‡ä»¤é›†ä¸Šé€²è¡Œäººå·¥è©•ä¼°ï¼Œ$GPT3_{SELF-INST}$ é¡¯ç¤ºå‡ºå»£æ³›çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå„ªæ–¼åœ¨å…¶ä»–å…¬é–‹å¯ç”¨æŒ‡ä»¤æ•¸æ“šé›†ä¸Šè¨“ç·´çš„æ¨¡å‹ï¼Œåªæ¯” InstrcutGPT001 è½å¾Œ 5%ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è²¢ç»ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Self-Instructï¼šä¸€ç¨®ç”¨æœ€å°‘çš„äººå·¥æ¨™è¨˜æ•¸æ“šå¼•å°æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„ä½œæ³•&lt;/li&gt;
&lt;li&gt;é€šéå¤§é‡çš„ instruction-tuning å¯¦é©—ï¼Œè­‰æ˜äº†æœ‰æ•ˆæ€§ã€‚&lt;/li&gt;
&lt;li&gt;ç™¼å¸ƒäº†ä¸€å€‹åŒ…å« 52K æŒ‡ä»¤çš„å¤§å‹ç¶œåˆè³‡æ–™é›†ï¼Œé‚„æœ‰ä¸€çµ„æ‰‹å‹•ç·¨å¯«çš„æ–°ä»»å‹™ï¼Œç”¨æ–¼å»ºæ§‹å’Œè©•ä¼°æœªä¾†çš„ instruction-following modelsã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;instruction-following-language-models&#34;&gt;Instruction-following language models&lt;/h3&gt;
&lt;p&gt;ä¸€ç³»åˆ—å·¥ä½œé¡¯ç¤ºï¼Œä½¿ç”¨ annotated &amp;ldquo;instructional&amp;rdquo; dataï¼Œå¯ä»¥ä½¿æ™®é€šèªè¨€æ¨¡å‹éµå¾ªä¸€èˆ¬èªè¨€çš„æŒ‡ä»¤ã€‚&lt;/p&gt;
&lt;p&gt;ä¹Ÿé¡¯ç¤ºå‡º &amp;ldquo;instructional&amp;rdquo; data çš„å¤§å°å’Œå¤šæ¨£æ€§ç›´æ¥å½±éŸ¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡çš„å·¥ä½œç›®çš„åœ¨æ¸›å°‘å°äººå·¥è¨»é‡‹è€…çš„ä¾è³´ã€‚&lt;/p&gt;
&lt;h3 id=&#34;language-models-for-data-generation-and-augmentation&#34;&gt;Language models for data generation and augmentation&lt;/h3&gt;
&lt;p&gt;è¨±å¤šå·¥ä½œä¾è³´ç”Ÿæˆå¼ LM ä¾†ç”Ÿæˆæ•¸æ“šæˆ–åš augmentationã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ä½œè€…çš„å·¥ä½œå¯è¢«è¦–ç‚ºä¸€ç¨® augmentationï¼Œä½†å’Œé€™äº›å·¥ä½œçš„å·®åˆ¥åœ¨æ–¼ä¸é™æ–¼ç‰¹å®šä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;Self-Instruct çš„ä¸€å€‹æ˜é¡¯å‹•æ©Ÿæ˜¯å¼•å°å‡ºæ–°çš„ä»»å‹™å®šç¾©ï¼Œè€Œé€™äº›ä»»å‹™å¯èƒ½é‚„æœªè¢« NLP çš„ç ”ç©¶è€…å®šç¾©éã€‚&lt;/p&gt;
&lt;h3 id=&#34;self-training&#34;&gt;Self-training&lt;/h3&gt;
&lt;p&gt;ä¸€ç¨®å…¸å‹çš„ self-training æ¡†æ¶é€éç¶“éè¨“ç·´çš„æ¨¡å‹ï¼Œå¹« unlabeled è³‡æ–™é€²è¡Œ labelï¼Œç„¶å¾Œç”¨é€™äº›è³‡æ–™æ”¹é€²æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ Self-Instruct å’Œ self-training æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹è™•ï¼Œä½†å¤šæ•¸ self-training çš„æ–¹æ³•éƒ½å‡è¨­äº†ä¸€å€‹ç‰¹å®šçš„ç›®æ¨™ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSelf-Instruct å¾é ­é–‹å§‹ç”Ÿå‡ºå„ç¨®ä»»å‹™ã€‚&lt;/p&gt;
&lt;h3 id=&#34;knowledge-distillation&#34;&gt;Knowledge distillation&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;é€™é‚Šæˆ‘æƒ³ä¸å¤ªé€šç‚ºä»€éº¼å¯ä»¥å’Œ Knowledge distillation æ‰¯ä¸Šé—œä¿‚&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Knowledge distillation é€šå¸¸æ¶‰åŠçŸ¥è­˜å¾è¼ƒå¤§æ¨¡å‹åˆ°è¼ƒå°æ¨¡å‹çš„è½‰ç§»&lt;/p&gt;
&lt;p&gt;Self-Instruct ä¹Ÿå¯ä»¥çœ‹åšæ˜¯ Knowledge distillation çš„ä¸€ç¨®å½¢å¼ï¼Œä½†å€åˆ¥å¦‚ä¸‹&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;distillation çš„ä¾†æºå’Œç›®æ¨™æ˜¯ç›¸åŒçš„ï¼Œå³æ¨¡å‹çš„çŸ¥è­˜è¢« distill åˆ°ä»–è‡ªå·±&lt;/li&gt;
&lt;li&gt;distill çš„å…§å®¹ä»¥ instruction task çš„å½¢å¼å‡ºç¾&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;æ¨™è¨˜å¤§è¦æ¨¡æŒ‡ä»¤è³‡æ–™å°äººé¡ä¾†èªªå¯èƒ½å…·æœ‰æŒ‘æˆ°æ€§ï¼Œå› ç‚ºä»–éœ€è¦&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å‰µæ„ï¼Œå¥½æå‡ºæ–°ä»»å‹™&lt;/li&gt;
&lt;li&gt;ç‚ºæ¯å€‹ä»»å‹™ç·¨å¯« labeled instances çš„å°ˆæ¥­çŸ¥è­˜&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;defining-instruction-data&#34;&gt;Defining Instruction Data&lt;/h3&gt;
&lt;p&gt;æˆ‘å€‘è¦ç”Ÿæˆçš„æŒ‡ä»¤è³‡æ–™é›†åŒ…å« {$I_t$}ï¼Œæ¯å€‹æŒ‡ä»¤ç”¨è‡ªç„¶èªè¨€å®šç¾©äº†ä»»å‹™ $t$ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹ä»»å‹™éƒ½æœ‰ä¸€å€‹æˆ–å¤šå€‹ input-output instances ($X_t,Y_t$)ã€‚&lt;/p&gt;
&lt;p&gt;çµ¦å®š task instruction $I_t$ï¼Œé‚„æœ‰ instance xï¼Œæ¨¡å‹ M è¦ç”Ÿå‡º yï¼š&lt;/p&gt;
&lt;p&gt;$M(I_t,x)=y, for (x,y) \in (X_t,Y_t)$&lt;/p&gt;
&lt;p&gt;å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œinstance input å’Œ instruction æ²’æœ‰åš´æ ¼åˆ†ç•Œã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ Instruction:&amp;ldquo;write an essay about school safety&amp;rdquo; x:&amp;quot;&amp;quot;ï¼Œå¯ä»¥è¢«æ”¹ç‚º Instruction:&amp;ldquo;write an essay about the following topic&amp;rdquo; x:&amp;ldquo;school safety&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;automatic-instruction-data-generation&#34;&gt;Automatic Instruction Data Generation&lt;/h3&gt;
&lt;p&gt;ç”ŸæˆæŒ‡ä»¤è³‡æ–™çš„ pipeline åˆ†æˆå››å€‹æ­¥é©Ÿï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æŒ‡ä»¤ç”Ÿæˆ&lt;/li&gt;
&lt;li&gt;è¾¨è­˜æŒ‡ä»¤æ˜¯å¦æ˜¯åˆ†é¡ä»»å‹™&lt;/li&gt;
&lt;li&gt;ç”¨ input-first æˆ– output-first åš instance generation&lt;/li&gt;
&lt;li&gt;éæ¿¾æ‰ä½å“è³ªçš„è³‡æ–™&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;instruction-generation&#34;&gt;Instruction Generation&lt;/h4&gt;
&lt;p&gt;Self-Instruct æ˜¯åŸºæ–¼ä¸€å€‹ç™¼ç¾ï¼Œä¹Ÿå°±æ˜¯å¤§å‹èªè¨€æ¨¡å‹å¯ä»¥é€é context ä¸­çš„ç¾æœ‰æŒ‡ä»¤ï¼Œç”Ÿå‡ºæ–°ç©çš„æŒ‡ä»¤ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºä½œè€…æä¾›äº†ä¸€ç¨®å¾ä¸€å°çµ„äººé¡ç·¨å¯«çš„æŒ‡ä»¤ä¸­ï¼Œä½¿æŒ‡ä»¤è³‡æ–™å¢é•·çš„åšæ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ä»–å€‘ç·¨å¯«çš„ 175 å€‹ä»»å‹™ (æ¯å€‹ä»»å‹™ 1 å€‹ instruction å’Œ 1 å€‹ instance) åˆå§‹åŒ– task poolã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ¯ä¸€å€‹ stepï¼Œä½œè€…å¾è£¡é¢ sample 8 å€‹ instructionsï¼Œä½œç‚º in-context çš„ç¯„ä¾‹ã€‚åœ¨é€™ 8 å€‹æŒ‡ä»¤ä¸­ï¼Œæœ‰ 6 æ¢ä¾†è‡ªäººå·¥ç·¨å¯«çš„ä»»å‹™ï¼Œå¦å¤–å…©æ¢ä¾†è‡ªå‰é¢æ­¥é©Ÿä¸­æ¨¡å‹ç”Ÿæˆçš„ä»»å‹™ï¼Œä»¥ä¿ƒé€²å¤šæ¨£æ€§ã€‚&lt;/p&gt;
&lt;h4 id=&#34;classification-task-identification&#34;&gt;Classification Task Identification&lt;/h4&gt;
&lt;p&gt;å› ç‚ºå°æ–¼åˆ†é¡å’Œéåˆ†é¡çš„ä»»å‹™ï¼Œä½œè€…æœƒæ¡å–å…©ç¨®åšæ³•ï¼Œæ‰€ä»¥ä½œè€…ä½¿ç”¨ä¾†è‡ª seed taks çš„ 12 æ¢åˆ†é¡æŒ‡ä»¤å’Œ 19 æ¢éåˆ†é¡æŒ‡ä»¤ï¼Œè®“ GPT3 é€é few-shot ä¾†åˆ¤åˆ¥ã€‚&lt;/p&gt;
&lt;h4 id=&#34;instance-generation&#34;&gt;Instance Generation&lt;/h4&gt;
&lt;p&gt;çµ¦äºˆæŒ‡ä»¤å’Œä»–å€‘çš„ä»»å‹™é¡åˆ¥ï¼Œä½œè€…ç¨ç«‹åœ°ç‚ºæ¯æ¢æŒ‡ä»¤ç”Ÿæˆ instanceã€‚&lt;/p&gt;
&lt;p&gt;é€™å…·å‚™æŒ‘æˆ°æ€§ï¼ŒåŸå› åœ¨æ–¼ä»–éœ€è¦æ¨¡å‹ç­è§£ç›®æ¨™ä»»å‹™æ˜¯ä»€éº¼ï¼Œæ ¹æ“šæŒ‡ä»¤æ‰¾å‡ºéœ€è¦é‚£äº›é¡å¤–çš„è¼¸å…¥å…§å®¹ï¼Œä¸¦ç”Ÿæˆä»–å€‘ã€‚ (æ¨¡å‹è¦æ ¹æ“š instruction ç”Ÿå‡º instance input)&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ï¼Œåœ¨ prompt ä¸­æ”¾å…¥å…¶ä»–åŒ…å« instruction-input-output çš„ä»»å‹™ç¯„ä¾‹çš„æ™‚å€™ï¼Œæ¨¡å‹å¯ä»¥å¯¦ç¾é€™é»ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨®è‡ªç„¶çš„æ–¹æ³•æ˜¯ Input-first Approachï¼Œå¯ä»¥è¦æ±‚èªè¨€æ¨¡å‹å…ˆæ ¹æ“šæŒ‡ä»¤æå‡º inputï¼Œå†ç”Ÿå‡ºç›¸æ‡‰çš„ outputã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œï¼Œé€™ç¨®æ–¹æ³•åœ¨åˆ†é¡ä»»å‹™ä¸Šï¼Œå¯èƒ½æœƒåå‘æ–¼ç”ŸæˆæŸç¨® labelã€‚æ‰€ä»¥ï¼Œå°æ–¼åˆ†é¡ä»»å‹™ï¼Œä½œè€…æ¡ç”¨ Output-first Approachï¼Œå…ˆç”Ÿæˆå¯èƒ½çš„ labelï¼Œåœ¨æ¯å€‹ label ä¸Šå†ç”Ÿæˆè¼¸å…¥ã€‚&lt;/p&gt;
&lt;h4 id=&#34;filtering-and-postprocessing&#34;&gt;Filtering and Postprocessing&lt;/h4&gt;
&lt;p&gt;ç‚ºäº†é¼“å‹µå¤šæ¨£æ€§ï¼Œåªæœ‰ç•¶æ–°çš„æŒ‡ä»¤å’Œä»»ä½•ç¾æœ‰çš„æŒ‡ä»¤çš„ ROUGE-L overlapping å°æ–¼ 0.7 çš„æ™‚å€™ï¼Œæ‰æœƒè¢«æ·»åŠ åˆ° task poolã€‚&lt;/p&gt;
&lt;p&gt;é‚„æ’é™¤äº†ä¸€äº›åŒ…å«äº†é€šå¸¸ä¸èƒ½è¢« LM è™•ç†çš„é—œéµå­— (e.g. images, pictures, graphs) çš„æŒ‡ä»¤ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç‚ºæ¯å€‹æŒ‡ä»¤ç”Ÿæˆæ–°çš„ instance çš„æ™‚å€™ï¼Œæœƒéæ¿¾æ‰å®Œå…¨ç›¸åŒæˆ–è€…æ˜¯è¼¸å…¥ç›¸åŒä½†è¼¸å‡ºä¸åŒçš„ instanceã€‚&lt;/p&gt;
&lt;h3 id=&#34;finetuning-the-lm-to-follow-instructions&#34;&gt;Finetuning the LM to Follow Instructions&lt;/h3&gt;
&lt;p&gt;åœ¨å‰µå»ºå¤§è¦æ¨¡æŒ‡ä»¤è³‡æ–™å¾Œï¼Œç”¨é€™äº›è³‡æ–™å°åŸå§‹èªè¨€æ¨¡å‹é€²è¡Œ fine-tuneã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºæ­¤ï¼Œå°‡ instruction å’Œ instance input é€£æ¥èµ·ä¾†ï¼Œä½œç‚º promptï¼Œç„¶å¾Œè¨“ç·´æ¨¡å‹é€éæ¨™æº–çš„ç›£ç£å¼å­¸ç¿’é€²è¡Œå¾®èª¿ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è®“æ¨¡å‹å°ä¸åŒçš„æ ¼å¼ robustï¼Œä½¿ç”¨å¤šå€‹æ¨¡æ¿å°‡æŒ‡ä»¤å’Œè¼¸å…¥ encode åœ¨ä¸€èµ·ã€‚&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ï¼ŒæŒ‡ä»¤å¯ä»¥æœ‰æˆ–æ²’æœ‰ Task: å‰å¢œã€è¼¸å…¥å¯ä»¥æœ‰æˆ–æ²’æœ‰ Input: å‰å¢œï¼Œæˆ–æ˜¯ä¸­é–“å¯ä»¥æœ‰ä¸åŒæ•¸é‡çš„æ›è¡Œä¹‹é¡çš„ã€‚&lt;/p&gt;
&lt;h2 id=&#34;self-instruct-data-from-gpt3&#34;&gt;Self-Instruct Data from GPT3&lt;/h2&gt;
&lt;p&gt;ä½œè€…é€é OpenAI API è¨ªå•æœ€å¤§çš„ GPT3 (davinci)&lt;/p&gt;
&lt;h3 id=&#34;statistics&#34;&gt;Statistics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;diversity&#34;&gt;Diversity&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;quality&#34;&gt;Quality&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;h3 id=&#34;gpt3_self-inst-fine-tuning-gpt3-on-its-own-instruction-data&#34;&gt;$GPT3_{SELF-INST}$: fine-tuning GPT3 on its own instruction data&lt;/h3&gt;
&lt;p&gt;ä½¿ç”¨ç”Ÿå‡ºä¾†çš„æŒ‡ä»¤è³‡æ–™ï¼Œå° GPT3 é€²è¡Œå¾®èª¿ã€‚&lt;/p&gt;
&lt;p&gt;å¾®èª¿æ˜¯é€é OpenAI finetuning API&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;h4 id=&#34;off-the-shelf-language-models&#34;&gt;Off-the-shelf language models&lt;/h4&gt;
&lt;p&gt;T5-LM å’Œ GPT3 æ˜¯æ™®é€š LM baselines (åªæœ‰ pre-trainingï¼Œæ²’æœ‰é¡å¤– fine-tune)&lt;/p&gt;
&lt;p&gt;é€™äº› baseline å°‡è¡¨æ˜ç¾æˆçš„ LM åœ¨é è¨“ç·´å¾Œï¼Œèƒ½å¤ ç«‹åˆ»è‡ªç„¶åœ°éµå¾ªæŒ‡ä»¤çš„ç¨‹åº¦ã€‚&lt;/p&gt;
&lt;h4 id=&#34;publicly-available-instruction-tuned-models&#34;&gt;Publicly-available instruction-tuned models&lt;/h4&gt;
&lt;p&gt;T0 å’Œ $T_k$-Instruct æ˜¯å…©å€‹ instruction-tuned modelsã€‚&lt;/p&gt;
&lt;p&gt;å…©è€…éƒ½æ˜¯å¾ T5 é€²è¡Œå¾®èª¿çš„ï¼Œå°é€™å…©ç¨®æ¨¡å‹ï¼Œéƒ½ä½¿ç”¨å…·æœ‰ 11B åƒæ•¸çš„æœ€å¤§ç‰ˆæœ¬ã€‚&lt;/p&gt;
&lt;h4 id=&#34;instruction-tuned-gpt3-models&#34;&gt;Instruction-tuned GPT3 models&lt;/h4&gt;
&lt;p&gt;ä½œè€…è©•ä¼°äº† InstructGPTï¼Œå®ƒæ˜¯ OpenAI åŸºæ–¼ GPT3 é–‹ç™¼çš„ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ SuperNI çš„å¯¦é©—ï¼Œåªèˆ‡ text-davinci-001 engine é€²è¡Œæ¯”è¼ƒï¼Œå› ç‚ºæ›´æ–°çš„ engine ç”¨æœ€æ–°çš„ç”¨æˆ¶è³‡æ–™ï¼Œè€Œä¸”å¾ˆå¯èƒ½å·²ç¶“çœ‹é SuperNIã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼æ–°ç·¨å¯«çš„æŒ‡ä»¤ï¼Œè©•ä¼°æ™‚å‰‡åŒ…å«äº† 001ã€002 å’Œ 003ï¼Œä»¥ç¢ºä¿å®Œæ•´æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æ¯”è¼ƒ Self-Instruct åœ¨å…¶ä»–å…¬é–‹å¯ç”¨çš„æŒ‡ä»¤è¨“ç·´é›†è³‡æ–™ï¼Œä½¿ç”¨ PromptSource å’Œ SuperNI çš„è³‡æ–™å¾®èª¿ GPT3ï¼Œé€™äº›è³‡æ–™ç”¨æ–¼è¨“ç·´ T0 å’Œ $T_k$-Instruct æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;åˆ†åˆ¥ç°¡ç¨±ç‚º T0 è¨“ç·´å’Œ SuperNI è¨“ç·´ã€‚&lt;/p&gt;
&lt;h3 id=&#34;experiment-1-zero-shot-generalization-on-superni-benchmark&#34;&gt;Experiment 1: Zero-Shot Generalization on SUPERNI benchmark&lt;/h3&gt;
&lt;p&gt;é¦–å…ˆä»¥ zero-shot çš„æ–¹å¼è©•ä¼°å…¸å‹ NLP ä»»å‹™éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;experiment-2-generalization-to-user-oriented-instructions-on-novel-tasks&#34;&gt;Experiment 2: Generalization to User-oriented Instructions on Novel Tasks&lt;/h3&gt;
&lt;p&gt;ç›¡ç®¡ SuperNI åœ¨ç¾æœ‰çš„ NLP ä»»å‹™å…·æœ‰å…¨é¢æ€§ï¼Œå¤šæ•¸çš„é€™äº›ä»»å‹™æ˜¯åˆæ–¼ç ”ç©¶ç†ç”±æå‡ºçš„ï¼Œè€Œä¸”åå‘åˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ›´å¥½çš„ç²å–æŒ‡ä»¤éµå¾ªæ¨¡å‹çš„å¯¦ç”¨åƒ¹å€¼ï¼Œä½œè€…ä¸­çš„ä¸€éƒ¨åˆ†äººç­–åŠƒäº†ä¸€çµ„é¢å‘ç”¨æˆ¶æ‡‰ç”¨çš„æ–°æŒ‡ä»¤é›†ã€‚&lt;/p&gt;
&lt;p&gt;ä»–å€‘å…ˆé‡å° Large LM å¯èƒ½å¯ä»¥æ‡‰ç”¨åˆ°çš„é ˜åŸŸé€²è¡Œ brainstormï¼Œä¸¦ä¸”åˆ¶å®šèˆ‡æ¯å€‹é ˜åŸŸç›¸é—œçš„ instruction å’Œ instanceã€‚&lt;/p&gt;
&lt;p&gt;ç¸½å…±å‰µå»ºäº† 252 æ¢æŒ‡ä»¤ï¼Œæ¯æ¢æŒ‡ä»¤æœ‰ 1 å€‹ instanceã€‚&lt;/p&gt;
&lt;h4 id=&#34;human-evaluation-setup&#34;&gt;Human evaluation setup&lt;/h4&gt;
&lt;p&gt;è©•ä¼°æ¨¡å‹åœ¨é€™äº›ä¸åŒä»»å‹™çš„æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾æ¥µå…·æŒ‘æˆ°æ€§ï¼Œå› ç‚ºä¸åŒçš„ä»»å‹™éœ€è¦ä¸åŒçš„å°ˆæ¥­çŸ¥è­˜ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç²å¾—æ›´å¿ å¯¦çš„è©•åƒ¹ï¼Œä½œè€…è«‹äº† instructions çš„ä½œè€…å°æ¨¡å‹çš„é æ¸¬çµæœé€²è¡Œè©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;å¯¦æ–½ä¸€å€‹ four-level rating systemï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rating A
&lt;ul&gt;
&lt;li&gt;å›è¦†æœ‰æ•ˆä¸”ä»¤äººæ»¿æ„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating B
&lt;ul&gt;
&lt;li&gt;å›è¦†å¯æ¥å—ï¼Œä½†å­˜åœ¨å¯ä»¥æ”¹é€²çš„åœ°æ–¹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating C
&lt;ul&gt;
&lt;li&gt;å›è¦†ç›¸é—œï¼Œä½†åœ¨å…§å®¹ä¸Šæœ‰é‡å¤§éŒ¯èª¤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating D
&lt;ul&gt;
&lt;li&gt;å›è¦†ä¸ç›¸é—œæˆ–ç„¡æ•ˆï¼ŒåŒ…å«é‡è¤‡è¼¸å…¥çš„éƒ¨åˆ†ï¼Œå®Œå…¨ç„¡é—œçš„è¼¸å‡ºã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;results-1&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å¦‚æœæŠŠ Rating B ä»¥ä¸Šè¦–ç‚ºæœ‰æ•ˆï¼Œ$GPT_{SELF-INST}$ åªå’Œ $InstructGPT_{001}$ ç›¸å·® 5%&lt;/p&gt;
&lt;h2 id=&#34;discussion-and-limitation&#34;&gt;Discussion and Limitation&lt;/h2&gt;
&lt;h3 id=&#34;why-does-self-instruct-work&#34;&gt;Why does SELF-INSTRUCT work?&lt;/h3&gt;
&lt;p&gt;å€¼å¾—åæ€çš„æ˜¯ï¼Œåœ¨æœ€è¿‘æˆåŠŸçš„ instruction-tuning LMs ä¸­ï¼Œé«˜å“è³ªçš„ human feedback æ‰®æ¼”çš„è§’è‰²ã€‚&lt;/p&gt;
&lt;p&gt;é€™è£¡æœ‰å…©å€‹æ¥µç«¯çš„å‡è¨­ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Human feedback æ˜¯ instruction-tuning ä¸­å¿…è¦ä¸”ä¸å¯æˆ–ç¼ºçš„è§’è‰²ï¼Œå› ç‚º LM éœ€è¦äº†è§£åœ¨é è¨“ç·´éç¨‹ä¸­æ²’å®Œå…¨äº†è§£åˆ°çš„å•é¡Œã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Human feedback æ˜¯ instruction-tuning ä¸€å€‹å¯é¸çš„æ–¹å‘ï¼Œå› ç‚º LM åœ¨é è¨“ç·´å°±å·²ç¶“å¾ˆç†Ÿæ‚‰æŒ‡ä»¤äº†ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é›–ç„¶ç¾å¯¦å¯èƒ½ä»‹æ–¼é€™å…©å€‹æ¥µç«¯ä¹‹é–“ï¼Œä½œè€…æ¨æ¸¬å¯èƒ½æ›´å‚¾å‘æ–¼ç¬¬äºŒç¨®å‡è¨­ï¼Œå°¤å…¶æ˜¯å°æ–¼è¼ƒå¤§çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;ç¬¬äºŒç¨®ï¼Œä¹Ÿæ˜¯äººé¡ç›´è¦ºï¼Œæ˜¯ Self- Instruct çš„é—œéµå‹•æ©Ÿï¼Œè€Œä¸”ä¹Ÿå¾æˆåŠŸçš„çµæœç²å¾—æ”¯æŒã€‚&lt;/p&gt;
&lt;h3 id=&#34;broader-impact&#34;&gt;Broader Impact&lt;/h3&gt;
&lt;p&gt;é™¤äº†æœ¬æ–‡çš„ç›´æ¥é—œæ³¨é»å¤–ï¼Œä½œè€…ç›¸ä¿¡ Self-Instruct å¯èƒ½æœ‰åŠ©æ–¼æ­éœ²å„ç¨® instruction tuning æ¨¡å‹ &amp;ldquo;å¹•å¾Œ&amp;rdquo; ç™¼ç”Ÿçš„äº‹æƒ…ã€‚&lt;/p&gt;
&lt;p&gt;ä¸å¹¸çš„æ˜¯ï¼Œç”±æ–¼ä»–å€‘çš„è³‡æ–™é›†å°šæœªç™¼å¸ƒï¼Œé€™ç¨®æ¥­ç•Œæ¨¡å‹ä»è™•æ–¼ API ç‰†ä¹‹å¾Œã€‚&lt;/p&gt;
&lt;p&gt;äººå€‘å°å…¶çµæ§‹ä»¥åŠç‚ºä½•èƒ½å±•ç¾ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›çŸ¥ä¹‹ç”šå°‘ã€‚&lt;/p&gt;
&lt;h3 id=&#34;limitations-of-self-instruct&#34;&gt;Limitations of Self-Instruct&lt;/h3&gt;
&lt;h4 id=&#34;tail-phenomena&#34;&gt;Tail phenomena&lt;/h4&gt;
&lt;p&gt;Self-Instruct ä¾è³´æ–¼ LMï¼Œç¹¼æ‰¿ LM çš„æ‰€æœ‰é™åˆ¶ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ç ”ç©¶é¡¯ç¤ºå‡º tail phenomena å° LM çš„æˆåŠŸæ§‹æˆåš´å³»çš„æŒ‘æˆ°ã€‚&lt;/p&gt;
&lt;p&gt;æ›å¥è©±èªªï¼ŒLM çš„æœ€å¤§æ”¶ç›Šå‡ºç¾æ–¼èªè¨€ä¸­æœ€é »ç¹å‡ºç¾çš„éƒ¨åˆ† (èªè¨€åˆ†ä½ˆçš„é ­éƒ¨)ï¼Œè€Œä½é »ç‡å‡ºç¾çš„ä¸Šä¸‹æ–‡ä¸­ç²å¾—çš„æ”¶ç›Šæœ€å°ã€‚&lt;/p&gt;
&lt;p&gt;åŒæ¨£çš„ï¼Œåœ¨é€™é …å·¥ä½œèƒŒæ™¯ä¸‹ï¼Œå¦‚æœ Self-Instruct å¤§éƒ¨åˆ†çš„æ”¶ç›Šåå‘é è¨“ç·´ corpus ä¸­é »ç¹å‡ºç¾çš„ä»»å‹™æˆ–æŒ‡ä»¤ï¼Œé‚£ä¹Ÿä¸ä»¤äººæ„Ÿåˆ°æ„å¤–ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œè©²æ–¹æ³•åœ¨ä¸å¸¸è¦‹å’Œæœ‰å‰µæ„çš„æŒ‡ä»¤ä¸‹ï¼Œå¯èƒ½æœƒé¡¯ç¾å‡ºè„†å¼±æ€§ã€‚&lt;/p&gt;
&lt;h4 id=&#34;dependence-on-large-models&#34;&gt;Dependence on large models&lt;/h4&gt;
&lt;p&gt;å› ç‚º Self-Instruct ä¾è³´æ–¼å¾ LM ä¸­æå–åˆçš„ inductive biasï¼Œå› æ­¤å®ƒå¯èƒ½é©åˆ larger modelã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœé€™æ˜¯å°çš„ï¼Œé€™æœƒå°é‚£äº›æ²’æœ‰å¤§é‡è¨ˆç®—è³‡æºçš„äººé€ æˆé˜»ç¤™ã€‚&lt;/p&gt;
&lt;h4 id=&#34;reinforcing-lm-biases&#34;&gt;Reinforcing LM biases&lt;/h4&gt;
&lt;p&gt;ä½œè€…æ“”å¿ƒé€™ç¨®è¿­ä»£ä½œæ³•å¯èƒ½æœƒç”¢ç”Ÿæ„æ–™ä¹‹å¤–çš„çµæœï¼Œæ¯”å¦‚å°‡æœ‰å•é¡Œçš„ç¤¾æœƒåè¦‹æ”¾å¤§ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Relative Position ä»‹ç´¹ &#43; è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/relative-position-%E4%BB%8B%E7%B4%B9--%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 24 Apr 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/relative-position-%E4%BB%8B%E7%B4%B9--%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;h2 id=&#34;èªªæ˜&#34;&gt;èªªæ˜&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡å¯«æ–¼ &lt;a class=&#34;link&#34; href=&#34;https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Swin Transformer è«–æ–‡é–±è®€&lt;/a&gt; ä¹‹å¾Œï¼Œç•¶æ™‚å° Relatvie position çš„ç†è§£ä¸å¤ æ¸…æ¥šï¼Œæœ¬æ–‡å°‡æœƒåšè§£é‡‹ï¼Œä¸¦é™„ä¸ŠåŸè«–æ–‡çš„ç­†è¨˜ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸‹å°‡æœƒå…ˆç”¨é•·åº¦ç‚º 3 çš„åºåˆ—ä½œç‚ºç¤ºç¯„ã€‚&lt;/p&gt;
&lt;h3 id=&#34;absolute-position-encodings&#34;&gt;Absolute Position Encodings&lt;/h3&gt;
&lt;p&gt;Absolute Position Encodings çš„åšæ³•æ˜¯æŠŠç”¨æŸç¨®æ–¹å¼ç”Ÿæˆæˆ–å¯å­¸ç¿’çš„å‘é‡åŠ åœ¨è¼¸å…¥ï¼Œç¬¬ä¸€å€‹ä½ç½®ç”¨ $w_1$ï¼Œç¬¬äºŒå€‹ä½ç½®ç”¨ $w_2$ï¼Œç¬¬ä¸‰å€‹ä½ç½®ç”¨ $w_3$ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/abs-pos.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;relative-position-encodings&#34;&gt;Relative Position Encodings&lt;/h3&gt;
&lt;p&gt;Relative Position Encodings é¡§åæ€ç¾©ï¼Œå°±æ˜¯æ”¹ç”¨ç›¸å°ä½ç½®ä¾†åšé€™äº›å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–ä¸­ï¼ŒPosition Encoding çš„éƒ¨åˆ†å¾ 3 å€‹å‘é‡è®Šæˆ 3*3 å€‹å‘é‡ï¼Œå› ç‚ºç¾åœ¨æœƒä»¥æ¯å€‹ token ç‚ºåŸºæº–ï¼Œç”Ÿå‡º 3 å€‹ç›¸å°ä½ç½®å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘ä»¥ $w_0$ï¼Œä»£è¡¨è™•æ–¼åŸé»ï¼Œ$w_x$ ä»£è¡¨å¾€å³ $x$ æ ¼ï¼Œ$w_{-x}$ ä»£è¡¨å¾€å·¦ $x$ æ ¼ï¼Œå…¶ä¸­ $x$ æ˜¯æ­£æ•´æ•¸ã€‚&lt;/p&gt;
&lt;p&gt;ç¬¬ä¸€å€‹ row æœ‰ $w_0$ã€$w_1$ã€$w_2$ï¼Œæ„æ€æ˜¯ä»¥ç¬¬ 0 å€‹å‘é‡ (I) ç‚ºåŸºæº–ï¼Œä»–çš„ä½ç½®æ˜¯ $w_0$ï¼Œå°ç¬¬ 0 å€‹å‘é‡ä¾†èªªï¼Œç¬¬ 1 å€‹å‘é‡ (like) æ˜¯ $w_1$ï¼Œç¬¬ 2 å€‹å‘é‡ (cat) æ˜¯ $w_2$ã€‚&lt;/p&gt;
&lt;p&gt;è¼ªæµä»¥ $n$ å€‹ token ç‚ºåŸºæº–ï¼Œå°±æœƒç”Ÿå‡º n*n å€‹ç›¸å°ä½ç½®å‘é‡ï¼Œè€Œä¸æ˜¯åŸå…ˆçš„ n å€‹çµ•å°ä½ç½®å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;å…¶ä¸­ $w_i$ å’Œ $w_j$ å¦‚æœ $i=j$ï¼Œä»–å€‘æœƒå…±ç”¨åŒæ¨£çš„ weightï¼Œä¸Šåœ–æ˜¯ä»¥ç›¸åŒé¡è‰²è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœåºåˆ—é•·åº¦æ˜¯ $n$ï¼Œå°±æœƒæœ‰ $2n-1$ å€‹å‘é‡è¦å­¸ã€‚&lt;/p&gt;
&lt;p&gt;n*n é€™å€‹æ•¸é‡ä½¿å…¶é©åˆåŠ å…¥åˆ° self-attentionï¼ŒåŸå§‹è«–æ–‡çš„åŠ å…¥æ–¹å¼å¯ä»¥åƒè€ƒä¸‹æ–¹è«–æ–‡ç­†è¨˜ï¼Œé€™é‚Šæ™šé»æœƒä»‹ç´¹å¾ŒçºŒè¡ç”Ÿçš„ç°¡åŒ–ç‰ˆã€‚&lt;/p&gt;
&lt;h3 id=&#34;swin-transformer-å¦‚ä½•å°å…¥-relative-position-encodings&#34;&gt;Swin Transformer å¦‚ä½•å°å…¥ Relative Position Encodings&lt;/h3&gt;
&lt;p&gt;Swin Transformer æ˜¯å€Ÿé‘’è¨±å¤š CNN æ¶æ§‹ï¼Œç‚ºäº† CV è€Œç¶“éä¿®æ”¹çš„ vision transformerã€‚&lt;/p&gt;
&lt;p&gt;å…¶ä¸­ä¸€å€‹é‡é»æ˜¯ï¼Œä»–æœƒåœ¨ä¸€å°å€å¡Šçš„ç‰¹å¾µåœ–ä¸Šåš self-attentionï¼Œè€Œä¸”æ˜¯ç”¨ Relative Position Encodingsã€‚&lt;/p&gt;
&lt;p&gt;å’Œå‰›å‰›çš„å·®åˆ¥åœ¨æ–¼ï¼Œç¾åœ¨è¦åœ¨äºŒç¶­ç©ºé–“åš Relative Position Encodingsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å‡è¨­æœ‰ä¸€å¼µ 2*2 çš„ feature mapï¼Œæˆ‘å€‘å…ˆè¨­å®šå¥½ feature map å„å€‹ token çš„çµ•å°ä½ç½®åº§æ¨™ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œæˆ‘å€‘è¼ªæµæŠŠ feature map çš„æ¯ä¸€å€‹ token ä½œç‚ºåŸºæº–é»ï¼ŒæŠŠ feature map çš„æ¯å€‹ token çš„åº§æ¨™æ¸›å»åŸºæº–é»çš„åº§æ¨™ï¼Œå°±å¯ä»¥å¾—åˆ°ç›¸å°ä½ç½®åº§æ¨™ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœæˆ‘å€‘æŠŠå››å€‹ç›¸å°ä½ç½®åº§æ¨™å„åˆ¥æ”¤å¹³ (æŒ‰ç…§å·¦ä¸Š -&amp;gt; å³ä¸Š -&amp;gt; å·¦ä¸‹ -&amp;gt; å³ä¸‹çš„é †åº)ï¼Œä¸¦ä¸”å¾ä¸Šåˆ°ä¸‹æ’å¥½ï¼Œä»–æœƒçœ‹èµ·ä¾†å¦‚ä¸‹åœ–ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æ­¤æ™‚æˆ‘å€‘å¹¾ä¹å®Œæˆäº†ç›¸å°ä½ç½®çš„è¡¨ï¼Œå’Œå‰›å‰›åºåˆ—ä¸€æ¨£ç”Ÿå‡ºäº† n*n å€‹ç›¸å°ä½ç½®ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘æ¥ä¸‹ä¾†è¦åšçš„äº‹æƒ…æ˜¯æŠŠé€™å€‹è¡¨çµ¦ç·¨è™Ÿï¼ŒæŠŠ (0, 0) éƒ½ç·¨æˆæŸå€‹æ•¸å­—ï¼ŒæŠŠ (1, 0) éƒ½ç·¨æˆæŸå€‹æ•¸å­—ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ­¤ä¹‹å‰ï¼Œå…ˆè€ƒæ…®ç¸½å…±æœƒæœ‰å¹¾ç¨®å¯èƒ½çš„ç›¸å°åº§æ¨™ï¼Œå°æ–¼é‚Šé•· $M$ çš„ feature map (é€™è£¡ M=2)ï¼Œå› ç‚ºå…©è»¸å¯èƒ½çš„æ•¸å­—çš†æœ‰ (2M-1) ç¨®ï¼Œå…±æœƒæœ‰ (2M-1)*(2M-1) ç¨®å¯èƒ½æ€§ï¼Œé€™è£¡ç­‰æ–¼ 9ã€‚&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥æˆ‘å€‘ç­‰ç­‰æœƒæŠŠæ‰€æœ‰åº§æ¨™ç·¨ç‚º 0~8ã€‚&lt;/p&gt;
&lt;p&gt;æƒ³å¾åº§æ¨™ç”Ÿå‡ºç·¨è™Ÿ 0~8 å¯ä»¥è€ƒæ…®æŠŠåº§æ¨™å…©è»¸çš„æ•¸å­—ç›¸åŠ ï¼Œä½†ç”±æ–¼æœ‰è² æ•¸çš„å­˜åœ¨ï¼Œè¦å…ˆæŠŠå…©è»¸çš„æ•¸å­—éƒ½è®Šæˆéè² æ•´æ•¸ï¼Œæ‰€ä»¥å…ˆæŠŠå…©è»¸çš„åº§æ¨™éƒ½å„åˆ¥åŠ  M-1ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æ­¤æ™‚å¦‚æœç›¸åŠ ï¼Œæœƒä½¿ (2, 1) å’Œ (1, 2) éƒ½å°æ‡‰åˆ°æ•¸å­— 3ï¼Œæ‰€ä»¥æˆ‘å€‘å…ˆæŠŠ row åº§æ¨™ä¹˜ä¸Š 2M-1 å†ç›¸åŠ ï¼Œæ­¤æ™‚å°±å¯ä»¥ç²å¾—ä¸€å€‹ n*n çš„ index table ï¼Œå°æ‡‰ä¸€çµ„ç›¸å°ä½ç½®å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Swin Transformer æ˜¯ç”¨ç°¡åŒ–ç‰ˆçš„ä½œæ³•ä¾†å¼•å…¥ç›¸å°ä½ç½®ï¼Œå…¬å¼å¦‚ä¸‹&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V$
&lt;ul&gt;
&lt;li&gt;$B$ æ˜¯ relative position biasï¼Œ$B \in R^{M^2 * M^2}$&lt;/li&gt;
&lt;li&gt;$a_{ij}$ æ˜¯ç´”é‡ï¼Œä¸æ˜¯å‘é‡ï¼Œå’ŒåŸå§‹è«–æ–‡ä¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è«–æ–‡å‡ºè™•&#34;&gt;è«–æ–‡å‡ºè™•&lt;/h2&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1803.02155.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Attention with Relative Position Representations&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ä¾è³´æ–¼ attention æ©Ÿåˆ¶çš„ Transformer åœ¨æ©Ÿå™¨ç¿»è­¯æ–¹é¢å–å¾— SOTAï¼Œä½†åœ¨çµæ§‹ä¸­æ²’æœ‰ç›¸å°æˆ–çµ•å°çš„ä½ç½®è³‡è¨Šï¼Œä»–éœ€è¦åœ¨è¼¸å…¥ä¸­æ·»åŠ çµ•å°ä½ç½®çš„è³‡è¨Šã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤æœ¬æ–‡æå‡ºä¸€ç¨®æ›¿ä»£æ–¹æ¡ˆï¼Œæ‹“å±• self-attention ï¼Œè€ƒæ…®ç›¸å°ä½ç½®çš„è¡¨ç¤ºï¼Œä¸¦åœ¨ä¸€äº›ä»»å‹™ä¸­ç²å¾—æ›´å¥½çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;å€¼å¾—ä¸€é¡Œçš„äº‹ï¼Œä½œè€…è§€å¯Ÿåˆ°çµåˆç›¸å°å’Œçµ•å°ä½ç½®ä¸æœƒé€²ä¸€æ­¥æé«˜ç¿»è­¯å“è³ªã€‚&lt;/p&gt;
&lt;p&gt;è©²æ©Ÿåˆ¶å¯ä»¥æ‹“å±•åˆ°ä»»æ„ graph-labeled çš„è¼¸å…¥&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Non-recurrent models ä¸ä¸€å®šæŒ‰é †åºè€ƒæ…®è¼¸å…¥å…ƒç´ ï¼Œå› æ­¤å¯èƒ½éœ€è¦æ˜ç¢ºçš„ position encoding æ‰æ‰èƒ½ç”¨åºåˆ—é †åºã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨®å¸¸è¦‹çš„æ–¹æ³•æ˜¯ä½¿ç”¨èˆ‡è¼¸å…¥å…ƒç´ çµåˆçš„ position encodingï¼Œä»¥å‘æ¨¡å‹å‚³é”ä½ç½®è³‡è¨Šã€‚&lt;/p&gt;
&lt;p&gt;å¯ä»¥æ˜¯ deterministic functionï¼Œæˆ–æ˜¯ learned representationsã€‚&lt;/p&gt;
&lt;p&gt;CNN å¯ä»¥æ•æ‰ kernel çš„ç›¸å°ä½ç½®è³‡è¨Šï¼Œä½†è¢«è­‰æ˜ä»å—ç›Šæ–¼ position encodingã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼æ—¢ä¸ä½¿ç”¨å·ç©ä¹Ÿä¸ä½¿ç”¨éæ­¸çš„ Transformerï¼Œçµåˆä½ç½®ä¿¡æ¯çš„ representation æ˜¯ä¸€å€‹ç‰¹åˆ¥é‡è¦çš„è€ƒæ…®å› ç´ ï¼Œå› ç‚ºè©²æ¨¡å‹åœ¨å…¶ä»–æ–¹é¢å°åºåˆ—æ’åºå®Œå…¨ä¸è®Šã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€ç¨®å°‡ç›¸å°ä½ç½®åˆä½µåˆ° Transformer çš„ self-attention çš„åšæ³•ï¼Œå³ä½¿å®Œå…¨æ›æ‰çµ•å°ä½ç½®ç·¨ç¢¼ï¼Œä¹Ÿä½¿å…©å€‹æ©Ÿå™¨ç¿»è­¯ä»»å‹™çš„å“è³ªæœ‰é¡¯è‘—æé«˜ã€‚&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;åŸå§‹ self-attention&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$z_i=\displaystyle\sum_{j=1}^n\alpha_{ij}(x_jW^V)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\alpha_{ij}=\frac{\text{exp } e_{ij}}{\sum_{k=1}^n\text{exp } e_{ik}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$e_{ij}=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;proposed-architecture&#34;&gt;Proposed Architecture&lt;/h2&gt;
&lt;h3 id=&#34;relation-aware-self-attention&#34;&gt;Relation-aware Self-Attention&lt;/h3&gt;
&lt;p&gt;æœ‰å…©å€‹è¦å¼•å…¥ relative position çš„åœ°æ–¹ï¼Œè€Œä¸”éƒ½æ˜¯å‘é‡&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$z_i = \displaystyle\sum_{j=1}^n \alpha_{ij}(x_jW^V+a_{ij}^V)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$e_{ij}=\frac{x_iW^Q(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;relative-position-representations&#34;&gt;Relative Position Representations&lt;/h3&gt;
&lt;p&gt;å¯ä»¥å¼•å…¥ clipï¼ŒæŠŠç·šæ€§åºåˆ—ä¸­ï¼Œé«˜æ–¼é•·åº¦ k çš„ä¿®å‰ªæˆæœ€å¤§å€¼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_{ij}^K=w_{clip(j-i,k)}^K$&lt;/li&gt;
&lt;li&gt;$a_{ij}^V=w_{clip(j-i,k)}^V$&lt;/li&gt;
&lt;li&gt;$clip(x,k)=max(-k,min(k,x))$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;model-variations&#34;&gt;Model Variations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;clipping çš„å¯¦é©—
&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;V å’Œ K çš„ ablation study
&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Swin Transformer è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Fri, 14 Apr 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.14030&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€å€‹æ–°çš„ vision Transformerï¼Œç¨±ä½œ Swin Transformerï¼Œå¯ä»¥è¢«ç”¨ä½œ computer vision ä¸­çš„ general-purpose backboneã€‚&lt;/p&gt;
&lt;p&gt;æŠŠ Transformer å¾ language ç§»åˆ° vision å…·å‚™æŒ‘æˆ°æ€§ï¼Œæ¯”å¦‚åŒä¸€å€‹ visual entity åœ¨å¤§å°ä¸Šå…·å‚™å¾ˆå¤§çš„ varianceã€‚é‚„æœ‰ high resolution ä¸‹ pixel å’Œ word çš„æ•¸é‡å·®ç•°å¤ªå¤§ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è§£æ±ºé€™äº›å·®ç•°ï¼Œä½œè€…æå‡º hierachical Transformerï¼Œç”¨ shifted windows ä¾†ç®—å‡º representationã€‚&lt;/p&gt;
&lt;p&gt;shifted windowing é€éæŠŠ self-attention é™åˆ¶åœ¨ non-overlapping çš„ local window å’Œå…è¨± cross-windows connection ä¾†æé«˜æ•ˆç‡ã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨® hierarchical architecture å¯ä»¥éˆæ´»åœ°åœ¨å„ç¨® scale ä¸‹æ“´å±• modelï¼Œé‚„å¯ä»¥å°åœ–åƒå¤§å°æœ‰ç·šæ€§çš„è¨ˆç®—æ™‚é–“è¤‡é›œåº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ViT æŠŠåœ–ç‰‡æ‰“æˆ patchï¼Œæ¯å€‹ patch æ˜¯ 16*16ï¼Œfeature maps ç”± single low resolution çš„è¼¸å…¥ç”Ÿæˆï¼Œè€Œä¸”ç”±æ–¼è‡ªæ³¨æ„åŠ›å§‹çµ‚éƒ½æ˜¯åœ¨å…¨å±€ä¸Šè¨ˆç®—çš„ (patch å’Œ patch é–“åšè‡ªæ³¨æ„åŠ›)ï¼Œæ‰€ä»¥æ™‚é–“è¤‡é›œåº¦æ˜¯ quadratic computation complexityã€‚&lt;/p&gt;
&lt;p&gt;Swin Transformer å¾å° patch é–‹å§‹ï¼Œä¸¦åœ¨æ›´æ·±çš„ Transformer layers åˆä½µç›¸é„°çš„ patchesã€‚&lt;/p&gt;
&lt;p&gt;æœ‰äº†é€™äº› hierarchical feature mapsï¼Œå¯ä»¥ç”¨åœ¨åƒæ˜¯ FPN æˆ–æ˜¯ U-Netã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹ Swin Transformer çš„é—œéµè¨­è¨ˆå› ç´ æ˜¯ shifted windowã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;é€é bridge ä¸åŒ layer çš„ windows ä¾†æä¾›ä»–å€‘é€£æ¥ã€‚&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;overall-architecture&#34;&gt;Overall Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Patch Merging
&lt;ul&gt;
&lt;li&gt;åŸæœ¬ç‰¹å¾µåœ–æ˜¯ H * W * C&lt;/li&gt;
&lt;li&gt;ä»¥ä¸Šä¸‹ stride=2 è¡Œèµ°ï¼Œæœƒå¾—åˆ°å››å¼µ H/2 * W/2 * C&lt;/li&gt;
&lt;li&gt;concatenate èµ·ä¾†ï¼Œè®Šæˆ H/2 * W/2 * 4C&lt;/li&gt;
&lt;li&gt;åš linearï¼Œè®Šæˆ H/2 * W/2 * 2C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;swin-transformer-block&#34;&gt;Swin Transformer block&lt;/h4&gt;
&lt;p&gt;Swin Transformer æ˜¯é€éæŠŠ Transformer block ä¸­çš„ multi-head self attention(MSA) æ›æˆåŸºæ–¼ shifted windows çš„ module æ§‹æˆã€‚&lt;/p&gt;
&lt;h3 id=&#34;shifted-window-based-self-attention&#34;&gt;Shifted Window based Self-Attention&lt;/h3&gt;
&lt;p&gt;æ¨™æº–çš„ Transformer æ¶æ§‹æœƒç®— global self-attentionï¼Œè¨ˆç®—æ‰€æœ‰ token é–“å½¼æ­¤çš„é—œä¿‚ï¼Œå°è‡´ quadratic complexityï¼Œä½¿å…¶ä¸é©ç”¨æ–¼éœ€è¦å¤§é‡ token çš„è¨±å¤š CV å•é¡Œ&lt;/p&gt;
&lt;h4 id=&#34;self-attention-in-non-overlapped-windows&#34;&gt;Self-attention in non-overlapped windows&lt;/h4&gt;
&lt;p&gt;åŸä¾†çš„åœ–ç‰‡æœƒä»¥ non-overlapping çš„æ–¹å¼åˆ‡å‰²ã€‚&lt;/p&gt;
&lt;p&gt;å‡è¨­æ¯å€‹ windows æœ‰ M * M å€‹ patchesï¼Œç„¶å¾Œä¸€å¼µåœ–åƒæœ‰ h * w å¡Š patchesï¼Œè¨ˆç®—è¤‡é›œåº¦å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega(MSA)=4hwC^2+2(hw)^2C$&lt;/li&gt;
&lt;li&gt;$\Omega(W-MSA)=4hwC^2+2M^2hwC$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;shifted-window-partitioning-in-successive-blocks&#34;&gt;Shifted window partitioning in successive blocks&lt;/h4&gt;
&lt;p&gt;window-based self-attention module ç¼ºä¹äº† windows é–“å½¼æ­¤çš„é€£æ¥ï¼Œæœƒé™åˆ¶æ¨¡å‹èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºäº†ä¸€ç¨® shifted window çš„æ–¹æ³•ï¼Œä¿æŒ non-overlapping windows çš„é«˜æ•ˆè¨ˆç®—ï¼ŒåŒæ™‚å¼•å…¥ windows é–“çš„é€£æ¥ã€‚&lt;/p&gt;
&lt;p&gt;å†å…©å€‹é€£çºŒçš„ windows é–“ï¼Œæœƒç§»å‹• $(âŒŠ \frac{M}{2} âŒ‹, âŒŠ \frac{M}{2} âŒ‹)$&lt;/p&gt;
&lt;h4 id=&#34;efficient-batch-computation-for-shifted-configuration&#34;&gt;Efficient batch computation for shifted configuration&lt;/h4&gt;
&lt;p&gt;shifted window æœ‰å€‹å•é¡Œæ˜¯ï¼Œæœƒå°è‡´æ›´å¤šçš„ windowsï¼Œå¾ $âŒˆ \frac{h}{M} âŒ‰ * âŒˆ \frac{w}{M} âŒ‰$ åˆ° $(âŒˆ \frac{h}{M} âŒ‰+1) * (âŒˆ \frac{w}{M} âŒ‰+1)$ï¼Œè€Œä¸”æœ‰äº› window æœƒå°æ–¼ M*Mã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£æœƒå°è‡´ç„¡æ³•æŠŠé€™äº›çµ¦å£“æˆä¸€å€‹ batch å¿«é€Ÿè¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨® naive çš„è§£æ³•å°±æ˜¯ç›´æ¥åœ¨å¤–é¢åŠ  zero paddingï¼Œä½†æœƒå¢åŠ è¨ˆç®—é‡ï¼Œç•¶ windows æ•¸é‡è¼ƒå°‘æ™‚ï¼Œè¨ˆç®—é‡æœƒè®Šå¾ˆå¯è§€ (å¾ 2 * 2 å€‹ windows è®Šæˆ 3 * 3 å€‹ windowsï¼Œå¢åŠ äº† 2.25 å€)&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºå¦å¤–ä¸€ç¨®å·§å¦™çš„åšæ³•ï¼ŒæŠŠä¸€äº›éƒ¨åˆ†æŒªç§»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½†ç¾åœ¨æœ‰äº› window è£¡æœ‰å¤šå€‹ä¸è©²ç›¸äº’åš attention çš„éƒ¨åˆ†ï¼Œæ‰€ä»¥è¦ç”¨ mask çš„æ–¹å¼è¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä¸åŒ windowsï¼Œåš self-attention å¾Œï¼ŒæŠŠä¸ç›¸å¹²çš„éƒ¨åˆ†åšçš„ attention æ¸›å»ä¸€å€‹å¾ˆå¤§çš„æ•¸å€¼ï¼Œæœ€å¾Œå†é softmaxã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/mask.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–ä¾†è‡ªä½œè€…åœ¨ github æä¾›çš„å¯è¦–åŒ–&lt;/p&gt;
&lt;p&gt;æœ€å¾Œå†æŠŠå®ƒæŒªå›åŸæœ¬çš„ä½ç½®ã€‚&lt;/p&gt;
&lt;h4 id=&#34;relative-position-bias&#34;&gt;Relative position bias&lt;/h4&gt;
&lt;p&gt;åƒè€ƒé€™å€‹: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_37541097/article/details/121119988&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/qq_37541097/article/details/121119988&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;architecture-variants&#34;&gt;Architecture Variants&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;window size é è¨­æ˜¯ M = 7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;query dimension of each head æ˜¯ d = 32&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;expansion layer of each MLP is $\alpha$ = 4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C æ˜¯ first stage çš„ hidden layers çš„ channel numbers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-T&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 6, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 0.25 å€&lt;/li&gt;
&lt;li&gt;complexity æ¥è¿‘ ResNet-50&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-S&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 0.5 å€&lt;/li&gt;
&lt;li&gt;complexity æ¥è¿‘ ResNet-101&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-B&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 128&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-L&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 192&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 2 å€&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;image-classification-on-imagenet-1k&#34;&gt;Image Classification on ImageNet-1K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;object-detection-on-coco&#34;&gt;Object Detection on COCO&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;semantic-segmentation-on-ade20k&#34;&gt;Semantic Segmentation on ADE20K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;åŸºæ–¼ self-attention çš„ shifted window æ˜¯ Swin Transformer é—œéµéƒ¨åˆ†ï¼Œè¢«é¡¯ç¤ºå‡ºä»–åœ¨ CV é ˜åŸŸæœ‰æ•ˆç‡ä¸”æœ‰æ•ˆï¼Œä¸¦æœŸæœ›æœªä¾†æŠŠå®ƒæ‡‰ç”¨åœ¨ NLPã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GIT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 29 Mar 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.14100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GIT: A Generative Image-to-text Transformer for Vision and Language&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•   
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;è¨­è¨ˆäº†ä¸€å€‹ Generative Image-to-text Transformerï¼Œçµ±ä¸€ vision-language tasksï¼Œåƒæ˜¯ image/video captioning æˆ–æ˜¯å•ç­”ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ generative models åœ¨é è¨“ç·´å’Œå¾®èª¿çš„æ™‚å€™æ˜¯åŒæ¨£çš„ç¶²è·¯æ¶æ§‹ï¼Œç¾æœ‰çš„å·¥ä½œé€šå¸¸éƒ½åŒ…å«è¤‡é›œçš„æ¶æ§‹ (uni/multi-modal encoder/decoder)ï¼Œ
è€Œä¸”ä¾è³´æ–¼å¤–éƒ¨æ¨¡çµ„ï¼Œæ¯”å¦‚ç‰©ä»¶åµæ¸¬æˆ– optical character recognition (OCR)ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ GITï¼Œæˆ‘å€‘ç°¡åŒ–ç‚º single language modeling task ä¸‹çš„ä¸€å€‹ image encoder å’Œä¸€å€‹ text decoderã€‚&lt;/p&gt;
&lt;p&gt;æ“´å¤§äº†é è¨“ç·´è³‡æ–™å’Œæ¨¡å‹å¤§å°ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨±å¤šå…·æœ‰æŒ‘æˆ°æ€§çš„ benchmarks ä¸Šå–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚é¦–æ¬¡åœ¨ TextCpas ä¸Šè¶…è¶Šäººé¡çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;æå‡ºäº†ä¸€ç¨® generation-based image classification and scene text recognition çš„æ–°æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;è¿‘å¹´ä¾†åœ¨ vision-languageï¼ˆVLï¼‰é è¨“ç·´æ–¹é¢å–å¾—äº†å·¨å¤§é€²å±•ï¼Œç‰¹åˆ¥æ˜¯åŸºæ–¼ image-text pairs çš„å¤§è¦æ¨¡æ•¸æ“šï¼Œä¾‹å¦‚ CLIPã€Florence å’Œ SimVLMã€‚&lt;/p&gt;
&lt;p&gt;å­¸ç¿’åˆ°çš„ representation å¾ˆå¥½çš„æé«˜äº†ä¸‹æ¸¸ä»»å‹™çš„æ€§èƒ½ï¼Œæ¯”å¦‚ image captioningã€visual question answering å’Œ image-text retrievalã€‚&lt;/p&gt;
&lt;p&gt;åœ¨é è¨“ç·´éç¨‹ä¸­ï¼ŒMasked Language Modeling (MLM) å’Œ Image-Text Matching (ITM) è¢«å»£æ³›ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™äº› loss å’Œä¸‹æ¸¸ä»»å‹™ä¸åŒï¼Œå¿…é ˆåš task-specific adaptationã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼Œ image captioning è¦ç§»é™¤ ITMï¼ŒVQA éœ€è¦é¡å¤–éš¨æ©Ÿåˆå§‹çš„ MLPã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ¸›å°‘é€™ç¨®å·®ç•°ï¼Œæœ€è¿‘çš„ç ”ç©¶è©¦åœ–ç‚ºé è¨“ç·´æ¨¡å‹è¨­è¨ˆ unified generative models ä¾†é è¨“ç·´ï¼Œå› ç‚ºå¤§å¤šæ•¸ VL çš„å•é¡Œå¯ä»¥è½‰åŒ–ç‚ºç”Ÿæˆå•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›æ–¹æ³•é€šå¸¸åˆ©ç”¨ multi-modal encoder å’Œ text decoderï¼Œä¸¦ç²¾å¿ƒè¨­è¨ˆ text input å’Œ text targetã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æ¨å‹•é€™æ–¹å‘çš„ç ”ç©¶ï¼Œä½œè€…è¨­è¨ˆäº†ä¸€å€‹ç°¡å–®çš„ Generative Image-to-text Transformerï¼Œç¨±ä½œ GITï¼ŒåªåŒ…å«ä¸€å€‹ image encoder å’Œ text decoderã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´ä»»å‹™åªæ˜¯æŠŠè¼¸å…¥çš„åœ–åƒæ˜ å°„åˆ°ç›¸é—œè¯çš„æ–‡å­—æè¿°ã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ä»–å¾ˆç°¡å–®ï¼Œä½†é‚„æ˜¯åœ¨çœ¾å¤šå…·æœ‰æŒ‘æˆ°æ€§çš„ benchmark å–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;image encoder æ˜¯ Swin-like vision transformerï¼Œåœ¨å¤§é‡çš„ image-text pairs ä¸Šåš pretrainï¼ŒåŸºæ–¼ contrastive taskã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¶ˆé™¤äº†ç¾æœ‰è¨±å¤šæ–¹æ³•ä¸­å° object detector çš„ä¾è³´ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†å°‡å…¶æ“´å±•åˆ°å½±ç‰‡é ˜åŸŸï¼Œæˆ‘å€‘æŠŠå¤šå€‹ frame çš„ç‰¹å¾µ concatenateï¼Œä½œç‚º video è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;text decoder æ˜¯ä¸€å€‹ç”¨ä¾†é æ¸¬ç›¸é—œè¯æ–‡å­—çš„ transformerã€‚&lt;/p&gt;
&lt;p&gt;æ•´å€‹ç¶²è·¯éƒ½æ˜¯åŸºæ–¼ language modeling task ä¾†è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ VQAï¼Œinput question è¢«çœ‹ä½œ text prefixï¼Œä¸¦ä»¥ auto-regressive çš„æ–¹æ³•ç”Ÿå‡ºç­”æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç¨® generation-based çš„ ImageNet classification æ–°æ–¹æ¡ˆï¼Œé æ¸¬æ¨™ç±¤ç›´æ¥æ ¹æ“šä½œè€…çš„ç”Ÿæˆæ¨¡å‹ï¼Œè€Œä¸ç”¨é å…ˆå®šç¾©è©å½™è¡¨ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘çš„ä½œæ³•å¾ˆç°¡å–®ï¼Œä½†åœ¨æ“´å¤§é è¨“ç·´è³‡æ–™å’Œæ¨¡å‹å¤§å°å¾Œï¼Œæˆæœé©šäººã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦è²¢ç»å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘å±•ç¤ºäº† GITï¼Œåƒ…ç”±ä¸€å€‹ image encoder å’Œä¸€å€‹ text decoder çµ„æˆï¼Œé€é language modeling taskï¼Œåœ¨ 0.8 billion image-text pairs ä¸Š pretrainã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨ image/video captioning å’Œ QA ä¸Šï¼Œæ²’æœ‰åŸºæ–¼ object detectorsï¼Œobject tags å’Œ OCRï¼Œå°±åœ¨å¤šå€‹ä»»å‹™ä¸Šå–å¾— SOTAã€‚è­‰æ˜ç°¡å–®çš„ç¶²è·¯æ¶æ§‹ä¹Ÿå¯ä»¥é€é scaling å–å¾—å¼·å¤§çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘è­‰æ˜ GIT é›–ç„¶ pretrain åœ¨ image-text pairsï¼Œä¹Ÿèƒ½åœ¨ video tasks ä¸Šå–å¾— SOTAï¼Œä¸éœ€è¦ video dedicated encodersã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘æå‡ºäº†ä¸€ç¨®æ–°çš„ generation-based image classification æ–¹æ¡ˆï¼Œåœ¨ ImageNet-1K ä¸Šï¼Œå–å¾—ä¸éŒ¯çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/table1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ VL pre-training ä¸­ï¼Œå¤š multi-task pre-training è¢«å»£æ³›ä½¿ç”¨ï¼Œè³¦äºˆç¶²è·¯å¤šç¨®æˆ–å¢å¼·çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼ŒMLM å’Œ ITM æ˜¯å»£æ³›æ¡ç”¨çš„é è¨“ç·´ä»»å‹™ï¼Œæœ€è¿‘ä¹Ÿæœ‰ç ”ç©¶åŠ å…¥ image-text contrastive lossã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼å¤šæ•¸ VL ä»»å‹™éƒ½å¯ä»¥è¡¨ç¤ºæˆ text generation taskï¼Œæ‰€ä»¥å¯ä»¥è¨“ç·´ä¸€å€‹ç”Ÿæˆæ¨¡å‹ä¾†æ”¯æŒå„ç¨®ä¸‹æ¸¸ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;è¼¸å…¥å’Œè¼¸å‡ºæ–‡æœ¬é€šå¸¸éƒ½æœƒç¶“éç²¾å¿ƒè¨­è¨ˆï¼Œä»¥é è¨“ç·´é€™æ¨£çš„ç”Ÿæˆæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ image representationï¼ŒFaster RCNN è¢«å¤§å¤šæ•¸ç¾æœ‰æ–¹æ³•ç”¨ä¾†æå–å€åŸŸç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;åŒæ™‚ï¼Œä¹Ÿå¾ˆå®¹æ˜“ä»¥ end-to-end çš„æ–¹æ³•è¨“ç·´æ•´å€‹ç¶²è·¯ã€‚&lt;/p&gt;
&lt;p&gt;é™¤äº† feature mapï¼Œobject tagsï¼Œä¹Ÿå¾ˆå¸¸è¢«ç”¨ä¾†æ–¹ä¾¿ transformer ç†è§£ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ¥æ˜¯ novel objectsã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼èˆ‡å ´æ™¯æ–‡æœ¬ç›¸é—œçš„ä»»å‹™ï¼Œèª¿ç”¨ OCR ä»¥ç”Ÿæˆå ´æ™¯æ–‡æœ¬ä½œç‚ºé™„åŠ ç¶²è·¯è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ text predictionï¼Œå¸¸ç”¨ transformer networkï¼Œçµåˆ cross-attention module ä¾†èåˆ image tokensã€‚&lt;/p&gt;
&lt;p&gt;æˆ–è€…åªæ˜¯å–®ç´” concatenate text tokens å’Œ image tokensï¼Œç„¶å¾Œç”¨ self-attentionã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘æœ‰ 9 å€‹ä¸åŒçš„ benchmarkï¼Œ3 ç¨®ä¸åŒæ¨¡å‹å¤§å°å’Œ 3 ç¨®ä¸åŒé è¨“ç·´è³‡æ–™è¦æ¨¡ã€‚&lt;/p&gt;
&lt;h2 id=&#34;generative-image-to-text-transformer&#34;&gt;Generative Image-to-text Transformer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;network-architecture&#34;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;image encoder åŸºæ–¼ contrastive pre-trained modelã€‚&lt;/p&gt;
&lt;p&gt;è¼¸å…¥æ˜¯åŸå§‹åœ–åƒï¼Œè¼¸å‡ºæ˜¯ compact 2D feature mapï¼Œè¢« flatten æˆ list of featuresã€‚&lt;/p&gt;
&lt;p&gt;é€éä¸€å€‹é¡å¤–çš„ linear layer å’Œä¸€å€‹ layernorm layerï¼Œimage features è¢« project åˆ° D dimensionsï¼Œä¹Ÿå°±æ˜¯ text encoder çš„ inputã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä½¿ç”¨åš contrastive tasks pretraining çš„ image encoderï¼Œå› ç‚ºæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜é€™ç¨® image encoder æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å¾Œé¢çš„ç« ç¯€ï¼Œé‚„è§€å¯Ÿåˆ° VL performence æ˜é¡¯åœ°éš¨è‘—æ›´å¼·çš„ image encoder è€Œæœ‰æ‰€æå‡ã€‚
é€™å’Œ object detection-based çš„æ–¹æ³•è§€å¯Ÿåˆ°çš„çµæœä¸€è‡´ã€‚&lt;/p&gt;
&lt;p&gt;CoCa çš„ concurrent work çµ±ä¸€äº† contrastive task å’Œ the generation taskï¼Œä½œç‚ºä¸€å€‹é è¨“ç·´éšæ®µã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„æ–¹æ³•ç›¸ç•¶æ–¼æ˜¯æŒ‰é †åºåˆ†é›¢å…©å€‹ä»»å‹™:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨ contrastive task è¨“ç·´ image encoder&lt;/li&gt;
&lt;li&gt;ç”¨ generation task pretrain image encoder å’Œ text decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;text decoder æ˜¯ä¸€å€‹ç”¨æ–¼é æ¸¬æ–‡æœ¬æè¿°çš„ transformer moduleï¼Œç”±å¤šå€‹ transformer block çµ„æˆï¼Œæ¯å€‹ transformer block ç”±ä¸€å€‹ self-attention layer å’Œ feed-forward layer çµ„æˆã€‚&lt;/p&gt;
&lt;p&gt;text è¢« tokenize å’Œ embed åˆ° D dimensionsï¼Œä¸¦æ·»åŠ  positional encoding å’Œ layernorm layerã€‚&lt;/p&gt;
&lt;p&gt;image features å’Œ text embeddings è¢« concatenate èµ·ä¾†ä½œç‚º transformer module çš„è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;text ä»¥ [BOS] é–‹å§‹ï¼Œä¸¦ä»¥ auto regressive çš„æ–¹å¼ decodeï¼Œç›´åˆ° [EOS] æˆ– maximum stepsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;attention mask æ ¹æ“šä¸Šåœ–è¨­è¨ˆï¼Œä½¿çš„ text token åªèƒ½ä¾è³´æ–¼å‰é¢çš„ text token å’Œ image tokenï¼Œè€Œ image token å¯ä»¥äº’ç›¸åš attentionã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œ unidirectional attention mask ä¸åŒï¼Œunidirectional attention mask ä¸¦éæ¯å€‹ image token éƒ½å¯ä»¥ä¾è³´æ–¼å…¶ä»–çš„ Image tokenã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…å¾ˆå¥½åœ°åˆå§‹åŒ– image encoderï¼Œå»éš¨æ©Ÿåˆå§‹åŒ– text decoderã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨®è¨­è¨ˆå‹•æ©Ÿæ˜¯åŸºæ–¼[MiniVLM: A Smaller and Faster Vision-Language Model]ï¼Œè©²ç ”ç©¶éš¨æ©Ÿåˆå§‹åŒ–é¡¯ç¤ºå‡ºèˆ‡ BERT åˆå§‹åŒ–ç›¸ä¼¼åœ°æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åŸå› å¯èƒ½åœ¨æ–¼ BERT åœ°åˆå§‹åŒ–ç„¡æ³•ç†è§£åœ–åƒä¿¡è™Ÿï¼Œé€™å°æ–¼ VL ä»»å‹™è‡³é—œé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;[Flamingo: a Visual Language Model for Few-Shot Learning] æ¡ç”¨äº†é¡ä¼¼çš„ image encoder + text decoderï¼Œä½†æ˜¯ä»–å€‘çš„ decoder ç¶“é pretrainï¼Œä¸¦ä¸”æœ‰ freezeï¼Œå¥½ä¿ç•™å¤§å‹èªè¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;GIT çš„æ‰€æœ‰åƒæ•¸éƒ½æœƒæ›´æ–°ï¼Œä»¥æ›´å¥½åœ°é©æ‡‰ VL çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€ç¨®æ¶æ§‹æ˜¯ cross-attention-based çš„ decoderï¼Œç”¨æ–¼ incorporate image signalsï¼Œè€Œä¸æ˜¯ concatenation å†ç”¨ self-attentionã€‚&lt;/p&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œlarge-scale çš„ pre-trainingï¼Œself-attention-based æœƒæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå°è¦æ¨¡çš„å‰‡æ˜¯ cross-attention-basedã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹åˆç†çš„è§£é‡‹æ˜¯ï¼Œç¶“éå……åˆ†è¨“ç·´ï¼Œdecoder å¯ä»¥å¾ˆå¥½åœ°è™•ç†åœ–åƒå’Œæ–‡æœ¬ï¼Œè€Œä¸” image token å¯ä»¥ç‚ºäº† text generation æ›´å¥½åœ°æ›´æ–°ã€‚&lt;/p&gt;
&lt;p&gt;è€Œ cross-attention è®“ image token æ²’è¾¦æ³• attend å½¼æ­¤ã€‚&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h3&gt;
&lt;p&gt;è¨“ç·´æ¡ç”¨ language modeling (LM) lossã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/for1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$I$ æ˜¯ image&lt;/li&gt;
&lt;li&gt;$y_i,i \in $ { $ 1,&amp;hellip;,N $ } æ˜¯æ–‡å­— tokenï¼Œ$y_0$ æ˜¯ [BOS]ï¼Œ$y_{N+1}$ æ˜¯ [EOS]&lt;/li&gt;
&lt;li&gt;CE æ˜¯æœ‰ 0.1 label smoothing çš„ cross-entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¦ä¸€ç¨®é¸æ“‡æ˜¯ MLMï¼Œåœ¨æ¯å€‹ epoch ä¸­é æ¸¬ 15% çš„è¼¸å…¥ tokenï¼Œè¦é æ¸¬æ‰€æœ‰ token è‡³å°‘éœ€è¦ 1 / 0.15 = 6.7 å€‹ epochsï¼Œå°æ–¼ LMï¼Œæ¯å€‹ epoch éƒ½å¯ä»¥é æ¸¬æ‰€æœ‰ tokenï¼Œå°æ–¼å¤§è¦æ¨¡é è¨“ç·´è³‡æ–™ä¾†èªªæ•ˆç‡æ›´é«˜ã€‚&lt;/p&gt;
&lt;p&gt;ablation studies é¡¯ç¤ºå‡º LM å¯ä»¥åœ¨æœ‰é™çš„ epoch å…§å¯¦ç¾æ›´å¥½çš„æ€§èƒ½ã€‚
åœ¨å¤§è¦æ¨¡è¨“ç·´ä¸­ï¼Œç”±æ–¼è¨ˆç®—è³‡è¨Šçš„é™åˆ¶ï¼Œåªæœ‰å…©å€‹ epochï¼Œæ‰€ä»¥é¸æ“‡ LMã€‚
èˆ‡æ­¤åŒæ™‚ï¼Œå¤§éƒ¨åˆ†æœ€è¿‘çš„ large-scale language model ä¹Ÿæ˜¯åŸºæ–¼ LMã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœæ²’æœ‰åœ–åƒè¼¸å…¥ï¼Œè©²æ¨¡å‹å°‡ç°¡åŒ–ç‚º decoder-only çš„èªè¨€æ¨¡å‹ï¼Œæ¶æ§‹é¡ä¼¼æ–¼ GPT-3ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œé€™ç¨®è¨­è¨ˆé‚„å¯ä»¥åˆ©ç”¨ text-only çš„è³‡æ–™ä¾†æå‡ scaled-up decoder çš„èƒ½åŠ›ï¼ŒæŠŠé€™ä¿ç•™çµ¦æœªä¾†çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;å°æ–¼ image captioningï¼Œç”±æ–¼è¨“ç·´æ•¸æ“šæ ¼å¼å’Œé è¨“ç·´ç›¸åŒï¼Œæ‰€ä»¥ç”¨åŒæ¨£çš„ LM task ä¾†å¾®èª¿ GITã€‚
å°æ–¼ visual question answeringï¼Œå•é¡Œå’Œ GT åœ¨å¾®èª¿çš„æ™‚å€™è¢«çœ‹åš special captionï¼Œä½† LM loss åƒ…ç”¨æ–¼ç­”æ¡ˆå’Œ [EOS]ã€‚&lt;/p&gt;
&lt;p&gt;æ¨ç†éç¨‹ä¸­ï¼Œquestion è¢«ç•¶ä½œ caption çš„ prefixï¼Œå®Œæˆçš„éƒ¨åˆ†æ˜¯é æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;VQAv2 ç¾æœ‰çš„å·¥ä½œæ”¶é›†å€™é¸ç­”æ¡ˆï¼Œå†é‡æ§‹æˆåˆ†é¡å•é¡Œï¼Œé æ¸¬ä¸€æ¬¡ã€‚
ä½œè€…çš„å·¥ä½œæœ‰æ›´å¤šæŒ‘æˆ°ï¼Œå› ç‚ºæ˜¯ç”Ÿæˆå¼çš„ï¼Œéœ€è¦ç”Ÿå‡ºè‡³å°‘å…©å€‹æ­£ç¢ºçš„ tokenï¼Œç­”æ¡ˆå’Œ [EOS]ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œè€ƒæ…®åˆ°è‡ªç”±å½¢å¼ç­”æ¡ˆçš„å¥½è™•ï¼Œä½œè€…é¸æ“‡äº†ç”Ÿæˆæ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ç”Ÿæˆæ¨¡å‹çš„é›£åº¦ï¼ŒVQAv2 æ¯”ç¾æœ‰çš„åˆ¤åˆ¥å·¥ä½œç•¥å·®ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼å’Œ scene-text related VQA ä»»å‹™ï¼Œç¾æœ‰æ–¹æ³•é€šå¸¸åˆ©ç”¨ OCR ç”Ÿæˆ 5 å€‹ scene text ä¸¦ç”¨ dynamic pointer network æ±ºå®šç•¶å‰è¼¸å‡ºæ‡‰è©²æ˜¯ OCR é‚„æ˜¯ general textã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç”±æ–¼ä½œè€…çš„æ–¹æ³•ä¸ä¾è³´æ–¼ OCRï¼Œå› æ­¤ä¹Ÿä¸ä¾è³´æ–¼ dynamic pointer networkã€‚&lt;/p&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œä½œè€…ç™¼ç¾æ¨¡å‹é€éå¤§è¦æ¨¡é è¨“ç·´è³‡æ–™å­¸æœƒå¦‚ä½•é–±è®€å ´æ™¯æ–‡æœ¬ï¼Œä¸¦ä¸”ä½œè€…çš„æ¨¡å‹ä¸æ˜¯å°ˆé–€ç‚ºäº†å½±ç‰‡é ˜åŸŸè¨­è¨ˆçš„ï¼Œä½†å¯ä»¥é€éç°¡å–®çš„æ¶æ§‹æ›´æ”¹å°±å–å¾—å…·æœ‰ç«¶çˆ­åŠ›æˆ–ç”šè‡³ SOTA çš„æˆæœï¼Œä¹Ÿå°±æ˜¯ä½œè€…å¯ä»¥å¾æ¯å€‹å½±ç‰‡æ¡æ¨£å¤šå€‹ frameï¼Œä¸¦é€é image encoder ç¨ç«‹åœ°ç‚ºæ¯å€‹ frame ç·¨ç¢¼ã€‚
æœ€å¾Œæ·»åŠ ä¸€å€‹ learnable temporal embedding (åˆå§‹åŒ–ç‚º 0)ï¼Œä¸¦ concatenate sampled frames çš„ç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„ç”¨æ–¼åœ–ç‰‡åˆ†é¡ï¼ŒæŠŠ class name ç”¨æ–¼ captionã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œç¾æœ‰å·¥ä½œä¸ä¸€æ¨£ï¼Œç¾æœ‰å·¥ä½œé€šå¸¸å…ˆå®šç¾©è©å½™è¡¨ï¼Œä¸¦ç”¨ç·šæ€§å±¤é æ¸¬æ¯å€‹é¡åˆ¥çš„å¯èƒ½æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç•¶æ–°æ•¸æ“šå’Œæ–°é¡åˆ¥è¢«æ·»åŠ åˆ°ç¾æœ‰æ•¸æ“šçš„æ™‚å€™ï¼Œé€™ç¨®æ–°ä¸€ä»£çš„æ–¹æ¡ˆæ˜¯æœ‰ç›Šçš„ï¼Œå› ç‚ºé€™æ¨£å¯ä»¥åœ¨ä¸å¼•å…¥æ–°åƒæ•¸çš„æƒ…æ³ä¸‹å°æ–°æ•¸æ“šé€²è¡Œè¨“ç·´ã€‚&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æ”¶é›† 0.8B çš„ image-text pairs ä¾†é è¨“ç·´ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image encoder æ˜¯æ ¹æ“š  pre-trained contrastive model åˆå§‹åŒ–çš„ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hidden dimension (D) = 768&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;text decoder æœ‰ 6 å€‹ randomly-initialized transformer blocks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å…±æœ‰ 0.7b çš„åƒæ•¸&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image decoder å’Œ text encoder çš„ learning rate å„åˆ¥æ˜¯ 1e-5 å’Œ 5e-5ï¼Œéƒ½ cosine decay åˆ° 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ¨è«–éšæ®µ beam size æ˜¯ 4ï¼Œlength penalty æ˜¯ 0.6ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supplementary materials å±•ç¤ºäº†å°æ¨¡å‹è®Šé«” (GITB and GITL) å’Œæ›´å¤§æ¨¡å‹ (GIT2) çš„çµæœ&lt;/p&gt;
&lt;h3 id=&#34;results-on-image-classification&#34;&gt;Results on Image Classification&lt;/h3&gt;
&lt;p&gt;è¼¸å‡ºå¿…é ˆèˆ‡é¡åˆ¥åç¨±å®Œå…¨åŒ¹é…ï¼Œç”šè‡³è€ƒæ…®å¤šæˆ–å°‘çš„ç©ºæ ¼ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ä¸çŸ¥é“è©å½™è¡¨ï¼Œç²¾ç¢ºåŒ¹è¢«æº–ç¢ºåº¦åªæœ‰ 1.93%ï¼Œå¦‚æœé æ¸¬åŒ…å« GT å°±å°ï¼Œé‚£æœ‰ 40.88%ã€‚&lt;/p&gt;
&lt;p&gt;é€šéå¾®èª¿æ¯å€‹é¡åˆ¥åªæœ‰ 1 shot æˆ– 5 shotï¼Œæº–ç¢ºåº¦æœƒé¡¯è‘—æé«˜ï¼Œ
è¡¨æ˜åªç”¨å°‘é‡è¨“ç·´æ¨£æœ¬ï¼Œä¹Ÿå¯ä»¥è¼•é¬†é©æ‡‰ä¸‹æ¸¸ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡ Flamingo ç›¸æ¯”ï¼ŒGIT å¯¦ç¾æ›´é«˜çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;p&gt;Flamingo åœ¨æ²’æœ‰åƒæ•¸æ›´æ–°çš„æƒ…æ³ä¸‹é€²è¡Œå°æ¨£æœ¬å­¸ç¿’ï¼Œä½†éœ€è¦é¡å¤–çš„ç¶²è·¯è¼¸å…¥ï¼Œå¯èƒ½æœƒå¢åŠ æ¨ç†æˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGIT é€éä¸€æ¬¡ lightweight fine-tuningï¼Œæ¨ç†éç¨‹ä¸­ä¸éœ€è¦é€™äº› training shotã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;h4 id=&#34;model-and-data-scaling&#34;&gt;Model and data scaling&lt;/h4&gt;
&lt;p&gt;å°æ–¼ç¶²è·¯æ¶æ§‹ï¼Œä½œè€…çš„æ¨¡å‹è¢«ç¨±ä½œ Hugeï¼ŒæŠŠ image encoder æ›æˆ CLIP çš„ ViT-B/16 å’Œ ViT-L/14 çš„å‰‡æ˜¯ Base å’Œ Largeã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å¯ä»¥çœ‹å‡ºè¼ƒå¤§çš„ image encoder å¸¶ä¾†çš„å¥½è™•ï¼Œä½†æ ¹æ“šå¯¦é©—ï¼Œ
ä½œè€…ç™¼ç¾å¾ˆé›£æœ‰æ•ˆåœ°æ“´å±• text decoderï¼ŒåŸå› å¯èƒ½æ˜¯ LM å¾ˆé›£ç”¨ limited amount of text ä¾†è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹å¯èƒ½çš„åŸå› æ˜¯ image encoder è² è²¬ object recognitionï¼Œè€Œ decoder è² è²¬ä»¥ NLP çš„æ–¹æ³•çµ„ç¹” object termsã€‚
å¾Œä¸€é …ä»»å‹™å¯èƒ½å¾ˆå®¹æ˜“ï¼Œå› ç‚ºå¤§å¤šæ•¸æè¿°éƒ½éµå¾ªç›¸ä¼¼çš„æ¨¡å¼ï¼Œæ¯”å¦‚ Object + verb + subjectï¼Œæ‰€ä»¥åªè¦ä¸€å€‹ small decoderï¼Œè¼ƒå¤§çš„ decoder å¯èƒ½æœƒå¢åŠ å­¸ç¿’é›£åº¦ã€‚&lt;/p&gt;
&lt;p&gt;Flamingo çš„ç ”ç©¶é¡¯ç¤ºæ›´å¤§çš„ Decoder å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†æ˜¯ä»–å€‘çš„ decoder æœ‰ pretrain éï¼Œè€Œä¸”åœ¨ VL é è¨“ç·´çš„æ™‚å€™ frozenï¼Œé¿é–‹äº†å¦‚ä½•æœ‰æ•ˆè¨“ç·´ decoder çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;LEMON çš„ transformer å¯ä»¥æ“´å±•åˆ° 32 å±¤ï¼Œå¯èƒ½æ˜¯å› ç‚ºä»–å€‘ä½¿ç”¨ MLM è€Œä¸æ˜¯ LMï¼Œå¾Œè€…å¯èƒ½æ›´åŠ å›°é›£ã€‚&lt;/p&gt;
&lt;h4 id=&#34;scene-text-in-pre-training-data&#34;&gt;Scene text in pre-training data&lt;/h4&gt;
&lt;p&gt;ç‚ºäº†ç­è§£ scene text comprehension çš„èƒ½åŠ›ï¼Œä½œè€…æª¢æŸ¥äº† pretrain data æœ‰å¤šå°‘ image-text pairs æœ‰ scene textã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ Microsoft Azure OCR API4 å°ä¸€äº›è³‡æ–™åš OCRï¼Œç„¶å¾ŒæŠŠ OCR çµæœå’Œ associated text åšæ¯”å°ï¼Œåªæœ‰åŒ…å«é•·åº¦è¶…é 5 å€‹å­—å…ƒçš„ OCR çµæœæ‰æœƒç®—æ¯”å°ã€‚
æœ‰ 15% çš„ CC12M å’Œ 31% çš„ä¸‹è¼‰åœ–åƒ(500K) åŒ…å« scene text æè¿°ã€‚
ç”±æ–¼ä»»å‹™æ˜¯è¨“ç·´é æ¸¬ textï¼Œç¶²è·¯é€æ¼¸å­¸æœƒé–±è®€ scene textã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;h3 id=&#34;limitations&#34;&gt;Limitations&lt;/h3&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œç›®å‰ä¸æ¸…æ¥šå¦‚ä½•æ§åˆ¶ç”Ÿæˆçš„ caption ä»¥åŠå¦‚ä½•åœ¨ä¸æ›´æ–°åƒæ•¸çš„æƒ…æ³ä¸‹åŸ·è¡Œ in-context learningï¼ŒæŠŠé€™ç•™çµ¦æœªä¾†çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h3 id=&#34;societal-impact&#34;&gt;Societal impact&lt;/h3&gt;
&lt;p&gt;è©²æ¨¡å‹åœ¨å¤§è¦æ¨¡æ•¸æ“šé›†ä¸Šé è¨“ç·´ï¼Œä¸èƒ½ä¿è­‰æ•¸æ“šä¸å« toxic languageï¼Œå¯èƒ½æœƒ poison outputã€‚&lt;/p&gt;
&lt;h2 id=&#34;å…¶ä»–&#34;&gt;å…¶ä»–&lt;/h2&gt;
&lt;h3 id=&#34;a3-network&#34;&gt;A.3 Network&lt;/h3&gt;
&lt;p&gt;è¬›è¶…åƒæ•¸&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/model.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>RoBERTa è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/roberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 22 Mar 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/roberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1907.11692&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â•
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç™¼ç¾ BERT è¨“ç·´ä¸è¶³ï¼Œä¸¦ä¸”ä½œè€…çš„æ¨¡å‹åœ¨ 4/9 çš„ GLUE ä»»å‹™, RACE å’Œ SQuAD å–å¾— SOTAã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;è‡ªç›£ç£çš„è¨“ç·´æ–¹æ³•å¸¶ä¾†äº†é¡¯è‘—çš„æ€§èƒ½æå‡ï¼Œä½†è¦ç¢ºå®šé€™ä¸€å †æ–¹æ³•ä¸­çš„å“ªäº›æ–¹é¢è²¢ç»æœ€å¤§ï¼Œå…·å‚™æŒ‘æˆ°æ€§ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´çš„è¨ˆç®—é‡æ˜¯æ˜‚è²´çš„ï¼Œä½¿ fine-tune å—é™ï¼Œè€Œä¸”é€šå¸¸éƒ½æ˜¯ç”¨ä¸åŒå¤§å°çš„ private training dataï¼Œä½¿è©•ä¼°æ¨¡å‹æ›´åŠ å›°é›£ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºäº†å° BERT é è¨“ç·´çš„ replication studyï¼ŒåŒ…æ‹¬å°è¶…åƒæ•¸çš„èª¿æ•´ï¼Œä»¥åŠå°è¨“ç·´é›†å¤§å°çš„ä»”ç´°è©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ BERT è¨“ç·´ä¸è¶³ï¼Œä¸¦æå‡ºäº†ä¸€ç¨®æ”¹é€²æ–¹æ³•ï¼Œç¨±ç‚º RoBERTaï¼Œå¯ä»¥é”åˆ°æˆ–è¶…éæ‰€æœ‰ post-BERT çš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä¿®æ”¹å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;è¨“ç·´æ¨¡å‹çš„æ™‚é–“æ›´é•·ï¼Œbatch æ›´å¤§ï¼Œç”¨æ›´å¤š data&lt;/li&gt;
&lt;li&gt;ç§»é™¤ next sentence prediction objective&lt;/li&gt;
&lt;li&gt;è¨“ç·´æ›´é•·çš„åºåˆ—&lt;/li&gt;
&lt;li&gt;å‹•æ…‹åœ°æ”¹è®Šç”¨æ–¼è¨“ç·´è³‡æ–™çš„ masking pattern&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è²¢ç»:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æå‡ºä¸€çµ„é‡è¦çš„ BERT è¨­è¨ˆé¸æ“‡å’Œè¨“ç·´ç­–ç•¥&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨äº†æ–°çš„ datasetï¼Œå«åš CCNEWSï¼Œä¸¦è­‰æ˜ç”¨æ›´å¤šçš„è³‡æ–™ä¾†é è¨“ç·´ï¼Œå¯ä»¥æé«˜ä¸‹æ¸¸ä»»å‹™çš„è¡¨ç¾&lt;/li&gt;
&lt;li&gt;è¨“ç·´è¡¨æ˜ï¼Œåœ¨æ­£ç¢ºçš„è¨­è¨ˆé¸æ“‡ä¸‹ï¼Œpretrained masked language model å’Œå…¶ä»–æœ€è¿‘çš„æ–¹æ³•æ¯”ï¼Œå…·æœ‰ç«¶çˆ­åŠ›&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;å° BERT åšå›é¡§&lt;/p&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L layers&lt;/li&gt;
&lt;li&gt;A self-attention heads&lt;/li&gt;
&lt;li&gt;H hidden dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;training-objectives&#34;&gt;Training Objectives&lt;/h3&gt;
&lt;p&gt;é è¨“ç·´çš„æ™‚å€™ï¼ŒBERT æœ‰å…©å€‹ç›®æ¨™: masked language modeling å’Œ next sentence prediction&lt;/p&gt;
&lt;h4 id=&#34;masked-language-model-mlm&#34;&gt;Masked Language Model (MLM)&lt;/h4&gt;
&lt;p&gt;BERT éš¨æ©Ÿé¸æ“‡ 15% çš„ token é€²è¡Œå¯èƒ½çš„æ›¿æ›&lt;/p&gt;
&lt;p&gt;80% æ›æˆ [MASK]ï¼Œ10% ä¿æŒä¸è®Šï¼Œ10% è¢«é¸ç‚ºä¸€å€‹éš¨ä¾¿çš„ vocabulary token&lt;/p&gt;
&lt;h4 id=&#34;next-sentence-prediction-nsp&#34;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;
&lt;p&gt;åˆ†é¡ç¬¬äºŒå¥æ˜¯ä¸æ˜¯ä¸‹ä¸€å¥ï¼Œæ˜¯äºŒå…ƒåˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;æ­£ä¾‹ç”±æå–é€£çºŒçš„å¥å­ç”¢ç”Ÿï¼Œè² ä¾‹ç”±ä¸åŒçš„ç‰‡æ®µé…å°ç”¢ç”Ÿã€‚&lt;/p&gt;
&lt;p&gt;æ­£ä¾‹å’Œè² ä¾‹ä»¥ç›¸ç­‰æ©Ÿç‡ç”¢ç”Ÿã€‚&lt;/p&gt;
&lt;h4 id=&#34;optimization&#34;&gt;Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;li&gt;$\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = 1e-6&lt;/li&gt;
&lt;li&gt;$L_2$ weight decay of 0.01&lt;/li&gt;
&lt;li&gt;Learning rate å‰ 10,000 step warm up åˆ° 1e-4ï¼Œç„¶å¾Œ linear decay&lt;/li&gt;
&lt;li&gt;å…¨éƒ¨çš„ layer å’Œ attention weight éƒ½ dropout 0.1&lt;/li&gt;
&lt;li&gt;GELU æ¿€æ´»å‡½æ•¸&lt;/li&gt;
&lt;li&gt;1,000,000 æ¬¡ updateï¼Œbatch size 256ï¼Œåºåˆ—é•·åº¦ 512&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data&#34;&gt;Data&lt;/h4&gt;
&lt;p&gt;BERT åœ¨ BookCorpus å’Œ English Wikipedia æ··å’Œçš„è³‡æ–™é›†ä¸Šè¨“ç·´ï¼Œå…±æœ‰ 16GB çš„æœªå£“ç¸®æ–‡æœ¬&lt;/p&gt;
&lt;h2 id=&#34;experimental-setup&#34;&gt;Experimental Setup&lt;/h2&gt;
&lt;p&gt;æè¿°å°æ–¼ BERT çš„ replication study çš„å¯¦é©—è¨­ç½®&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;ä½œè€…ç”¨ FAIRSEQ é‡æ–°å¯¦ç¾äº† BERTã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦éµå¾ª [Background-Optimization] ä¸­çš„ BERT åŸå§‹è¶…åƒæ•¸ï¼Œä½† peak learning rate å’Œ warmup step é™¤å¤–ï¼Œä»–å€‘é‡å°æ¯å€‹è¨­ç½®å–®ç¨èª¿æ•´ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾è¨“ç·´å° Adam epsilon éå¸¸æ•æ„Ÿã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾è¨­ç½® $\beta_2$ = 0.98ï¼Œåœ¨å¤§ batch size çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥æé«˜è¨“ç·´æ™‚çš„ç©©å®šæ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç”¨æœ€å¤š 512 å€‹ token é è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä¸æœƒéš¨æ©Ÿæ³¨å…¥çŸ­åºåˆ—ï¼Œä¹Ÿä¸æœƒç‚ºå‰ 90% çš„æ›´æ–°ç¸®çŸ­è¼¸å…¥çš„é•·åº¦ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åªè¨“ç·´ full-length çš„ sequencesã€‚&lt;/p&gt;
&lt;h3 id=&#34;data-1&#34;&gt;Data&lt;/h3&gt;
&lt;p&gt;BERT-style çš„é è¨“ç·´ä»°è³´å¤§é‡æ–‡æœ¬ã€‚&lt;/p&gt;
&lt;p&gt;å·²æœ‰ç ”ç©¶è­‰æ˜å¢åŠ æ•¸æ“šé‡å¯ä»¥æé«˜ end-task çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;å·²æœ‰ä¸€äº›ç ”ç©¶ï¼Œç”¨æ¯”åŸå§‹ BERT æ›´å¤šæ¨£æ›´å¤§çš„æ•¸æ“šé›†ï¼Œä½†ä¸æ˜¯æ‰€æœ‰çš„æ•¸æ“šé›†éƒ½æœ‰å…¬é–‹ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶ç”¨äº†äº”å€‹ä¸åŒå¤§å°å’Œé ˜åŸŸçš„è‹±æ–‡æ–‡æœ¬ï¼Œå…±æœ‰è¶…é 160 GB çš„æœªå£“ç¸®æ–‡æœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ä»¥ä¸‹æ•¸æ“šé›†:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BookCorpus + English Wikipedia
&lt;ul&gt;
&lt;li&gt;BERT åŸæœ¬ä½¿ç”¨çš„ã€‚&lt;/li&gt;
&lt;li&gt;16 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CC-News
&lt;ul&gt;
&lt;li&gt;ä½œè€…å¾ CommonCrawl News dataset çš„è‹±æ–‡éƒ¨åˆ†ä¸­è’é›†ï¼ŒåŒ…å«äº† 2016 å¹´ 9 æœˆåˆ° 2019 å¹´ 2 æœˆçš„ 6300 è¬ç¯‡è‹±æ–‡æ–°èã€‚&lt;/li&gt;
&lt;li&gt;éæ¿¾å¾Œæœ‰ 76 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OpenWebText
&lt;ul&gt;
&lt;li&gt;WebText çš„é–‹æºé‡å»ºç‰ˆï¼Œå¾ Reddit ä¸Šè‡³å°‘æœ‰ 3 å€‹ upvotes çš„ shared URLs æå–å‡ºçš„ Web å…§å®¹ã€‚&lt;/li&gt;
&lt;li&gt;38 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stories
&lt;ul&gt;
&lt;li&gt;åŒ…å« CommonCrawl data çš„ä¸€å€‹å­é›†åˆï¼Œç¶“ééæ¿¾ï¼Œä»¥åŒ¹é… story-like style of Winograd schemas&lt;/li&gt;
&lt;li&gt;31 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;ä½¿ç”¨ä»¥ä¸‹ä¸‰å€‹ benchmarks è©•ä¼°é è¨“ç·´æ¨¡å‹&lt;/p&gt;
&lt;h4 id=&#34;glue&#34;&gt;GLUE&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The General Language Understanding Evaluation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”¨æ–¼è©•ä¼°è‡ªç„¶èªè¨€ç†è§£çš„ 9 å€‹æ•¸æ“šé›†çš„é›†åˆï¼Œä»»å‹™è¢«å®šç¾©ç‚º single-sentence åˆ†é¡æˆ– sentence-pair åˆ†é¡ä»»å‹™ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;finetune çš„æµç¨‹éµå¾ªåŸå§‹ BERT paper&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;squad&#34;&gt;SQuAD&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Stanford Question Answering Dataset&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æä¾›ä¸€æ®µ context ä»¥åŠä¸€å€‹å•é¡Œ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å…·æœ‰å…©å€‹ç‰ˆæœ¬ V1.1 å’Œ V2.0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;V1.1
&lt;ul&gt;
&lt;li&gt;context ç¸½æ˜¯åŒ…å«ä¸€å€‹ç­”æ¡ˆ&lt;/li&gt;
&lt;li&gt;è©•ä¼° V1.1 çš„æ™‚å€™ï¼Œä½œè€…æ¡ç”¨å’Œ BERT ç›¸åŒçš„ span prediction method&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V2.0
&lt;ul&gt;
&lt;li&gt;ä¸€äº›å•é¡Œåœ¨æä¾›çš„ context ä¸­æ²’æœ‰å›ç­”ï¼Œä½¿ä»»å‹™æ›´æœ‰æŒ‘æˆ°æ€§&lt;/li&gt;
&lt;li&gt;è©•ä¼° V2.0 çš„æ™‚å€™ï¼Œä½œè€…æœƒç”¨ä¸€å€‹é¡å¤–çš„äºŒå…ƒåˆ†é¡å™¨é æ¸¬å•é¡Œæ˜¯å¦å¯ä»¥å›ç­”ï¼Œåœ¨è©•ä¼°çš„æ™‚å€™ï¼Œåªé æ¸¬è¢«åˆ†é¡ç‚ºå¯å›ç­”çš„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;race&#34;&gt;RACE&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The ReAding Comprehension from Examinations&lt;/li&gt;
&lt;li&gt;å¤§å‹é–±è®€ç†è§£æ•¸æ“šé›†ï¼Œæœ‰è¶…é 28,000 ç¯‡æ–‡ç«  ä»¥åŠå°‡è¿‘ 100,000 å€‹å•é¡Œ&lt;/li&gt;
&lt;li&gt;å¾ä¸­åœ‹çš„è‹±æ–‡è€ƒè©¦è’é›†çš„ï¼Œé€™äº›è€ƒè©¦æ˜¯ç‚ºåœ‹ä¸­ç”Ÿå’Œé«˜ä¸­ç”Ÿè¨­è¨ˆçš„&lt;/li&gt;
&lt;li&gt;æ¯ç¯‡æ–‡ç« éƒ½èˆ‡å¤šå€‹å•é¡Œç›¸é—œè¯&lt;/li&gt;
&lt;li&gt;å°æ¯å€‹å•é¡Œï¼Œè¦å¾å››å€‹é¸é …ä¸­é¸å‡ºä¸€å€‹å°çš„&lt;/li&gt;
&lt;li&gt;context æ¯”èµ·å…¶ä»–é–±è®€ç†è§£çš„æ•¸æ“šé›†è¦é•·ï¼Œè€Œä¸”è¦æ¨ç†çš„å•é¡Œæ¯”ä¾‹å¾ˆå¤§&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training-procedure-analysis&#34;&gt;Training Procedure Analysis&lt;/h2&gt;
&lt;p&gt;æ¢è¨å“ªäº›é¸æ“‡å°æˆåŠŸé è¨“ç·´ BERT å¾ˆé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æŠŠæ¶æ§‹å›ºå®šï¼Œä¹Ÿå°±æ˜¯è¨“ç·´å’Œ$BERT_{BASE}$ (L=12, H=768, A=12, 110M params)ä¸€æ¨£æ¶æ§‹çš„ BERT models&lt;/p&gt;
&lt;h3 id=&#34;static-vs-dynamic-masking&#34;&gt;Static vs. Dynamic Masking&lt;/h3&gt;
&lt;p&gt;BERT åœ¨ preprocessing çš„æ™‚å€™è™•ç† maskingï¼Œç”¢ç”Ÿå–®å€‹ static maskã€‚
ä½œè€…ç‚ºäº†é¿å…åœ¨æ¯å€‹ epoch éƒ½å°æ¯å€‹ instance ç”¨ç›¸åŒçš„ maskï¼Œå°‡æ•¸æ“šè¤‡è£½äº† 10 æ¬¡ï¼Œåœ¨ 40 å€‹ epochs è£¡ï¼Œä»¥ 10 ç¨®ä¸åŒçš„æ–¹å¼ maskã€‚æ‰€ä»¥ä¸€æ¬¡è¨“ç·´éç¨‹ä¸­ï¼Œç›¸åŒçš„ mask æœƒå‡ºç¾å››æ¬¡ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æœƒä»¥ä¸Šè¿°ç­–ç•¥å’Œ Dynamic masking é€²è¡Œæ¯”è¼ƒï¼ŒDynamic masking æ˜¯åœ¨æ¯æ¬¡é¤µ model å‰ï¼Œæ‰ç”Ÿæˆ maskã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ Dynamic Masking ç›¸æ¯” staticï¼Œè¦ä¸æ˜¯å·®ä¸å¤šï¼Œå°±æ˜¯ç•¥å¥½ï¼ŒåŸºæ–¼çµæœå’Œæ•ˆç‡çš„å„ªå‹¢è€ƒé‡ï¼Œå…¶ä»–å¯¦é©—ä¸­éƒ½ç”¨ dynamic maskingã€‚&lt;/p&gt;
&lt;h3 id=&#34;model-input-format-and-next-sentence-prediction&#34;&gt;Model Input Format and Next Sentence Prediction&lt;/h3&gt;
&lt;p&gt;åŸå§‹çš„ BERT é è¨“ç·´ä¸­ï¼Œå…©å€‹å¥å­è¦ä¸æ˜¯åŒä¸€å€‹æ–‡ä»¶çš„é€£çºŒå¥å­(p = 0.5)ï¼Œä¸ç„¶å°±æ˜¯ä¸åŒçš„ document åšæ¡æ¨£&lt;/p&gt;
&lt;p&gt;ä»¥å¾€æœ‰ç ”ç©¶æŒ‡å‡ºç§»é™¤ NSP æœƒæå®³æ€§èƒ½ï¼Œä½†ä¹Ÿæœ‰ç ”ç©¶è³ªç–‘å¿…è¦æ€§ï¼Œæ‰€ä»¥æœ¬æ–‡æ¯”è¼ƒäº†å¹¾ç¨®æ›¿ä»£è¨“ç·´æ ¼å¼ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SEGMENT-PAIR+NSP
&lt;ul&gt;
&lt;li&gt;æœ€åŸå§‹çš„æ–¹æ³•ï¼Œæ¯å€‹ segment å¯ä»¥æœ‰å¤šå€‹è‡ªç„¶å¥å­&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SENTENCE-PAIR+NSP
&lt;ul&gt;
&lt;li&gt;åªåŒ…å«ä¸€å°å¥å­ï¼Œç”±æ–¼è¼¸å…¥æ˜é¡¯å°‘æ–¼ 512 tokenï¼Œæ‰€ä»¥æœƒå¢åŠ  batch size è®“ token ç¸½æ•¸å’Œå‰è€…å·®ä¸å¤š&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FULL-SENTENCES
&lt;ul&gt;
&lt;li&gt;åŒ…å«å¾ä¸€å€‹æˆ–å¤šå€‹æ–‡ä»¶ä¸­é€£çºŒæ¡æ¨£çš„å®Œæ•´å¥å­ï¼Œå¯èƒ½æœƒè·¨è¶Šæ–‡ä»¶é‚Šç•Œï¼Œåœ¨æ–‡ä»¶é‚Šç•Œé–“æœƒåŠ å€‹é¡å¤–çš„åˆ†éš”ç¬¦&lt;/li&gt;
&lt;li&gt;ç§»é™¤äº† NSP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DOC-SENTENCES
&lt;ul&gt;
&lt;li&gt;å’Œ FULL-SENTENCES å·®ä¸å¤šï¼Œä½†ä¸èƒ½è·¨è¶Š documentï¼Œåœ¨ document å°¾å·´çš„éƒ¨åˆ†æœƒå®¹æ˜“å°‘æ–¼ 512ï¼Œæ‰€ä»¥æœƒå‹•æ…‹å¢åŠ  batch sizeï¼Œè®“ token ç¸½æ•¸å’Œ FULL-SENTENCES å·®ä¸å¤š&lt;/li&gt;
&lt;li&gt;ç§»é™¤äº† NSP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ç™¼ç¾ DOC-SENTENCES æ˜¯æœ€æ£’çš„ï¼Œä½†ç”±æ–¼ DOC-SENTENCES æœƒè®“ batch sizes å¤§å°å¯è®Šï¼Œæ‰€ä»¥å…¶ä»–å¯¦é©—æœƒç”¨ FULL-SENTENCESï¼Œæ¯”è¼ƒå¥½å’Œå…¶ä»–ç›¸é—œå·¥ä½œæ¯”è¼ƒã€‚&lt;/p&gt;
&lt;h3 id=&#34;training-with-large-batches&#34;&gt;Training with large batches&lt;/h3&gt;
&lt;p&gt;æ ¹æ“šéå»ç¥ç¶“ç¶²è·¯æ©Ÿå™¨ç¿»è­¯çš„å·¥ä½œï¼Œç•¶ learning rate é©ç•¶å¢åŠ çš„æ™‚å€™ï¼Œç”¨éå¸¸å¤§çš„çš„ mini-bathces å¯ä»¥æé«˜ optimization çš„é€Ÿåº¦å’Œ end-task æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ç ”ç©¶ä¹Ÿé¡¯ç¤º BERT é©ç”¨æ–¼ large batch trainingã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;text-encoding&#34;&gt;Text Encoding&lt;/h3&gt;
&lt;p&gt;Byte-Pair Encoding (BPE) æ˜¯ä¸€ç¨®ä»‹æ–¼å­—ç¬¦ç´šåˆ¥å’Œè©ç´šåˆ¥è¡¨ç¤ºä¹‹é–“çš„æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒå…è¨±è™•ç†è‡ªç„¶èªè¨€èªæ–™åº«ä¸­å¸¸è¦‹çš„å¤§è©å½™é‡ã€‚&lt;/p&gt;
&lt;p&gt;BPE ä¸ä¾è³´æ–¼å®Œæ•´çš„å–®è©ï¼Œè€Œæ˜¯ä¾é  subwords unitsï¼Œé€šéå°è¨“ç·´èªæ–™é€²è¡Œçµ±è¨ˆåˆ†æä¾†æå–é€™äº› subwords unitsã€‚&lt;/p&gt;
&lt;p&gt;BPE è©å½™è¡¨çš„å¤§å°é€šå¸¸åœ¨ 10K-100K çš„ subword unitsã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ &amp;ldquo;Language Models are Unsupervised Multitask Learners&amp;rdquo; æ–‡ä¸­ï¼Œæåˆ°äº†ä¸€ç¨®å·§å¦™çš„ BPE å¯¦ç¾ï¼Œä¸æ˜¯ç”¨ unicode charactersï¼Œè€Œæ˜¯ç”¨ bytes ä½œç‚º base subword unitsã€‚å¯ä»¥ç”Ÿå‡º 50K å¤§å°çš„è©å½™è¡¨ï¼Œè€Œä¸”ä¸ç”¨å¼•å…¥ä»»ä½•çš„ &amp;ldquo;unknown&amp;rdquo;ã€‚&lt;/p&gt;
&lt;p&gt;åŸå§‹çš„ BERT ç”¨ character-level BPE vocabularyï¼Œå¤§å°ç‚º 30Kã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è€ƒæ…®ç”¨ 50K byte-level BPE vocabularyï¼Œè€Œä¸å°è¼¸å…¥åšé¡å¤–çš„ preprocessing æˆ– tokenizationï¼Œ&amp;ldquo;Language Models are Unsupervised Multitask Learners&amp;rdquo; çš„ç ”ç©¶é¡¯ç¤ºé€™äº› Encoding çš„æ–¹æ³•åœ¨æœ€çµ‚æ•ˆèƒ½ä¸Šä¸¦ç„¡å¤ªå¤§å·®åˆ¥ï¼Œåªåœ¨æŸäº›ä»»å‹™ä¸Š end-task performance è¡¨ç¾ç¨å·®ã€‚&lt;/p&gt;
&lt;p&gt;ä½†ä½œè€…ç›¸ä¿¡ universal encoding scheme çš„å„ªå‹¢è¶…éäº†è¼•å¾®çš„æ€§èƒ½ä¸‹é™ï¼Œå…¶ä»–å¯¦é©—ä¹Ÿæœƒç”¨é€™ç¨®é‚Šç¢¼æ–¹å¼ã€‚&lt;/p&gt;
&lt;h2 id=&#34;roberta&#34;&gt;RoBERTa&lt;/h2&gt;
&lt;p&gt;æ•´ç†ä¸Šé¢èªªçš„æ”¹é€²ã€‚&lt;/p&gt;
&lt;p&gt;RoBERTa ç”¨ä»¥ä¸‹é…ç½®:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;dynamic masking&lt;/li&gt;
&lt;li&gt;FULL-SENTENCES without NSP loss&lt;/li&gt;
&lt;li&gt;large mini-batches&lt;/li&gt;
&lt;li&gt;larger byte-level BPE&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;æ­¤å¤–ï¼Œé‚„èª¿æŸ¥äº†å…©å€‹ä¹‹å‰çš„å·¥ä½œæ²’å¼·èª¿çš„é‡è¦å› ç´ :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨æ–¼é è¨“ç·´çš„ data&lt;/li&gt;
&lt;li&gt;è¨“ç·´é data çš„æ¬¡æ•¸&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç‚ºäº†æŠŠé€™äº›å› ç´ çš„é‡è¦æ€§å’Œå…¶ä»–æ¨¡å‹é¸æ“‡åˆ†éš”é–‹ï¼Œå…ˆæŒ‰ç…§ $BERT_{LARGE}$ (L = 24, H = 1024, A = 16, 355M parameters) è¨“ç·´ RoBERTaã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ BOOKCORPUS plus WIKIPEDIA dataset é€²è¡Œäº† 100K step çš„é è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ§åˆ¶ training data çš„æƒ…æ³ä¸‹ï¼Œ RoBERTa æ¯” $BERT_{LARGE}$ çš„çµæœæœ‰å¤§å¹…åº¦çš„æ”¹é€²ï¼Œé‡ç”³äº†å‰é¢è¨­è¨ˆé¸æ“‡çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;æ¥ä¸‹ä¾†ï¼Œçµåˆä¹‹å‰èªªçš„é¡å¤– datasetï¼Œä¸¦ç”¨ç›¸åŒçš„æ­¥æ•¸(100K) è¨“ç·´ RoBERTaï¼Œè§€å¯Ÿåˆ°ä¸‹æ¸¸ä»»å‹™çš„æ€§èƒ½é€²ä¸€æ­¥æé«˜ï¼Œé©—è­‰äº†æ•¸æ“šå¤§å°å’Œå¤šæ¨£æ€§çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œï¼Œå° RoBERTa åšæ›´é•·æ™‚é–“çš„é è¨“ç·´ï¼Œå°‡æ­¥æ•¸æé«˜åˆ° 300K å’Œ 500Kï¼Œå†æ¬¡è§€å¯Ÿåˆ°ä¸‹æ¸¸ä»»å‹™æ€§èƒ½é¡¯è‘—æå‡ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä¹Ÿæ³¨æ„åˆ°ï¼Œå³ä½¿æ˜¯ä»–å€‘è¨“ç·´æ™‚é–“æœ€é•·çš„æ¨¡å‹ï¼Œä¹Ÿä¸æœƒ overfit ä»–å€‘çš„æ•¸æ“šã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡çš„å…¶ä»–éƒ¨åˆ†åœ¨ä¸‰å€‹ benchmark è©•ä¼°å¥½å£: GLUEã€SQuaD å’Œ RACE&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;glue-results&#34;&gt;GLUE Results&lt;/h3&gt;
&lt;p&gt;é›–ç„¶å¾ˆå¤š GLUE æ’è¡Œæ¦œçš„æäº¤éƒ½æ˜¯ depend on multi-task finetuningï¼Œä½†ä½œè€…çš„ submission æ˜¯ depends only on single-task finetuningã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œå°æ–¼ RTEã€STS å’Œ MRPCï¼Œå¾ MNLI çš„æ¨¡å‹å¾®èª¿æœƒæ¯” baseline çš„ RoBERTa æœ‰å¹«åŠ©è¨±å¤šã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç¬¬ä¸€å€‹è¨­ç½® (single-task, dev) ä¸­ï¼ŒRoBERTa åœ¨æ‰€æœ‰ 9 å€‹ GLUE ä»»å‹™ dev set ä¸Šéƒ½å–å¾—äº†æœ€å…ˆé€²çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç¬¬äºŒå€‹è¨­ç½® (ensembles, test) ä¸­ï¼Œä½œè€…å°‡ RoBERTa æäº¤åˆ° GLUE æ’è¡Œæ¦œï¼Œä¸¦åœ¨ 9 å€‹ä»»å‹™ä¸­çš„ 4 å€‹ä¸Šå–å¾—äº† SOTA å’Œè¿„ä»Šç‚ºæ­¢çš„æœ€é«˜å¹³å‡åˆ†ã€‚&lt;/p&gt;
&lt;p&gt;é€™ä»¤äººèˆˆå¥®çš„åœ°æ–¹åœ¨æ–¼ï¼Œèˆ‡å¤šæ•¸ top submissions ä¸åŒï¼ŒRoBERTa ä¸æ˜¯ depend on multi-tasking finetuning&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;åœ¨é è¨“ç·´ BERT æ¨¡å‹æ™‚ï¼Œä½œè€…ä»”ç´°è©•ä¼°äº†è¨±å¤šè¨­è¨ˆæ±ºç­–ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ï¼Œé€šéå°æ¨¡å‹é€²è¡Œæ›´é•·æ™‚é–“çš„è¨“ç·´ã€ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡è™•ç†æ›´å¤šçš„æ•¸æ“šã€å»é™¤ NSPã€è¨“ç·´æ›´é•·çš„åºåˆ—ã€dynamic maskingï¼Œå¯ä»¥é¡¯è‘—æé«˜æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æ”¹é€²çš„é è¨“ç·´ç¨‹åºï¼Œæˆ‘å€‘ç¨±ä¹‹ç‚º RoBERTaï¼Œåœ¨ GLUEã€RACE å’Œ SQuAD ä¸Šå¯¦ç¾äº† SOTAï¼Œè€Œç„¡éœ€ç‚º GLUE é€²è¡Œå¤šä»»å‹™å¾®èª¿æˆ–ç‚º SQuAD æä¾›é¡å¤–çš„æ•¸æ“šã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›çµæœèªªæ˜äº†é€™äº›ä»¥å‰è¢«å¿½è¦–çš„è¨­è¨ˆæ±ºç­–çš„é‡è¦æ€§ï¼Œä¸¦è¡¨æ˜ BERT çš„é è¨“ç·´ç›®æ¨™èˆ‡æœ€è¿‘æå‡ºçš„æ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ä»ç„¶å…·æœ‰ç«¶çˆ­åŠ›ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PatentSBERTa è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/patentsberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 15 Mar 2023 15:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/patentsberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.11933&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬ç ”ç©¶æä¾›äº†ä¸€å€‹è¨ˆç®—  patent-to-patent (p2p) technological similarity çš„æœ‰æ•ˆæ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä¸¦æå‡ºä¸€å€‹ hybrid frameworkï¼Œç”¨æ–¼æŠŠ p2p ç›¸ä¼¼æ€§çš„çµæœæ‡‰ç”¨æ–¼ semantic search å’Œ automated patent classificationã€‚&lt;/p&gt;
&lt;p&gt;æŠŠ Sentence-BERT (SBERT) ç”¨åœ¨ claims ä¸Šä¾†ä½œ embeddingsã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æå‡ embedding çš„å“è³ªï¼Œä½¿ç”¨åŸºæ–¼ SBERT å’Œ RoBERT çš„ transformer modelï¼Œç„¶å¾Œå†ç”¨ augmented approach åœ¨  in-domain supervised patent claims data(ç›¸å°æ–¼ out-domain) ä¾† fine-tune SBERTã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ KNN(Nearest Neighbors) ä¾†æ ¹æ“š p2p similarity åˆ†é¡æ¨¡å‹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;å‚³çµ±ä¸Šçš„ p2p ç›¸ä¼¼åº¦æ˜¯åŸºæ–¼é—œéµå­—ã€æŠ€è¡“é¡åˆ¥ç­‰ metadata æ±ºå®šçš„ï¼Œä½†è¿‘æœŸ semantic-based çš„æ–¹æ³•ä¹Ÿè¶Šä¾†è¶Šå—æ­¡è¿ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç›®å‰é‡åˆ°çš„å•é¡Œ
&lt;ol&gt;
&lt;li&gt;BERT ç”¨ä¾†è¨ˆç®— p2p ç›¸ä¼¼æ€§çš„æˆæœ¬å¾ˆé«˜&lt;/li&gt;
&lt;li&gt;åŸºæ–¼ generic text çš„ pre-trained model åœ¨é‡åˆ°ç‰¹å®šé ˜åŸŸçš„å°ˆæ¥­è¡“èªæ™‚å¯èƒ½æœƒé‡åˆ°ä¾·é™ã€‚&lt;/li&gt;
&lt;li&gt;åœ¨å°ˆåˆ©åš multi-label classification (MLC) æ˜¯å€‹æŒ‘æˆ°&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;è²¢ç»
&lt;ol&gt;
&lt;li&gt;æä¾›ä¸€å€‹å¿«é€Ÿé«˜æ•ˆçš„æ¡†æ¶ï¼Œåˆ©ç”¨ Transformer æ¶æ§‹è¨ˆç®— p2p ç›¸ä¼¼åº¦&lt;/li&gt;
&lt;li&gt;é€é augmented SBERTï¼Œå°‡ transformer model fine-tune åˆ° domain-specific language&lt;/li&gt;
&lt;li&gt;æå‡ºä¸€å€‹åŸºæ–¼ Transformer å’Œ å‚³çµ± ML æ¨¡å‹çš„æ··å’Œæ¶æ§‹ï¼Œå¯ä»¥æ‰“æ•— multi-label å’Œ multi-class çš„å°ˆåˆ©åˆ†é¡ SOTA æ¨¡å‹&lt;/li&gt;
&lt;li&gt;ç”¨ç°¡å–®çš„ KNN é€²è¡Œå°ˆåˆ©åˆ†é¡ï¼Œæä¾›äº†ä¸€ç¨®ç°¡å–®çš„æ–¹æ³•ä¾†æª¢æŸ¥ã€ç†è§£å’Œè§£é‡‹æ¨¡å‹çš„é æ¸¬çµæœ&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;h3 id=&#34;dataset-description&#34;&gt;Dataset Description&lt;/h3&gt;
&lt;p&gt;æœ¬ç ”ç©¶ä½¿ç”¨ PatentsView datasetï¼ŒPatentsView å¹³å°å»ºç«‹åœ¨ä¸€å€‹å®šæœŸæ›´æ–°çš„ database ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;dataset å·²ç”¨æ–¼ä¹‹å‰é¡ä¼¼çš„ç ”ç©¶ï¼Œæ¯”å¦‚ DeepPatentã€PatentBERTã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶ä½¿ç”¨äº† 2013-2017 çš„æ‰€æœ‰å°ˆåˆ©ï¼Œé€™äº›å°ˆåˆ©è‡³å°‘è¦åœ¨ BigQuery ä¸Šæœ‰ä¸€æ¢ claimã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶çš„ record æœ‰ 1,492,294 é …å°ˆåˆ©ï¼Œä¸¦ç”¨ 8% ä½œç‚ºæ¸¬è©¦é›†ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œæœ¬ç ”ç©¶åˆªé™¤äº†æœ‰é‡è¤‡å°ˆåˆ© ID å’Œ claim text çš„ recordã€‚&lt;/p&gt;
&lt;h3 id=&#34;textual-data-patent-claims&#34;&gt;Textual Data: Patent Claims&lt;/h3&gt;
&lt;p&gt;æœ¬ç ”ç©¶ä½¿ç”¨ claim ä½œç‚ºè¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;claim è¢«èªç‚ºæ˜¯æº–å‚™å°ˆåˆ©æ–‡ä»¶çš„åˆå§‹æ¡†æ¶ï¼Œå…¶ä»–æ–‡ä»¶éƒ½æ˜¯æ ¹æ“š claim æº–å‚™çš„ï¼Œ
å› æ­¤ï¼Œclaim æ¯”å…¶ä»–æ–‡ä»¶åŒ…å«æ›´å…¨é¢å’Œæº–ç¢ºçš„è¨Šæ¯ã€‚&lt;/p&gt;
&lt;p&gt;claim å…·æœ‰å±¤æ¬¡çµæ§‹ï¼Œfirst claim è¢«è¦–ç‚ºè©²æ¶æ§‹çš„ä¸»å¹¹ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶åƒ…ä½¿ç”¨ first claimï¼Œä½†åœ¨ä»¥å¾Œçš„ç ”ç©¶ä¸­ï¼Œå¸Œæœ›æ ¹æ“š tree structure çµ„åˆæ‰€æœ‰ claimï¼Œä¸¦è¨ˆç®— semantic similarityï¼Œä¸¦åšå¤šæ¨™ç±¤åˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç ”ç©¶æ¨£æœ¬ä¸­ï¼Œ claim å¹³å‡æœ‰ 17 å€‹ã€‚&lt;/p&gt;
&lt;p&gt;claim çš„å¹³å‡é•·åº¦æ˜¯ 162ï¼Œæœ¬ç ”ç©¶ä¸­ï¼ŒBERT çš„ max_seq_length æ˜¯ 510ã€‚&lt;/p&gt;
&lt;h3 id=&#34;patent-classification-cpc-classes&#34;&gt;Patent Classification: CPC Classes&lt;/h3&gt;
&lt;p&gt;CPCç³»çµ±å’ŒIPCï¼ˆåœ‹éš›å°ˆåˆ©åˆ†é¡ï¼‰ç³»çµ±æ˜¯æœ€å¸¸ç”¨çš„å…©ç¨®åˆ†é¡ç³»çµ±ï¼ŒCPC æ˜¯ IPC ç³»çµ±çš„æ›´å…·é«”å’Œè©³ç´°çš„ç‰ˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;CPC å…·æœ‰ç”¨æ–¼åˆ†é¡çš„å±¤æ¬¡çµæ§‹ï¼ŒåŒ…æ‹¬ Sectionã€Classã€Subclass å’Œ Groupï¼Œ
åœ¨å­é¡ç´šåˆ¥ï¼ŒCPC æœ‰ 667 å€‹æ¨™ç±¤ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ•¸æ“šé›†ä¸­æˆ‘å€‘æœ‰ 663 å€‹æ¨™ç±¤ï¼Œå…¶ä¸­ 159 å€‹åœ¨æ•¸æ“šé›†ä¸­çš„æ¨£æœ¬å°‘æ–¼ 350 å€‹ï¼Œé€™ç¨®æ¨™ç±¤åˆ†ä½ˆå°è‡´äº† KNN ä¸å¥½è™•ç†ï¼Œä¸€èˆ¬ä¾†èªªï¼Œéš¨è‘— instance æ•¸é‡çš„å¢åŠ ï¼Œæˆ‘å€‘å¯ä»¥æé«˜æ¨¡å‹çš„æº–ç¢ºæ€§ã€‚&lt;/p&gt;
&lt;h2 id=&#34;method-and-experimental-setup&#34;&gt;Method and experimental setup&lt;/h2&gt;
&lt;p&gt;Pretrained Language Models (LMs) åœ¨ NLP ä¸­è®Šå¾—ååˆ†æµè¡Œã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ pairwise sentence semantic similarityï¼ŒSBERT å’Œ BERT æ˜¯å…©ç¨®å…·æœ‰é¡¯è‘—ä¸åŒæ•ˆæœçš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;BERT é€šå¸¸å¯ä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨å¯¦éš›æ‡‰ç”¨ä¸Šä¾†èªªå¤ªæ…¢äº†ã€‚&lt;/p&gt;
&lt;p&gt;SBERT åœ¨å¯¦éš›æ‡‰ç”¨ä¸Šè¡¨ç¾é‚„è¡Œï¼Œä½†éœ€è¦ in-domain training data ä¸¦ä¸” finetuneã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/PatentSBERTa/Bi_vs_Cross-Encoder.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/PatentSBERTa/approach.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–æ˜¯ Augmented SBERT In-domain approachã€‚&lt;/p&gt;
&lt;p&gt;in-domain sentence pairs é€é cross-encoder ä¾†æ¨™è¨˜ï¼Œå‡è¨­æœ‰ n å€‹ in-domain sentencesï¼Œæœƒæœ‰ $C_2^n$ çµ„å¯èƒ½çš„çµ„åˆã€‚&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„çµ„åˆä¸¦ä¸æœƒæé«˜æ€§èƒ½ï¼Œæ‰€ä»¥è¦æœ‰æ­£ç¢ºçš„æ¡æ¨£ç­–ç•¥ï¼Œæ‰å¯æå‡æ€§èƒ½çš„åŒæ™‚ä¹Ÿæ¸›å°‘è¨ˆç®—é–‹éŠ·ã€‚&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–é‚£ç¨®çµåˆ cross-encoder å’Œ bi-encoder çš„ä½œæ³•è¢«ç¨±ç‚º Augmented SBERT (AugSBERT)ï¼Œ
æ¶‰åŠä»¥ä¸‹ä¸‰å€‹æ­¥é©Ÿ:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨è³‡æ–™é›† Fine-tune RoBERTa ä»¥ç”Ÿå‡º cross-encoder&lt;/li&gt;
&lt;li&gt;ç”¨ cross-encoder ä¾†æŠŠæœªæ¨™è¨˜çš„è³‡æ–™æ¨™è¨˜ï¼ŒåŒæ™‚åŸºæ–¼æŸç¨®ç‰¹å®šçš„æ¡æ¨£ç­–ç•¥ï¼Œå¾ 652,653 ç¨®å¯èƒ½çš„çµ„åˆä¸­æŒ‘é¸ 3432 çµ„&lt;/li&gt;
&lt;li&gt;æŠŠè³‡æ–™é›† + é¡å¤–çš„ 3432 çµ„è³‡æ–™ä¸€èµ·æ‹¿ä¾†è¨“ç·´ SBERT&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;p2p-similarity-and-semantic-search&#34;&gt;P2P similarity and semantic search&lt;/h3&gt;
&lt;p&gt;Patent Semantic Search (PSS) æ˜¯å°ˆåˆ©åˆ†æçš„åŸºç¤éƒ¨åˆ†ã€‚&lt;/p&gt;
&lt;p&gt;Transformer æ¨¡å‹ç­‰èªç¾©ç›¸ä¼¼æ€§çš„è§£æ³•æ˜¯ä¸€ç¨®æ–°è§£æ³•ï¼Œå¯ä»¥ç”¨ä¾†è§£æ±ºåŸºæ–¼é—œéµå­—çš„æœå°‹æ–¹æ³•ä¸­ï¼Œ query terms å’Œå°ˆåˆ©å…§å®¹ä¸åŒ¹é…çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è©•ä¼°æ¨¡å‹çš„æº–ç¢ºæ€§ï¼Œæœªä¾†çš„ç ”ç©¶ä¸­ï¼Œä½œè€…å¸Œæœ›é€šé Mean Reciprocal Rank (MRR) ä¾†è©•ä¼°åˆ†é¡çµæœã€‚&lt;/p&gt;
&lt;h3 id=&#34;cpc-prediction&#34;&gt;CPC Prediction&lt;/h3&gt;
&lt;p&gt;Top-N æº–ç¢ºåº¦ç­‰æ–¼ GT èˆ‡é æ¸¬æœ‰æœ€é«˜æ¦‚ç‡çš„ä»»ä½• N å€‹é æ¸¬åŒ¹é…çš„é »ç‡ï¼Œ
æ‰€ä»¥ Top-5 å°±æ˜¯æœ€é«˜çš„äº”å€‹åˆ†é¡ä¸­ä¸€å€‹å°±æœ‰ä¸­ã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡ä½¿ç”¨  augmented SBERT  ç²å¾— SOTA çš„å°ˆåˆ©æ–‡æœ¬ embeddingã€‚&lt;/p&gt;
&lt;p&gt;ä»‹ç´¹äº†ä¸€ç¨® augmented çš„æ–¹æ³•ï¼ŒæŠŠ SBERT å¾®èª¿åˆ°é©åˆ patent claims çš„ domainã€‚&lt;/p&gt;
&lt;p&gt;SBERT çš„ä¸€å€‹ä¸»è¦å„ªé»æ˜¯å¯ä»¥æœ‰æ•ˆç‡åœ°ç²å¾— embedding distanceï¼Œä½¿æˆ‘å€‘èƒ½å¤ ç‚ºå¤§çš„å°ˆåˆ©è³‡æ–™é›†å»ºæ§‹ p2p similarityã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶åŸºæ–¼æ–‡æœ¬çš„ p2p similarity çš„æœ‰ç”¨æ€§å·²ç¶“åœ¨å„ç¨®æ‡‰ç”¨æ–¹é¢å¾—åˆ°è­‰æ˜ï¼Œä½†æœ¬æ–‡é€²ä¸€æ­¥è­‰æ˜ä½œè€…çš„ transformer-based p2p similarity å¯ä»¥è¢«ç”¨åœ¨ SOTA çš„å°ˆåˆ©åˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”ä½¿ç”¨ç°¡å–®çš„ KNN æ–¹æ³•ï¼Œæª¢æŸ¥ä»–å€‘å¯ä»¥ä½¿æ¨¡å‹æ±ºç­–å…·å‚™ understandable å’Œ explainableã€‚&lt;/p&gt;
&lt;h2 id=&#34;limitations--future-research&#34;&gt;Limitations &amp;amp; Future Research&lt;/h2&gt;
&lt;p&gt;æœªä¾†å¸Œæœ›ç”¨ Annoy(Approximate Nearest Neighbor Oh Yeah!) ä¾†æ¸¬è©¦æ›´å¤§æ¨£æœ¬çš„æ¨¡å‹ä¸¦æ¯”è¼ƒçµæœã€‚&lt;/p&gt;
&lt;p&gt;Annoy(Approximate Nearest Neighbor Oh Yeah!) æ˜¯æƒ³å°‹æ‰¾è¿‘ä¼¼ç›¸ä¼¼è€Œä¸æ˜¯ç²¾ç¢ºç›¸ä¼¼çš„å¥å­ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Sentence-BERT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/sentence-bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 12 Mar 2023 10:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/sentence-bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1908.10084&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;BERT å’Œ RoBERTa åœ¨ semantic textual similarity (STS) ä¸Šå¤ªèŠ±æ™‚é–“ï¼Œå› ç‚ºä»–éœ€è¦å°‡å…©å€‹å¥å­éƒ½è¼¸å…¥ç¶²è·¯ï¼Œä¸¦ä¸”å…©å…©æ¯”å°ã€‚&lt;/p&gt;
&lt;p&gt;Sentence-BERT(SBERT) å°é è¨“ç·´çš„ BERT ä½œäº†ä¸€äº›ä¿®æ”¹ï¼Œé€é siamese å’Œ triplet network çš„çµæ§‹ä¾†ç”Ÿå‡ºæœ‰æ„ç¾©çš„ embeddingsï¼Œä½¿å…¶æœ€å¾Œå¯ä»¥é€é cosine-similarity æ¯”è¼ƒç›¸ä¼¼åº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;SBERT ä½¿ BERT å¯ä»¥ç”¨æ–¼æŸäº›è¿„ä»Šç‚ºæ­¢ä¸é©ç”¨æ–¼ BERT çš„ä»»å‹™ï¼Œæ¯”å¦‚ large-scale semantic similarity comparisonã€clustering é‚„æœ‰ information retrieval via semantic searchã€‚&lt;/p&gt;
&lt;p&gt;ä»¥å¾€çš„ç›¸é—œç ”ç©¶æ˜¯æŠŠå–®å€‹å¥å­è¼¸å…¥ BERTï¼Œæœ€å¾Œ average BERT output layerï¼Œæˆ–æ˜¯ä½¿ç”¨ç¬¬ä¸€å€‹ outputï¼Œä½†é€™æ¨£æœƒç”¢ç”Ÿç³Ÿç³•çš„ sentence embeddingsã€‚&lt;/p&gt;
&lt;p&gt;SentEval æ˜¯ä¸€å€‹ evaluation toolkit for sentence embeddings&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;BERT é€éè¼¸å…¥å…©å€‹å¥å­ï¼Œä»¥ [SEP] éš”é–‹ï¼Œå¯ä»¥åœ¨ STS å–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;ä½†é€™æ¨£ç„¡æ³•è¨ˆç®—ç¨ç«‹çš„ sentence embeddingï¼Œæ‰€ä»¥éå¾€çš„ç ”ç©¶äººå“¡æŠŠå–®å€‹å¥å­è¼¸å…¥ BERTï¼Œæœ€å¾Œ average BERT output layerï¼Œæˆ–æ˜¯ä½¿ç”¨ç¬¬ä¸€å€‹ outputã€‚&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;SBERT åœ¨ BERT / RoBERTa çš„è¼¸å‡ºä¸­æ·»åŠ äº† poolingï¼Œä½œè€…å˜—è©¦äº†ä¸‰ç¨®ç­–ç•¥ï¼ŒCLS-token çš„è¼¸å‡ºã€æ‰€ä»¥è¼¸å‡ºå‘é‡çš„å¹³å‡ã€max-over-time of the output vectorsï¼Œé»˜èªæ˜¯ MEANã€‚&lt;/p&gt;
&lt;p&gt;å¯¦é©—ä»¥ä¸‹çµæ§‹å’Œç›®æ¨™å‡½æ•¸:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Classification Objective Function&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/COF-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/COF-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression Objective Function&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç”¨ mean squared-error loss&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/ROF.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Triplet Objective Function&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/TOF.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-details&#34;&gt;Training Details&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset
&lt;ul&gt;
&lt;li&gt;SNLI çµåˆ Multi-Genre NLI
&lt;ul&gt;
&lt;li&gt;SNLI: 570,000 å€‹ å¥å­ pairï¼Œæœ‰ä¸‰é¡ï¼Œcontradiction, eintailment, and neutral&lt;/li&gt;
&lt;li&gt;MultiNLI: 430,000 å€‹å¥å­ pair&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3-way softmax Classification Objective Function&lt;/li&gt;
&lt;li&gt;1-epoch&lt;/li&gt;
&lt;li&gt;batch-size: 16&lt;/li&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;li&gt;lr: 2e-5&lt;/li&gt;
&lt;li&gt;warm-up: è¶…é 10% of the training data&lt;/li&gt;
&lt;li&gt;é»˜èª pooling ç­–ç•¥: MEAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;å­¸ç¿’ä¸€å€‹è¤‡é›œçš„å›æ­¸å‡½æ•¸åˆ†æ STS å¸¸æ˜¯ SOTAï¼Œä½†æ˜¯ç”±æ–¼ä»–æ˜¯ pair-wiseï¼Œé‡åˆ° combinatorial explosionï¼Œä¸å¥½æ‹“å±•ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç”¨ cosine-similarity æ¯”è¼ƒå…©å€‹ embeddings çš„ç›¸ä¼¼åº¦ï¼Œä¹Ÿç”¨ negative Manhatten å’Œ negative Euclidean distancesï¼Œä½†å¾—åˆ°å·®ä¸å¤šçš„çµæœã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;ç”¨ BERT ç”Ÿå‡ºçš„ embeddings ä¸é©åˆå¸¸è¦‹çš„ç›¸ä¼¼åº¦æ¸¬é‡æ–¹æ³•ï¼Œæ¯”å¦‚ cosine-similarityã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡º SBERT æ”¹é€²ï¼Œåœ¨ siamese / triplet ç¶²è·¯æ¶æ§‹ä¸­å¾®èª¿ BERTã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ RoBERTa æ›¿æ›æ‰ BERT ä¸¦æ²’æœ‰ä»€éº¼é¡¯è‘—æ”¹é€²ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PatentBERT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/patentbert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 02 Mar 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/patentbert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.02124&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;æŠŠ fine-tune BERT æ‡‰ç”¨åœ¨å°ˆåˆ©åˆ†é¡ä¸Šï¼Œç•¶æ‡‰ç”¨æ–¼è¶…é 200 è¬ä»¶å°ˆåˆ©çš„è³‡æ–™é›†æ™‚ï¼Œè©²æ–¹æ³•è¶…è¶Šäº†çµåˆ word-embedding çš„ CNN çš„ SOTA ä½œæ³•ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;è²¢ç»:
&lt;ol&gt;
&lt;li&gt;ä¸€å€‹ç”¨é è¨“ç·´çš„ BERT å» fine-tune çš„ SOTA æ–¹æ³•&lt;/li&gt;
&lt;li&gt;ä¸€å€‹å«åš USPTO-3M çš„å¤§å‹è³‡æ–™é›†ï¼Œå±¬æ–¼ CPC subclass levelï¼Œä¸¦æä¾› SQL èªå¥è®“å¾ŒçºŒçš„ç ”ç©¶è€…ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;èˆ‡å‚³çµ±è§€å¿µç›¸åï¼Œåªéœ€è¦ claim å°±è¶³ä»¥å®Œæˆåˆ†é¡ä»»å‹™&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;å°ˆåˆ©åˆ†é¡æ˜¯ä¸€å€‹ multi-label çš„åˆ†é¡ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼æ¨™ç±¤çš„æ•¸é‡å¯èƒ½å¾ˆå¤§ï¼Œæ‰€ä»¥æ˜¯å€‹å…·æœ‰æŒ‘æˆ°æ€§çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æº–å‚™äº†ä¸€å€‹åŸºæ–¼ CPC çš„æ–°è³‡æ–™é›†ï¼Œæœ‰è¶…éä¸‰ç™¾è¬é …ç¾åœ‹å°ˆåˆ©ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPC
&lt;ul&gt;
&lt;li&gt;Cooperative Patent Classification&lt;/li&gt;
&lt;li&gt;æ˜¯ IPC æ›´å…·é«”å’Œè©³ç´°çš„ç‰ˆæœ¬&lt;/li&gt;
&lt;li&gt;å¯é è¦‹å°‡å–ä»£ IPC æˆç‚ºæ–°çš„æ¨™æº–
&lt;ul&gt;
&lt;li&gt;åªæ˜¯ç”±æ–¼ CLEP-IP ç«¶è³½ï¼Œå¤§éƒ¨åˆ†è«–æ–‡éƒ½åŸºæ–¼ IPC
&lt;ul&gt;
&lt;li&gt;è³‡æ–™é›†åŒ…å« 1978 åˆ° 2009 æäº¤çš„å°ˆåˆ©&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IPC
&lt;ul&gt;
&lt;li&gt;International Patent Classification&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…çš„ dataset åŸºæ–¼ patent claims&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patent claims
&lt;ul&gt;
&lt;li&gt;é‡è¦æ€§åœ¨éå¾€è¢«ä½ä¼°&lt;/li&gt;
&lt;li&gt;åœ¨èµ·è‰å°ˆåˆ©ç”³è«‹æ™‚ï¼Œå°ˆåˆ©æ¥­è€…æœƒå…ˆèµ·è‰ patent claims&lt;/li&gt;
&lt;li&gt;å°ˆåˆ©æ–‡ä»¶çš„å…¶é¤˜éƒ¨åˆ†ç”± claim åšå»¶ä¼¸&lt;/li&gt;
&lt;li&gt;åœ¨å°ˆåˆ©æ³•ä¸­ï¼Œclaims å®šç¾©äº†å°ˆåˆ©ç™¼æ˜çš„ç•Œç·šï¼Œç¢ºå®šäº†å°ˆåˆ©æ¬Šç¯„åœ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç‚ºä½¿æ¨¡å‹æ›´ç°¡å–®ï¼Œåªé—œæ³¨ patent claimsï¼Œä¸¦ä¸”åƒ…ç”¨ç¬¬ä¸€é … claimã€‚&lt;/p&gt;
&lt;h1 id=&#34;ç›¸é—œå·¥ä½œ&#34;&gt;ç›¸é—œå·¥ä½œ&lt;/h1&gt;
&lt;p&gt;éå¾€æœ‰äº›ç ”ç©¶åªé¡¯ç¤ºäº† precisionï¼Œä½†æ²’æœ‰ F1 value æˆ– recallï¼Œé›£ä»¥å…¬å¹³æ¯”è¼ƒã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ DeepPatent&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;éå¾€è³‡æ–™åŸºæ–¼ CLEF-IP æˆ– patent officesã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾åœ¨ BigQuery ç”¨ Google Patents Public Datasets æ›´å®¹æ˜“ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”å¯ç”¨ SQL statementsï¼Œä½œè€…èªç‚ºæ¯”å…±äº«å‚³çµ±è³‡æ–™é›†æ›´å¥½ï¼ŒåŸå› å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Seperation of concerns
&lt;ul&gt;
&lt;li&gt;å¦‚æœè³‡æ–™åŒ…å«å‰è™•ç†æˆ–å¾Œè™•ç†ï¼Œå…¶ä»–ç ”ç©¶äººå“¡éœ€è¦ä¸åŒæ“ä½œæ™‚æœƒå¾ˆé ­ç—›ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Clarity and flexibility
&lt;ul&gt;
&lt;li&gt;SQL statement ç²¾ç¢ºä¸”å®¹æ˜“æ ¹æ“šä¸åŒæ¢ä»¶é€²è¡Œä¿®æ”¹ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åœ¨å’Œ DeepPatent æ¯”è¼ƒçš„æ™‚å€™ï¼Œå¯ä»¥çš„è©±ï¼Œæœƒç”¨ USPTO2M é€²è¡Œæ¸¬è©¦ï¼Œå¦‚æœä¸è¡Œï¼Œæ‰æœƒåˆä½µä¾†è‡ª USPTO-3M çš„è³‡æ–™ï¼Œæ¯”å¦‚ USPTO-2M æ²’æœ‰ claims çš„æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ¯”è¼ƒ claim å¦‚ä½•å½±éŸ¿æ€§èƒ½ï¼Œå°‡åˆä½µå…©å€‹è³‡æ–™é›†ã€‚&lt;/p&gt;
&lt;h1 id=&#34;method--experimental-setup&#34;&gt;Method &amp;amp; Experimental Setup&lt;/h1&gt;
&lt;p&gt;ç”¨ BERT-Base å°±å¯ä»¥æ‰“æ•— DeepPatentã€‚&lt;/p&gt;
&lt;p&gt;éµå¾ª BERT Project ä¸­çµ¦çš„ fine-tune ç¯„ä¾‹ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº† multilabelï¼Œç”¨ sigmoid cross entropy with logits function è€Œä¸æ˜¯ç”¨ softmaxã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/PatentBERT/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;å°ˆåˆ©åˆ†é¡ä½œç‚ºå…·æœ‰æŒ‘æˆ°æ€§çš„ä»»å‹™ï¼Œå¹¾åå¹´ä¾†ä¸€ç›´æ²’æœ‰ä»¤äººæ»¿æ„çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€å€‹åŸºæ–¼ fine-tune BERT çš„æ–¹æ³•ï¼Œæ€§èƒ½å„ªæ–¼ DeepPatentã€‚&lt;/p&gt;
&lt;p&gt;ä¸¦ä¸”çµæœè¡¨æ˜åªç”¨ patent claim å°±å¯ä»¥å®Œæˆåˆ†é¡ä»»å‹™ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MAE è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</link>
        <pubDate>Wed, 15 Feb 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.06377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;é€™ç¯‡è«–æ–‡é¡¯ç¤ºå‡º MAE æ˜¯ CV ä¸­çš„ scalable self-supervised learnersã€‚&lt;/p&gt;
&lt;p&gt;MAE çš„æ–¹æ³•å¾ˆç°¡å–®&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;éš¨æ©Ÿè“‹ä½è¼¸å…¥å½±åƒçš„ä¸€äº› patch&lt;/li&gt;
&lt;li&gt;é‡å»º missing pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å…·å‚™å…©å€‹æ ¸å¿ƒè¨­è¨ˆ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;éå°ç¨±çš„ encoder-decoder æ¶æ§‹ï¼Œencoder åªä½œç”¨æ–¼å¯è¦‹çš„ patch å­é›†åˆ(æ²’æœ‰ mask tokens)ï¼Œlightweight decoder å‰‡æ ¹æ“š latent representation å’Œ make tokens ä¾†é‡å»ºåœ–ç‰‡ã€‚&lt;/li&gt;
&lt;li&gt;ç•¶é®ä½é«˜æ¯”ä¾‹(æ¯”å¦‚ 75%)çš„å½±åƒæ™‚ï¼Œæœƒå¾—åˆ°ä¸€å€‹ nontrivial å’Œ meaningful çš„ self-supervisory task&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;çµåˆé€™å…©é»è¨­è¨ˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¨“ç·´å¤§æ¨¡å‹ã€‚
ä»¥ ViT-Huge ç”¨ ImageNet-1K è¨“ç·´(è¨“ç·´é›†ä¸€ç™¾å¤šè¬å¼µç…§ç‰‡)å¯é”åˆ° 87.8% çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/intro.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ CV ä¸­ï¼Œå¸¸éœ€è¦å¤§é‡ labeled imagesã€‚
NLP ä¸­ï¼Œè‡ªç›£ç£é è¨“ç·´è™•ç†äº†éœ€è¦å¤§é‡æ¨™è¨»è³‡æ–™çš„å•é¡Œã€‚
masked autoencoders æ˜¯ä¸€ç¨®æ›´ general çš„ denoising autoencoders çš„å½¢å¼ã€‚
BERT éå¸¸æˆåŠŸï¼Œautoencoding methods åœ¨ CV çš„ç ”ç©¶å»è½å¾Œ NLPï¼Œä½œè€…æ€è€ƒæ˜¯ä»€éº¼è®“ masked autoencoding åœ¨ CV å’Œ NLP ç”¢ç”Ÿä¸åŒã€‚
æœ‰ä»¥ä¸‹è§€é»&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç›´åˆ°å‰é™£å­ï¼ŒCV ä¸­çš„ CNN æ˜¯ä¸»æµï¼Œä½†å·ç©å±¤ä¸å¥½å¼•å…¥ mask tokens æˆ– positional embedding é€™äº› indicatorã€‚ä½†é€™äº›å¯ä»¥é€é ViT ä¾†è§£æ±ºï¼Œä¸æ‡‰æˆç‚ºå•é¡Œã€‚&lt;/li&gt;
&lt;li&gt;èªè¨€å’Œè¦–è¦ºçš„ Information density ä¸åŒï¼Œèªè¨€æ˜¯ highly semantic å’Œ information-denseï¼Œä½¿å¡«å­—æœ¬èº«ä¸æ˜¯å¾ˆç°¡å–®çš„äº‹æƒ…ï¼Œä½†å½±åƒå«æœ‰å¤§é‡å†—é¤˜çš„è¨Šæ¯ï¼Œç¼ºå¤±çš„éƒ¨åˆ†æ¯”è¼ƒå¥½å¾ç›¸é„°çš„ patch é‡å»ºï¼Œæ¯”å¦‚ç›´æ¥æ’å€¼ï¼Œæ‰€ä»¥ä½œè€…ç”¨ä¸€ç¨®ç°¡å–®çš„ç­–ç•¥ï¼Œéš¨æ©Ÿ mask å¾ˆå¤§ä¸€éƒ¨åˆ†çš„ patchï¼Œå‰µé€ ä¸€å€‹å…·æœ‰æŒ‘æˆ°æ€§çš„è‡ªç›£ç£ä»»å‹™ï¼Œå¼·è¿«æ¨¡å‹é—œæ³¨ global çš„è³‡è¨Šã€‚&lt;/li&gt;
&lt;li&gt;é—œæ–¼ decoderï¼ŒCV é‚„åŸ pixelï¼Œpixel å±¬æ–¼ lower semantic levelï¼ŒNLP é‚„åŸ wordï¼Œword çš„ semantic information è¼ƒé«˜ã€‚ä½œè€…ç™¼ç¾ï¼Œé›–ç„¶åœ¨ BERT ä¸­ï¼Œå¯ä»¥ç”¨ç°¡å–®çš„ decoder é‚„åŸ(ä¸€å€‹ MLP)ï¼Œä½† CV ä¸­ decoder çš„è¨­è¨ˆå°±å¾ˆé‡è¦ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åŸºæ–¼ä»¥ä¸Šè§€é»ï¼Œä½œè€…æå‡º MAEï¼Œéš¨æ©Ÿé®ä½å¤§é‡çš„ patchï¼Œä¸¦åœ¨ pixel space é‡å»ºå¤±å»çš„ patchã€‚è€Œä¸”æ˜¯éå°ç¨± encoder-decoder æ¶æ§‹ï¼Œencoder åªæœƒçœ‹åˆ°å¯è¦‹çš„ patchï¼Œä½† docoder é™¤äº† latent representationï¼Œé‚„æœƒçœ‹åˆ° mask tokensã€‚é€™ç¨®è¨­è¨ˆåœ¨éå¸¸é«˜çš„æ©è“‹ç‡(æ¯”å¦‚ 75%)ä¸‹ä¸ä½†å¯ä»¥æé«˜æº–ç¢ºåº¦ï¼Œé‚„å¯ä»¥è®“ encoder åªè™•ç†è¼ƒå°‘æ¯”ä¾‹(æ¯”å¦‚ 25%)çš„ patchï¼Œå°‡è¨“ç·´æ™‚é–“æ¸›å°‘ 3 å€æˆ–æ›´å¤šï¼Œä½¿ MAE å¯ä»¥è¼•é¬†æ“´å±•æˆæ›´å¤§çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨é€™æ¨£çš„æ¶æ§‹ä¸‹ï¼Œç”¨ MAE çš„ pre-trainingï¼Œå¯ä»¥è¨“ç·´éå¸¸åƒ data çš„æ¨¡å‹ï¼Œæ¯”å¦‚ ViT-Large/-Hugeï¼Œè€Œåªä½¿ç”¨ ImageNet-1Kã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ ImageNet-1K åœ¨ vanilla ViT-Huge ä¸Š fine-tune å¯é”åˆ° 87.8% æº–ç¢ºåº¦ï¼Œæ¯”ä»¥å¾€åªä½¿ç”¨ ImageNet-1K çš„çµæœéƒ½é«˜ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ obejct detectionã€instance segmentationã€semantic segmentation ä¸Šåš transfer learning éƒ½é”åˆ°ä¸éŒ¯çš„æ•ˆæœï¼Œå¯ä»¥æ‰“æ•—ç”¨ç›£ç£å¼é è¨“ç·´æ¨¡å‹çš„å°æ‰‹ã€‚&lt;/p&gt;
&lt;h1 id=&#34;ç›¸é—œå·¥ä½œ&#34;&gt;ç›¸é—œå·¥ä½œ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoding
&lt;ul&gt;
&lt;li&gt;MAE æ˜¯ä¸€ç¨® denoising autoencoding çš„å½¢å¼ï¼Œä½†å’Œ DAE é‚„æ˜¯å·®åˆ¥å¾ˆå¤§ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masked image encoding
&lt;ul&gt;
&lt;li&gt;iGPTã€ViTã€BEiT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Masking&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å’Œ ViT ä¸€æ¨£ï¼ŒæŠŠåœ–ç‰‡åˆ‡æˆå¤šå€‹ patchï¼Œå°æ–¼ patch å‡å‹»éš¨æ©Ÿåœ°æ¡æ¨£ä¿ç•™ï¼Œå‰©ä¸‹åœ°é®ä½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViT&lt;/li&gt;
&lt;li&gt;ä¹Ÿæœ‰ positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer block&lt;/li&gt;
&lt;li&gt;è¼¸å…¥
&lt;ul&gt;
&lt;li&gt;encoded visible patches&lt;/li&gt;
&lt;li&gt;mask tokens
&lt;ul&gt;
&lt;li&gt;shared, learned vector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;éƒ½æœƒåŠ å…¥ positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç”¨ç›¸è¼ƒ encoder è¼•é‡çš„è§£ç¢¼å™¨ï¼Œæ‰€æœ‰çš„ patch ç”±é€™å€‹è¼•é‡çš„ decoder è™•ç†ï¼Œæ¸›å°‘é è¨“ç·´æ™‚é–“&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconstruction target&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decoder çš„æœ€å¾Œä¸€å±¤æ˜¯ linear projectionï¼Œä¹‹å¾Œå† reshape æˆä½ è¦çš„  patch&lt;/li&gt;
&lt;li&gt;loss function
&lt;ul&gt;
&lt;li&gt;mean squared error(MSE)&lt;/li&gt;
&lt;li&gt;åªç®— masked patched çš„ MSEï¼Œåƒ BERT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple implementation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å…ˆå–å¾—ä¸€ç³»åˆ— token(patch åš linear projection + positional embedding)&lt;/li&gt;
&lt;li&gt;randomly shuffleï¼Œæ ¹æ“šæ¯”ä¾‹ç§»é™¤å°¾ç«¯ä¸€éƒ¨ä»½&lt;/li&gt;
&lt;li&gt;encoding å¾Œï¼Œå°¾ç«¯æ¥ä¸Š mask tokensï¼Œä¸¦ä¸” unshuffle&lt;/li&gt;
&lt;li&gt;åŠ ä¸Š positional embedding å¾Œï¼Œçµ¦ decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;imagenet-experiments&#34;&gt;ImageNet Experiments&lt;/h1&gt;
&lt;p&gt;åœ¨ ImageNet-1K ä¸Šåšè‡ªç›£ç£çš„é è¨“ç·´ï¼Œç„¶å¾Œåš&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;end-to-end fine-tuning
&lt;ul&gt;
&lt;li&gt;æ‰€æœ‰åƒæ•¸éƒ½å¯æ”¹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;linear probing
&lt;ul&gt;
&lt;li&gt;åªæ”¹æœ€å¾Œä¸€å±¤ç·šæ€§å±¤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/vit-mae.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/ratio-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;optimal masking ratio æ„å¤–åœ°é«˜ï¼Œç›¸æ¯” BERT åªæœ‰ 15%&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/fine-tune-blocks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;è¨è«–å’Œçµè«–&#34;&gt;è¨è«–å’Œçµè«–&lt;/h1&gt;
&lt;p&gt;åœ¨ CV å¯¦ç”¨çš„é è¨“ç·´åšæ³•ä¸»æµæ˜¯ç›£ç£å¼çš„ï¼ŒCV ä¸­è‡ªç›£ç£çš„åšæ³•å¯èƒ½æ­£è·Ÿè‘— NLP çš„è»Œè·¡èµ°ã€‚&lt;/p&gt;
&lt;p&gt;è¦ä»”ç´°è™•ç†åœ–åƒå’Œèªè¨€çš„å€åˆ¥ï¼Œä½œè€…å»é™¤åœ–ç‰‡ä¸­å¾ˆå¯èƒ½ä¸æ§‹æˆ semantic segment çš„éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ç§»é™¤æŸå€‹ objectã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ViT è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 12 Feb 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.11929&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;åœ¨ CV é ˜åŸŸ transformer è¡¨ç¾æœ‰é™ï¼Œç›®å‰ attention å¸¸å¸¸æ˜¯å’Œå·ç©ç¥ç¶“ç¶²è·¯ä¸€èµ·ç”¨ï¼Œæˆ–æ˜¯ç”¨ä¾†æŠŠä¸€äº›å·ç©å±¤æ›æˆ self-attentionï¼Œä½†æ•´é«”æ¶æ§‹ä¸è®Šã€‚é€™ç¯‡è«–æ–‡æƒ³å±•ç¾ä¸€å€‹ç´” Transformer å¯ä»¥ç›´æ¥åœ¨å½±åƒåˆ†é¡ä¸Šè¡¨ç¾å¾ˆå¥½ã€‚å¦‚æœç”¨å¤§é‡è³‡æ–™ä½œé è¨“ç·´ï¼Œå†é·ç§»åˆ°ä¸­å°å‹çš„è³‡æ–™é›†ï¼Œå¯ä»¥å’Œ SOTA çš„ CNN è¡¨ç¾å¾—ä¸€æ¨£å¥½ï¼Œé‚„éœ€è¦è¼ƒå°‘çš„è¨“ç·´è³‡æºä½œè¨“ç·´ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;self-attention-based æ¶æ§‹ï¼Œç‰¹åˆ¥æ˜¯ Transformerï¼Œå·²ç¶“æ˜¯ NLP çš„é‡è¦é¸æ“‡ã€‚ä¸»æµçš„ä½œæ³•æ˜¯åœ¨å¤§å‹æ–‡å­—è³‡æ–™é›†ä¸Šä½œè¨“ç·´ï¼Œå†é‡å°å°å‹ä»»å‹™è³‡æ–™é›†ä½œ fine-tuneã€‚ç”±æ–¼ Transformer çš„è¨ˆç®—æ•ˆç‡é«˜ï¼Œé‚„æœ‰å¯æ“´å±•æ€§ï¼Œå¯ä»¥ train ä¸€äº›å¾ˆå¤§çš„ modelï¼Œéš¨è‘— model å’Œè³‡æ–™é›†å¢å¤§ï¼Œç›®å‰é‚„æ²’çœ‹å‡ºé£½å’Œçš„ç¾è±¡ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œåœ¨ CVï¼ŒCNN é‚„æ˜¯ä¸»æµï¼Œä¸€äº›å·¥ä½œå˜—è©¦ç”¨ self-attention çµåˆ CNN-like çš„æ¶æ§‹ï¼Œæ¯”å¦‚æŠŠ feature map ç•¶ transformer çš„è¼¸å…¥ï¼Œå› ç‚ºåŸå§‹ pixel å¤ªå¤šï¼Œæˆ–ç”šè‡³æŠŠå·ç©å±¤å…¨æ›æˆ self-attentionï¼Œé›–ç„¶å¾Œè€…ç†è«–ä¸Šæ•ˆç‡å¾ˆé«˜(åŸè«–æ–‡ä¸­æœ‰å¦å¤– cite å…©ç¯‡ä½œæ³•)ï¼Œä½†å› ç‚ºä»–å€‘åšæ³•ç‰¹æ®Šï¼Œåœ¨ç¾ä»£ç¡¬é«”ä¸Šå¾ˆé›£åŠ é€Ÿï¼Œæ‰€ä»¥ç„¡æ³•å¾ˆæœ‰æ•ˆåœ°æ“´å±•ã€‚åœ¨ large-scale çš„å½±åƒè­˜åˆ¥ä¸Šï¼Œ ResNet-like çš„æ¶æ§‹é‚„æ˜¯ SOTAã€‚&lt;/p&gt;
&lt;p&gt;è©²å¯¦é©—ç›´æ¥æŠŠä¸€å€‹æ¨™æº–çš„ Transformer ä½œç”¨æ–¼åœ–ç‰‡ä¸Šï¼Œåªä½œæœ€å°‘çš„ä¿®æ”¹ã€‚æŠŠå½±åƒåˆ†æˆå¤šå€‹ patchï¼Œä¸¦æŠŠå®ƒå€‘è®Šæˆä¸€ç³»åˆ—çš„ linear embeddingï¼Œç•¶ä½œ NLP ä¸­çš„ tokens(words) ä¾†è™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;ç•¶åœ¨ä¸­å‹å¤§å°çš„è³‡æ–™é›†(e.g. ImageNet)ä¸Šè¨“ç·´ï¼Œå¦‚æœæ²’æœ‰ strong regularizationï¼ŒViT æœƒç•¥è¼¸åŒç­‰å¤§å°çš„ ResNets&lt;/p&gt;
&lt;p&gt;é€™ç¯‡è«–æ–‡åœ¨æ›´å¤§çš„è³‡æ–™é›†(14M-300M çš„å½±åƒ)ä¸Šè¨“ç·´ï¼Œå°±æ‰“æ•—äº† inductive biasã€‚åœ¨å¤§é‡è³‡æ–™ä¸Šä½œé è¨“ç·´å°±å¾ˆè®šã€‚&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;å¤§å‹çš„ Transformer-based æ¨¡å‹å¸¸å¸¸æ˜¯å…ˆåœ¨å¤§è³‡æ–™é›†ä¸Šé è¨“ç·´ç„¶å¾Œæ ¹æ“šä»»å‹™ fine-tuneï¼Œæ¯”å¦‚ BERT å’Œ GPTã€‚&lt;/p&gt;
&lt;p&gt;è¦æŠŠ self-attention ç”¨åœ¨ CV ä¸Šï¼Œæœ€ç°¡å–®çš„åšæ³•å°±æ˜¯æŠŠæ¯å€‹ Pixel ç•¶ä¸€å€‹å…ƒç´ ï¼Œä½† self-attention æ˜¯å¹³æ–¹è¤‡é›œåº¦ï¼Œåœ¨ç¾å¯¦çš„åœ–ç‰‡å¾ˆé›£æ‡‰ç”¨ã€‚ä¸€å€‹æ‡‰ç”¨ Transformer çš„åšæ³•æ˜¯åªæŠŠ self-attention ç”¨åœ¨ local neighborhoodï¼Œå¦å¤–ä¸€å€‹æ˜¯ç”¨ Sparse Transformerï¼Œé‚„æœ‰ä¸€å †ç‰¹æ®Šçš„æ–¹æ³•ï¼Œé›–ç„¶è¡¨ç¾ä¸éŒ¯ï¼Œä½†è¦ç”¨ç¡¬é«”åŠ é€Ÿèµ·ä¾†ä¸å®¹æ˜“ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹æœ‰é—œçš„æ¨¡å‹æ˜¯ iGPTï¼Œåœ¨ reduce image resolution å’Œ color space å¾ŒæŠŠ transformer æ‡‰ç”¨åœ¨ image pixels ä¸Šã€‚å®ƒç”¨éç›£ç£å¼è¨“ç·´å¾Œï¼Œå† fine-tune æˆ–åš linear probing(åªæ›´æ–°æœ€å¾Œçš„ linear layer) åˆ†é¡ä»»å‹™ï¼Œè¡¨ç¾å¾ˆå¥½ã€‚&lt;/p&gt;
&lt;p&gt;å·²ç¶“æœ‰é¡ä¼¼çš„å·¥ä½œäº†ï¼ŒæŠ½å– patches of size 2 * 2ï¼Œæœ€å¾Œå†æ¥ full self-attentionï¼ŒåŸºæœ¬ä¸Šå’Œ ViT éå¸¸åƒï¼Œé€™ç¯‡è«–æ–‡é€²ä¸€æ­¥è­‰æ˜äº†ä½œå¤§è¦æ¨¡çš„é è¨“ç·´å¯ä»¥è®“ Transformer å’Œ SOTA çš„ CNN ç›¸æ¯”ï¼Œè€Œä¸” ViT å› ç‚º patch æ¯”è¼ƒå¤§ï¼Œå¯ä»¥è™•ç† medium-resolution çš„åœ–ç‰‡ã€‚é€™å•é¡Œæ˜¯å¯é æœŸçš„ï¼Œå› ç‚º Transformer ç¼ºå°‘äº†ä¸€äº› inductive biasesã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inductive biases
&lt;ul&gt;
&lt;li&gt;ä¸€äº›å‡è¨­&lt;/li&gt;
&lt;li&gt;æ¯”å¦‚ CNN å¸¸æœ‰å››å€‹å‡è¨­
&lt;ul&gt;
&lt;li&gt;locality&lt;/li&gt;
&lt;li&gt;translation invariance with pooling layers
&lt;ul&gt;
&lt;li&gt;å¹³ç§»ä¸è®Šæ€§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation equivariance
&lt;ul&gt;
&lt;li&gt;f(g(x)) = g(f(x))&lt;/li&gt;
&lt;li&gt;å·ç©å’Œå¹³ç§»çš„å…ˆå¾Œé †åºæ²’å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;æ¨¡å‹ç›¡å¯èƒ½é¡ä¼¼åŸå§‹ Transformerï¼Œé€™æ¨£å¯ä»¥æŠŠä¸€äº› NLP ä¸ŠæˆåŠŸçš„ Transformer æ¶æ§‹æ‹¿ä¾†ç”¨ï¼Œé‚„å¯ä»¥ç”¨ä¸€äº›å¾ˆæœ‰æ•ˆç‡çš„ implementation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-process.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;embedding ç¶­åº¦æ˜¯ 768 = 16 * 16 * 3
position embedding çš„åšæ³•æ˜¯ standard learnable 1D positional embeddingsï¼Œå°±æ˜¯ BERT çš„åšæ³•ï¼Œç°¡å–®ä¾†èªªå°±æ˜¯ç”Ÿå‡ºä¸€å¼µå¯ä»¥è¨“ç·´çš„è¡¨ï¼Œ(åºåˆ—é•·åº¦, embedding size)ï¼Œä½œè€…ä¹Ÿæœ‰å˜—è©¦å…¶ä»–æ–¹æ³•ï¼Œä½†ç™¼ç¾æˆæ•ˆå·®ä¸å¤šï¼Œæ¯”å¦‚ 2D positional embeddingï¼Œæ¦‚å¿µå°±æ˜¯å¾ç”Ÿå‡º(åºåˆ—é•·åº¦, embedding size)è®Šæˆç”Ÿå‡º 2 å€‹(sqrt(åºåˆ—é•·åº¦), embedding size)ã€‚&lt;/p&gt;
&lt;p&gt;[class] çš„æ¦‚å¿µæ˜¯ NLP å‡ºä¾†çš„ï¼ŒResNet-like çš„æ¶æ§‹å¸¸è¦‹çš„åšæ³•ä¹Ÿæœ‰é€šé globally average-pooling (GAP)ä¾†ç”Ÿå‡ºå‘é‡ï¼Œå†æ¥ä¸Šåˆ†é¡å™¨åšé æ¸¬ã€‚å¯¦é©—ç™¼ç¾ç›´æ¥åœ¨ transformer çš„è¼¸å‡ºåš GAP å’Œ [class] éƒ½å¯ä»¥é”åˆ°ä¸éŒ¯çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-gap.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-acc.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;æ‹¿æ¨™æº–çš„ Transformer ä¾†ä½œ Image recognitionï¼Œå’Œä»¥å¾€ç”¨ self-attention åœ¨ CV çš„æ–¹æ³•ä¸ä¸€æ¨£ï¼Œé™¤äº†ä¸€é–‹å§‹çš„ initial patch extractionï¼Œæ²’æœ‰å¼•å…¥å…¶ä»–å½±åƒç‰¹æœ‰çš„ inductive biasesã€‚ç›´æ¥æŠŠåœ–ç‰‡ç•¶æˆæ˜¯ä¸€ç³»åˆ—çš„ patchï¼Œç„¶å¾Œç›´æ¥ç”¨ Transformer encoder ç•¶ä¸€èˆ¬ NLP ä»»å‹™è™•ç†ã€‚åœ¨å¾ˆå¤šå½±åƒåˆ†é¡è¨“ç·´é›†ä¸Šè¡¨ç¾å¾—æ›´å¥½é‚„åœ¨ pre-train ä¸Šç›¸å°ä¾¿å®œã€‚&lt;/p&gt;
&lt;p&gt;é‚„æœ‰ä¸€äº›å€¼å¾—æŒ‘æˆ°çš„åœ°æ–¹ï¼Œæ¯”å¦‚æŠŠ ViT æ‡‰ç”¨åœ¨å…¶ä»– CV ä»»å‹™ï¼Œæ¯”å¦‚ detection å’Œ segmentationã€‚å¦ä¸€å€‹æŒ‘æˆ°æ˜¯æ¢ç´¢è‡ªç›£ç£é è¨“ç·´çš„æ–¹æ³•ã€‚é€™ç¯‡è«–æ–‡å…¶å¯¦æœ‰å¯¦é©—è‡ªç›£ç£ï¼Œè¡¨ç¾ OKï¼Œä½†å’Œç›£ç£å¼é‚„æ˜¯æœ‰å¾ˆå¤§çš„è½å·®ã€‚æ“´å¤§ ViT å¯èƒ½æœ‰æ›´å¥½çš„çµæœã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>InstructGPT</title>
        <link>https://roykesydon.github.io/Blog/p/instructgpt/</link>
        <pubDate>Fri, 27 Jan 2023 17:39:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/instructgpt/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;æŠŠèªè¨€æ¨¡å‹è®Šå¤§ä¸ä»£è¡¨ä»–å€‘æœƒæ›´å¥½åœ°éµå¾ªç”¨æˆ¶çš„æ„åœ–ã€‚&lt;/p&gt;
&lt;p&gt;å¤§çš„èªè¨€æ¨¡å‹æœ‰å¯èƒ½æœƒç”Ÿæˆ untruthful, toxic, not helpful çš„ç­”æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;è©²è«–æ–‡é€é fine-tuning with human feedback ä¾†è§£æ±ºé€™å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ä¸€é–‹å§‹æº–å‚™ä¸€ç³»åˆ—äººå·¥æ¨™è¨»çš„ promptsï¼Œç„¶å¾Œç”¨é€™ dataset å° GPT-3 åš fine-tuneã€‚&lt;/p&gt;
&lt;p&gt;æ¥ä¸‹ä¾†å†è’é›†ä¸€å€‹ datasetï¼Œå­˜æ”¾ rankings of model outputsï¼Œç”±äººå·¥åˆ¤æ–·è¼¸å‡ºå¥½å£ï¼Œå†ç”¨ RL æŠŠå‰›å‰› fine-tune éçš„ model ç¹¼çºŒ fine-tuneã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œæœ‰ 1.3B åƒæ•¸çš„ InstructGPT è¡¨ç¾çš„çµæœæ¯” 175B åƒæ•¸çš„ GPT-3 é‚„å¥½ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Large language models(LMs) å¯ä»¥é€é &amp;ldquo;prompt&amp;rdquo; ä¾†åŸ·è¡Œå„ç¨® NLP ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ä½†é€™äº›æ¨¡å‹ä¹Ÿå¸¸æœ‰ä¸€äº›éç›®çš„æ€§çš„è¡Œç‚ºï¼Œè«¸å¦‚æé€ äº‹å¯¦ç­‰ç­‰ã€‚&lt;/p&gt;
&lt;p&gt;åŸå› æ˜¯å‡ºåœ¨ç›®æ¨™å‡½æ•¸ä¸Šï¼Œå¤šæ•¸ LMs çš„ç›®æ¨™å‡½æ•¸æ˜¯æ ¹æ“šç¶²è·¯ä¸Šçš„æ–‡æœ¬ç”Ÿå‡ºä¸‹ä¸€å€‹å­—è©ã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œã€Œæ ¹æ“šä½¿ç”¨è€…æŒ‡ä»¤ç”Ÿå‡ºå®‰å…¨ä¸”æœ‰å¹«åŠ©çš„ç­”æ¡ˆä¸åŒã€ã€‚&lt;/p&gt;
&lt;p&gt;ä¸Šè¿°çš„å·®ç•°ä½¿èªè¨€æ¨¡å‹çš„ç›®æ¨™æ˜¯ misalignedã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„ç›®æ¨™æ˜¯ç”Ÿå‡º helpfulã€ honest(æ²’æœ‰èª¤å°æ€§è³‡è¨Š)ã€harmless çš„ modelã€‚&lt;/p&gt;
&lt;p&gt;å…·é«”ä½œæ³•ï¼Œä½¿ç”¨ reinforcement learning from human feedback(RLHF)ã€‚&lt;/p&gt;
&lt;h2 id=&#34;è¨“ç·´æ­¥é©Ÿ&#34;&gt;è¨“ç·´æ­¥é©Ÿ&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-train-step.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;çµæœ&#34;&gt;çµæœ&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labelers æ˜é¡¯åå¥½ InstructGPT çš„ç­”æ¡ˆï¼Œå‹é GPT-3 çš„ç­”æ¡ˆ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT çš„ç­”æ¡ˆåœ¨ truthfulness å‹é GPT-3 çš„ç­”æ¡ˆ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT çš„ç­”æ¡ˆåœ¨ toxicity ä¸Šå°å‹ GPT-3 çš„ç­”æ¡ˆï¼Œä½†åœ¨ bias ä¸Šæ²’æœ‰&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;methods&#34;&gt;Methods&lt;/h1&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;æ¨™è¨»äººå“¡å¯«å¾ˆå¤š prompts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plain:
&lt;ul&gt;
&lt;li&gt;éš¨ä¾¿å¯«ä»»æ„ä»»å‹™&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Few-shot:
&lt;ul&gt;
&lt;li&gt;æƒ³å€‹ instructionï¼Œä¸¦å¯« multiple query/response pairs for that instruction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User-based:
&lt;ul&gt;
&lt;li&gt;æ ¹æ“šä¸€äº›ç”³è«‹ä½¿ç”¨ OpenAI API çš„ç”¨æˆ¶ï¼Œæå‡ºæœ‰é—œçš„ prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç„¶å¾Œæ ¹æ“šé€™å€‹è¨“ç·´åˆæ­¥æ¨¡å‹ï¼Œä¸¦æŠŠé€™å€‹åˆæ­¥æ¨¡å‹æ”¾åˆ°ä»–å€‘çš„ Playground çµ¦ç”¨æˆ¶ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;å†æŠŠç”¨æˆ¶å•çš„å•é¡Œè’é›†å›ä¾†ï¼Œä¸¦åšç¯©é¸ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´ SFT çš„æ¨¡å‹ç”¨ 13k training prompts&lt;/p&gt;
&lt;p&gt;è¨“ç·´ RM çš„æ¨¡å‹ç”¨ 33k training prompts&lt;/p&gt;
&lt;p&gt;è¨“ç·´ PPO çš„æ¨¡å‹ç”¨ 31k training prompts&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Supervised fine-tuning(SFT)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ‹¿ GPT-3 å»è¨“ç·´ 16 å€‹ epochs&lt;/li&gt;
&lt;li&gt;è·‘ä¸€å€‹ epoch å°±ç™¼ç¾ overfittingï¼Œä½†ç™¼ç¾è¨“ç·´æ›´å¤š epoches å°å¾Œé¢çš„ RM æœ‰ç”¨ï¼Œè€Œä¸”é€™å€‹ model ä¹Ÿåªæ˜¯éæ¸¡ç”¢å“&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward modeling(RM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æŠŠ SFT å¾Œé¢çš„ unembedding layer å»é™¤æ‰ï¼Œæ¥ä¸Šç·šæ€§å±¤ï¼Œæœ€å¾Œè¼¸å‡ºä¸€å€‹ scalar reward&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”¨ 6B RMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é€™æ¨¡å‹æœƒåƒ prompt å’Œ response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;äººå·¥æ¨™è¨˜çš„æ˜¯æ’åºï¼Œä¸æ˜¯åˆ†æ•¸&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å°æ¯å€‹ prompt ç”Ÿå‡º 9 å€‹ç­”æ¡ˆ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åŸæœ¬æ˜¯ 4 å€‹ï¼Œä½†æ’ 9 å€‹èŠ±çš„æ™‚é–“å¯èƒ½ä¸æœƒåˆ° 4 å€‹çš„å…©å€ï¼Œå› ç‚ºä¸»è¦å¿ƒåŠ›æœƒèŠ±åœ¨è®€ promptã€‚ä½†æ¨™è¨»è¨Šæ¯æœƒå¤šå¾ˆå¤šï¼Œå› ç‚ºéƒ½æ˜¯å…©å…©æ¯”è¼ƒã€‚&lt;/li&gt;
&lt;li&gt;è€Œä¸”åœ¨ loss ä¸­æœ€å¤šåªè¦ä¸Ÿå…¥ RM 9 æ¬¡ï¼Œå› ç‚ºå¯ä»¥é‡ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pairwise Ranking Loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å°ä¸€å€‹ prompt(å‡è¨­æ˜¯ x)ï¼Œå–å‡ºä¸€å°å›è¦†(å‡è¨­æ˜¯ $y_w$ å’Œ $y_l$)ï¼Œç®—å‡º RM(x, $y_w$) å’Œ RM(x, $y_l$)ï¼Œå‡è¨­ $y_w$ æ¯” $y_l$ æ’åºé«˜ï¼Œè®“ RM(x, $y_w$) - RM(x, $y_l$) çš„æ•¸å€¼è¶Šå¤§è¶Šå¥½&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-reward-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement learning(RL)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-rl-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\beta$ é‚£é …æ˜¯ KL divergence&lt;/li&gt;
&lt;li&gt;$\gamma$ é‚£é …æ˜¯ä¸æƒ³è¦è®“é€™ model å¤ªå°ˆæ³¨åœ¨å¾®èª¿çš„ä»»å‹™ï¼Œè€Œå¤±å»åŸæœ¬åœ¨å…¶ä»– NLP ä»»å‹™ä¹Ÿè¡¨ç¾å¾ˆå¥½çš„åŠŸèƒ½ã€‚
&lt;ul&gt;
&lt;li&gt;$D_{pretrain}$ æ˜¯ pretraining distribution&lt;/li&gt;
&lt;li&gt;å¦‚æœ $\gamma$ ç‚º 0ï¼Œåœ¨è©²å¯¦é©—ä¸­å«åš PPOï¼Œå¦å‰‡ï¼Œç¨±ç‚º PPO-ptx&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GPT ä¸‰éƒ¨æ›²</title>
        <link>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</link>
        <pubDate>Thu, 19 Jan 2023 01:50:07 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</guid>
        <description>&lt;p&gt;GPT æœ¬è³ªä¸Šå°±æ˜¯ Transformer çš„ decoder&lt;/p&gt;
&lt;h1 id=&#34;gpt-1&#34;&gt;GPT-1&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ç”¨ semi-supervisedï¼Œå¾Œä¾†è¢«æ­¸ç‚º self-supervised&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h2&gt;
&lt;p&gt;$L_1(U)=\sum_i logP(u_i|u_{i-k},&amp;hellip;,u_{i-1};\theta)$&lt;/p&gt;
&lt;p&gt;$U= \{ u_1,&amp;hellip;,u_n \}$&lt;/p&gt;
&lt;p&gt;$U$ æ˜¯ä¸€ç³»åˆ—æœªæ¨™è¨˜çš„æ–‡æœ¬ token&lt;/p&gt;
&lt;p&gt;$k$ æ˜¯çª—å£å¤§å°&lt;/p&gt;
&lt;h3 id=&#34;æ¨¡å‹å¤§è‡´æ¶æ§‹&#34;&gt;æ¨¡å‹å¤§è‡´æ¶æ§‹&lt;/h3&gt;
&lt;p&gt;$h_0=UW_e+W_p$&lt;/p&gt;
&lt;p&gt;$h_1=transformer \_ block(h_{i-1})\forall i \in[1,n]$&lt;/p&gt;
&lt;p&gt;$P(u)=softmax(h_nW^T_e)$&lt;/p&gt;
&lt;p&gt;$U=\{u_{-k},&amp;hellip;,u_{-1}\}$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h2&gt;
&lt;p&gt;$P(y|x^1,&amp;hellip;,x^m)=softmax(h^m_lW_y)$&lt;/p&gt;
&lt;p&gt;$L2(C)=\sum_{(x,y)}log P(y|x^1,&amp;hellip;,x^m)$&lt;/p&gt;
&lt;p&gt;$L_3(C)=L_2(C)+\lambda*L_1(C)$&lt;/p&gt;
&lt;p&gt;$C$ æ˜¯ labeled çš„è³‡æ–™é›†ï¼Œå¾®èª¿åŸºæœ¬ä¸Šå°±æ˜¯åœ¨å¾Œé¢åŠ ä¸Šç·šæ€§å±¤&lt;/p&gt;
&lt;p&gt;ä½œè€…æœ€å¤§åŒ– likelihood çš„æ™‚å€™æ˜¯ç”¨ $L_3$ è€Œéå–®ç´”çš„ $L_2$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;å¾®èª¿æ‡‰ç”¨ç¯„ä¾‹&#34;&gt;å¾®èª¿æ‡‰ç”¨ç¯„ä¾‹&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-1-tasks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;è³‡æ–™é›†&#34;&gt;è³‡æ–™é›†&lt;/h2&gt;
&lt;p&gt;ç”¨ BooksCorpus è¨“ç·´å‡ºä¾†çš„&lt;/p&gt;
&lt;p&gt;æœ‰è¶…é 7000 æœ¬æœªå‡ºç‰ˆçš„æ›¸&lt;/p&gt;
&lt;h2 id=&#34;æ¨¡å‹çµæ§‹&#34;&gt;æ¨¡å‹çµæ§‹&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;12 å±¤ transformer çš„ decoder&lt;/li&gt;
&lt;li&gt;768 ç¶­ word embedding&lt;/li&gt;
&lt;li&gt;12 å€‹ attention heads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;å’Œ-bert-base-æ¯”è¼ƒ&#34;&gt;å’Œ BERT BASE æ¯”è¼ƒ&lt;/h2&gt;
&lt;p&gt;BERT è«–æ–‡æ¯”è¼ƒæ™šå‡ºï¼Œä½† BASE çš„æ¨¡å‹æ¶æ§‹å’Œ GPT æœ‰ç›¸ä¼¼ä¹‹è™•ï¼Œ&lt;/p&gt;
&lt;p&gt;BASE æ˜¯ 12 å±¤çš„ decoderï¼Œword embedding å’Œ attention head çš„ç¶­åº¦æˆ–æ•¸é‡å’Œ GPT-1 ç›¸åŒ&lt;/p&gt;
&lt;h1 id=&#34;gpt-2&#34;&gt;GPT-2&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/paper/language-models-are-unsupervised-multitask&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Unsupervised Multitask Learner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GPT-2 é™¤äº†ç”¨æ›´å¤§çš„çš„æ¨¡å‹å’Œæ›´å¤§çš„è³‡æ–™é›†ï¼ŒæŠŠé‡é»æ”¾åœ¨ zero-shot ä¸Šï¼Œé›–ç„¶åœ¨ GPT-1 çš„è«–æ–‡å°±æœ‰æé zero-shot&lt;/p&gt;
&lt;h2 id=&#34;è³‡æ–™é›†-1&#34;&gt;è³‡æ–™é›†&lt;/h2&gt;
&lt;p&gt;é€™æ¬¡åšäº†ä¸€å€‹å«åš WebText çš„è³‡æ–™é›†ï¼Œæœ‰ç™¾è¬ç´šåˆ¥çš„ç¶²é &lt;/p&gt;
&lt;h3 id=&#34;common-crawl&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;å¤§å‹çˆ¬èŸ²å°ˆæ¡ˆï¼Œæœ‰å¤§é‡ç¶²é è³‡æ–™ï¼Œä½†å……æ–¥äº†åƒåœ¾è¨Šæ¯&lt;/p&gt;
&lt;h3 id=&#34;webtext&#34;&gt;WebText&lt;/h3&gt;
&lt;p&gt;WebText çš„è³‡æ–™ä¾†æºæ˜¯ reddit ä¸Šçš„å¤–éƒ¨é€£çµï¼Œåªè¦æœ‰è‡³å°‘ä¸‰å€‹ karmaï¼Œå°±æœƒè¢«æ¡ç´ï¼Œç”±æ­¤å–å¾—å“è³ªè¼ƒå¥½çš„ç¶²é è³‡æ–™ã€‚é€éé€™ç¨®æ–¹æ³•ï¼Œå–å¾—äº† 4500 è¬å€‹é€£çµã€‚ä¸¦ç”¨Dragnet (Peters &amp;amp; Lecocq, 2013) and Newspaper content extractors æŠŠæ–‡å­—è¨Šæ¯å¾ HTML ä¸­æŠ“å‡ºä¾†&lt;/p&gt;
&lt;h2 id=&#34;æ¶æ§‹&#34;&gt;æ¶æ§‹&lt;/h2&gt;
&lt;p&gt;å’ŒåŸæœ¬å·®ä¸å¤šï¼Œè®Šæˆæœ‰ 1.5B åƒæ•¸çš„ Transformer decoder&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h2&gt;
&lt;p&gt;ä¸éœ€è¦ä¸‹æ¸¸ä»»å‹™çš„æ¨™è¨˜è³‡æ–™&lt;/p&gt;
&lt;p&gt;æ”¹æŠŠä»»å‹™è¼¸å…¥é€²æ¨¡å‹&lt;/p&gt;
&lt;h3 id=&#34;ç›®å‰å•é¡Œ&#34;&gt;ç›®å‰å•é¡Œ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ç¾åœ¨çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸å¤ªå¥½&lt;/li&gt;
&lt;li&gt;Multitask learning
åœ¨ NLP ä¸Šä¸å¤ªå¸¸ç”¨ï¼ŒNLP ç¾åœ¨ä¸»æµé‚„æ˜¯åœ¨é è¨“ç·´æ¨¡å‹ä¸Šåšå¾®èª¿ä»¥æ‡‰å°ä¸‹æ¸¸ä»»å‹™
&lt;ul&gt;
&lt;li&gt;å°æ¯å€‹ä¸‹æ¸¸ä»»å‹™éƒ½å¾—é‡æ–°è¨“ç·´æ¨¡å‹&lt;/li&gt;
&lt;li&gt;å¾—è’é›† labeled è³‡æ–™&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;çµæœ&#34;&gt;çµæœ&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;gpt-3&#34;&gt;GPT-3&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;æ‘˜è¦&#34;&gt;æ‘˜è¦&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;æœ‰ 175B çš„åƒæ•¸ï¼Œç”±æ–¼æ¨¡å‹æ¥µå¤§ï¼Œè¦åœ¨å­ä»»å‹™å¾®èª¿æœƒæˆæœ¬å¾ˆå¤§ï¼Œæ‰€ä»¥ä¸åšä»»ä½•æ¢¯åº¦æ›´æ–°&lt;/li&gt;
&lt;li&gt;åœ¨å¾ˆå¤š NLP ä»»å‹™æœ‰å‚‘å‡ºçš„æˆæœ&lt;/li&gt;
&lt;li&gt;å¯ä»¥ç”Ÿå‡ºäººé¡é›£ä»¥å€åˆ†çš„æ–°èæ–‡ç« &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ç›®å‰æœ‰çš„å•é¡Œ&#34;&gt;ç›®å‰æœ‰çš„å•é¡Œ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;è¦åœ¨å­ä»»å‹™å¾®èª¿ï¼Œéœ€è¦è³‡æ–™é›†&lt;/li&gt;
&lt;li&gt;å¾®èª¿å¾Œåœ¨æœ‰äº›å­ä»»å‹™ä¸Šè¡¨ç¾å¥½ä¸ä»£è¡¨ä½ é è¨“ç·´æ¨¡å‹ä¸€å®šæ³›åŒ–èƒ½åŠ›é«˜&lt;/li&gt;
&lt;li&gt;äººé¡ä¸éœ€è¦å¤§é‡ labeled è³‡æ–™å»å®Œæˆå°ä»»å‹™&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è©•ä¼°æ–¹å¼&#34;&gt;è©•ä¼°æ–¹å¼&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;åˆ†ç‚ºä¸‰ç¨®ï¼Œfew / one / zero-shot learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;æ¶æ§‹-1&#34;&gt;æ¶æ§‹&lt;/h2&gt;
&lt;p&gt;åŸºæœ¬ä¸Š GPT-3 å’Œ GPT-2 æ¶æ§‹ä¸€æ¨£&lt;/p&gt;
&lt;h3 id=&#34;ç›¸åŒ&#34;&gt;ç›¸åŒ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;modified initialization&lt;/li&gt;
&lt;li&gt;pre-normalization&lt;/li&gt;
&lt;li&gt;reversible tokenization described therein&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ä¸åŒ&#34;&gt;ä¸åŒ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠ Sparse Transformer çš„ä¸€äº›ä¿®æ”¹æ‹¿éä¾†ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;GPT-3 Small æ˜¯ GPT-1 çš„å¤§å°
GPT-3 Medium æ˜¯ BERT Large çš„å¤§å°
GPT-3 XL å’Œ GPT-2 ç›¸è¿‘ï¼Œæ¯”è¼ƒæ·ºä¹Ÿæ¯”è¼ƒå¯¬&lt;/p&gt;
&lt;h4 id=&#34;batch-size-å¤§å°&#34;&gt;Batch Size å¤§å°&lt;/h4&gt;
&lt;p&gt;æ¨¡å‹å°çš„æ™‚å€™éœ€è¦å°ä¸€é»ï¼Œé€éé€™ç¨®é¡å¤–çš„ noise ä¾†é¿å… overfitting(ä¸ç¢ºå®šæ˜¯ä¸æ˜¯çŒœæƒ³)&lt;/p&gt;
&lt;h2 id=&#34;è³‡æ–™é›†-2&#34;&gt;è³‡æ–™é›†&lt;/h2&gt;
&lt;h3 id=&#34;common-crawl-1&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;æ¶æ§‹æ¯” GPT-2 å¤§å¾ˆå¤šï¼Œæ‰€ä»¥å›é ­è€ƒæ…®é€™å€‹è³‡æ–™é›†&lt;/p&gt;
&lt;h4 id=&#34;ä¸‰æ­¥é©Ÿ&#34;&gt;ä¸‰æ­¥é©Ÿ&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;å…ˆéæ¿¾ï¼Œé€é reddit é‚£å€‹é«˜å“è³ªçš„è³‡æ–™é›†ï¼Œä¾†è¨“ç·´ä¸€å€‹æ¨¡å‹åˆ†é¡é«˜å“è³ªå’Œä½å“è³ªçš„ç¶²é ã€‚&lt;/li&gt;
&lt;li&gt;é€é LSH æ¼”ç®—æ³•æŠŠç›¸ä¼¼çš„æ–‡æœ¬éæ¿¾æ‰&lt;/li&gt;
&lt;li&gt;æŠŠä¸€äº›å·²çŸ¥é«˜å“è³ªçš„è³‡æ–™é›†ä¹ŸåŠ é€²ä¾†&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;é€™æ˜¯ä¸€å€‹ Batch è£¡æœ‰ 60% ä¾†è‡ª Common Crawl(filtered) çš„æ„æ€
Wikipedia é›–ç„¶ç¸½é‡æ¯”è¼ƒå°‘ï¼Œä½†ä¹Ÿæœ‰ 3% çš„æ¡æ¨£ç‡&lt;/p&gt;
&lt;h2 id=&#34;çµæœ-1&#34;&gt;çµæœ&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;è¨ˆç®—é‡æŒ‡æ•¸å¢é•·ï¼Œloss å»æ˜¯ç·šæ€§çš„å¾€ä¸‹é™&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;paper è£¡æœ‰å¾ˆå¤šä»»å‹™çš„å¯¦é©—çµæœï¼Œé€™é‚Šå°±ä¸é™„ä¸Šäº†&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;åœ¨æ–‡æœ¬ç”Ÿæˆä¸Šé‚„æ˜¯æ¯”è¼ƒå¼±ï¼Œç”Ÿå¾ˆé•·çš„æ±è¥¿ï¼Œå¯èƒ½æœƒé‡è¤‡è‡ªå·±èªªéçš„è©±ã€å¤±å»é€£è²«æ€§ã€è‡ªç›¸çŸ›ç›¾ç­‰ç­‰&lt;/p&gt;
&lt;p&gt;åœ¨æœ‰äº›é›™å‘æ€§çš„ä»»å‹™ä¸Šå¯èƒ½è¡¨ç¾æ›´å·®&lt;/p&gt;
&lt;h2 id=&#34;å½±éŸ¿&#34;&gt;å½±éŸ¿&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;å¯èƒ½è¢«ç”¨ä¾†æ•£å¸ƒä¸å¯¦æ¶ˆæ¯ã€åƒåœ¾éƒµä»¶ç­‰ç­‰&lt;/li&gt;
&lt;li&gt;åè¦‹&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;çµè«–&#34;&gt;çµè«–&lt;/h2&gt;
&lt;p&gt;åœ¨å¾ˆå¤š NLP ä»»å‹™å¯ä»¥åšåˆ°æ¥è¿‘ SOTA å¾®èª¿æ¨¡å‹çš„æˆæœ&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
