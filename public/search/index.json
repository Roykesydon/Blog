[{"content":"linear equation $a_1x_1+a_2x_2+\u0026hellip;+a_nx_n = b$ $a$ 是 coefficient $x$ 是 variables $b$ 是 constant term Systems of linear equations m equations, n variables\n$a_{11}x_1+a_{12}x_2+\u0026hellip;+a_{1n}x_n = b_1\\\\ a_{21}x_1+a_{22}x_2+\u0026hellip;+a_{2n}x_n = b_2\\\\ \u0026hellip;\\\\ a_{m1}x_1+a_{m2}x_2+\u0026hellip;+a_{mn}x_n = b_m$\nsolution\n$[s_1~s_2~\u0026hellip;~s_n]^T$ 是一組解，代換到 $x_1$~$x_n$ 後滿足所有 equation 的向量 所有 Systems of linear equations 都有\nno solution exactly one solution infinitely many solutions consistent/inconsistent\n如果有一組以上的解就是 consistent 無解就是 inconsistent equivalent\n如果兩組 Systems of linear equations 的 solution set 一樣，稱為 equivalent elementary row operations\n不會影響 solution set types Interchange 兩 row 互換 Scaling 某 row 乘某個 nonzero scalar Row addition 把某 row 乘某個 scalar 後加到某 row property 所有 elementary row operations 都是 reversible 用來求解 coefficient matrix\n$a_{11}x_1+a_{12}x_2+\u0026hellip;+a_{1n}x_n = b_1\\\\ a_{21}x_1+a_{22}x_2+\u0026hellip;+a_{2n}x_n = b_2\\\\ \u0026hellip;\\\\ a_{m1}x_1+a_{m2}x_2+\u0026hellip;+a_{mn}x_n = b_m$ 可以拆為 $Ax=b$\n$A=\\begin{bmatrix} a_{11}\u0026amp; a_{12}\u0026amp;\u0026hellip;\u0026amp; a_{1n} \\\\ a_{21}\u0026amp; a_{22}\u0026amp;\u0026hellip;\u0026amp; a_{2n} \\\\ \u0026hellip;\u0026amp; \u0026hellip;\u0026amp;\u0026hellip;\u0026amp; \u0026hellip; \\\\ a_{m1}\u0026amp; a_{m2}\u0026amp;\u0026hellip;\u0026amp; a_{mn} \\end{bmatrix}$\nA 就是 coefficient matrix $x=\\begin{bmatrix} x_1\\\\ x_2\\\\ \u0026hellip;\\\\ x_n \\end{bmatrix}$\n$x$ 是 variable vector $[A|b]=\\begin{bmatrix} a_{11}\u0026amp; a_{12}\u0026amp;\u0026hellip;\u0026amp; a_{1n} \u0026amp; b_1 \\\\ a_{21}\u0026amp; a_{22}\u0026amp;\u0026hellip;\u0026amp; a_{2n} \u0026amp; b_2\\\\ \u0026hellip;\u0026amp; \u0026hellip;\u0026amp;\u0026hellip;\u0026amp; \u0026hellip; \u0026amp; \u0026hellip;\\\\ a_{m1}\u0026amp; a_{m2}\u0026amp;\u0026hellip;\u0026amp; a_{mn} \u0026amp; b_m \\end{bmatrix}$\n叫做 augmented matrix ","date":"2023-02-21T15:42:47+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-ii/","title":"線性代數 - II"},{"content":"matrix a rectangular array of scalars\nsize\nm by n 叫做 square if m = n equal\n兩個矩陣的 size 和每個 entry 都一樣 submatrix\n從一個大矩陣刪掉 rows 或 columns addition\n兩個大小相同的矩陣，每個對應位置的 entry 兩兩相加 scalar multiplication\n一個矩陣的所有 entry 乘以某個 scalar zero matrix\n所有 entry 都是 0，該矩陣常以 $O_{n \\times m}$ 來表示 性質 $A = O + A$ $0 \\cdot A = O $ subtraction\n$A-B=A+(-B)$ transpose\n$A^T$ 的 $(i,j)$-entry 是 $A$ 的 $(j,i)$-entry Properties $(A+B)^T=A^T+B^T$ $(sA)^T=sA^T$ $(A^T)^T=A$ vectors type\nrow vector 只有 1 row 的 matrix column vector 只有 1 column 的 matrix components\nthe entries of a vector 用 the $i$ th component 代表 $v_i$ addition, scalar multiplication\n和 matrix 一樣 矩陣表示\n一個矩陣常被表示為 a stack of row vectors a cross list of column vectors linear combination $c_1u_1+c_2u_2+\u0026hellip;+c_ku_k$\nscalars\n$c_1,c_2,\u0026hellip;,c_k$ 又被稱作 linear combination 的 coefficients vectors\n$u_1,u_2,\u0026hellip;,u_k$ 如果 $u,v$ 非平行二維向量，則二維空間中所有向量皆是 $u,v$ 的 linear combination，且是 unique 的\nstandard vectors $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \u0026hellip; \\\\ 0 \\end{bmatrix} ,e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ \u0026hellip; \\\\ 0 \\end{bmatrix},\u0026hellip;, e_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ \u0026hellip; \\\\ 1 \\end{bmatrix}$\n$R^n$ 的任何一個向量都可以被 standard vectors 表示成 uniquely linearly combined\n矩陣向量乘法 $Av=v_1a_1+v_2a_2+\u0026hellip;+v_na_n$ Identity Matrix 對整數 n，$n \\times n$ identity matrix $I_n$ 每個 columns 是 standard vectors $e_1, e_2, \u0026hellip;, e_n$ in $R^n$ Stochastic Matrix 對整數 n，$n \\times n$ stochastic matrix 所有 entry 都必須非負 每個 column 的 entry 總和必須是 unity (相加為 1) ","date":"2023-02-21T14:42:47+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8-i/","title":"線性代數 - I"},{"content":"Process Scheduling 可能時機\nrunning -\u0026gt; waiting running -\u0026gt; ready waiting -\u0026gt; ready running -\u0026gt; terminate Process Scheduler\nPreemptive scheduler (Time slice) 可以被搶占 Non-Preemptive scheduler 又稱 cooperative scheduling 只可能出現在時機 1 或 4 Classification fo Processes(related to scheduling)\nInteractive Processes (50 - 150 ms) Batch Processes Real time Processes Hard Soft Classification of Processes(related to CPU usage)\nCPU Bound I/O Bound Standard Scheduling Algorithm FCFS SJF SRTF Priority Based Highest Response Ratio Next Round Robin Virtual RR Multi-Level Queue Scheduler Multi-Level Feed Back Queue Scheduler Rotating Staircase Deadline Scheduler UNIX SVR3 Scheduler 有 32 個 runqueue，每個 runqueue 負責 4 個 priority values\n128 Priority values\n0-49: Kernel 50-127: User $Priority_j=Base_j+CPU_j(i)+nice_j$\nBase: 0-127 $CPU_j(i) = DR * CPU_j(i-1)$ DR = $\\frac{1}{2}$ nice: -20 ~ +19 可以用 nice 和 renice 改 process nice value Schedtool Query \u0026amp; set per process scheduling parameters\nScheduling Policy Real time SCHED_RR SCHED_FIFO Conventional SCHED_NORMAL (default) SCHED_BATCH (CPU intensive) SCHED_ISO (unused) SCHED_IDLEPRIO (low pri jobs) Nice Value (-20 to +19) Static Priority (1-99) CPU affinity process 想運行在某個指定的 CPU 上，不被轉移到其他 CPU，才不會降低指定 CPU 的 cache 命中率 soft CPU affinity hard CPU affinity cpus_allowed 一個用來指定 CPU 的 mask 1 schedtool \u0026lt;PID\u0026gt; ","date":"2023-02-20T21:12:52+08:00","permalink":"https://roykesydon.github.io/Blog/p/process-scheduling/","title":"Process Scheduling"},{"content":"RL 方法 Policy-based learn 做事的 actor Value-based 不直接 learn policy，而是 Learn critic，負責批評 Q-learning 屬於這種 Critic 不直接決定 action 給予 actor $\\pi$，評估 actor $\\pi$ 有多好 critic 的 output 依賴於 actor 的表現 State Value Function State value function $V^{\\pi}(s)$ 用 actor $\\pi$，看到 s 後玩到結束，cumulated reward expectation 是多少 評估方法 Monte-Carlo(MC) based approach\ncritic 看 $\\pi$ 玩遊戲 訓練一個 network，看到不同的 state ，輸出 cumulated reward(直到遊戲結束，以下稱為 $G_a$)，解 regression 問題 Temporal-difference(TD) approach\nMC 的方法至少要玩到遊戲結束才可以 update network，但有些遊戲超長 TD 只需要 {$s_t,a_t,r_t,s_{t+1}$} $V^{\\pi}(s_t)=V^{\\pi}(s_{t+1})+r_t$ MS v.s. TD\nMC Larger variance 每次的輸出差異很大 TD smaller variance 相較 $G_a$ 較小，因為這邊的 random variable 是 r，但 $G_a$ 是由很多 r 組合而成 V 可能估得不準確 那 learn 出來的結果自然也不准 較常見 Another Critic State-action value function $Q^\\pi(s,a)$\n又叫 Q function 當用 actor $\\pi$ 時，在 state s 採取 a 這個 action 後的 cumulated reward expectation 有一個要注意的地方是，actor 看到 s 不一定會採取 a 只要有 Q function，就可以找到\u0026quot;更好的\u0026quot; policy，再替換掉原本的 policy \u0026ldquo;更好的\u0026quot;定義 $V^{\\pi^{\u0026rsquo;}} \\ge V^{\\pi}(s), \\text{for all state s}$ $\\pi^{\u0026rsquo;}(s)=arg \\underset{a}{max}Q^{\\pi}(s,a)$ $\\pi^{\u0026rsquo;}$ 沒有多餘的參數，就單純靠 Q function 推出來 這邊如果 a 是 continuous 的會有問題，等等解決 這樣就可以達到\u0026quot;更好的\u0026quot;policy，不過就不列證明了 Basic Tip Target network 在 training 的時候，把其中一個 Q 固定住，不然要學的 target 是不固定的，會不好 train Exploration policy 完全 depend on Q function 如果 action 總是固定，這不是好的 data collection 方法，要在 s 採取 a 過，才比較好估計 Q(s, a)，如果 Q function 是 table 就根本不可能估出來，network 也會有一樣的問題，只是沒那麼嚴重。 解法 Epsilon Greedy $a=\\begin{cases} arg \\underset{a}{max}Q(s,a), \u0026amp; \\text{with probability } 1-\\varepsilon \\\\ random, \u0026amp; otherwise \\end{cases}$ 通常 $\\varepsilon$ 會隨時間遞減，因為你一開始 train 的時候不知道怎麼比較好 Boltzmann Exploration $P(a|s)=\\frac{exp(Q(s,a))}{\\sum_a exp(Q(s,a))}$ Replay Buffer 把一堆的 {$s_t,a_t,r_t,s_{t+1}$} 存放在一個 buffer {$s_t,a_t,r_t,s_{t+1}$} 簡稱為 exp 裡面的 exp 可能來自於不同的 policy 在 buffer 裝滿的時候才把舊的資料丟掉 每次從 buffer 隨機挑一個 batch 出來，update Q function 好處 跟環境作互動很花時間，這樣可以減少跟環境作互動的次數 本來就希望 batch 裡的 data 越 diverse 越好，不會希望 batch 裡的 data 都是同性質的 issue 我們要觀察 $\\pi$ 的 value，混雜了一些不是 $\\pi$ 的 exp 到底有沒有關係? 理論上沒問題，但李老師沒解釋 Typical Q-learning 演算法 初始化 Q-fucntion Q，target Q-function $\\hat{Q}=Q$ 在每個 episode 對於每個 time step t 給 state $s_t$，根據 Q 執行 action $a_t$ (epsilon greedy) 獲得 reward $r_t$，到達 $s_{t+1}$ 把 {$s_t,a_t,r_t,s_{t+1}$} 存到 buffer 從 buffer sample {$s_t,a_t,r_t,s_{t+1}$}(通常是一個 batch) Target $y=r_i+\\underset{a}{max}\\hat{Q}(s_{i+1},a)$ Update Q 的參數，好讓 $Q(s_i,a_i)$ 更接近 y(regression) 每 C 步 reset $\\hat{Q}=Q$ ","date":"2023-02-20T16:21:23+08:00","permalink":"https://roykesydon.github.io/Blog/p/q-learning/","title":"Q-learning"},{"content":"On/Off-policy On-policy 學習的 agent 和與環境互動的 agent 是同一個 Off-policy 學習的 agent 和與環境互動的 agent 是不同個 想從 On-policy 轉 Off-policy On-policy 每次都要重新蒐集資料，很花時間 由另一個 $\\pi_{\\theta^{\u0026rsquo;}}$ 去 train $\\theta$，$\\theta^{\u0026rsquo;}$是固定的，所以我們可以 re-use sample data Importance Sampling 是一個 general 的想法，不限於 RL\n$E_{x \\text{\\textasciitilde} p}[f(x)]\\approx \\frac{1}{N}\\displaystyle\\sum_{i=1}^N f(x^i)$\n$x^i$ is sampled from p(x) 我們遇到的問題是沒辦法從 p 來 sample data，只能透過 q(x) 去 sample $x^i$\n可以把上式改寫成 $E_{x \\text{\\textasciitilde} p}[f(x)]=E_{x \\text{\\textasciitilde} q}[f(x)\\frac{p(x)}{q(x)}]$\nIssue 雖然理論上 q 可以任意選，只要不要 q(x) 是 0 的時候 p(x) 不是 0，實作上 p 和 q 不能差太多，不然會有問題\n這兩項的 Variance 不一樣，如果 p 除以 q 差距很大，右邊的 Variance 會很大，如果 sample 不夠多次就會有問題 轉換 原本\n$\\triangledown \\overline{R_{\\theta}}=E_{\\tau \\text{\\textasciitilde}p_{\\theta}(\\tau)}[R(\\tau)\\triangledown log p_{\\theta} (\\tau)]$ 改為\n$\\triangledown \\overline{R_{\\theta}}=E_{\\tau \\text{\\textasciitilde}p_{\\theta^{\u0026rsquo;}}(\\tau)}[\\frac{p_{\\theta}(\\tau)}{p_{\\theta^{\u0026rsquo;}}(\\tau)}R(\\tau)\\triangledown log p_{\\theta} (\\tau)]$ 從 $\\theta^{\u0026rsquo;}$ sample 資料 更新 $\\theta$ 多次 Advantage function 原本\n$E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta}}[A^{\\theta}(s_t,a_t)\\triangledown log p_\\theta(a_t^n|s_t^n)]$ 改為\n$E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta^{\u0026rsquo;}}}[\\frac{P_\\theta(s_t,a_t)}{P_{\\theta^{\u0026rsquo;}}(s_t,a_t)}A^{\\theta^{\u0026rsquo;}}(s_t,a_t)\\triangledown log p_\\theta(a_t^n|s_t^n)]$ 要注意 Advantage 的結果要由 $\\theta^{\u0026rsquo;}$ 得出，是 $\\theta^{\u0026rsquo;}$在和環境互動 新的 objective function\n$J^{\\theta^{\u0026rsquo;}}(\\theta)=E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta^{\u0026rsquo;}}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{\u0026rsquo;}}(a_t|s_t)}A^{\\theta^{\u0026rsquo;}}(s_t,a_t)]$ PPO 確保 $\\theta$ 和 $\\theta^{\u0026rsquo;}$ 不會差太多 $J_{PPO}^{\\theta^{\u0026rsquo;}}(\\theta)=J^{\\theta^{\u0026rsquo;}}(\\theta)-\\beta KL(\\theta, \\theta^{\u0026rsquo;})$ 前身 TRPO Trust Region Policy Optimization $J_{TRPO}^{\\theta^{\u0026rsquo;}}(\\theta)=E_{(s_t,a_t)\\text{\\textasciitilde}\\pi_{\\theta^{\u0026rsquo;}}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{\u0026rsquo;}}(a_t|s_t)}A^{\\theta^{\u0026rsquo;}}(s_t,a_t)], KL(\\theta, \\theta^{\u0026rsquo;})\u0026lt;\\delta$ constrain 很難處理 KL divergence 這邊不是 $\\theta$ 和 $\\theta^{\u0026rsquo;}$ 參數上的距離，而是 behavior 的距離 參數上的距離是指這兩個參數有多像 是給同樣的 state 生出 action 的 distribution 要像 algorithm 初始參數 $\\theta^0$ 每個 iteration 用 $\\theta^k$ 和環境互動，蒐集{$s_t,a_t$}，並計算 advantage $A^{\\theta^k}(s_t,a_t)$\n找出 theta 最佳化 $J_{PPO}(\\theta)$\n$J_{PPO}^{\\theta^{k}}(\\theta)=J^{\\theta^{k}}(\\theta)-\\beta KL(\\theta, \\theta^{k})$ 可以更新很多次 動態調整 $\\beta$\nAdaptive KL Penalty 設可接受的 KL 數值範圍 if $KL(\\theta,\\theta^k)\u0026gt;KL_{max},\\text{increase} \\beta$ if $KL(\\theta,\\theta^k)\u0026lt;KL_{min},\\text{decrease} \\beta$ PPO2 PPO\n$J_{PPO}^{\\theta^{k}}(\\theta)=J^{\\theta^{k}}(\\theta)-\\beta KL(\\theta, \\theta^{k})$ PPO2\n$J_{PPO2}^{\\theta^{k}}(\\theta)\\approx \\displaystyle\\sum_{(s_t,a_t)}min(\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta^k}(a_t|s_t)}A^{\\theta^k}(s_t,a_t), \\\\ clip(\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta^k}(a_t|s_t)}, 1-\\varepsilon, 1+\\varepsilon)A^{\\theta^k}(s_t,a_t))$ ","date":"2023-02-20T12:35:56+08:00","permalink":"https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/","title":"Proximal Policy Optimization(PPO)"},{"content":"Basic Components Actor Policy $\\pi$ is a network with parameter $\\theta$ Env Reward Function Trajectory 在一場遊戲，把 env 輸出的 s 和 actor 輸出的 a 串起來，是一個 Trajectory Trajectory $\\tau$ = {$s_1,a_1,s_2,a_2,\u0026hellip;,s_T,a_T$} $p_{\\theta}(\\tau)=p(s_1)\\displaystyle\\prod_{t=1}^Tp_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$ Update $\\theta \\leftarrow \\theta + \\eta \\triangledown \\overline{R}_{\\theta}$\n$\\triangledown \\overline{R_{\\theta}} = \\displaystyle\\sum_{\\tau} R(\\tau) \\triangledown p_{\\theta} (\\tau) \\\\ =\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{t=1}^{T_n}R(\\tau^n)\\triangledown log p_{\\theta} (a_t^n|s_t^n)$\n實作 常見公式 $\\triangledown f(x)=f(x)\\triangledown logf(x)$ 用當前模型蒐集一堆 Trajectory 更新模型 回到第一步 細節 做一個分類問題，把 state 當作分類器的 Input，把 action 當作分類器的 ground truth 作訓練 在實作分類問題的時候，objective function 都會寫成 minimize cross entropy，就是 maximize log likelihood RL 和一般分類的區別是，要記得在 loss 前面乘上 $R(\\tau^n)$ Tip Add a Baseline $R(\\tau^n)$ 有可能永遠都為正 此時等於告訴 Model 說，今天不管是什麼 action，都要提高它的機率。不一定會有問題，因為雖然都是正的，但正的量有大有小，可能某些 action 上升的幅度會更大。因為我們是在做 sampling，不一定會 sample 到某些 action，本來想的情況是所有的 trajectory 都會出現才沒問題。 解法: 希望 reward 不要總是正的 $\\triangledown \\overline{R_{\\theta}}\\approx \\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\displaystyle\\sum_{t=1}^{T_n}(R(\\tau^n)-b)\\triangledown log p_{\\theta}(a_t^n|s_t^n)$ $b \\approx E[R(\\tau)]$ Assign Suitable Credit 原本整場遊戲的所有 action 都會乘上 $R(\\tau)$，但這不太公平，因為就算結果是好的，不代表所有 action 都是對的，反之亦然。在理想的情況下，如果 sample 夠多，就可以解決這問題。 解法 只計算從這個 action 後的 reward 總和 因為前面的 reward 和你做了什麼沒關係 接續解法 1，把比較未來的 reward 做 discount 乘某個小於 1 的 $\\gamma^{t^{\u0026rsquo;}-t}$ Advantage function base 可以是 state-dependent，可以根據 network 得出，以後再說 $(Reward-b)$ 可以合起來看做 Advantage function $A^{\\theta}(s_t,a_t)$ 這邊 Reward 不管你是什麼形式，有沒有 discount。 它的意義是，這個 action 相較於其他的 action 有多好，而不是絕對好 這個 A 通常可以由某個類神經網路估計，那個類神經網路叫做 critic，以後講 Actor-Critic 的時候再說 ","date":"2023-02-19T17:16:14+08:00","permalink":"https://roykesydon.github.io/Blog/p/policy-gradient/","title":"Policy Gradient"},{"content":"paper: Masked Autoencoders Are Scalable Vision Learners\nAbstract 這篇論文顯示出 MAE 是 CV 中的 scalable self-supervised learners。\nMAE 的方法很簡單\n隨機蓋住輸入影像的一些 patch 重建 missing pixels 具備兩個核心設計\n非對稱的 encoder-decoder 架構，encoder 只作用於可見的 patch 子集合(沒有 mask tokens)，lightweight decoder 則根據 latent representation 和 make tokens 來重建圖片。 當遮住高比例(比如 75%)的影像時，會得到一個 nontrivial 和 meaningful 的 self-supervisory task 結合這兩點設計，可以有效地訓練大模型。 以 ViT-Huge 用 ImageNet-1K 訓練(訓練集一百多萬張照片)可達到 87.8% 的準確度。\nIntroduction 在 CV 中，常需要大量 labeled images。 NLP 中，自監督預訓練處理了需要大量標註資料的問題。 masked autoencoders 是一種更 general 的 denoising autoencoders 的形式。 BERT 非常成功，autoencoding methods 在 CV 的研究卻落後 NLP，作者思考是什麼讓 masked autoencoding 在 CV 和 NLP 產生不同。 有以下觀點\n直到前陣子，CV 中的 CNN 是主流，但卷積層不好引入 mask tokens 或 positional embedding 這些 indicator。但這些可以透過 ViT 來解決，不應成為問題。 語言和視覺的 Information density 不同，語言是 highly semantic 和 information-dense，使填字本身不是很簡單的事情，但影像含有大量冗餘的訊息，缺失的部分比較好從相鄰的 patch 重建，比如直接插值，所以作者用一種簡單的策略，隨機 mask 很大一部分的 patch，創造一個具有挑戰性的自監督任務，強迫模型關注 global 的資訊。 關於 decoder，CV 還原 pixel，pixel 屬於 lower semantic level，NLP 還原 word，word 的 semantic information 較高。作者發現，雖然在 BERT 中，可以用簡單的 decoder 還原(一個 MLP)，但 CV 中 decoder 的設計就很重要。 基於以上觀點，作者提出 MAE，隨機遮住大量的 patch，並在 pixel space 重建失去的 patch。而且是非對稱 encoder-decoder 架構，encoder 只會看到可見的 patch，但 docoder 除了 latent representation，還會看到 mask tokens。這種設計在非常高的掩蓋率(比如 75%)下不但可以提高準確度，還可以讓 encoder 只處理較少比例(比如 25%)的 patch，將訓練時間減少 3 倍或更多，使 MAE 可以輕鬆擴展成更大的模型。\n在這樣的架構下，用 MAE 的 pre-training，可以訓練非常吃 data 的模型，比如 ViT-Large/-Huge，而只使用 ImageNet-1K。\n用 ImageNet-1K 在 vanilla ViT-Huge 上 fine-tune 可達到 87.8% 準確度，比以往只使用 ImageNet-1K 的結果都高。\n在 obejct detection、instance segmentation、semantic segmentation 上做 transfer learning 都達到不錯的效果，可以打敗用監督式預訓練模型的對手。\n相關工作 Autoencoding MAE 是一種 denoising autoencoding 的形式，但和 DAE 還是差別很大。 Masked image encoding iGPT、ViT、BEiT Approach Masking\n和 ViT 一樣，把圖片切成多個 patch，對於 patch 均勻隨機地採樣保留，剩下地遮住 MAE encoder\nViT 也有 positional embedding MAE decoder\nTransformer block 輸入 encoded visible patches mask tokens shared, learned vector 都會加入 positional embedding 用相較 encoder 輕量的解碼器，所有的 patch 由這個輕量的 decoder 處理，減少預訓練時間 Reconstruction target\ndecoder 的最後一層是 linear projection，之後再 reshape 成你要的 patch loss function mean squared error(MSE) 只算 masked patched 的 MSE，像 BERT Simple implementation\n先取得一系列 token(patch 做 linear projection + positional embedding) randomly shuffle，根據比例移除尾端一部份 encoding 後，尾端接上 mask tokens，並且 unshuffle 加上 positional embedding 後，給 decoder ImageNet Experiments 在 ImageNet-1K 上做自監督的預訓練，然後做\nend-to-end fine-tuning 所有參數都可改 linear probing 只改最後一層線性層 optimal masking ratio 意外地高，相比 BERT 只有 15%\n討論和結論 在 CV 實用的預訓練做法主流是監督式的，CV 中自監督的做法可能正跟著 NLP 的軌跡走。\n要仔細處理圖像和語言的區別，作者去除圖片中很可能不構成 semantic segment 的部分，而不是移除某個 object。\n","date":"2023-02-15T16:08:46+08:00","permalink":"https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/","title":"MAE 論文"},{"content":"paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAbstract 在 CV 領域 transformer 表現有限，目前 attention 常常是和卷積神經網路一起用，或是用來把一些卷積層換成 self-attention，但整體架構不變。這篇論文想展現一個純 Transformer 可以直接在影像分類上表現很好。如果用大量資料作預訓練，再遷移到中小型的資料集，可以和 SOTA 的 CNN 表現得一樣好，還需要較少的訓練資源作訓練。\nIntroduction self-attention-based 架構，特別是 Transformer，已經是 NLP 的重要選擇。主流的作法是在大型文字資料集上作訓練，再針對小型任務資料集作 fine-tune。由於 Transformer 的計算效率高，還有可擴展性，可以 train 一些很大的 model，隨著 model 和資料集增大，目前還沒看出飽和的現象。\n然而在 CV，CNN 還是主流，一些工作嘗試用 self-attention 結合 CNN-like 的架構，比如把 feature map 當 transformer 的輸入，因為原始 pixel 太多，或甚至把卷積層全換成 self-attention，雖然後者理論上效率很高(原論文中有另外 cite 兩篇作法)，但因為他們做法特殊，在現代硬體上很難加速，所以無法很有效地擴展。在 large-scale 的影像識別上， ResNet-like 的架構還是 SOTA。\n該實驗直接把一個標準的 Transformer 作用於圖片上，只作最少的修改。把影像分成多個 patch，並把它們變成一系列的 linear embedding，當作 NLP 中的 tokens(words) 來處理。\n當在中型大小的資料集(e.g. ImageNet)上訓練，如果沒有 strong regularization，ViT 會略輸同等大小的 ResNets\n這篇論文在更大的資料集(14M-300M 的影像)上訓練，就打敗了 inductive bias。在大量資料上作預訓練就很讚。\nRelated Work 大型的 Transformer-based 模型常常是先在大資料集上預訓練然後根據任務 fine-tune，比如 BERT 和 GPT。\n要把 self-attention 用在 CV 上，最簡單的做法就是把每個 Pixel 當一個元素，但 self-attention 是平方複雜度，在現實的圖片很難應用。一個應用 Transformer 的做法是只把 self-attention 用在 local neighborhood，另外一個是用 Sparse Transformer，還有一堆特殊的方法，雖然表現不錯，但要用硬體加速起來不容易。\n另一個有關的模型是 iGPT，在 reduce image resolution 和 color space 後把 transformer 應用在 image pixels 上。它用非監督式訓練後，再 fine-tune 或做 linear probing(只更新最後的 linear layer) 分類任務，表現很好。\n已經有類似的工作了，抽取 patches of size 2 * 2，最後再接 full self-attention，基本上和 ViT 非常像，這篇論文進一步證明了作大規模的預訓練可以讓 Transformer 和 SOTA 的 CNN 相比，而且 ViT 因為 patch 比較大，可以處理 medium-resolution 的圖片。這問題是可預期的，因為 Transformer 缺少了一些 inductive biases。\ninductive biases 一些假設 比如 CNN 常有四個假設 locality translation invariance with pooling layers 平移不變性 translation equivariance f(g(x)) = g(f(x)) 卷積和平移的先後順序沒差 Method 模型盡可能類似原始 Transformer，這樣可以把一些 NLP 上成功的 Transformer 架構拿來用，還可以用一些很有效率的 implementation\nembedding 維度是 768 = 16 * 16 * 3 position embedding 的做法是 standard learnable 1D positional embeddings，就是 BERT 的做法，簡單來說就是生出一張可以訓練的表，(序列長度, embedding size)，作者也有嘗試其他方法，但發現成效差不多，比如 2D positional embedding，概念就是從生出(序列長度, embedding size)變成生出 2 個(sqrt(序列長度), embedding size)。\n[class] 的概念是 NLP 出來的，ResNet-like 的架構常見的做法也有通過 globally average-pooling (GAP)來生出向量，再接上分類器做預測。實驗發現直接在 transformer 的輸出做 GAP 和 [class] 都可以達到不錯的效果。\nConclusion 拿標準的 Transformer 來作 Image recognition，和以往用 self-attention 在 CV 的方法不一樣，除了一開始的 initial patch extraction，沒有引入其他影像特有的 inductive biases。直接把圖片當成是一系列的 patch，然後直接用 Transformer encoder 當一般 NLP 任務處理。在很多影像分類訓練集上表現得更好還在 pre-train 上相對便宜。\n還有一些值得挑戰的地方，比如把 ViT 應用在其他 CV 任務，比如 detection 和 segmentation。另一個挑戰是探索自監督預訓練的方法。這篇論文其實有實驗自監督，表現 OK，但和監督式還是有很大的落差。擴大 ViT 可能有更好的結果。\n","date":"2023-02-12T00:27:55+08:00","permalink":"https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/","title":"ViT 論文"},{"content":"隨機變數之和 Z=X+Y\n$p_Z(z)=\\displaystyle\\sum_{x=-\\infty}^{\\infty}p_{X,Y}(x,z-x)\\\\ =\\displaystyle\\sum_{y=-\\infty}^{\\infty}p_{X,Y}(z-y,y)$\n$f_Z(z)=\\int_{-\\infty}^{\\infty}f_{X,Y}(x,z-x)dx\\\\ =\\int_{-\\infty}^{\\infty}f_{X,Y}(z-y,y)dy$\n如果 X, Y 獨立\n離散\n$p_Z(z)=\\displaystyle\\sum_{x=-\\infty}^{\\infty}p_{X}(x)\\cdot p_Y(z-x)\\\\ =\\displaystyle\\sum_{y=-\\infty}^{\\infty}p_{X}(z-y)\\cdot p_Y(y)$ 這兩個等式是 discrete convolution $=p_X(z) * p_Y(z)$ 連續\n$f_Z(z)=\\int_{-\\infty}^{\\infty}f_{X}(x) f_Y(z-x) dx\\\\ =\\int_{-\\infty}^{\\infty}f_{X}(z-y) f_Y(y) dy$ 這兩個等式是 continuous convolution $=f_X(z) * f_Y(z)$ 如果有 n 個獨立隨機變數\n$X=X_1+X_2+\u0026hellip;+X_n$ 如果 $X_1,\u0026hellip;,X_n$ 獨立 $p_X(x)=p_{X_1}(x) * p_{X_2}(x) * p_{X_3}(x) * \u0026hellip; * p_{X_n}(x)$ 連續做 convolution $f_X(x)=f_{X_1}(x) * f_{X_2}(x) * f_{X_3}(x) * \u0026hellip; * f_{X_n}(x)$ 連續做 convolution MGF moment generating function\nconvolution 很難算\n流程\n如果有多個連續 convolution 也適用下面流程，全部一次一起相乘 給定 $p_{X_1}(x), p_{X_2}(x)$，目標是求 $p_{X_1}(x) * p{X_2}(x)$\n轉換到 MGF\n$\\phi_{X_1}(s)=E \\lbrack e^{sX_1} \\rbrack\\\\ = \\displaystyle\\sum_{x=-\\infty}^{\\infty}e^{sx}\\cdot p_{X_1}(x)$\n$\\phi_{X_2}(s)=E \\lbrack e^{sX_2} \\rbrack$\n相乘 $\\phi_{X_1}(s) \\cdot \\phi_{X_2}(s)$\n逆轉換\n查表 $\\phi_X(s)$ 定義\n$\\phi_X(s)=E \\lbrack e^{sX} \\rbrack = \\begin{cases} \\displaystyle\\sum_{x=-\\infty}^{\\infty} e^{sx} \\cdot p_{X}(x) \u0026amp; 離散, \\\\ \\int_{-\\infty}^{\\infty} e^{sx} \\cdot f_{X}(x)dx \u0026amp; 連續 \\end{cases}$ 性質\nY = aX + b $\\phi_Y(s) = e^{sb} \\cdot \\phi_X(as) $ 常見離散機率分佈的 MGF\n$X$~$Bernoulli(p)$ $\\phi_X(s)=1-p+pe^s$ $X$~$BIN(n, p)$ 作 n 次實驗成功次數等於個實驗室成功次數的總和 $X = X_1 + X_2 + \u0026hellip; + X_n, X_i 獨立, Xi$~$Bernoulli(p)$ $\\phi_{X_i}(s)=1-p+pe^s$ $\\phi_{X}(s)=\\lbrack 1-p+pe^s \\rbrack ^n$ $X$~$Geometric(p)$ 自行推導 $X$~$Pascal(k,p)$ 看到第 k 次成功，花的總實驗室次數等於第 1 號成功花多少次 + 第 2 號 +\u0026hellip;+ 第 k 號 $X = X_1 + X_2 + \u0026hellip; + X_n, X_i 獨立, Xi$~$Gemetric(p)$ $X$~$Exponential(\\lambda)$ 自行推導 $X$~$Erlang(n,\\lambda)$ $X = X_1 + X_2 + \u0026hellip; + X_n, X_i 獨立, Xi$~$Exponential(\\lambda)$ 多個隨機變數之和 獨立隨機變數之和 $X_1, X_2, \u0026hellip;$獨立，且各自有一模一樣的機率分佈 { $X_i$ } $I.I.D.$\nIndependently and Identically Distributed $X = X_1+X_2+\u0026hellip;+X_n$，n 為常數，請問 X 的機率分佈\n$p_X(x)=p_{X_1}(x) * p_{X_1}(x) * p_{X_1}(x) * \u0026hellip; * p_{X_1}(x)$ $f_X(x)=f_{X_1}(x) * f_{X_1}(x) * f_{X_1}(x) * \u0026hellip; * f_{X_1}(x)$ 因為他們機率分佈一模一樣，所以底下都是 $X_1$ $\\phi_X(s)=\\lbrack \\phi_{X_1}(s) \\rbrack ^n$ e.g. 假設壽司理想重量是 13g，抓飯量是常態分佈，期望值是 14，標準差是 3，每天要作 100 個，每天飯量的機率分佈是?\n$X_i$ : 第 i 個壽司的飯量，{ $X_i$ } I.I.D. $X_i$~$N(14,9)\\\\ \\Rightarrow \\phi_{X_i}(s)=\\phi_{X_1}(s)\\\\ =e^{\\mu S + \\frac{\\sigma^2}{2}s^2} = e^{14 s + \\frac{9}{2}s^2}$ $X=X_1+X_2+\u0026hellip;+X_{100}$ $\\phi_X(s)=\\lbrack \\phi_{X_1}(s) \\rbrack^{100}\\\\ =e^{1400 s + \\frac{900}{2}s^2}$ 這個東西是 $X$~$N(1400,900)$ 的 MGF，所以可以逆推回來機率分佈 隨機變數之獨立隨機變數和 $X_1,X_2,\u0026hellip;I.I.D.$\n$X = X_1 + X_2 + \u0026hellip; + X_N$\nN 本身也是隨機變數，其機率分佈已知\n$\\phi_X(s)=\\phi_N(ln(\\phi_{X_1}(s)))$\n中央極限定理 central limit theorem(CLT)\n若 $X_1,X_2,\u0026hellip;,X_n$ 為 $I.I.D.$，當 n 趨近於無窮大時\n$X=X_1+X_2+\u0026hellip;+X_n$~$N(\\mu_{X_1+X_2\u0026hellip;+X_n}, \\sigma^2_{X_1+X_2+\u0026hellip;+X_n})$ $\\mu_{X_1+X_2+\u0026hellip;+X_n}=\\mu_{X_1}+\\mu_{X_2}+\u0026hellip;+\\mu_{X_n}=n\\mu_{X_1}$ $\\sigma^2_{X_1+X_2+\u0026hellip;+X_n}=\\sigma^2_{X_1}+\\sigma^2_{X_2}+\u0026hellip;+\\sigma^2_{X_n}=n\\sigma^2_{X_1}$ 應用\n要處理多個獨立的隨機變數的和時，可以用 CLT 將其機率分佈近似為常態分佈後計算機率 比如雜訊常當作常態分佈 如果某機率分佈等於多個獨立隨機變數的和，此機率分佈可以用常態分佈近似，再算機率 e.g. $X$~$BIN(100,0.3)$ $X=X_1+X_2+\u0026hellip;+X_100$ {$X_i$} $I.I.D., X_i$~$Bernoulli(0.3)$ 範例\n天團粉絲有 0.2 的機率買 CD，共有100萬個粉絲，發售 CD 超過 200800 張的機率為何 $X$~$BIN(1000000,0.2)$ $P(X\u0026gt;200800)=\\displaystyle\\sum_{x=200801}^{10^6}(\\overset{1000000}{x})0.2^x0.8^{10^6-x}$ $(\\overset{1000000}{x})=\\frac{1000000!}{200801!799199!}$ 算不出來 $X=X_1+X_2+\u0026hellip;+X_{1000000}, X_i$~$Bernoulli(0.2)\\\\ \\Rightarrow \\mu_{X_1}=0.2, \\sigma_{X_1}^2=0.16$ By CLT $\\Rightarrow X$~$N(200000,160000)$ $P(X\u0026gt;200800)\\\\ =P(\\frac{X-200000}{400} \u0026gt; \\frac{200800-200000}{400})\\\\ =P(Z\u0026gt;2) =Q(2) \\approx0.023$ De Moivre - Laplace Formula 如果是離散的隨機變數和，可以算的更精確 $P(k_1 \\le X \\le k_2) \\approx \\Phi(\\frac{k_2+0.5-n\\mu_{X_1}}{\\sqrt{n}\\sigma_{X_1}}) - \\Phi(\\frac{k_1-0.5-n\\mu_{X_1}}{\\sqrt{n}\\sigma_{X_1}})$ ","date":"2023-02-05T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iv/","title":"機率論 - IV"},{"content":"隨機變數的函數 隨機變數 X 的任意函數 g(x) 也是一個隨機變數，常被稱為 Derived Random Variable 求 g(x) 的機率分佈 X 是離散 直接推 g(X) 的 PMF X 是離散隨機變數，Y = g(X) 也是離散隨機變數 $p_{g(X)}(y) = \\displaystyle\\sum_{會讓g(x)=y 的所有x}p_X(x)$ X 是連續 先推 g(x) 的 CDF，再微分得 PDF\n先算 g(X) 的 CDF $F_{g(X)}(y)=P\\lbrack g(X) \\le y \\rbrack$ 若 g(X) 可以微分，再對 y 微分得 PDF $f_{g(X)}(y)=\\frac{d}{dy}F_{g(X)}(y)$ e.g. 若 Y=3X+2，請問 Y 的 PDF 與 $f_X(x) 的關係?$\n$F_Y(y)=P(Y \\le y)\\\\ =P(3X+2 \\le y)\\\\ =P(X \\le \\frac{y-2}{3})\\\\ =F_X(\\frac{y-2}{3})$ $f_Y(y)=\\frac{d}{dy}F_Y(y)\\\\ =\\frac{d}{dy}F_X(\\frac{y-2}{3})\\\\ =\\frac{dF_X(\\frac{y-2}{3})}{d(\\frac{y-2}{3})} \\cdot \\frac{d \\frac{y-2}{3}}{dy}\\\\ =f_X(\\frac{y-2}{3}) \\cdot \\frac{1}{3}$ 若 Y=aX+b\n$f_Y(y)=\\frac{1}{|a|}f_X(\\frac{y-b}{a})$ 條件機率分佈 若 X 是離散隨機變數，PMF 是 $p_X(x)$，某事件 B 已發生 PMF: $p_{X|B}(x)= x = \\begin{cases} x \\in B: \u0026amp; \\frac{p_X(x)}{p(B)}, \\ x \\notin B: \u0026amp; 0 \\end{cases}$ CDF: $F_{X|B}(x)\\\\ =\\displaystyle\\sum_{u \\le x}p_{X|B}(u)\\\\ =\\displaystyle\\sum_{u \\le x, u \\in B} \\frac{p_X(u)}{P(B)}$ 若 X 是連續隨機變數，某事件 B 已發生 PDF: $f_{X|B}(x)\\\\ =\\begin{cases} x \\in B: \u0026amp; \\frac{f_X(x)}{P(B)}, \\ x \\notin B: \u0026amp; 0 \\end{cases}$ CDF: $F_{X|B}(x)\\\\ =\\int_{-\\infty \\le u \\le x, u \\in B} \\frac{f_X(u)}{P(B)} du$ 條件期望值 Conditional Excpectation $E \\lbrack X|B \\rbrack\\\\ =\\begin{cases} \\displaystyle\\sum_{x=-\\infty}^{\\infty} x \\cdot p_{X|B}(x) \u0026amp; 離散, \\\\ \\int_{-\\infty}^{\\infty} x \\cdot f_{X|B}(x)dx \u0026amp; 連續 \\end{cases}$\n$E \\lbrack g(X)|B \\rbrack\\\\ =\\begin{cases} \\displaystyle\\sum_{x=-\\infty}^{\\infty} g(x) \\cdot p_{X|B}(x) \u0026amp; 離散, \\\\ \\int_{-\\infty}^{\\infty} g(x) \\cdot f_{X|B}(x)dx \u0026amp; 連續 \\end{cases}$\n$Var(X|B) = E\\lbrack X^2 | B \\rbrack - (\\mu_{X|B})^2$\n失憶性 Memoryless Geometric 和 Exponential 機率分佈都有失憶性 不管事情已經進行多久，對於事情之後的進行一點影響都沒有 聯合機率分佈 joint probability distribution 同時考慮多個隨機變數的機率分佈 Joint PMF X, Y 皆為離散，聯合PMF\n$p_{X,Y}(x,y)=P(X=x, Y=y)$ 性質\n$0 \\le p_{X,Y}(x,y) \\le 1$ $\\Sigma^{\\infty}{x=-\\infty}\\Sigma^{\\infty}{y=-\\infty} p_{X,Y}(x,y)=1$ X, Y 獨立 $P_{X,Y}(x,y)\\\\ =P(X=x,Y=y)\\\\ =P_X(x)P_Y(y)$ 對任何事件 B $P(B)=\\Sigma_{(x,y)\\in B}P_{X,Y}(x,y)$ Joint CDF $F_{X,Y}(x,y)=P(X \\le x, Y \\le y)$\n性質\n$0 \\le F_{X,Y}(x, y) \\le 1$ 若 $x_1 \\le x_2$ 且 $y_1 \\le y_2$，則 $F_{X,Y}(x_1,y_1) \\le F_{X,Y} (x_2, y_2)$ $F_{X,Y}(x, \\infty) = F_X(x)$ $F_{X,Y}(\\infty, y) = F_Y(y)$ $F_{X,Y}(\\infty, \\infty) = 1$ $F_{X,Y}(x, -\\infty)\\\\ = P(X \\le x, Y \\le -\\infty)\\\\ \\le P(Y \\le -\\infty) \\\\ = 0$ $F_{X,Y}(-\\infty, y) = 0$ $P(x_1 \u0026lt; X \\le x_2, y_1 \u0026lt; Y \\le y_2)\\\\ =F_{X,Y}(x_2,y_2)-F_{X,Y}(x_2,y_1)-F_{X,Y}(x_1,y_2)+F_{X,Y}(x_1,y_1)$ Joint PDF $f_{X,Y}(x,y)= \\frac{\\partial^2F_{X,Y}(x,y)}{\\partial x \\partial y}$\n$F_{X,Y}(x,y) = \\int_{-\\infty}^{x} \\int_{-\\infty}^{y} f_{X,Y}(u,v)dv du$\n性質\n$f_{X,Y}(x,y) \\ge 0$ $\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dxdy=1$ 如果 X,Y 獨立 $f_{X,Y}(x,y)=f_X(x) \\cdot f_Y(y)$ 對任何事件 B $P(B)=\\int\\int_{(x,y)\\in B}f_{X,Y}(x,y)dxdy$ 邊際 PMF Marginal PMF 已知聯合 PMF : $p_{X,Y}(x,y)$，求 $p_X(x), p_Y(y)$，稱為邊際 PMF $p_X(x)=\\displaystyle\\sum_{y=-\\infty}^{\\infty}P_{X,Y}(x,y)$ $p_Y(y)=\\displaystyle\\sum_{x=-\\infty}^{\\infty}P_{X,Y}(x,y)$ 已知聯合 PDF : $p_{X,Y}(x,y)$，求 $f_X(x), f_Y(y)$，稱為邊際 PDF $f_X(x)=\\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dy$ $f_Y(y)=\\int_{-\\infty}^{\\infty}f_{X,Y}(x,y)dx$ 雙變數期望值 聯合 PMF 下的期望值\n$E\\lbrack h(X,Y) \\rbrack = \\displaystyle\\sum_{x=-\\infty}^{\\infty}\\displaystyle\\sum_{y=-\\infty}^{\\infty}h(x,y)\\cdot p_{X,Y}(x,y)$ h(X,Y) 也可以只和 X 有關，比如它可以是 $x^2$ 聯合 PDF 下的期望值\n$E\\lbrack h(X,Y) \\rbrack = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}h(x,y)\\cdot f_{X,Y}(x,y) dxdy$ e.g. 已知 $f_{X,Y}(x,y)=\\begin{cases} 0.5, \u0026amp; \\text{if } 0 \\le y \\le x \\le 2, \\\\ 0, \u0026amp; otherwise \\end{cases}$ $E \\lbrack X + Y \\rbrack \\\\ = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x+y)\\cdot f_{X,Y}(x,y) dxdy\\\\ = \\int_{0}^{2}\\int_{y}^{2}(x+y)\\cdot 0.5 dxdy$ 期望值性質\n$E\\lbrack \\alpha h_1(X,Y)+ \\beta h_2(X,Y) \\rbrack\\\\ =\\alpha E\\lbrack h_1(X,Y)\\rbrack + \\beta E\\lbrack h_2(X,Y) \\rbrack$ 若 X,Y 獨立 $E\\lbrack g(X)h(Y) \\rbrack = E \\lbrack g(X) \\rbrack \\cdot E \\lbrack h(Y) \\rbrack$ Variance 性質\n$Var(X+Y)=Var(X)+Var(Y)+2 \\cdot Cov(X,Y)$ $Cov(X,Y)=E\\lbrack (X-\\mu_X)(Y -\\mu_Y) \\rbrack$ 如果 X, Y 獨立 $2E\\lbrack (X-\\mu_X)(Y -\\mu_Y) \\rbrack \\\\ = 2E\\lbrack (X-\\mu_X) \\rbrack E\\lbrack (Y -\\mu_Y) \\rbrack \\\\ = 0$ ","date":"2023-02-02T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-iii/","title":"機率論 - III"},{"content":"機率密度函數 PDF probability density function PMF 在 連續R.V. 上，假如 $X\\text{\\textasciitilde}[0,1)$，$p_X(0.7)$ = 0，因為有無窮多個數字 公式 $f_X(x)=\\lim\\limits_{\\Delta x \\rightarrow 0} \\frac{P(x \\le X \\le x + \\Delta x)}{\\Delta x} \\\\ = \\lim\\limits_{\\Delta x \\rightarrow 0} \\frac{F_X(x+\\Delta x) - F_X(x)}{\\Delta x} \\\\ = F^{\\prime}_X(x) $ 和 CDF 的關係 $CDF: F_X(x) = PDF: f_X(x)$ $\\int^x_{-\\infty}$ 可以從 PDF 轉到 CDF\n$\\frac{d}{dx} 可以從 CDF 轉到 PDF$\n跟機率的關係 $P(a \u0026lt; X \\le b) = F_X(b) - F_X(a) \\\\ = \\int^b_{-\\infty} f_X(x)dx - \\int^a_{-\\infty} f_X(x)dx \\\\ = \\int^a_b f_X(x)dx$ $f_X(x)=\\lim\\limits_{\\Delta x \\rightarrow 0} \\frac{P(x \\le X \\le x + \\Delta x)}{\\Delta x}$ 當 $\\Delta x$ 很小時 $P(x \\le X \\le x + \\Delta x) \\approx f_X(x) \\cdot \\Delta x$ 性質 $f_X(x) = F^{\\prime}_X(x)$ $F_X(x)=\\int^x_{-\\infty}f_X(u)du$ $P(a \\le X \\le b)=\\int^b_a f_X(x) dx$ $\\int^{\\infty}_{-\\infty}f_X(x)dx=1$ $f_X(x) \\ge 0$ $f_X(x)$ 可以比 1 大 連續機率分佈 Uniform 機率分佈 $X \\text{\\textasciitilde}UNIF(a,b)$ PDF $f_X(x) = \\begin{cases} \\frac{1}{b-a} \u0026amp; ,a \\le x \\le b \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 0 \u0026amp; ,x \\le a \\\\ \\frac{x-a}{b-a} \u0026amp; ,a \u0026lt; x \\le b\\\\ 1 \u0026amp; ,x \u0026gt; b \\end{cases}$ Exponential 機率分佈 有失憶性(memoryless)，常被用來 model 有這種性質的事情 $X \\text{\\textasciitilde}Exponential(\\lambda)$ PDF $f_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} \u0026amp; ,x \\ge 0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = 1-e^{-\\lambda x}$ Erlang 機率分佈 Gamma Distribution $X \\text{\\textasciitilde}Erlang(n,\\lambda)$ PDF $f_X(x) = \\begin{cases} \\frac{1}{(n-1)!}\\lambda^n x^{n-1} e^{-\\lambda x} \u0026amp; ,x \\ge 0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ $f_X(x)=(\\lambda e^{-\\lambda x}) * (\\lambda e^{-\\lambda x}) * \u0026hellip; * (\\lambda e^{-\\lambda x})$ 自己和自己做 n 次 convolution CDF $F_X(x) = \\begin{cases} 1 - \\Sigma^{n-1}_{k=0}\\frac{(\\lambda x)^k}{k!}e^{-\\lambda x} \u0026amp; ,x \\ge 0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ 常見用法 用來 model 一件有多個關卡事情的總時間，而每個關卡所需時間是隨機的 關卡數: n 每關卡所需時間之機率分佈 $Exponential(\\lambda)$ e.g. 打電動過三關所需時間 $Erlang(3, \\lambda)$ Normal 機率分佈 (常態分佈) 在自然界常出現\n常被用做「很多隨機量的總和」的機率模型\n又稱 Gaussian 機率分佈\n$X \\text{\\textasciitilde}Gaussian(\\mu,\\sigma)$\nPDF\n$f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ 也常用 $X \\text{\\textasciitilde}N(\\mu,\\sigma^2)$\n注意 $\\sigma$ 不一樣 CDF\n太難算，積不出來\n針對某組特別的 $\\mu, \\sigma$ 的 CDF 建表，把其他常態分佈的 CDF 和這組產生關聯 標準常態分佈\n$Z \\text{\\textasciitilde}N(0,1)$ $f_Z(z)=\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{z^2}{2}}$ CDF 表示為 $\\Phi(z)$ $\\Phi(z)=\\int^z_{-\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{u^2}{2}}du$\n積不出來，以數值方法近似出來後建表給人家查 查 standard normal table e.g. $F_Z(1.325)=?$\n查表 $F_Z(1.32)=0.9066$，$F_Z(1.33)=0.9082$ 用內插約略得 0.9074 性質\n$\\Phi(-z) = 1 - \\Phi(z)$ 任意 $\\mu, \\sigma$ 的 CDF\n對任何 $X \\text{\\textasciitilde}N(\\mu,\\sigma^2)$ $\\frac{X-\\mu}{\\sigma}\\text{\\textasciitilde}N(0,1)$ $F_X(x)=\\Phi(\\frac{x-\\mu}{\\sigma})$ 期望值 Expectation 大數法則 $P(A)=\\lim\\limits_{N \\rightarrow \\infty}\\frac{N_A}{N}$ 基本上期望值是利用大數法則算的 mean 值，雖然平均值是 R.V.，但當實驗無窮多次時，會收斂到常數，因此以這為估算值 Mean 值又稱做期望值 離散隨機變數 $E\\lbrack X \\rbrack=\\mu_X=\\displaystyle\\sum^{\\infty}_{x=-\\infty}x \\cdot P_X(x)$ 離散隨機變數的函數的期望值 對離散隨機變數 X 而言，其任意函數 g(x) 也是一隨機變數，也有期望值 $g(X)$ 的期望值定義為 $E \\lbrack g(X) \\rbrack=\\displaystyle\\sum^{\\infty}_{x=-\\infty}g(x)\\cdot P_X(x)$ 性質 $E\\lbrack \\alpha g(X) \\rbrack = \\alpha \\cdot E \\lbrack g(X) \\rbrack$ $E\\lbrack \\alpha g(X) + \\beta h(X) \\rbrack \\\\ =\\alpha \\cdot E \\lbrack g(X) \\rbrack + \\beta \\cdot E \\lbrack h(X) \\rbrack$ $E\\lbrack \\alpha \\rbrack = \\alpha$ 常見隨機變數函數的期望值 $X$ 的 $n^{th} moment$ $E \\lbrack X^n \\rbrack = \\displaystyle\\sum^{\\infty}_{x=-\\infty}x^n \\cdot P_X(x)$ X 的變異數(variance) $E \\lbrack (X-\\mu_X)^2 \\rbrack = \\displaystyle\\sum^{\\infty}_{x=-\\infty} (x-\\mu_X)^2 \\cdot P_X(x)$ 變異數 Variance Variance 通常符號表示為 $\\sigma^2_X=E \\lbrack (X-\\mu_X)^2 \\rbrack$ 隱含隨機變數 X 多「亂」的資訊 variance 大的話，X 不見得接近 $\\mu_X$ 變異數開根號是標準差(standard deviation) $\\sigma_X = \\sqrt{Variance} \\ge 0$ 算法 $\\sigma^2_X=E \\lbrack X^2 \\rbrack - \\mu^2_X\\\\ \\Rightarrow E \\lbrack X^2 \\rbrack = \\sigma^2_X + \\mu^2_X$\n常見離散分佈的期望值 / 變異數 $X\\text{\\textasciitilde}Bernouli(p)$\n$\\mu_X=1 \\cdot p + 0 \\cdot (1-p) \\\\ = p$\n$\\sigma^2_X = E \\lbrack X^2 \\rbrack - \\mu^2_X \\\\ = \\displaystyle\\sum^1_{x=0}x^2\\cdot p_X(x)-\\mu_X^2 \\\\ =1^2 \\cdot p + 0^2 \\cdot (1-p) - p^2\\\\ =p(1-p)$\n$X$~$BIN(n,p)$\n$\\mu_X = np$\n$\\sigma^2_X = np(1-p)$\n$X$~$GEO(p)$\n$\\mu_X = \\frac{1}{p}$\n$\\sigma^2_X = \\frac{(1-p)}{p^2}$\n$X$~$PASKAL(k,p)$\n$\\mu_X = \\frac{k}{p}$\n$\\sigma^2_X = \\frac{k(1-p)}{p^2}$\n$X$~$POI(\\alpha)$\n$\\mu_X = \\alpha$\n$\\sigma^2_X = \\alpha$\n$X$~$UNIF(a,b)$\n$\\mu_X = \\frac{a+b}{2}$\n$\\sigma^2_X = \\frac{1}{12}(b-a)(b-a+2)$\n連續隨機變數 對連續的隨機變數 X 而言，將 X 的值以 $\\Delta$ 為單位無條件捨去來近似，以隨機變數 Y 表示(當 $\\Delta \\rightarrow$ 0 時，$X \\approx Y$)，然後再當做 PMF 處理。\n$E \\lbrack X \\rbrack = \\int^{\\infty}_{-\\infty}xf_X(x)dx$ 連續隨機變數的函數的期望值 對連續隨機變數 X 而言，其任意函數 g(x) 也是一隨機變數，也有期望值 $g(X)$ 的期望值定義為 $E \\lbrack g(X) \\rbrack=\\int^{\\infty}_{-\\infty}g(x)\\cdot f_X(x)dx$ 性質 $E\\lbrack \\alpha g(X) \\rbrack = \\alpha \\cdot E \\lbrack g(X) \\rbrack$ $E\\lbrack \\alpha g(X) + \\beta h(X) \\rbrack \\\\ =\\alpha \\cdot E \\lbrack g(X) \\rbrack + \\beta \\cdot E \\lbrack h(X) \\rbrack$ $E\\lbrack \\alpha \\rbrack = \\alpha$ 常見隨機變數函數的期望值 $X$ 的 $n^{th} moment$ $E \\lbrack X^n \\rbrack = \\int^{\\infty}_{-\\infty}x^n \\cdot f_X(x)dx$ X 的變異數(variance) $E \\lbrack (X-\\mu_X)^2 \\rbrack = \\int^{\\infty}_{-\\infty} (x-\\mu_X)^2 \\cdot f_X(x)dx$ 變異數 Variance 和離散隨機變數的資訊一樣 常見連續分佈之期望值/變異數 $X$~$Exponential(\\lambda)$\n$\\mu_X = \\frac{1}{\\lambda}$\n$\\sigma^2_X = \\frac{1}{\\lambda^2}$\n$X$~$Erlang(n, \\lambda)$\n$\\mu_X = \\frac{n}{\\lambda}$\n$\\sigma^2_X = \\frac{n}{\\lambda^2}$\n$X$~$Gaussian(\\mu,\\sigma)$\n$\\mu_X = \\mu$\n$\\sigma^2_X = \\sigma^2$\n$X$~$UNIF(a,b)$\n$\\mu_X = \\frac{a+b}{2}$\n$\\sigma^2_X = \\frac{1}{12}(b-a)^2$\n","date":"2023-02-01T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-ii/","title":"機率論 - II"},{"content":"集合論 名詞 子集合(Subset) B 是 C 的子集(B 不能等於 C) $B \\subset C$ 補集(Complement) C 是 A 的補集 $C=A^C$ 不相交(Disjoint) $X \\cap Y = \\{\\}$ 互斥(Mutually Exclusive) 一群集合 $X_1, X_2, \u0026hellip;, X_n$ 中任選兩個集合 $X_i, X_j$ 都不相交，則 $X_1, X_2, \u0026hellip;, X_n$ 這群集合互斥 公式 De Morgan\u0026rsquo;s Law ${(A \\cup B)}^C=A^C \\cap B^C$ 機率名詞 Outcome (結果) 實驗中可能的結果 Sample Space (樣本空間) 機率實驗所有可能的結果的集合，常以 $S$ 表示 Event (事件) 對於實驗結果的某種敘述 事件可以看做是 outcome 的集合，也是 sample space 的子集 機率是一個函數，其自變數是 event，故可看做是一個映射 公理 Axioms 對任何事件 $A$ 而言, $P(A) \\geq 0$\n$P(S) = 1$\n事件 $A_1, A_2, \u0026hellip;$ 互斥 $\\Rightarrow$ $P(A_1 \\cup A_2 \\cup A_3 \\cup \u0026hellip;)$\n$=P(A_1)+P(A_2)+P(A_3)+\u0026hellip;$\n衍生公式 Boole\u0026rsquo;s 不等式\n對任意 $n$ 個事件 $A_1, A_2, \u0026hellip;, A_n$ 而言 $P(\\cup^n_{i=1}A_i \\leq \\Sigma^n_{i=1}P(A_i))$ Bonferroni\u0026rsquo;s 不等式\n對任意 $n$ 個事件 $A_1, A_2, \u0026hellip;, A_n$ 而言 $P(\\cap^n_{i=1} A_i) \\geq 1 - \\Sigma^n_{i=1} P(A^C_i)$ 條件機率 公式 $P(X|Y) = \\frac{P(X \\cap Y)}{P(Y)}$ $P(X \\cap Y) = P(X|Y) * {P(Y)} = P(Y|X) * P(X)$ 性質 $P(X|Y) \\geq 0$ $P(Y|Y) = 1$ $A, B$ 互斥 $\\Rightarrow P(A \\cup B |Y) = \\frac{P(A)}{P(Y)} + \\frac{P(B)}{P(Y)} = P(A|Y)+P(B|Y)$ 定理 Total Probability 定理 若 $C_1, C_2, \u0026hellip;, C_n$ 互斥且 $C_1 \\cup C_2 \\cup \u0026hellip; \\cup C_n = S$，則對任意事件 $A$ $P(A) = P(A|C_1)P(C_1) + P(A|C_2)P(C_2) + \u0026hellip; + P(A|C_n)P(C_n)$ Bayes\u0026rsquo; Rule 貝式定理 若 $C_1, C_2, \u0026hellip;, C_n$ 互斥且 $C_1 \\cup C_2 \\cup \u0026hellip; \\cup C_n = S$，則對任意事件 $A$ $P(C_j|A)=\\frac{P(A|C_j) * P(C_j)}{\\Sigma^n_{i=1}P(A|C_i)*P(C_i)}$\n$= \\frac{P(C_j \\cap A)}{P(A)}$\n獨立性 Independence 若兩事件 $A, B$ 之機率滿足\n$P(A \\cap B) = P(A) * P(B)$ 或以 $P(A|B) = P(A)$ 表示 則 $A, B$ 兩事件稱為機率上的獨立事件\n若事件 $A_1, A_2, \u0026hellip; A_n$ 滿足下列條件，則稱此 $n$ 事件獨立 $(n\u0026gt;2)$\n從中任選 $m$ 事件 $A_{i_1}, A_{i_2}, \u0026hellip; A_{i_m}$ 均滿足 $P(A_{i_1} \\cap A_{i_2} \\cap \u0026hellip; \\cap A_{i_m}) = P(A_{i_1})P(A_{i_2})\u0026hellip;P(A_{i_m}) , m=2, 3, \u0026hellip;, n$ 排列組合 二項式係數(binomial coefficient) $(^n_k)$ 有 $n$ 個異物，從中取出 $k$ 個 多項式係數(multinomial coefficient) $\\frac{n!}{n_1!n_2!\u0026hellip;n_m!}$ 有 m 種異物，每次選物從中選一後放回，依序選 n 次，共有 $m^n$ 種 outcome，在所有實驗結果中，第一種出現 $n_1$ 次，以此類推，這樣的實驗結果有多少種 隨機變數 Random Variable, R.V. 用來把 outcome 數字化的表示方式 通常用大寫英文字母 是將 outcome 轉成對應數字的函數 $X: S \\rightarrow R$ 從樣本空間映射到實數 隨機變數的函數，也是一個隨機變數 種類 離散隨機變數 (Discrete R.V.)\n值是有限個，或是「可數的」無窮多個 連續隨機變數 (Continuous R.V.)\n值有無窮多個，而且「不可數」 可數、不可數 可數 包含的東西可一個個被數，總有一天會被數到 e.g. 正偶數集合 不可數的 不管怎麼數，裡面一定有個東西會沒數到 e.g. 0~1 之間的所有數字 累積分佈函數 CDF cumulative distribution function\n對任一個隨機變數 $X$，定義 CDF 為\n$F_X(x) \\overset{def}{=}P(X \\leq x)$ 永遠用 $F$ 表示 常見用途\n算 X 落在某範圍的機率 $P(A \u0026lt; X \\le b) = F_X(b)-F_X(a)$ $P(A \\le X \\le b) = F_X(b)-F_X(a)+P(X=a)$ $P(A \u0026lt; X \u0026lt; b) = P(A \u0026lt; X \\le b^-)$ 性質 離散隨機變數的 CDF $F_X(x^+)=F_X(x)$ $F_X(x^-)=F_X(x)-P(X=x)$ 連續隨機變數的 CDF $F_X(x^-)=F_X(x)=F_X(x^+)$ 共同 $F_X(- \\infty)=P(X \\le - \\infty)=0$ $F_X(\\infty)=P(X \\le \\infty) = 1$ $0 \\le F_X(x) \\le 1$ 機率質量函數 PMF probability mass function 對任一個「離散」隨機變數 $X$，其 PMF 為 $p_X(x) \\overset{def}{=}P(X=x)$ PMF 和 CDF 的關係 對任何 $x$ $F_X(x) = \\displaystyle\\sum^{\\lfloor x \\rfloor}_{n=-\\infty}p_X(n)$ $P_X(x)=F_X(x^+)-F_X(x^-)$ 機率分佈(Probability Distribution) PMF 和 PDF 都是一種機率分佈 將總和為 1 的機率分佈在點上 離散機率分佈 Bernoulli 機率分佈 1 次實驗，2 種結果，在意某結果發生與否 $X \\text{\\textasciitilde}Bernoulli(p)$ PMF $p_X(x) = \\begin{cases} p \u0026amp; ,x=1 \\\\ 1-p \u0026amp; x=0 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 0 \u0026amp; ,x\u0026lt;0 \\\\ 1-p \u0026amp; 0 \\leq x \u0026lt;1 \\\\ 1 \u0026amp; ,x \\geq 1 \\end{cases}$ Binomial 機率分佈 實驗成功機率為 p，做 n 次實驗，X 表成功次數 $X \\text{\\textasciitilde}BIN(p)$ PMF $p_X(x) = (^n_x)p^x(1-p)^{n-x}$ 成功 $x$ 次 CDF $F_X(x) = \\displaystyle\\sum^{\\lfloor x \\rfloor}_{m=-\\infty} (^n_m)\\cdot p^m \\cdot (1-p)^{n-m}$ Uniform 機率分佈 1 次實驗，n 種結果，各結果機率均等，在意某結果發生否 $X \\text{\\textasciitilde}UNIF(a,b)$ PMF $p_X(x) = \\begin{cases} \\frac{1}{b-a+1} \u0026amp; ,x=a,a+1,\u0026hellip;,b \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 0 \u0026amp; ,x\u0026lt;a \\\\ \\frac{\\lfloor x \\rfloor - a + 1}{b-a+1} \u0026amp; ,a \\leq x\u0026lt; b\\\\ 1 \u0026amp; ,x \\geq b \\end{cases}$ Geometric 機率分佈 若實驗成功機率為 p，到成功為止，做了 X 次嘗試 有失憶性 $X \\text{\\textasciitilde}Geometric(p)$ PMF $p_X(x) = \\begin{cases} (1-p)^{x-1} \\cdot p \u0026amp; ,x=1, 2, 3, \u0026hellip; \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = \\begin{cases} 1-(1-p)^{\\lfloor x \\rfloor} \u0026amp; ,x \\ge 1 \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ Pascal 機率分佈 若實驗成功機率為 p，到第 k 次成功為止，共做了 X 次嘗試 $X \\text{\\textasciitilde}Pascal(k, p)$ PMF $p_X(x) = \\begin{cases} \\binom{x-1}{k-1}(1-p)^{x-k} p^k \u0026amp; ,x=k, k+1, \u0026hellip; \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ CDF $F_X(x) = P(X \\le x) \\\\ = P(在 x 次實驗中 \\ge k 次成功)\\\\ = P(Y \\ge k), Y~BIN (x,p) \\\\ $ 故 Pascal 又稱 Negative Binomial Poisson 機率分佈 已知某事發生速率為每單位時間 $\\lambda$ 次，觀察時間為 $T$ 時間單位，$X$ 為該觀察時間內發生該事的總次數。 $X \\text{\\textasciitilde}POI(\\lambda T)$ 有時候也會以 $\\mu$ 來表示 $\\lambda T$ PMF $p_X(x) = e^{-\\lambda T} \\cdot \\frac{(\\lambda T)^x}{x!}$ CDF $F_X(x) = \\begin{cases} \\displaystyle\\sum^{\\lfloor x \\rfloor}_{n=-\\infty}e^{-\\lambda T} \\cdot \\frac{(\\lambda T)^n}{n!} \u0026amp; ,x = 0,1,2,\u0026hellip; \\\\ 0 \u0026amp; ,otherwise \\end{cases}$ ","date":"2023-01-31T15:18:41+08:00","permalink":"https://roykesydon.github.io/Blog/p/%E6%A9%9F%E7%8E%87%E8%AB%96-i/","title":"機率論 - I"},{"content":"Share information between processes 透過硬碟上的文件溝通 超慢 透過 kernel buffer 滿快的，但這樣要一直在 user mode 和 kernel mode 來回切換，因為kernel buffer 在 kernel space 透過 shared memory region shared memory region 在 user space Mechanisms Signals\nCommunication\nData transfer Byte Stream Pipes FIFOs(Named Pipes) stream sockets Message Passing SystemV MsgQ POSIX MsgQ datagram sockets Shared Memory SystemV S.M POSIX S.M Memory Mapping anonymous memory mapping memory mapped file Synchronization\nPipes Related processes parent-child sibling Executing on same machine 用法 cmd1 | cmd2 cmd1 不是輸出到 stdout，而是由 kernel 維護的 buffer，也就是 pipe cmd 不是從 stdin 獲取輸入，而是從 pipe 獲取 cmd1 | cmd2 | \u0026hellip; | cmdn Named Pipes / FIFOs Related / Unrelated processes\nExecuting on same machine\ncreat a FIFO\ncommands mkfifo mknod 嘗試寫入或讀取 FIFO 時，會被 redirect 到 pipe\nSignal Handling Signal Used by OS to notify running process some event has occured without the process needing to pull for that event process 收到 signal 後會先停止執行並執行 signal handler A process did something SIGSEGV(11), SIGFPE(8), SIGILL(4), SIGPIPE(13)\u0026hellip; A process wants to tell another process something SIGCHILD(17) child process terminated User sends sig to foreground processes Ctrl + C SIGINT(2) Ctrl + \\ SIGQUIT(3) Ctrl + Z SIGTSTP(20) disposition 決定 process 遇到 signal 時該怎麼處理\nTerm teminate process Ign ignore Core terminate the process and dump core Stop stop the process Cont continue the process if it is stopped Signal can\u0026rsquo;t not be caught SIGKILL(9) SIGSTOP(19) Commands trap\n可以 handle signal\nkill kill - L 可以看到 standard signal 和 real-time signal\nstandard signal 開頭是 SIG，realt-time signal 是 SIGRT\n","date":"2023-01-28T15:31:50+08:00","permalink":"https://roykesydon.github.io/Blog/p/ipc--inter-process-communication/","title":"IPC -- Inter-Process Communication"},{"content":"paper: Training language models to follow instructions with human feedback\nAbstract 把語言模型變大不代表他們會更好地遵循用戶的意圖。\n大的語言模型有可能會生成 untruthful, toxic, not helpful 的答案。\n該論文透過 fine-tuning with human feedback 來解決這問題。\n一開始準備一系列人工標註的 prompts，然後用這 dataset 對 GPT-3 做 fine-tune。\n接下來再蒐集一個 dataset，存放 rankings of model outputs，由人工判斷輸出好壞，再用 RL 把剛剛 fine-tune 過的 model 繼續 fine-tune。\n最後有 1.3B 參數的 InstructGPT 表現的結果比 175B 參數的 GPT-3 還好。\nIntroduction Large language models(LMs) 可以透過 \u0026ldquo;prompt\u0026rdquo; 來執行各種 NLP 任務。\n但這些模型也常有一些非目的性的行為，諸如捏造事實等等。\n原因是出在目標函數上，多數 LMs 的目標函數是根據網路上的文本生出下一個字詞。\n這和「根據使用者指令生出安全且有幫助的答案不同」。\n上述的差異使語言模型的目標是 misaligned。\n作者的目標是生出 helpful、 honest(沒有誤導性資訊)、harmless 的 model。\n具體作法，使用 reinforcement learning from human feedback(RLHF)。\n訓練步驟 結果 Labelers 明顯偏好 InstructGPT 的答案，勝過 GPT-3 的答案\nInstructGPT 的答案在 truthfulness 勝過 GPT-3 的答案\nInstructGPT 的答案在 toxicity 上小勝 GPT-3 的答案，但在 bias 上沒有\nMethods Dataset 標註人員寫很多 prompts\nPlain: 隨便寫任意任務 Few-shot: 想個 instruction，並寫 multiple query/response pairs for that instruction User-based: 根據一些申請使用 OpenAI API 的用戶，提出有關的 prompts 然後根據這個訓練初步模型，並把這個初步模型放到他們的 Playground 給用戶使用。\n再把用戶問的問題蒐集回來，並做篩選。\n訓練 SFT 的模型用 13k training prompts\n訓練 RM 的模型用 33k training prompts\n訓練 PPO 的模型用 31k training prompts\nModel Supervised fine-tuning(SFT)\n拿 GPT-3 去訓練 16 個 epochs 跑一個 epoch 就發現 overfitting，但發現訓練更多 epoches 對後面的 RM 有用，而且這個 model 也只是過渡產品 Reward modeling(RM)\n把 SFT 後面的 unembedding layer 去除掉，接上線性層，最後輸出一個 scalar reward\n用 6B RMs\n這模型會吃 prompt 和 response\n人工標記的是排序，不是分數\n對每個 prompt 生出 9 個答案\n原本是 4 個，但排 9 個花的時間可能不會到 4 個的兩倍，因為主要心力會花在讀 prompt。但標註訊息會多很多，因為都是兩兩比較。 而且在 loss 中最多只要丟入 RM 9 次，因為可以重用 Pairwise Ranking Loss\n對一個 prompt(假設是 x)，取出一對回覆(假設是 $y_w$ 和 $y_l$)，算出 RM(x, $y_w$) 和 RM(x, $y_l$)，假設 $y_w$ 比 $y_l$ 排序高，讓 RM(x, $y_w$) - RM(x, $y_l$) 的數值越大越好 Reinforcement learning(RL)\nPPO\n$\\beta$ 那項是 KL divergence $\\gamma$ 那項是不想要讓這 model 太專注在微調的任務，而失去原本在其他 NLP 任務也表現很好的功能。 $D_{pretrain}$ 是 pretraining distribution 如果 $\\gamma$ 為 0，在該實驗中叫做 PPO，否則，稱為 PPO-ptx Result ","date":"2023-01-27T17:39:12+08:00","permalink":"https://roykesydon.github.io/Blog/p/instructgpt/","title":"InstructGPT"},{"content":"介紹 一種用於自動化找超參數的方法，用在採樣昂貴而且是黑盒子的情況\n流程 取樣一些資料點 生出一個 Surrogate Model(可採用 Gaussian Process) 反覆做以下事情 用 Acquisition Function 挑選下一個要採樣的點 重新評估 Surrogate Model Gaussian Process 最終的 prediction 是一個 distribution 而不是單一個數字 生成方法需借助 kernel function，常用 RBF(Radial Basis Function)\n$K(x, x^{\u0026rsquo;}|\\tau)=\\sigma^2exp(-\\frac{1}{2}(\\frac{x-x^{\u0026rsquo;}}{l})^2)$\n$\\sigma$ 和 $l$ 是兩個可以調整的超參數\nAcquisition Function 可用超參數來調節 exploitation 和 exploitation\nUCB(Upper confidence bound) PI(probability of improvement) EI(Expected improvement) ","date":"2023-01-26T01:36:53+08:00","permalink":"https://roykesydon.github.io/Blog/p/bayesian-optimization/","title":"Bayesian Optimization"},{"content":"PPFDT per process file descriptor table 每個 process 都有 存放 file descriptors file descriptors 是一個唯一的整數，用來識別作業系統上的 open file 0, 1, 2 是 Standard input / ouput / error 大小受限於 OPEN_MAX，亦即能同時間能開的最多檔案數 Redirection Input redirection $ wc \u0026lt; /etc/passwd 把 wc 的 PPFDT 的 stdin 改成 /etc/passwd 如果是 $ wc /etc/passwd，則是在 PPFDT 追加 /etc/passwd Ouput redirection $ wc \u0026gt; f1 把 wc 的 PPFDT 的 stdout 改成 f1 Input \u0026amp; output redirection 兩個可以同時用\n$ cat \u0026lt; f1 \u0026gt; f2 \u0026gt;\u0026gt; 可以 append $ \u0026lt; f1 cat \u0026gt; f2 可以亂換位置 Error redirection $ find / -name f1 2\u0026gt; error 1\u0026gt; outputs 這樣就會把那些 Permission denied 的給到 errors，成功的給到 outputs 2\u0026gt;/dev/null /dev/null 會把丟進來的東西都丟棄 Copy Descripter 這兩者等價 $ cat f1 1\u0026gt;op_err 2\u0026gt;op_err $ cat f1 1\u0026gt;op_err 2\u0026gt;\u0026amp;1 make 2 a copy of 1 ","date":"2023-01-21T02:20:43+08:00","permalink":"https://roykesydon.github.io/Blog/p/io-redirection/","title":"IO Redirection"},{"content":"Compile C 4-steps pre-processing compilation assembly linking Types of Object Files Executable object file Relocatable object file Shared object file Core file Formats of Object Files a.out initial version of UNIX COFF SVR3 UNIX PE Win. NT ELF SVR4 Linux ELF format of a program ELF Header Program Header Table .text .rodata .data .bss .symtab .rel.text .rel.data .debug .line .strtab Section Header Table 可參考: http://ccckmit.wikidot.com/lk:elf\nProcess Instance of a program running on a computer\nProcess Control Block task_struct\nProcess Identification PID, PPID, SID, UID, EUID.. Process State Information Process Control Information ","date":"2023-01-21T00:08:25+08:00","permalink":"https://roykesydon.github.io/Blog/p/process-management/","title":"Process Management"},{"content":"Features Process control Variables Flow control Functions File \u0026amp; cmd name completions Cmd line editng Cmd history Command Mode Interactive Non- Interactive Command Type internal / Builtin command\n指令的程式碼是 shell 的一部分 e.g., cd, exit 不會產生 child process 有些 internal command，比如 echo, pwd，會 internal 和 external 都有實作 external command\n指令的程式碼在硬碟上的某個 binary file e.g., clear, ls 會產生 child process Common Commands 比較實用或常用的\ngrep\n找字詞\ngrep \u0026lt;string/pattern\u0026gt; -i 大小寫不敏感 -v 不包含關鍵字的 cut 找 column\n-f 找哪些 column -d 分隔符是什麼 比較兩個檔案\ncomm\n顯示 file1 獨有的列、 file2 獨有的列、file1 和 file2 共有的列\ncmp, diff\n回傳不一樣的列資訊\nunset\n把指定的變數移除掉\ntee\n吃 stdin 輸出到 stdout 和其他檔案\nless\n讀檔案用\nExpansions White space Control Operators ; 讓指令接著執行 \u0026amp; 放在結尾，讓指令在背景執行 \u0026amp;\u0026amp; logical AND || logical OR\n前面失敗才會跑後面\n# 註解用 \\ escape special characters 放結尾好換行繼續輸入 $? 一個特別的變數，有上個指令的 exit code Shell variables User defined Env var Shell history File Globing *, ?, [], -, ! ","date":"2023-01-19T23:00:02+08:00","permalink":"https://roykesydon.github.io/Blog/p/shell/","title":"Shell"},{"content":"GPT 本質上就是 Transformer 的 decoder\nGPT-1 paper: Improving Language Understanding by Generative Pre-Training\n用 semi-supervised，後來被歸為 self-supervised\nUnsupervised pre-training $L_1(U)=\\sum_i logP(u_i|u_{i-k},\u0026hellip;,u_{i-1};\\theta)$\n$U= \\{ u_1,\u0026hellip;,u_n \\}$\n$U$ 是一系列未標記的文本 token\n$k$ 是窗口大小\n模型大致架構 $h_0=UW_e+W_p$\n$h_1=transformer \\_ block(h_{i-1})\\forall i \\in[1,n]$\n$P(u)=softmax(h_nW^T_e)$\n$U=\\{u_{-k},\u0026hellip;,u_{-1}\\}$\nSupervised fine-tuning $P(y|x^1,\u0026hellip;,x^m)=softmax(h^m_lW_y)$\n$L2(C)=\\sum_{(x,y)}log P(y|x^1,\u0026hellip;,x^m)$\n$L_3(C)=L_2(C)+\\lambda*L_1(C)$\n$C$ 是 labeled 的資料集，微調基本上就是在後面加上線性層\n作者最大化 likelihood 的時候是用 $L_3$ 而非單純的 $L_2$\n微調應用範例 資料集 用 BooksCorpus 訓練出來的\n有超過 7000 本未出版的書\n模型結構 12 層 transformer 的 decoder 768 維 word embedding 12 個 attention heads 和 BERT BASE 比較 BERT 論文比較晚出，但 BASE 的模型架構和 GPT 有相似之處，\nBASE 是 12 層的 decoder，word embedding 和 attention head 的維度或數量和 GPT-1 相同\nGPT-2 paper: Language Models are Unsupervised Multitask Learner\nGPT-2 除了用更大的的模型和更大的資料集，把重點放在 zero-shot 上，雖然在 GPT-1 的論文就有提過 zero-shot\n資料集 這次做了一個叫做 WebText 的資料集，有百萬級別的網頁\nCommon Crawl 大型爬蟲專案，有大量網頁資料，但充斥了垃圾訊息\nWebText WebText 的資料來源是 reddit 上的外部連結，只要有至少三個 karma，就會被採納，由此取得品質較好的網頁資料。透過這種方法，取得了 4500 萬個連結。並用Dragnet (Peters \u0026amp; Lecocq, 2013) and Newspaper content extractors 把文字訊息從 HTML 中抓出來\n架構 和原本差不多，變成有 1.5B 參數的 Transformer decoder\nzero-shot 不需要下游任務的標記資料\n改把任務輸入進模型\n目前問題 現在的模型泛化能力不太好 Multitask learning 在 NLP 上不太常用，NLP 現在主流還是在預訓練模型上做微調以應對下游任務 對每個下游任務都得重新訓練模型 得蒐集 labeled 資料 結果 GPT-3 paper: Language Models are Few-Shot Learners\n摘要 有 175B 的參數，由於模型極大，要在子任務微調會成本很大，所以不做任何梯度更新 在很多 NLP 任務有傑出的成果 可以生出人類難以區分的新聞文章 目前有的問題 要在子任務微調，需要資料集 微調後在有些子任務上表現好不代表你預訓練模型一定泛化能力高 人類不需要大量 labeled 資料去完成小任務 評估方式 分為三種，few / one / zero-shot learning 架構 基本上 GPT-3 和 GPT-2 架構一樣\n相同 modified initialization pre-normalization reversible tokenization described therein 不同 把 Sparse Transformer 的一些修改拿過來用 GPT-3 Small 是 GPT-1 的大小 GPT-3 Medium 是 BERT Large 的大小 GPT-3 XL 和 GPT-2 相近，比較淺也比較寬\nBatch Size 大小 模型小的時候需要小一點，透過這種額外的 noise 來避免 overfitting(不確定是不是猜想)\n資料集 Common Crawl 架構比 GPT-2 大很多，所以回頭考慮這個資料集\n三步驟 先過濾，透過 reddit 那個高品質的資料集，來訓練一個模型分類高品質和低品質的網頁。 透過 LSH 演算法把相似的文本過濾掉 把一些已知高品質的資料集也加進來 這是一個 Batch 裡有 60% 來自 Common Crawl(filtered) 的意思 Wikipedia 雖然總量比較少，但也有 3% 的採樣率\n結果 計算量指數增長，loss 卻是線性的往下降\npaper 裡有很多任務的實驗結果，這邊就不附上了\nLimitations 在文本生成上還是比較弱，生很長的東西，可能會重複自己說過的話、失去連貫性、自相矛盾等等\n在有些雙向性的任務上可能表現更差\n影響 可能被用來散布不實消息、垃圾郵件等等 偏見 結論 在很多 NLP 任務可以做到接近 SOTA 微調模型的成果\n","date":"2023-01-19T01:50:07+08:00","permalink":"https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/","title":"GPT 三部曲"},{"content":"VM A software implementation of a machine\nSystem VM 提供可以執行 GuestOS 的 complete system platform Process VM 像一個一般的 app 一樣在 hostOS 跑，支援單一個 process Hypervisor 又稱虛擬機器監視器（英語：virtual machine monitor，縮寫為VMM） 用來管理 VM\n允許多個 GuestOS 跑在 host computer\nType-1\nbare-metal hypervisors 直接在硬體上執行 Type-2\nhosted hypervisors 在 hostOS 上執行 directories Binary\ne.g., bin, sbin, lib, opt bin: 有關 user 的指令 sbin: 管理員會用的指令 opt: optional software，多數機器中這是空的 Configuration\ne.g., boot, etc, Data\ne.g., home, root, srv, media, mnt, temp In memory 字面上的意思，不在 hard disk，在 memory\ne.g., dev, proc, sys System Resources\ne.g., usr Variable Data\ne.g., var ","date":"2023-01-19T01:50:07+08:00","permalink":"https://roykesydon.github.io/Blog/p/linux-%E7%91%A3%E4%BA%8B/","title":"Linux 瑣事"}]