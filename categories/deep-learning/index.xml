<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deep Learning on Roykesydon</title>
        <link>https://roykesydon.github.io/Blog/categories/deep-learning/</link>
        <description>Recent content in Deep Learning on Roykesydon</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 31 Mar 2024 00:27:55 +0800</lastBuildDate><atom:link href="https://roykesydon.github.io/Blog/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>ğŸ¦‘SQUIDğŸ¦‘ è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 31 Mar 2024 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/squid-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.13495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Radiography imaging protocols (æ”¾å°„ç·šæˆåƒå”å®š) æœƒå°ˆæ³¨æ–¼ç‰¹å®šçš„èº«é«”å€åŸŸï¼Œå› æ­¤æœƒåœ¨æ‚£è€…é–“ç”¢ç”Ÿå¤§é‡ç›¸ä¼¼çš„ç…§ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†åˆ©ç”¨é€™ç¨® structed informationï¼Œä½œè€…æå‡ºäº† Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (SQUID)ï¼Œå®ƒå¯ä»¥æŠŠå›ºæœ‰çš„äººé«”çµæ§‹åˆ†é¡ç‚ºåè¦†å‡ºç¾çš„ patternã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ¨ç†ç‹€æ…‹ä¸‹ï¼Œå®ƒå¯ä»¥è­˜åˆ¥åœ–ç‰‡ä¸­çš„ç•°å¸¸æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”è¼ƒå…©å€‹ chest X-ray benchmarkï¼ŒSQUID åœ¨éç›£ç£ç•°å¸¸æª¢æ¸¬ä¸Šè¶…è¶Šäº† 13 ç¨® SOTA æ–¹æ³•è‡³å°‘ 5 å€‹ç™¾åˆ†é»ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„å‰µå»ºäº†ä¸€å€‹æ–°çš„è³‡æ–™é›† (DigitAnatomy)ï¼Œè©²è³‡æ–™é›†çµåˆäº†èƒ¸è…”è§£å‰–å­¸ä¸­çš„ spatial correlation å’Œ consistent shape é€™å…©å€‹ç‰¹æ€§ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;æ”¾å°„ç·šæˆåƒå’Œä¸€èˆ¬åœ–ç‰‡çš„å·®åˆ¥
&lt;ul&gt;
&lt;li&gt;ä¸€èˆ¬çš„ photographic imaging å’Œ radiography imaging æ˜¯ä¸åŒçš„ã€‚ä¸€èˆ¬çš„åœ–ç‰‡ç‰©é«”ï¼Œæˆ‘å€‘æœƒå‡è¨­ translation invariance (å¹³ç§»ä¸è®Šæ€§)ï¼Œç„¡è«–è²“åœ¨å·¦å³ï¼Œéƒ½æ˜¯è²“ã€‚ä½†æ˜¯åœ¨æ”¾å°„ç·šæˆåƒä¸­ï¼Œçµæ§‹çš„ç›¸å°ä½ç½®å’Œæ–¹å‘æ˜¯è¾¨åˆ¥æ­£å¸¸å’Œç•°å¸¸çš„é‡è¦ç‰¹å¾µã€‚&lt;/li&gt;
&lt;li&gt;è€Œä¸”ç”±æ–¼ radiography imaging protocols ä»¥ç›¸ç•¶ä¸€è‡´çš„æ–¹å‘è©•ä¼°æ‚£è€…ï¼Œæˆåƒåœ¨ä¸åŒçš„è¨­å‚™è£½é€ å•†ã€è¨­æ–½ä½ç½®é‚„æœ‰æ‚£è€…çš„æƒ…æ³ä¸‹ï¼Œéƒ½å…·æœ‰å¾ˆå¤§çš„ç›¸ä¼¼æ€§ã€‚åƒé€™æ¨£åè¦†å‡ºç¾ä¸”ä¸€è‡´çš„çµæ§‹ï¼Œæœ‰åŠ©æ–¼åˆ†æå•é¡Œï¼Œæ˜¯æ”¾å°„ç·šæˆåƒçš„å„ªå‹¢ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æœ‰å¤šé …ç ”ç©¶è­‰æ˜äº†è¨±å¤šå…ˆé©—çŸ¥è­˜åœ¨å¢å¼·æ·±åº¦å­¸ç¿’æ¨¡å‹æ€§èƒ½ä¸Šçš„å„ªå‹¢ï¼Œæ¯”å¦‚æ·»åŠ  location featuresã€ä¿®æ”¹ç›®æ¨™å‡½æ•¸é‚„æœ‰ç´„æŸç›¸å°æ–¼ç…§ç‰‡ä¸­ landmarks çš„ç›¸å°åº§æ¨™ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æƒ³è§£æ±ºçš„å•é¡Œ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¤šé” 80% çš„è‡¨åºŠéŒ¯èª¤æ˜¯ç”±æ–¼æ”¾å°„ç§‘é†«ç”Ÿæ¼æ‰ç•°å¸¸è€Œé€ æˆã€‚&lt;/li&gt;
&lt;li&gt;æœ¬æ–‡æƒ³å›ç­”ä¸€å€‹é—œéµå•é¡Œï¼šæœ‰æ²’æœ‰è¾¦æ³•åˆ©ç”¨ anatomical patterns çš„ consistency å’Œ spatial informationï¼Œåœ¨æ²’æœ‰æ‰‹å‹•æ¨™è¨»çš„æƒ…æ³ä¸‹ï¼ŒåŠ å¼·æ·±åº¦å­¸ç¿’æ¨¡å‹çš„ç•°å¸¸æª¢æ¸¬èƒ½åŠ›ï¼Ÿéç›£ç£çš„ç•°å¸¸æª¢æ¸¬åªç”¨å¥åº·çš„åœ–ç‰‡é€²è¡Œè¨“ç·´ï¼Œä¸ç”¨ç–¾ç—…è¨ºæ–·æˆ–ä»»ä½• labelã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SQUID è§£æ±ºè¾¦æ³•&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ¬æ–‡ä¸åƒå…ˆå‰çš„ç•°å¸¸æª¢æ¸¬æ–¹æ³•ï¼Œæœ¬æ–‡æŠŠ task åˆ¶å®šç‚º in-painting task (åœ–åƒä¿®å¾©)ï¼Œå¥½åˆ©ç”¨æ”¾å°„ç·šæˆåƒçš„å¤–è§€ã€ä½ç½®ã€å¸ƒå±€ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½œè€…æå‡ºäº† SQUIDï¼Œåœ¨è¨“ç·´éç¨‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥é€éç©ºé–“ä¸­ç¶“å¸¸å‡ºç¾çš„ anoatomical patterns ä¾†å‹•æ…‹ç¶­è­·ä¸€å€‹ visual pattern dictionaryã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”±æ–¼è§£å‰–å­¸çš„ consistencyï¼Œå¥åº·æˆåƒä¸­çš„èº«é«”å€åŸŸæœƒå‘ˆç¾é¡ä¼¼çš„ visual patternï¼Œä½¿ unique pattern çš„æ•¸é‡æ˜¯å¯æ§çš„ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨æ¨ç†éšæ®µï¼Œç”±æ–¼ dictionary ä¸å­˜åœ¨ anomaly patternï¼Œå› æ­¤å¦‚æœå­˜åœ¨ç•°å¸¸ï¼Œç”¢ç”Ÿçš„æ”¾å°„ç·šæˆåƒæœƒå’Œç¾å¯¦æœ‰æ‰€å·®è·ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯ä»¥é€éå€åˆ†ä¿®å¾©ä»»å‹™çš„å“è³ªä¾†è­˜åˆ¥ç•°å¸¸ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯¦é©—å‡è¨­&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç•°å¸¸æª¢æ¸¬çš„æˆåŠŸåŸºæ–¼å…©å€‹å‡è¨­
&lt;ul&gt;
&lt;li&gt;è³‡æ–™ä¸­å¾ˆå°‘ç•°å¸¸åœ–ç‰‡&lt;/li&gt;
&lt;li&gt;ç•°å¸¸å’Œæ­£å¸¸æœ‰é¡¯è‘—ä¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯¦é©—&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨å…©å€‹å¤§è¦æ¨¡ã€å…¬é–‹çš„æ”¾å°„ç·šæˆåƒè³‡æ–™é›†ä¸Šå¯¦é©—
&lt;ul&gt;
&lt;li&gt;ZhangLab
&lt;ul&gt;
&lt;li&gt;åœ¨éç›£ç£æ–¹é¢è´ SOTA è¶…é 5 å€‹ç™¾åˆ†é»&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;æ¯”æœ€è¿‘çš„ 13 ç¨®æ–¹æ³•æé«˜ 10 å€‹ç™¾åˆ†é»&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ–°è³‡æ–™é›†&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‰µå»ºäº† DigitAnatomy è³‡æ–™é›†ï¼Œé—¡æ˜èƒ¸è…”è§£å‰–çµæ§‹çš„ spatial correlation å’Œ consistent shapeã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è²¢ç»ç¸½çµ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨èƒ¸è…”æ”¾å°„ç·šæˆåƒçš„æ–°éç›£ç£ SOTA ç•°å¸¸æª¢æ¸¬æ–¹æ³•&lt;/li&gt;
&lt;li&gt;æ–°çš„ç¶œåˆè³‡æ–™é›†&lt;/li&gt;
&lt;li&gt;ç™¼æ˜æ–°æ–¹æ³•æ‰“æ•—ä¸»æµéç›£ç£ç•°å¸¸æª¢æ¸¬æ–¹æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Anomaly detection in natural imaging
&lt;ul&gt;
&lt;li&gt;è­˜åˆ¥åé›¢æ­£å¸¸è³‡æ–™åˆ†ä½ˆçš„ç½•è¦‹äº‹ä»¶&lt;/li&gt;
&lt;li&gt;ç”±æ–¼ç•°å¸¸æ¨£æœ¬çš„ç¼ºä¹ï¼Œå¾Œä¾†çš„å·¥ä½œéƒ½åˆ¶å®šç‚ºéç›£ç£å­¸ç¿’å•é¡Œ&lt;/li&gt;
&lt;li&gt;å¤§è‡´åˆ†ç‚ºå…©é¡
&lt;ul&gt;
&lt;li&gt;reconstruction-based
&lt;ul&gt;
&lt;li&gt;æ¢å¾©åŸå§‹è¼¸å…¥ä¸¦åˆ†æé‡å»ºèª¤å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;density-based
&lt;ul&gt;
&lt;li&gt;é€éä¼°è¨ˆæ­£å¸¸è³‡æ–™çš„åˆ†ä½ˆä¾†é æ¸¬ç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä¸éé€™äº›æ–¹æ³•éƒ½æ²’è¾¦æ³•è§£é‡‹å¯èƒ½çš„ç•°å¸¸ï¼Œæœ¬æ–‡é€éç¶­è­· visual pattern memory ä¾†è§£æ±ºé€™å€‹å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Anomaly detection in medical imaging
&lt;ul&gt;
&lt;li&gt;åŸºæ–¼ç›£ç£å­¸ç¿’çš„æ–¹æ³•å¤šåŠç”¨æ–¼æª¢æ¸¬ç‰¹å®šç¨®é¡çš„ç•°å¸¸ï¼Œæ¯”å¦‚è…«ç˜¤&lt;/li&gt;
&lt;li&gt;æœ€è¿‘æå‡ºäº†ä¸€äº›ç„¡ç›£ç£æ–¹æ³•ä¾†æª¢æ¸¬ä¸€èˆ¬ç•°å¸¸ï¼Œå’Œ GAN æœ‰é—œï¼Œä½†æ˜¯é€™äº›æ–¹æ³•éœ€è¦æœ‰é—œæ–¼ç•°å¸¸ç¨®é¡çš„å¼·å¤§å…ˆé©—çŸ¥è­˜å’Œå‡è¨­æ‰èƒ½ä½¿å¢å¼·æœ‰æ•ˆ&lt;/li&gt;
&lt;li&gt;å’Œä¸€èˆ¬çš„ç…§ç‰‡ä¸åŒï¼ŒRadiography imaging protocols ç”Ÿæˆå…·ä¸€è‡´æ€§çš„åœ–ç‰‡ï¼Œç•°å¸¸çš„è®ŠåŒ–æ¯”è¼ƒå¾®å¦™ (subtle)ï¼Œæª¢æ¸¬èµ·ä¾†æ›´å…·æŒ‘æˆ°ï¼Œä½œè€…åˆ©ç”¨æ”¾å°„ç·šæˆåƒçš„ç‰¹æ€§ï¼Œå¤§å¤§æé«˜æª¢æ¸¬æ€§èƒ½ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory networks
&lt;ul&gt;
&lt;li&gt;éå¾€æœ‰ä¸€äº›æœ‰é—œæ–¼æŠŠ Memory modules ç´å…¥ç¥ç¶“ç¶²è·¯çš„ç ”ç©¶ï¼Œå…¶ä¸­æœ‰æ¡ç”¨åˆ° Memory Matrixã€‚æœ¬æ–‡å…‹æœäº† Memory matrix çš„ä¾·é™æ€§ï¼Œä¸¦æå‡ºä¸€ç¨®æœ‰æ•ˆä¸”é«˜æ•ˆç‡çš„çš„ memory queueã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;squid&#34;&gt;SQUID&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Feature extraction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠåœ–ç‰‡åˆ‡æˆ N x N å€‹ non-overlapping patchesï¼Œç„¶å¾Œé¤µå…¥ä¸€å€‹ encoder åšç‰¹å¾µæå–ï¼Œé€™è£¡æ˜¯ç”¨ CNN æå–ï¼Œä½†è¦ç”¨å…¶ä»– backbone ä¹Ÿå¯ä»¥&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image reconstruction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é€™è£¡æœƒç”¨ teacher å’Œ student generator
&lt;ul&gt;
&lt;li&gt;teacher
&lt;ul&gt;
&lt;li&gt;ç›´æ¥ç”¨ encoder çš„ feature é‡å»ºåœ–ç‰‡&lt;/li&gt;
&lt;li&gt;æœ¬è³ªä¸Šæ˜¯ auto-encoder&lt;/li&gt;
&lt;li&gt;ä½œç‚º regularizer ä¾†é¿å… student generator é‡è¤‡ç”Ÿæˆç›¸åŒçš„æ­£å¸¸åœ–ç‰‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;student
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨ in-painting block å¢å¼·å¾Œçš„ feature ä¾†é‡å»ºï¼Œæœ€å¾Œæœƒè¢«ç”¨åœ¨ discrimination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å…©å€‹ generator æœƒåœ¨æ¯å€‹ up-sampling level ç”¨ knowledge distillation paradigm ä¾†çµåˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anomaly discrimination&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨ adversarial learning å¾Œï¼Œä½¿ç”¨ discriminator ä¾†å€åˆ†æ­£å¸¸å’Œç•°å¸¸&lt;/li&gt;
&lt;li&gt;ç”¨ 2 å€‹ generator ä¾†ç”Ÿæˆåœ–ç‰‡ï¼Œå†ç”¨ discriminator ä¾†å€åˆ†ï¼Œåªæœ‰ student generator æœƒæ¥æ”¶ discriminator çš„æ¢¯åº¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inventing-memory-queue-as-dictionary&#34;&gt;Inventing Memory Queue as Dictionary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Motivation
&lt;ul&gt;
&lt;li&gt;Memory Matrix è¢«å»£æ³›æ¡ç”¨
&lt;ul&gt;
&lt;li&gt;Feature æœƒé€éåœ¨ Memory matrix åšåŠ æ¬Šå¹³å‡ä¾†å¼·åŒ–&lt;/li&gt;
&lt;li&gt;ç¼ºé»
&lt;ul&gt;
&lt;li&gt;é€™æ¨£çš„å¢å¼·æ–¹æ³•æ˜¯å°æ•´å¼µåœ–ç‰‡çš„æå‡ºçš„ç‰¹å¾µåšçš„ï¼Œä¸Ÿæ£„äº†åœ–ç‰‡ä¸­çš„ spatial informationã€‚å°è‡´ä»–ç„¡æ³•æ„ŸçŸ¥åˆ°æ”¾å°„ç·šæˆåƒä¸­çš„ä¸€è‡´æ€§çµæ§‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Space-aware memory
&lt;ul&gt;
&lt;li&gt;ç‚ºäº†åˆ©ç”¨ç©ºé–“è³‡è¨Šï¼Œä½œè€…åªå°‡ patch è€Œä¸æ˜¯æ•´å¼µåœ–ç‰‡å‚³éåˆ° modelï¼Œè®“ patch åªèƒ½å­˜å– Memory matrix ä¸­å°æ‡‰åˆ°çš„å€æ®µï¼Œä½œè€…æŠŠé€™ç¨®ç­–ç•¥ç¨±ç‚º Space-aware memoryï¼Œè€Œä¸”é‚„å¯ä»¥åŠ å¿«é€Ÿåº¦ï¼Œå› ç‚ºä¸ç”¨å­˜å–æ•´å€‹ Memory matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Memory queue
&lt;ul&gt;
&lt;li&gt;åœ¨ learning-based Memory matrix ä¸­ï¼Œnormal patterns æ˜¯ç”± matrix ä¸­çš„ learned basis çµ„åˆè€Œæˆï¼Œä½†çµ„åˆå‡ºä¾†çš„æ±è¥¿å’Œç¾å¯¦ç…§ç‰‡çš„ç‰¹å¾µç¸½æœƒæœ‰åˆ†ä½ˆå·®è·ï¼Œä½¿å¾ŒçºŒçš„å½±åƒç”Ÿæˆè®Šå¾—å›°é›£&lt;/li&gt;
&lt;li&gt;ä½œè€…æå‡º memory queueï¼Œç”¨ä¾†åœ¨è¨“ç·´æœŸé–“å„²å­˜çœŸå¯¦çš„å½±åƒ featureï¼Œå¾è€Œå‘ˆç¾å’Œå½±åƒç‰¹å¾µç›¸åŒçš„åˆ†ä½ˆã€‚å®ƒåœ¨è¨“ç·´æœŸé–“æœƒæŠŠå…ˆå‰çœ‹åˆ°çš„ç‰¹å¾µç›´æ¥è¤‡è£½åˆ° queue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gumbel shrinkage
&lt;ul&gt;
&lt;li&gt;æ§åˆ¶ memory matrix ä¸­ activated pattern çš„æ•¸é‡æ˜¯æœ‰åˆ©çš„ï¼Œä½†å¦‚æœç”¨ hard shrinkage threashold æœƒç„¡æ³•è™•ç†æ‰¾ä¸åˆ°åˆé© entry çš„æƒ…æ³ã€‚ä¸€ç¨®è‡ªç„¶çš„è§£æ³•æ˜¯è®“æ¢¯åº¦æµéå‰ k å€‹ç›¸ä¼¼çš„ entryï¼Œå…¶é¤˜çš„ä¸æ›´æ–°ã€‚ä½†é€™æ¨£åˆæœƒå°è‡´æœªå•Ÿå‹•çš„ entry ç„¡æ³•æ¥æ”¶ä»»ä½•æ¢¯åº¦ä¸¦æ›´æ–°ï¼Œå› æ­¤æå‡ºäº† Gumbel shrinkage schema
&lt;ul&gt;
&lt;li&gt;$w&amp;rsquo; = sg(hs(w,topk(w)) - \phi(w)) + \phi(w)$
&lt;ul&gt;
&lt;li&gt;$w$ ä»£è¡¨ feature å’Œ entry çš„ç›¸ä¼¼åº¦&lt;/li&gt;
&lt;li&gt;$sg(\cdot)$ ä»£è¡¨ stop-gradientï¼Œä¸è¨ˆç®—è¼¸å…¥çš„æ¢¯åº¦&lt;/li&gt;
&lt;li&gt;$hs(\cdot, t)$ ä»£è¡¨ hard shrinkageï¼Œæœ‰å€‹ threshold $t$&lt;/li&gt;
&lt;li&gt;$\phi(\cdot)$ ä»£è¡¨ softmax&lt;/li&gt;
&lt;li&gt;é€™æ¨£ä¿ç•™äº† top k ä½œç‚º w çš„æœ€çµ‚çµæœï¼Œåˆç”¨ softmax å°æ‰€æœ‰ entry é€²è¡Œæ›´æ–°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;formulating-anomaly-detection-as-in-painting&#34;&gt;Formulating Anomaly Detection as In-painting&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Motivation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image in-painting æœ€åˆæ˜¯ç”¨ä¾†æ¢å¾©å…·æœ‰ neighboring context çš„åœ–ç‰‡å€å¡Šï¼Œå› æ­¤æ ¹æ“šæ­¤ç›´è¦ºï¼Œæƒ³æŠŠç•°å¸¸åœ–ç‰‡ä¿®å¾©æˆæ­£å¸¸åœ–ç‰‡ä¾†å¯¦ç¾æª¢æ¸¬&lt;/li&gt;
&lt;li&gt;åœ¨ä¿®å¾©åƒç´ çš„æ™‚å€™ï¼Œç‰¹åˆ¥æ˜¯ç”¨æ·±åº¦ç¶²è·¯ï¼Œå®¹æ˜“æœ‰ boundary artifactsï¼Œåœ¨ pixel ç´šåˆ¥çš„ä¿®å¾©ä¸­ï¼Œé€™äº› boundary artifacts æœƒå°è‡´å¤§é‡èª¤å ±
&lt;ul&gt;
&lt;li&gt;artifact ä¸­ç¿»å¥½åƒæ˜¯ã€Œå½å½±ã€ï¼Œå°±æ˜¯é‡å»ºçš„æ™‚å€™æœƒå‘ˆç¾æœ‰é»åƒæ£‹ç›¤çš„æ•ˆæ‡‰&lt;/li&gt;
&lt;li&gt;ä½œè€…é¸æ“‡åœ¨ feature level é€²è¡Œ in-paintingï¼Œé¿é–‹é€™å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In-painting block&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æœƒå…ˆæŠŠæ¯å€‹ patch $F_{1,1}$ ~ $F_{w,h}$ éƒ½å…ˆæ‰¾åˆ°æœ€æ¥è¿‘çš„ normal patterns $N_{1,1}$ ~ $N_{w,h}$&lt;/li&gt;
&lt;li&gt;å› ç‚º N æ˜¯ä¹‹å‰è¨“ç·´è³‡æ–™ä¸­æå–çš„ç‰¹å¾µçµ„æˆçš„ï¼Œä¸å—ç•¶å‰è¼¸å…¥å½±åƒçš„å½±éŸ¿ã€‚ç‚ºäº†å°å…¥è¼¸å…¥åœ–ç‰‡çš„ç‰¹å¾µï¼Œä½œè€…æŠŠ F å’Œ N ç”¨ transformer block ä¾†çµåˆ
&lt;ul&gt;
&lt;li&gt;å°æ–¼æ¯å€‹ patch $F_{i,j}$ï¼ŒæœƒæŠŠå…¶ç•¶ä½œä¸­å¿ƒï¼Œç”¨ç›¸é„°çš„ 8 å€‹ N patch ä¾†é‡æ–°å®šç¾© $F_{i,j}$ï¼ŒæŠŠé€™ 8 å€‹ N patch ä½œç‚º key å’Œ valueï¼Œ$F_{i,j}$ ä½œç‚º query&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æœ€å¾Œæœƒåœ¨ in-painting block çš„å‰å¾Œåš point-wise convolution (1x1)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Masked shortcut&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯¦é©—çµæœè¡¨æ˜ï¼Œç›´æ¥åš residual connection æœƒé™ä½ä¿®å¾©çš„æ€§èƒ½ï¼Œä½œè€…æ¡ç”¨ random binary mask åœ¨ training æœŸé–“ gate shortcut feature
&lt;ul&gt;
&lt;li&gt;$F&amp;rsquo;=(1-\delta)\cdot F + \delta \cdot inpaint(F)$
&lt;ul&gt;
&lt;li&gt;$\delta$~$Bernoulli(\rho)$
&lt;ul&gt;
&lt;li&gt;$\rho$ gating probability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç²å¾— F&amp;rsquo; å¾Œï¼ŒåŸå§‹çš„ F æœƒè¢«æ›´æ–°é€² memory&lt;/li&gt;
&lt;li&gt;åœ¨æ¨è«–éšæ®µï¼Œæœƒ disable shortcutï¼Œä½¿ $F&amp;rsquo;=inpaint(F)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anomaly-discrimination&#34;&gt;Anomaly Discrimination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discriminator è©•ä¼°åœ–ç‰‡ç¾ä¸ç¾å¯¦ï¼Œä¸ç¾å¯¦è¡¨ç¤ºç•°å¸¸&lt;/li&gt;
&lt;li&gt;å› ç‚º Generator åªåœ¨æ­£å¸¸åœ–ç‰‡è¨“ç·´ï¼Œæ‰€ä»¥ Memory Queue ä¹Ÿåªæœ‰ normal pattern&lt;/li&gt;
&lt;li&gt;ç¨å¾®ç¸½çµ
&lt;ul&gt;
&lt;li&gt;in-painting block æœƒæŠŠ patch å¼·åŒ–ç‚ºç›¸ä¼¼çš„ normal feature&lt;/li&gt;
&lt;li&gt;student generator æœƒæ ¹æ“š &amp;ldquo;normal&amp;rdquo; feature é‡å»ºå‡º &amp;ldquo;normal&amp;rdquo; image&lt;/li&gt;
&lt;li&gt;å¦‚æœæ²’æœ‰ç•°å¸¸çš„è©±ï¼Œé‚£ input å’Œé‡å»ºçš„ image åœ¨èªæ„ä¸Šæ‡‰è©²ç›¸å·®å¾ˆå°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç•°å¸¸åˆ†æ•¸ $A$ çš„ç®—æ³•
&lt;ul&gt;
&lt;li&gt;$A=\phi(\frac{D(G_s(E(I)))-\mu}{\sigma})$
&lt;ul&gt;
&lt;li&gt;$\phi(\cdot)$ æ˜¯ sigmoid function&lt;/li&gt;
&lt;li&gt;$\mu$ å’Œ $\sigma$ æ˜¯æ ¹æ“š training samples ç®—å‡ºçš„ç•°å¸¸åˆ†æ•¸çš„å¹³å‡å€¼å’Œæ¨™æº–å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_t = (I-G_t (E(I)))^2$&lt;/li&gt;
&lt;li&gt;$\mathcal L_s = (I-G_s (E(I)))^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Knowledge distillation
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dist} = \sum_{l}^{i=1} (F^i_t-F^i_s)^2$
&lt;ul&gt;
&lt;li&gt;$l$ æ˜¯ levels of features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial loss
&lt;ul&gt;
&lt;li&gt;é¡ä¼¼ DCGAN&lt;/li&gt;
&lt;li&gt;$\mathcal L_{gen} = log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discriminator
&lt;ul&gt;
&lt;li&gt;$\mathcal L_{dis} = log(D(I)) + log(1-D(G_s(E(I))))$&lt;/li&gt;
&lt;li&gt;æŠŠ real image æ©Ÿç‡æ‹‰é«˜ï¼ŒæŠŠ fake image æ©Ÿç‡æ‹‰ä½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Total loss
&lt;ul&gt;
&lt;li&gt;minimize generative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_t \mathcal L_t + \lambda_s \mathcal L_s + \lambda_{dist} \mathcal L_{dist} + \lambda_{gen} \mathcal L_{gen}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;maximize discriminative loss
&lt;ul&gt;
&lt;li&gt;$\lambda_{dis} \mathcal L_{dis}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;new-benchmark&#34;&gt;New Benchmark&lt;/h3&gt;
&lt;p&gt;æå‡ºä¸€å€‹æ–°è³‡æ–™é›† - DigitAnatomyã€‚ã€‚å¦‚æœåŒ…å«æ­£ç¢ºé †åºçš„é˜¿æ‹‰ä¼¯æ•¸å­— 1~9 å‰‡è¦–ç‚ºæ­£å¸¸ï¼Œç•°å¸¸åŒ…æ‹¬ç¼ºå¤±ã€äº‚åºã€ç¿»è½‰å’Œ zero digitã€‚&lt;/p&gt;
&lt;p&gt;è©²è³‡æ–™é›†å°æ–¼æ”¾å°„ç·šæˆåƒå°¤å…¶æœ‰åˆ©ï¼ŒåŸå› å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial correlation and consistent shape&lt;/li&gt;
&lt;li&gt;æ”¾å°„ç·šæˆåƒè¦æ¨™è¨˜éœ€è¦å°ˆæ¥­çŸ¥è­˜ï¼Œä½†æ•¸å­—å®¹æ˜“ debug&lt;/li&gt;
&lt;li&gt;è©²è³‡æ–™é›†å¾ˆå®¹æ˜“å°±å¯ä»¥ç²å¾—æ¨¡æ“¬ç•°å¸¸çš„ ground truth&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;public-benchmarks&#34;&gt;Public Benchmarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ZhangLab Chest X-ray
&lt;ul&gt;
&lt;li&gt;åŒ…å«å¥åº·å’Œè‚ºç‚çš„å½±åƒ&lt;/li&gt;
&lt;li&gt;è¨“ç·´é›†
&lt;ul&gt;
&lt;li&gt;1349 å¼µæ­£å¸¸&lt;/li&gt;
&lt;li&gt;3883 å¼µç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ¸¬è©¦é›†
&lt;ul&gt;
&lt;li&gt;234 å¼µæ­£å¸¸&lt;/li&gt;
&lt;li&gt;390 å¼µç•°å¸¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ä½œè€…å¾è¨“ç·´é›†éš¨æ©ŸæŒ‘ 200 å¼µåšç‚ºèª¿æ•´è¶…åƒæ•¸çš„ validation set&lt;/li&gt;
&lt;li&gt;å½±åƒéƒ½èª¿æ•´ç‚º 128x128&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stanford CheXpert
&lt;ul&gt;
&lt;li&gt;å° front-view PA å½±åƒé€²è¡Œè©•ä¼°ï¼Œå…±æœ‰ 12 ç¨®ç•°å¸¸&lt;/li&gt;
&lt;li&gt;æœ‰ 5249 å¼µæ­£å¸¸å’Œ 23671 å¼µç•°å¸¸ç”¨ä½œè¨“ç·´
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨å’Œ ZhangLab ç›¸åŒçš„è¶…åƒæ•¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç”¨è¨“ç·´é›†çš„ 250 å¼µæ­£å¸¸å’Œ 250 å¼µç•°å¸¸é€²è¡Œæ¸¬è©¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;baselines-and-metrics&#34;&gt;Baselines and Metrics&lt;/h3&gt;
&lt;p&gt;è€ƒæ…® 13 å€‹ä¸»è¦çš„ baseline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç¶“å…¸ UAD (unsupervised anomaly detection)
&lt;ul&gt;
&lt;li&gt;Auto-encoderã€VAE&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;é†«å­¸å½±åƒçš„ SOTA
&lt;ul&gt;
&lt;li&gt;Ganomalyã€f-AnoGANã€IFã€SALAD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æœ€è¿‘çš„ UAD
&lt;ul&gt;
&lt;li&gt;MemAEã€CutPasteã€M-KDã€PANDAã€PaDiMã€IGD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;é™¤éæœ‰ç‰¹åˆ¥è¨»æ˜ï¼Œä¸ç„¶éƒ½æ˜¯å¾é ­ç¨ç«‹è¨“ç·´è‡³å°‘ä¸‰æ¬¡&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;interpreting-squid-on-digitanatomy&#34;&gt;Interpreting SQUID on DigitAnatomy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ DigitAnatomy çš„å¯¦é©—ä¸­ï¼Œæ•…æ„æ³¨å…¥ç•°å¸¸åˆ°æ­£å¸¸åœ–ç‰‡ä¸­ï¼Œæ¸¬è©¦æ¨¡å‹æ˜¯å¦å¯ä»¥é‡å»ºæ­£å¸¸åœ–ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;SQUID é‡å»ºå‡ºçš„åœ–ç‰‡æ¯”å…¶ä»– baseline æœ‰æ›´å¤šæœ‰æ„ç¾©çš„è¨Šæ¯ï¼Œä¸»è¦æ­¸åŠŸæ–¼ space-aware memoryï¼Œå…¶ç”¢ç”Ÿç¨ç‰¹çš„ patternï¼Œè€Œä¸”å’Œç©ºé–“è¨Šæ¯ç›¸é—œè¯ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€æ—¦å‡ºç¾ç•°å¸¸ï¼Œin-painting block æœƒå¾å­—å…¸ä¸­æ‰¾å‡ºå‰ k å€‹ç›¸è¿‘çš„ï¼ŒæŠŠç•°å¸¸ç‰¹å¾µå¢å¼·åˆ°å…¶å°æ‡‰çš„æ­£å¸¸ç‰¹å¾µï¼Œå…¶ä»–æ–¹æ³•ä¸å…·å‚™æ­¤èƒ½åŠ›ï¼Œæ‰€ä»¥ä»–å€‘é‡å»ºå‡ºæœ‰ç¼ºé™·çš„åœ–åƒã€‚&lt;/p&gt;
&lt;p&gt;GAN å‚¾å‘æ–¼é‡å»ºè¨“ç·´æ¨£æœ¬å¹³å‡å¾—åˆ°çš„å½±åƒã€‚
MemAE å—ç›Šæ–¼ Memory matrixï¼Œè¡¨ç¾è¼ƒå¥½ï¼Œä½†å°æ–¼ç¼ºå¤±æ•¸å­—çš„ç•°å¸¸æ•ˆæœä¸ä½³ã€‚&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-squid-on-chest-radiography&#34;&gt;Benchmarking SQUID on Chest Radiography&lt;/h3&gt;
&lt;h4 id=&#34;limitation&#34;&gt;Limitation&lt;/h4&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ç›®å‰çš„ SQUID æ²’è¾¦æ³•åœ¨åƒç´ å±¤ç´šç²¾ç¢ºå®šä½ç•°å¸¸ã€‚é€™å¯ä»¥ç†è§£ï¼Œå› ç‚º SQUID æ˜¯ä¸€ç¨®éç›£ç£æ–¹æ³•ï¼Œä¸éœ€è¦æ¨™è¨»ã€‚&lt;/p&gt;
&lt;p&gt;é‚£äº›åƒç´ ç´šåˆ¥çš„ç•°å¸¸æª¢æ¸¬æœƒé­é‡æ”¾å¤§é›œè¨Šçš„å½±éŸ¿ï¼Œä½†æ˜¯ç”±æ–¼ SQUID æ˜¯åœ¨ç‰¹å¾µå±¤ç´šé€²è¡Œçš„ï¼Œæ¯”åƒç´ ç´šåˆ¥æ›´åŠ  robustã€‚&lt;/p&gt;
&lt;h3 id=&#34;ablating-key-properties-in-squid&#34;&gt;Ablating Key Properties in SQUID&lt;/h3&gt;
&lt;h4 id=&#34;component-study&#34;&gt;Component study&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;hyper-parameter-robustness&#34;&gt;Hyper-parameter robustness&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig9.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;disease-free-training-requirement&#34;&gt;Disease-free training requirement?&lt;/h4&gt;
&lt;p&gt;ç”¨æ–¼é†«å­¸ç•°å¸¸æª¢æ¸¬çš„éç›£ç£æ–¹æ³•ä¸¦ä¸å¸¸è¦‹ï¼Œå› ç‚ºæ‰€è¬‚çš„ UAD æ–¹æ³•ä¸¦ä¸æ˜¯ã€Œéç›£ç£ã€çš„ï¼Œå› ç‚ºä»–å€‘å¿…é ˆåªåœ¨ç„¡ç–¾ç—…å½±åƒä¸Šä½œè¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å¯¦è¸ä¸­ï¼Œè¦ç²å¾—å¥åº·åœ–ç‰‡éœ€è¦ manual annotationã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨“ç·´é›†ä¸­è€ƒæ…® disease-free å¾ 100% - 50% çš„æƒ…æ³ï¼ŒæŠŠ SQUID çš„ robust å’Œå¦å¤–ä¸‰å€‹ baseline é€²è¡Œæ¯”è¼ƒã€‚&lt;/p&gt;
&lt;p&gt;SQUID çš„ memory queue å¯ä»¥è‡ªå‹•å¿½ç•¥å°‘æ•¸çš„ anatomical patternsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SQUID/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MRL è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/mrl-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 26 Feb 2024 00:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mrl-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.13147&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Matryoshka Representation Learning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æƒ³è¨­è¨ˆ flexible çš„ representation å¥½é©æ‡‰ä¸åŒçš„ä¸‹æ¸¸ä»»å‹™&lt;/p&gt;
&lt;p&gt;MRL æ ¹æ“šä¸åŒçš„ granularities ä¾† encode è³‡è¨Šï¼Œä¸¦å…è¨±ä¸€å€‹ single embedding ä¾†é©æ‡‰ä¸‹æ¸¸ä»»å‹™çš„é‹ç®—é™åˆ¶&lt;/p&gt;
&lt;p&gt;MRL å­¸ç¿’å¾ coarse-to-fine çš„ representationï¼Œè‡³å°‘å’Œç¨ç«‹è¨“ç·´ä½ç¶­åº¦çš„ representation æœ‰ä¸€æ¨£çš„æº–ç¢ºåº¦&lt;/p&gt;
&lt;p&gt;åœ¨ç›¸åŒç²¾æº–åº¦ä¸‹ï¼ŒMRL çš„ embedding ç¸®å° 14 å€&lt;/p&gt;
&lt;p&gt;åœ¨ ImageNet-1K å’Œ 4K ä¸Šé€²è¡Œå¤§è¦æ¨¡æª¢ç´¢æ™‚ï¼Œå¯¦éš›é€Ÿåº¦æå‡ 14 å€&lt;/p&gt;
&lt;p&gt;long-tail few-shot learning çš„æº–ç¢ºåº¦æé«˜ 2%ï¼Œä¸¦ä¸”å’ŒåŸæœ‰ representation ä¸€æ¨£ robust&lt;/p&gt;
&lt;p&gt;æœ€å¾Œï¼Œä½œè€…è­‰æ˜ MRL å¯ä»¥ç„¡ç¸«åœ°æ“´å±•åˆ°å„ç¨®æ¨¡å¼çš„ç¶²è·¯è¦æ¨¡è³‡æ–™é›†ï¼ŒåŒ…å« Vision, Vision + Language, å’Œ Language&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;deep representation çš„ deployment æœ‰å…©å€‹æ­¥é©Ÿï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æ˜‚è²´çš„ forward-pass è¨ˆç®— representation&lt;/li&gt;
&lt;li&gt;representations åœ¨ä¸‹æ¸¸çš„åˆ©ç”¨ (æ¯”å¦‚æª¢ç´¢)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åœ¨ web-scale ä¸Šï¼Œé€™ç¨®åˆ©ç”¨çš„æˆæœ¬è“‹éäº†ç‰¹å¾µè¨ˆç®—æˆæœ¬&lt;/p&gt;
&lt;p&gt;å¸¸è¦‹çš„åšæ³•çš„ representation çš„å‰›æ€§è¿«ä½¿å¤šå€‹ä»»å‹™ä¸­ä½¿ç”¨é«˜ç¶­åµŒå…¥å‘é‡&lt;/p&gt;
&lt;p&gt;äººé¡å°è‡ªç„¶ä¸–ç•Œçš„æ„ŸçŸ¥æ˜¯ç”±ç²—åˆ°ç´°çš„ç²’åº¦&lt;/p&gt;
&lt;p&gt;ç„¶è€Œï¼Œæˆ–è¨±æ˜¯åŸºæ–¼æ¢¯åº¦è¨“ç·´çš„ inductive-biasï¼Œæ·±åº¦å­¸ç¿’å‚¾å‘æ–¼å°‡ã€Œè³‡è¨Šã€æ“´æ•£åˆ°æ•´å€‹è¡¨ç¤ºå‘é‡ä¸­&lt;/p&gt;
&lt;p&gt;é€šå¸¸é€éè¨“ç·´å¤šå€‹ä½ç¶­æ¨¡å‹ã€è¯åˆå„ªåŒ–ä¸åŒå®¹é‡çš„å­ç¶²è·¯ã€äº‹å¾Œå£“ç¸®ï¼Œåœ¨ç¾æœ‰çš„ fixed representation ä¸Šå¯¦ç¾å½ˆæ€§&lt;/p&gt;
&lt;p&gt;é€™äº›æŠ€è¡“ä¸­çš„æ¯ä¸€ç¨®éƒ½é›£ä»¥æ»¿è¶³è‡ªé©æ‡‰å¤§è¦æ¨¡éƒ¨å±¬çš„è¦æ±‚ï¼ŒåŸºæ–¼é–‹éŠ· / ç¶­è­·è€ƒé‡ç­‰ç­‰&lt;/p&gt;
&lt;p&gt;MRL ä»¥ nested çš„æ–¹å¼ä¾†å­¸ç¿’ O(log(d)) å€‹ç›¸åŒçš„é«˜ç¶­å‘é‡ã€ä½†ä¸åŒ capacity çš„ representationï¼Œå› æ­¤è¢«ç¨±ç‚º Matryoshka&lt;/p&gt;
&lt;p&gt;Matryoshka Representation æé«˜äº†å¤§è¦æ¨¡åˆ†é¡å’Œæª¢ç´¢çš„æ•ˆç‡ï¼Œè€Œä¸æœƒé¡¯è‘—å¤±å»æº–ç¢ºåº¦&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡é‡é»é—œæ³¨åœ¨æ©Ÿå™¨å­¸ç¿’ç³»çµ±çš„å…©å€‹é—œéµæ¨¡çµ„ï¼šå¤§è¦æ¨¡åˆ†é¡å’Œæª¢ç´¢&lt;/p&gt;
&lt;p&gt;åœ¨åˆ†é¡ä¸Šï¼Œä½œè€…ä½¿ç”¨ variable-size representations çµåˆ adaptive cascades ä¾†é¡¯è‘—æ¸›å°‘å¯¦ç¾ç‰¹å®šç²¾åº¦æ‰€éœ€åµŒå…¥çš„å¹³å‡ç¶­åº¦&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ï¼Œåœ¨ ImageNet-1K ä¸Šï¼ŒMRL + Adaptive classification åœ¨å’Œ baseline åŒç²¾æº–åº¦çš„æƒ…æ³ä¸‹å°‡ representation ç¸®å° 14 å€&lt;/p&gt;
&lt;p&gt;å°æ–¼æª¢ç´¢ï¼Œå…ˆç”¨ query embedding ä¸€é–‹å§‹çš„ few dimensions ä¾†æ¸›å°‘ retrieval candidatesï¼Œç„¶å¾Œå†ç”¨æ›´å¤šçš„ dimensions ä¾† re-rank retrieved set&lt;/p&gt;
&lt;p&gt;MRL çš„æª¢ç´¢ç²¾ç¢ºåº¦å’Œ single-shot retrieval ç›¸ç•¶&lt;/p&gt;
&lt;p&gt;ä¸»è¦è²¢ç»å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æå‡ºäº† MRL ä¾†ç²å¾— filexible çš„ representation for adaptive deployment&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨ MRL é€²è¡Œå¤§è¦æ¨¡åˆ†é¡å’Œæª¢ç´¢ï¼Œé€Ÿåº¦æé«˜ 14 å€è€Œä¸”ä¸€æ¨£æº–ç¢º&lt;/li&gt;
&lt;li&gt;å¯ä»¥åœ¨è·¨ modalities é‚„æœ‰å¯æ¥å— web-scale è³‡æ–™çš„æƒ…æ³ä¸‹ç„¡ç¸«èª¿æ•´ MRL&lt;/li&gt;
&lt;li&gt;åœ¨å…¶ä»–ä¸‹æ¸¸ä»»å‹™çš„èƒŒæ™¯ä¸‹é€²ä¸€æ­¥åˆ†æ MRL çš„è¡¨ç¤º&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;efficient-classification-and-retrieval&#34;&gt;Efficient Classification and Retrieval&lt;/h3&gt;
&lt;p&gt;åœ¨æ¨ç†éç¨‹ä¸­ï¼Œåˆ†é¡å’Œæª¢ç´¢çš„æ•ˆç‡å¯ä»¥å¾å…©å€‹æ–¹é¢ç ”ç©¶ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æ·±åº¦ç‰¹å¾µçš„é«˜ä½†æ†å®šçš„æˆæœ¬&lt;/li&gt;
&lt;li&gt;éš¨è‘—æ¨™ç±¤ç©ºé–“å’Œæ•¸æ“šå¤§å°è€Œè®ŠåŒ–çš„æœç´¢æˆæœ¬&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç¬¬ä¸€å€‹å•é¡Œå¯ä»¥é€éä¸åŒçš„æ¼”ç®—æ³•è¨­è¨ˆé«˜æ•ˆçš„ç¥ç¶“ç¶²è·¯ä¾†è§£æ±º&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯ï¼Œä¼´éš¨è‘—å¼·å¤§çš„ featurizerï¼Œå¤šæ•¸ scale ç›¸é—œçš„å•é¡Œå‡ºåœ¨ æ¨™ç±¤æ•¸é‡(L)ã€è³‡æ–™å¤§å°(N)ã€æˆ–æ˜¯è¡¨ç¤ºç¶­åº¦(d) é€™ç¨® linear dependence ä¸Š&lt;/p&gt;
&lt;p&gt;è®“ RAM, disk å’Œ CPU éƒ½åŒæ™‚æœ‰å·¨å¤§çš„å£“åŠ›&lt;/p&gt;
&lt;p&gt;æ¨™ç±¤æ•¸é‡åœ¨è¨ˆç®—å’Œ RAM æ–¹é¢å·²ç¶“å¾—åˆ°äº†å¾ˆå¥½çš„ç ”ç©¶ï¼Œå¯ä»¥é€é Approximate Nearest Neighbor Search (ANNS) æˆ– leveraging the underlying hierarchy ä¾†è§£æ±º&lt;/p&gt;
&lt;p&gt;åœ¨è¡¨ç¤ºå¤§å°æ–¹é¢ï¼Œé™ç¶­ã€Hash å’Œç‰¹å¾µé¸æ“‡ç­‰æŠ€è¡“å¸¸ç”¨æ–¼ç·©è§£ O(d) çš„å¢é•·è¦æ¨¡ï¼Œä½†ä»£åƒ¹æ˜¯ç²¾ç¢ºåº¦çš„é¡¯è‘—é™ä½&lt;/p&gt;
&lt;p&gt;ANNS ä½¿ç”¨æˆ¶å¯ä»¥å¾è³‡æ–™åº«ä¸­å–å¾—å’Œè«‹æ±‚æœ€ç›¸ä¼¼çš„æ–‡ä»¶æˆ–åœ–ç‰‡&lt;/p&gt;
&lt;p&gt;å»£æ³›æ¡ç”¨çš„ HNSW å¯ä»¥è®“ O(d log(N))  å’Œæº–ç¢ºæœç´¢ O(dN) ä¸€æ¨£ç²¾æº–ï¼Œä½†ä»£åƒ¹æ˜¯éœ€è¦åœ¨ RAM å’Œ disk æ‰¿æ“” graph-based index çš„é–‹éŠ·&lt;/p&gt;
&lt;p&gt;MRL è§£æ±ºå° d çš„ç·šæ€§ä¾è³´å•é¡Œï¼Œä½ç¶­åº¦çš„ Mayryoshka representations å’Œç¨ç«‹è¨“ç·´çš„ representations ä¸€æ¨£æº–ç¢ºï¼Œè€Œä¸éœ€è¦å¤šæ¬¡æ˜‚è²´çš„å‰å‘å‚³é&lt;/p&gt;
&lt;h2 id=&#34;matryoshka-representation-learning&#34;&gt;Matryoshka Representation Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MRL/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æœ‰åˆ†å…©ç¨®è¨“ç·´æ–¹æ³•ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Matryoshka Representation Learning (MRL)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨å¾Œé¢æ¥ 9 å±¤ MLPï¼Œä¸æ˜¯ä¸²è¯ï¼Œæ˜¯ä¸¦è¯&lt;/li&gt;
&lt;li&gt;æ¯”å¦‚ mlp(768,8)&amp;hellip;., mlp(768,2048)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Efficient Matryoshka Representation Learning (MRL-E)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨å¾Œé¢åªæ¥ 1 å±¤ï¼Œç„¶å¾Œå–å‰é¢ç¶­åº¦å¾—åˆ°ä¸€å€‹å‘é‡ï¼Œä»¥æ­¤é¡æ¨&lt;/li&gt;
&lt;li&gt;æ¯”å¦‚ mlp(768, 2048)ï¼Œå¯èƒ½å–å‰ 16 ç¶­ç•¶ä¸€å€‹å‘é‡ï¼Œç„¶å¾Œå†å–å‰ 64 ç¶­ç•¶ä¸€å€‹å‘é‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>REALM è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/realm-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Fri, 05 Jan 2024 00:00:13 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/realm-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2002.08909&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;REALM: Retrieval-Augmented Language Model Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç‚ºäº†ç”¨æ›´åŠ æ¨¡çµ„åŒ–å’Œå¯è§£é‡‹çš„æ–¹å¼ç²å–çŸ¥è­˜ï¼Œä½œè€…ç”¨ latent knowledge retriever ä¾†åŠ å¼·èªè¨€æ¨¡å‹çš„é è¨“ç·´ï¼Œlatent knowledge retriever å…è¨±æ¨¡å‹æª¢ç´¢ large corpus çš„ documentï¼Œä¸¦åœ¨é è¨“ç·´ã€å¾®èª¿å’Œæ¨ç†æ™‚ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/REALM/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨è«¸å¦‚ BERT çš„èªè¨€æ¨¡å‹ä¸­ï¼Œå­¸åˆ°çš„ world knowledge éš±å¼åœ°å„²å­˜åœ¨ç¥ç¶“ç¶²è·¯çš„åƒæ•¸ä¸­ï¼Œä½¿å…¶å¾ˆé›£ç¢ºå®šå„²å­˜äº†å“ªäº›çŸ¥è­˜ä»¥åŠå„²å­˜åœ¨ä½•è™•ã€‚è€Œä¸”å„²å­˜ç©ºé–“å—ç¶²è·¯å¤§å°é™åˆ¶ï¼Œä½†è¨“ç·´æ›´å¤§çš„ç¶²è·¯å¯èƒ½éå¸¸æ˜‚è²´&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç‚ºäº†ä»¥æ›´åŠ å¯è§£é‡‹å’Œæ¨¡çµ„åŒ–çš„æ–¹å¼ç²å–çŸ¥è­˜ï¼Œæå‡ºäº†ä¸€å€‹æ–°ç©çš„æ¡†æ¶ï¼ŒRetrieval-Augmented Language Model (REALM) é è¨“ç·´ï¼Œé€é learned textual knowledge retriever ä¾†åŠ å¼·é è¨“ç·´&lt;/p&gt;
&lt;p&gt;å’ŒæŠŠçŸ¥è­˜å„²å­˜åœ¨åƒæ•¸ä¸­çš„æ¨¡å‹ç›¸æ¯”ï¼Œè©²æ–¹æ³•é¡¯ç¤ºåœ°æ­ç¤º world knowledge æ‰®æ¼”çš„è§’è‰²ï¼Œåšæ³•æ˜¯è¦æ±‚æ¨¡å‹åœ¨æ¨ç†éç¨‹ä¸­æ±ºå®šå“ªäº›çŸ¥è­˜ä¾† retrieveï¼Œä¸¦ç”¨åœ¨æ¨ç†éšæ®µ&lt;/p&gt;
&lt;p&gt;åœ¨æ¯æ¬¡é æ¸¬å‰ï¼Œèªè¨€æ¨¡å‹ä½¿ç”¨ retriever åœ¨ large corpus æœç´¢æ–‡ä»¶ï¼Œä¸¦ç”¨é€™äº›æ–‡ä»¶å¹«åŠ©é æ¸¬&lt;/p&gt;
&lt;p&gt;End to End çš„å­¸ç¿’éœ€è¦è€ƒæ…®æ•´å€‹æª¢ç´¢æ­¥é©Ÿï¼Œå¥½é€²è¡Œåå‘å‚³æ’­&lt;/p&gt;
&lt;p&gt;REALM æœ‰å€‹é—œéµç›´è¦ºï¼Œå°±æ˜¯è¨“ç·´ retriever çš„æ™‚å€™ç”¨çš„æ˜¯ unsupervised text çš„ performance-based signalï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä¸€å€‹å¯ä»¥æé«˜èªè¨€æ¨¡å‹è¤‡é›œæ€§çš„æª¢ç´¢æ˜¯æœ‰å¹«åŠ©ï¼Œä¸”è©²è¢«çå‹µçš„&lt;/li&gt;
&lt;li&gt;è³‡è¨Šä¸è¶³çš„æª¢ç´¢æ‡‰è©²å—åˆ°æ‡²ç½°&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æ¯”å¦‚åœ– Fig.1 æ‰¾åˆ°çš„æ–‡ä»¶å°±è©²ç²å¾—çå‹µ&lt;/p&gt;
&lt;p&gt;ä½œè€…å°‡ retrieve-then-predict çš„æ–¹æ³•å»ºæ¨¡ä¸¦è¦–ä½œ latent variable language model ä¸¦å„ªåŒ– marginal likelihood&lt;/p&gt;
&lt;p&gt;ä½†åœ¨é è¨“ç·´æœŸé–“è¦è¨“ç·´å¤§è¦æ¨¡çš„ retrieval module æˆç‚ºå•é¡Œï¼Œå› ç‚º Retriever å¾—ç‚ºæ¯å€‹é è¨“ç·´æ­¥é©Ÿè€ƒæ…®æ•¸ç™¾è¬å€‹å€™é¸æ–‡æª”ï¼Œè€Œä¸”å¿…é ˆæ ¹æ“šæ±ºç­–åå‘å‚³æ’­ã€‚ç‚ºäº†è§£æ±ºé€™å•é¡Œï¼Œä½œè€…å»ºæ§‹äº† retrieverï¼Œä»¥ä¾¿å¿«å–å’ŒéåŒæ­¥æ›´æ–°æ¯å€‹æ–‡ä»¶çš„è¨ˆç®—ï¼Œä¸¦å°‡æœ€ä½³æ–‡ä»¶çš„é¸æ“‡è¡¨ç¤ºç‚º Maximum Inner Product Search (MIPS)&lt;/p&gt;
&lt;p&gt;é€éåœ¨ Open-QA ä»»å‹™ä¸Šä½¿ç”¨ REALM é è¨“ç·´çš„ model ä¾† finetune é€²è¡Œè©•ä¼°ï¼ŒOpenQA æ˜¯æœ€ knowledge-intensive çš„ä»»å‹™ä¹‹ä¸€&lt;/p&gt;
&lt;p&gt;ä½œè€…æŒ‘äº†ä¸‰å€‹æµè¡Œçš„ Open-QA benchmarkï¼Œæ¯”å¦‚ NaturalQuestions-Openã€WebQuestionsã€CuratedTrecï¼Œä¸¦å’Œ SOTA Open-QA model æ¯”è¼ƒ&lt;/p&gt;
&lt;p&gt;åœ¨ä¸‰å€‹åŸºæº–éƒ½å–å¾—äº† SOTA çš„çµæœï¼Œabsolute accuracy æ˜é¡¯é«˜æ–¼å…ˆå‰ç³»çµ± 4-16%&lt;/p&gt;
&lt;p&gt;ä¹Ÿå±•ç¤ºäº† REALM çš„ qualitative benefitï¼Œæ¯”å¦‚å¯è§£é‡‹æ€§å’Œæ¨¡çµ„åŒ–&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;open-domain-question-answering-open-qa&#34;&gt;Open-domain question answering (Open-QA)&lt;/h3&gt;
&lt;p&gt;OpenQA çš„ open æ˜¯æŒ‡æ¨¡å‹ä¸æœƒæ¥æ”¶åˆ°åŒ…å«ç­”æ¡ˆçš„é æä¾›æ–‡ä»¶ï¼Œå’Œå‚³çµ±çš„é–±è®€ç†è§£ä¸åŒ&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/REALM/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;realms-generative-process&#34;&gt;REALMâ€™s generative process&lt;/h3&gt;
&lt;p&gt;é è¨“ç·´åš masked language modelingï¼Œå¾®èª¿çš„ä»»å‹™æ˜¯ Open-QA&lt;/p&gt;
&lt;h3 id=&#34;model-architecture&#34;&gt;Model architecture&lt;/h3&gt;
&lt;p&gt;neural knowledge retriever æ˜¯ $p(z|x)$&lt;/p&gt;
&lt;p&gt;knowledge-augmented encoder æ˜¯ $p(y|z,x)$&lt;/p&gt;
&lt;h4 id=&#34;knowledge-retriever&#34;&gt;Knowledge Retriever&lt;/h4&gt;
&lt;p&gt;$p(z|x)=\frac{\text{exp }f(x,z)}{\sum_{z&amp;rsquo;}\text{exp }f(x,z&amp;rsquo;)}$&lt;/p&gt;
&lt;p&gt;$f(x,z) = Embed_{input}(x)^T Embed_{doc}(z)$&lt;/p&gt;
&lt;p&gt;$join_{BERT}(x)=[CLS]x[SEP]$&lt;/p&gt;
&lt;p&gt;$join_{BERT}(x_1, x_2)=[CLS]x_1[SEP]x_2[SEP]$&lt;/p&gt;
&lt;p&gt;$Embed_{input}(x)=W_{input}BERT_{CLS}(join_{BERT}(x))$&lt;/p&gt;
&lt;p&gt;$Embed_{doc}(z)=W_{doc}BERT_{CLS}(join_{BERT}(z_{title}, z_{body}))$&lt;/p&gt;
&lt;h4 id=&#34;knowledge-augmented-encoder&#34;&gt;Knowledge-Augmented Encoder&lt;/h4&gt;
&lt;p&gt;Finetune:&lt;/p&gt;
&lt;p&gt;$p(y|z,x) \propto \displaystyle\sum_{s\in S(z,y)}exp(MLP([h_{START(s)};h_{END(s)}]))$&lt;/p&gt;
&lt;p&gt;$h_{START(s)}=BERT_{START(s)}(join_{BERT}(x,z_{body}))$&lt;/p&gt;
&lt;p&gt;$h_{END(s)}=BERT_{END(s)}(join_{BERT}(x,z_{body}))$&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;p&gt;é—œéµçš„è¨ˆç®—æŒ‘æˆ°æ˜¯ $p(y|x)=\sum_{z\in Z}p(y|x,z)p(z,x)$ï¼Œæ¶‰åŠåˆ° Z knowledge corpus ä¸­çš„ æ‰€æœ‰ document zï¼Œå› æ­¤åªå–æ©Ÿç‡ $p(z|x)$ æœ€é«˜çš„ top k æ–‡ä»¶ä¾†æ±‚å’Œï¼Œè€ƒé‡åˆ°å¤šæ•¸æ–‡ä»¶çš„æ©Ÿç‡æ‡‰è©²ç‚º 0ï¼Œé€™æ˜¯åˆç†çš„&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯ä¾ç„¶éœ€è¦ä¸€å€‹æœ‰æ•ˆçš„æ–¹æ³•ä¾†å°‹æ‰¾å‰ k å€‹æ–‡ä»¶&lt;/p&gt;
&lt;p&gt;å‰é¢çš„ $f(x,z)$ æ˜¯ä¸€å€‹å…§ç©ï¼Œå¯ä»¥ç”¨  Maximum Inner Product Search (MIPS) æ¼”ç®—æ³•ä¾†å°‹æ‰¾è¿‘ä¼¼å‰ k å€‹æ–‡æª”&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç”¨ MIPSï¼Œè¦å…ˆç”¨ä¸€ç¨® embedding å‡½å¼ä¾†å¹« z encodeï¼Œä½†æ˜¯å¦‚æœæ›´æ–°äº†é€™å€‹å‡½å¼ï¼Œè³‡æ–™çµæ§‹å’Œ $p(z|x)$ åˆæœƒä¸ä¸€è‡´&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œæ¯æ¬¡å° embedding å‡½å¼æ›´æ–°å¾Œï¼Œsearch index éƒ½æœƒè®Š &amp;ldquo;stale&amp;rdquo; (é™³èˆŠ)&lt;/p&gt;
&lt;p&gt;æ¯éš”æ•¸ç™¾å€‹è¨“ç·´æ­¥é©ŸéåŒæ­¥é‡æ–° embed å’Œ index æ‰€æœ‰ document ä¾†åˆ·æ–° index&lt;/p&gt;
&lt;p&gt;MIPS çš„ index åœ¨åˆ·æ–°ä¹‹å‰æœƒæœ‰é» staleï¼Œä½†å®ƒåªç”¨æ–¼æŒ‘é¸å‰ k å€‹æ–‡ä»¶&lt;/p&gt;
&lt;p&gt;çµæœè­‰æ˜ï¼Œåªè¦åœ¨æ°ç•¶çš„åˆ·æ–°ç‡ä¸‹ï¼Œé‚„æ˜¯å¯ä»¥ç©©å®š optimize&lt;/p&gt;
&lt;h4 id=&#34;what-does-the-retriever-learn&#34;&gt;What does the retriever learn?&lt;/h4&gt;
&lt;p&gt;é€™è£¡å±•ç¤ºäº†å®ƒå¦‚ä½•çå‹µæé«˜é æ¸¬æº–ç¢ºæ€§çš„æª¢ç´¢&lt;/p&gt;
&lt;p&gt;$\triangledown \text{log }p(y|x)=\displaystyle\sum_{z\in Z}r(z)\triangledown f(x,z)$&lt;/p&gt;
&lt;p&gt;$r(z)=[\frac{p(y|z,x)}{p(y|x)}-1]p(z|x)$&lt;/p&gt;
&lt;p&gt;å¦‚æœ $r(z)$ æ˜¯æ­£çš„ï¼Œæœƒè®“ f(x,z) æé«˜ï¼Œå¦å‰‡æ¸›ä½&lt;/p&gt;
&lt;p&gt;$r(z)$ åªæœ‰åœ¨ $p(y|z,x)&amp;gt;p(y|x)$ çš„æƒ…æ³ä¸‹æ‰æœƒæ˜¯æ­£çš„&lt;/p&gt;
&lt;p&gt;$p(y|x)$ æ˜¯ $p(y|x,z)$ åœ¨éš¨æ©Ÿå–æ¨£çš„æƒ…æ³ä¸‹çš„æœŸæœ›å€¼&lt;/p&gt;
&lt;p&gt;åªè¦æ–‡æª” z å¥½éé æœŸï¼Œå°±æœƒæŒçºŒæ­£é¢æ›´æ–°&lt;/p&gt;
&lt;h3 id=&#34;injecting-inductive-biases-into-pre-training&#34;&gt;Injecting inductive biases into pre-training&lt;/h3&gt;
&lt;p&gt;åœ¨ç™¼å±• REALM çš„éç¨‹ä¸­ï¼Œä½œè€…ç™¼ç¾å¹¾å€‹é¡å¤–ç­–ç•¥ï¼Œå¯ä»¥é€²ä¸€æ­¥å¼•å°æ¨¡å‹é€²è¡Œæœ‰æ„ç¾©çš„æª¢ç´¢&lt;/p&gt;
&lt;h4 id=&#34;salient-span-masking&#34;&gt;Salient span masking&lt;/h4&gt;
&lt;p&gt;æœ‰ä¸€äº› MLM span åªéœ€è¦ local contextï¼Œä½†ä½œè€…æƒ³å°ˆæ³¨æ–¼ world knowledge&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥ä½œè€…æœƒ mask ä¸€äº› salient spanï¼Œæ¯”å¦‚ã€Œè‹±åœ‹ã€ã€ã€Œ1969 å¹´ 7 æœˆã€&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨åœ¨ CoNLL-2003 ä¸Šè¨“ç·´çš„ BERT-based taggerï¼Œä¾†æ‰¾å‡º named entitiesï¼Œä¸¦ç”¨æ­£è¦è¡¨é”å¼ä¾†æ‰¾å‡ºæ—¥æœŸ&lt;/p&gt;
&lt;p&gt;çµæœè¡¨æ˜é€™é¡¯è‘—å„ªæ–¼å…¶ä»– mask ç­–ç•¥&lt;/p&gt;
&lt;h4 id=&#34;null-document&#34;&gt;Null document&lt;/h4&gt;
&lt;p&gt;é›–ç„¶ Salient span masking è¡¨ç¾å¾ˆå¥½ï¼Œä½†ä¸æ˜¯æ‰€æœ‰ mask éƒ½éœ€è¦ world knowledge&lt;/p&gt;
&lt;p&gt;é€éå‘å‰ top k æ–‡æª”ä¸­å¤šåŠ ä¸€å€‹ç©ºçš„æ–‡æª”ï¼Œå…è¨±åœ¨ä¸éœ€è¦æª¢ç´¢æœ‰ç©ºç™½é¸é …&lt;/p&gt;
&lt;h4 id=&#34;prohibiting-trivial-retrievals&#34;&gt;Prohibiting trivial retrievals&lt;/h4&gt;
&lt;p&gt;å¦‚æœ mask çš„å¥å­ä¾†è‡ªæ–‡ä»¶ zï¼Œå¯ä»¥é€éæŸ¥çœ‹ z ä¸­ x çš„ unmasked ç‰ˆæœ¬ä¾†è¼•é¬†é æ¸¬ yï¼Œä½¿ p(z|x) å‡ºç¾è¼ƒå¤§çš„æ¢¯åº¦ï¼Œå¦‚æœé€™ç¨®æƒ…æ³å¤ªé »ç¹ï¼Œæœƒä½¿ retriever æœ€çµ‚å­¸æœƒçš„æ±è¥¿åå‘ exact matchï¼Œè€Œä¸æœƒæ•ç²å…¶ä»–å½¢å¼çš„ç›¸é—œæ€§&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œåœ¨é è¨“ç·´æœŸé–“æ’é™¤äº† trivial candidate&lt;/p&gt;
&lt;h4 id=&#34;initialization&#34;&gt;Initialization&lt;/h4&gt;
&lt;p&gt;åœ¨è¨“ç·´é–‹å§‹æ™‚ï¼Œå¦‚æœ input å’Œ document å„è‡ªçš„ Embedding function æ²’æœ‰è‰¯å¥½çš„æ•ˆèƒ½ï¼Œæœƒä½¿æª¢ç´¢åˆ°çš„ z å’Œ x ç„¡é—œ&lt;/p&gt;
&lt;p&gt;æœƒå°è‡´æ¨¡å‹å­¸ç¿’å¿½ç•¥æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼Œæª¢ç´¢å™¨å°±ä¸æœƒæ”¶åˆ°æœ‰æ„ç¾©çš„æ¢¯åº¦ï¼Œä¹Ÿç„¡æ³•æ”¹é€²&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é¿å… cold-start problemï¼Œä½œè€…æ¡ç”¨ warm-start æ–¹æ¡ˆï¼Œå…ˆä»¥  Inverse Cloze Task (ICT) é€™ç¨®ç°¡å–®çš„ç›®æ¨™ä¾†è™•ç†å…©å€‹ embedding functionï¼Œçµ¦å®šä¸€å€‹å¥å­ï¼Œè¨“ç·´æ¨¡å‹æª¢ç´¢å¥å­ä¾†è‡ªçš„æ–‡æª”&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/REALM/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/REALM/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;open-qa-benchmarks&#34;&gt;Open-QA Benchmarks&lt;/h3&gt;
&lt;p&gt;æœ¬æ–‡å°‡é‡é»æ”¾åœ¨å•é¡Œä½œè€…ä¸çŸ¥é“ç­”æ¡ˆçš„è³‡æ–™é›†ï¼Œæ¯”è¼ƒèƒ½åæ˜ ç¾å¯¦ä¸­çš„å•é¡Œ&lt;/p&gt;
&lt;h4 id=&#34;naturalquestions-open&#34;&gt;NaturalQuestions-Open&lt;/h4&gt;
&lt;p&gt;ç”±è‡ªç„¶ç™¼ç”Ÿçš„ google æŸ¥è©¢å’Œç­”æ¡ˆçµ„æˆï¼Œæ¯å€‹ç­”æ¡ˆé‚„å¸¶æœ‰ answer type&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡åªç”¨å±¬æ–¼ &amp;ldquo;short answer type&amp;rdquo; çš„å•é¡Œï¼Œæœ€å¤šæœ‰ 5 å€‹ token&lt;/p&gt;
&lt;h4 id=&#34;webquestions&#34;&gt;WebQuestions&lt;/h4&gt;
&lt;p&gt;å¾ Google Suggest API æ”¶é›†çš„&lt;/p&gt;
&lt;h4 id=&#34;curatedtrec&#34;&gt;CuratedTrec&lt;/h4&gt;
&lt;p&gt;å¾ MSNSearch å’Œ AskJeeves ç­‰ç¶²ç«™ä¸ŠçœŸå¯¦ä½¿ç”¨è€…æŸ¥è©¢ä¸­æå–çš„å•ç­”&lt;/p&gt;
&lt;h3 id=&#34;approaches-compared&#34;&gt;Approaches compared&lt;/h3&gt;
&lt;h4 id=&#34;retrieval-based-open-qa&#34;&gt;Retrieval-based Open-QA&lt;/h4&gt;
&lt;p&gt;æœ€è¿‘çš„ä¸€äº›æ–¹æ³•æå‡ºç”¨ MIPS index ä¾†å¯¦ç¾å¯è¨“ç·´çš„æª¢ç´¢&lt;/p&gt;
&lt;p&gt;ORQA èˆ‡ REALM é¡ä¼¼&lt;/p&gt;
&lt;p&gt;ä½† REALM æå‡ºäº†æ›´æ–°ç©çš„æ¨¡å‹é è¨“ç·´æ­¥é©Ÿï¼Œä¸¦åå‘å‚³æ’­åˆ° MIPS index ä¸­ï¼Œè€Œä¸ç”¨å›ºå®šçš„ index&lt;/p&gt;
&lt;p&gt;ä¸Šé¢æŒ‡çš„æ‡‰è©²æ˜¯æœ‰é—œæ–¼éåŒæ­¥æ›´æ–° index çš„éƒ¨åˆ†ï¼Œé‚„å¯ä»¥æ¢¯åº¦æ›´æ–°&lt;/p&gt;
&lt;p&gt;å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒREALM å’Œ OrQA çš„é è¨“ç·´éƒ½æ˜¯ç”¨ ICT åˆå§‹åŒ–çš„&lt;/p&gt;
&lt;h4 id=&#34;generation-based-open-qa&#34;&gt;Generation-based Open-QA&lt;/h4&gt;
&lt;p&gt;Open-QA çš„æ–°èˆˆæ›¿ä»£æ–¹æ¡ˆæ˜¯å°‡å…¶å»ºæ¨¡ç‚ºåºåˆ—é æ¸¬ä»»å‹™ï¼Œåªéœ€å°å•é¡Œé€²è¡Œç·¨ç¢¼ï¼Œç„¶å¾Œæ ¹æ“šç·¨ç¢¼é€å€‹æ¨™è¨˜ç·¨ç¢¼ç­”æ¡ˆ&lt;/p&gt;
&lt;p&gt;GPT2 å¯ä»¥é€é sequence to sequence ç›´æ¥ç”¢ç”Ÿç­”æ¡ˆï¼Œè€Œä¸éœ€è¦çµ¦æŒ‡å®šçš„ä¸Šä¸‹æ–‡ï¼Œä½†å¯èƒ½ç”±æ–¼ç¼ºä¹ finetune è€Œæ²’æœ‰ç«¶çˆ­åŠ›&lt;/p&gt;
&lt;p&gt;åŒæ™‚ï¼ŒT5 è¡¨æ˜ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆè€Œä¸å¾çµ¦å®šä¸Šä¸‹æ–‡ä¸­æ˜ç¢ºæå–æ˜¯å¯è¡Œçš„æ–¹æ³•ï¼Œä½†ä»–å€‘åªåœ¨æä¾›ä¸Šä¸‹æ–‡æ–‡æª”çš„é–±è®€ç†è§£ä»»å‹™ä¸Šé€²è¡Œå¯¦é©—&lt;/p&gt;
&lt;p&gt;ç‚ºäº†å’Œæœ€å…·ç«¶çˆ­åŠ›çš„ baseline æ¯”è¼ƒï¼Œæœ¬æ–‡èˆ‡é‡å° Open-QA finetune çš„ T5 é€²è¡Œæ¯”è¼ƒ&lt;/p&gt;
&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h3&gt;
&lt;h4 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h4&gt;
&lt;p&gt;æ–‡ä»¶è¢«è²ªå©ªåœ°åˆ†å‰²æˆå¤šé” 288 å€‹ BERT wordpieces çš„ chunkï¼Œç”¢ç”Ÿè¶…é 1300 è¬å€‹ retrieval candidates&lt;/p&gt;
&lt;p&gt;åœ¨å¾®èª¿æ¨ç†éç¨‹ä¸­ï¼Œè€ƒæ…®å‰äº”å€‹å€™é¸è€…&lt;/p&gt;
&lt;h4 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h4&gt;
&lt;p&gt;ä½¿ç”¨ BERT çš„é è¨­å„ªåŒ–å™¨åœ¨ 64 å€‹ Google Cloud TPU ä¸Šé è¨“ç·´ 20 è¬æ­¥&lt;/p&gt;
&lt;p&gt;MIPS åœ¨ 16 å€‹ TPU ä¸Šä¸¦è¡Œ&lt;/p&gt;
&lt;h3 id=&#34;main-results&#34;&gt;Main results&lt;/h3&gt;
&lt;p&gt;REALM åœ¨ table.1 æ˜é¡¯å„ªæ–¼ä¹‹å‰çš„æ‰€æœ‰æ–¹æ³•&lt;/p&gt;
&lt;p&gt;REALM æœ€ç›´æ¥çš„æ¯”è¼ƒæ˜¯ ORQAï¼Œå¾®èª¿è¨­å®šã€è¶…åƒæ•¸å’Œè¨“ç·´è³‡æ–™éƒ½ç›¸é€š&lt;/p&gt;
&lt;p&gt;REALM å°æ¯” ORQA çš„æ”¹é€²ç´”ç²¹æ˜¯æ›´å¥½çš„é è¨“ç·´æ–¹æ³•&lt;/p&gt;
&lt;p&gt;è€Œä¸”æœ¬æ–‡è¡¨æ˜ä»–å€‘çš„é è¨“ç·´æ–¹æ³•å¯ä»¥ç”¨åœ¨ single-corpus setting æˆ– seperate corpus setting&lt;/p&gt;
&lt;p&gt;å…©è€…çš„å·®åˆ¥åœ¨ X å’Œ Z ä¾†æºä¸€ä¸ä¸€æ¨£&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;table.2 å±•ç¾äº†å»é™¤ REALM é—œéµçµ„ä»¶å¾Œçš„çµæœ&lt;/p&gt;
&lt;p&gt;é‚„å±•ç¾äº† finetune å‰ gold answer å‡ºç¾åœ¨å‰äº”å€‹æª¢ç´¢ä¸­çš„é »ç‡&lt;/p&gt;
&lt;h4 id=&#34;masking-scheme&#34;&gt;Masking scheme&lt;/h4&gt;
&lt;p&gt;salient span masking åœ¨å…ˆå‰æ¨™æº–çš„ BERT è¨“ç·´ä¸­å°šæœªè¢«è­‰æ˜å…·æœ‰å½±éŸ¿åŠ›ï¼Œä½†å°æ–¼ REALM è‡³é—œé‡è¦&lt;/p&gt;
&lt;h4 id=&#34;mips-index-refresh-rate&#34;&gt;MIPS index refresh rate&lt;/h4&gt;
&lt;p&gt;åœ¨é è¨“ç·´æœŸé–“ï¼Œé‹è¡Œä¸¦è¡Œéç¨‹ä¾†é‡æ–° embed document å’Œé‡å»º MIPS index&lt;/p&gt;
&lt;p&gt;å°è‡´æ¯å¤§ç´„ 500 å€‹è¨“ç·´æ­¥é©Ÿåˆ·æ–°ä¸€æ¬¡ index&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è­‰æ˜é »ç¹åˆ·æ–°çš„é‡è¦æ€§ï¼Œä¹Ÿå’Œè¼ƒæ…¢åˆ·æ–°ç‡æ¯”è¼ƒ&lt;/p&gt;
&lt;p&gt;table.2 é¡¯ç¤ºï¼Œstale index å¯èƒ½æœƒå‚·å®³æ¨¡å‹ï¼Œé€²ä¸€æ­¥æ¸›å°‘é€™ç¨®éæ™‚æ€§å¯ä»¥æä¾›æ›´å¥½çš„æœ€ä½³åŒ–&lt;/p&gt;
&lt;h2 id=&#34;discussion-and-related-work&#34;&gt;Discussion and Related Work&lt;/h2&gt;
&lt;h3 id=&#34;scalable-grounded-neural-memory&#34;&gt;Scalable grounded neural memory&lt;/h3&gt;
&lt;p&gt;document index å¯ä»¥è¢«è¦–ç‚ºä¸€ç¨® memoryï¼Œkey æ˜¯ document embedding&lt;/p&gt;
&lt;p&gt;å¾é€™è§’åº¦ä¾†çœ‹ï¼Œæœ¬æ–‡çš„å·¥ä½œå’Œ product key memory æœ‰å…±åŒçš„å‹•æ©Ÿï¼Œå®ƒèƒ½å¤ åœ¨è¨˜æ†¶é«”ç¶²è·¯ä¸­å¯¦ç¾ä½æ–¼ç·šæ€§çš„å­˜å–&lt;/p&gt;
&lt;p&gt;ä¸€å€‹ä¸»è¦çš„å€åˆ¥æ˜¯æœ¬æ–‡çš„è¨˜æ†¶é«”æ˜¯æœ‰æ ¹æ“šçš„ï¼Œæ¯å€‹ memory éƒ½å’Œä¸€å€‹æ–‡æª”ç›¸é—œè¯ï¼Œè€Œä¸æ˜¯å’Œ unnamed value vector ç›¸é—œè¯&lt;/p&gt;
&lt;p&gt;é€™ç¨®ç¨‹åº¦çš„å¯è§£é‡‹æ€§å° Open-QA è‡³é—œé‡è¦ï¼Œåœ¨é€™äº›æ‡‰ç”¨ç¨‹å¼ä¸­ï¼Œç”¨æˆ¶éœ€è¦å‡ºè™•æ‰èƒ½ä½¿é æ¸¬ç­”æ¡ˆå€¼å¾—ä¿¡è³´&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
</description>
        </item>
        <item>
        <title>DPR è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/dpr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 28 Dec 2023 00:00:13 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/dpr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2004.04906&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Dense Passage Retrieval for Open-Domain Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Open-domain question answering ä¾è³´æœ‰æ•ˆçš„ passage retrieval ä¾†é¸æ“‡ candidate contextï¼Œå‚³çµ±ä¸Šç”¨çš„ sparse vector space modelsï¼Œæœ‰ TF-IDFã€BM25 ç­‰ç­‰&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡é¡¯ç¤ºå‡ºæª¢ç´¢å¯¦éš›ä¸Šå¯ä»¥åªç”¨ dense representationï¼Œembedding æ˜¯å¾å°‘é‡çš„ question å’Œ passage å­¸åˆ°çš„ï¼Œåˆ©ç”¨ç°¡å–®çš„ dual-encoder framework&lt;/p&gt;
&lt;p&gt;åœ¨å»£æ³›çš„ open-domain QA è³‡æ–™é›†ä¸Šï¼Œæœ¬æ–‡çš„ dense retriever åœ¨ top-20 passage retrieval accuracy ä¸Šæ¯” Lucene BM25 å¥½ 9%-19%&lt;/p&gt;
&lt;p&gt;ä¸¦å¹«åŠ©ä½œè€…çš„ end-to-end QA system åœ¨ multiple open-domain QA benchmarks ä¸Šå–å¾— SOTA&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;æ—©æœŸé–±è®€ç†è§£æ¨¡å‹æå‡ºäº†ç°¡å–®çš„ two-stage frameworkï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ä¸€å€‹ context retriever å…ˆé¸å®šä¸€äº› passage å­é›†åˆï¼Œå…¶ä¸­æŸä¸€äº› passage åŒ…å«ç­”æ¡ˆ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¸€å€‹ machine readerï¼Œå¯ä»¥å¾¹åº•æª¢æŸ¥é¸å‡ºçš„ context ä¸¦æ‰¾åˆ°ç­”æ¡ˆ&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç›¡ç®¡å°‡ open-domain QA ç°¡åŒ–ç‚º machine reading æ˜¯ä¸€å€‹åˆç†çš„ç­–ç•¥ï¼Œä½†æ˜¯å¯¦éš›ä¸Šç¶“å¸¸çœ‹åˆ°åš´é‡çš„æ€§èƒ½ä¸‹é™ï¼Œé¡¯ç¤ºå‡ºéœ€è¦æ”¹é€² retrieval&lt;/p&gt;
&lt;p&gt;open-domain QA ä¸­çš„ retrieval é€šå¸¸ç”¨ TF-IDF æˆ– BM25 ä¾†å¯¦ç¾ï¼Œå¯ä»¥é€é inverted index æœ‰æ•ˆåœ° match keywordsï¼Œè€Œä¸”æŠŠ question å’Œ context è¡¨ç¤ºç‚º high-dimensional sparse vectors&lt;/p&gt;
&lt;p&gt;ç›¸åçš„ï¼ŒDense latent semantic encoding åœ¨è¨­è¨ˆä¸Šå’Œ sparse representation æ˜¯äº’è£œçš„&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ï¼Œç”±å®Œå…¨ä¸åŒçš„ token çµ„æˆçš„å…©å€‹åŒç¾©è©ä¾ç„¶å¯ä»¥æ˜ å°„åˆ°æ¥è¿‘çš„å‘é‡&lt;/p&gt;
&lt;p&gt;term-based system ç›¸è¼ƒ dense retrieval system å¾ˆé›£å°‡æ¯”å¦‚ã€Œå£äººã€å’Œã€Œæƒ¡æ£ã€åŒ¹é…&lt;/p&gt;
&lt;p&gt;Dense encoding ä¹Ÿå¯ä»¥é€éèª¿æ•´ embedding function ä¾†å­¸ç¿’ï¼Œå°æ–¼ task-specific çš„ representation æä¾›äº†å½ˆæ€§&lt;/p&gt;
&lt;p&gt;é€éç‰¹æ®Šçš„ in-memory data structure å’Œ indexing schemeï¼Œå¯ä»¥ç”¨ maximum inner product search (MIPS) ä¾†å¿«é€Ÿæª¢ç´¢&lt;/p&gt;
&lt;p&gt;ç„¶è€Œï¼Œäººå€‘æ™®éèªç‚ºå­¸ç¿’è‰¯å¥½çš„ dense vector representation éœ€è¦å¤§é‡çš„ QA labeled pair&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œåœ¨ ORQA ä¹‹å‰ï¼ŒDense retrieval å¾æ²’è¢«è­‰æ˜éå¯ä»¥åœ¨ open-domain QA ä¸Šè´é TF-IDF æˆ– BM25&lt;/p&gt;
&lt;p&gt;ORQA æå‡ºäº† ICT ä¾†åšé¡å¤–çš„é è¨“ç·´&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ ORQA è­‰æ˜äº† dense retrieval å¯ä»¥è¶…è¶Š BM25ï¼Œåœ¨å¤šå€‹ open-domain QA è³‡æ–™é›†ä¸Šå–å¾— SOTAï¼Œä½†ä»–ä¹Ÿå­˜åœ¨å…©å€‹å¼±é»ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ICT é è¨“ç·´æ˜¯ compute-intensiveï¼Œè€Œä¸”ä¸ç¢ºå®š regular sentence æ˜¯å¦èƒ½å¾ˆå¥½çš„æ›¿ä»£ objective function ä¸­çš„ question&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”±æ–¼ context encoder æ²’æœ‰ç”¨ QA pair é€²è¡Œ finetuneï¼Œå› æ­¤ç›¸æ‡‰çš„ representation å¯èƒ½ä¸æ˜¯æœ€ä½³çš„&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œæœ¬æ–‡å°‡è§£æ±ºä¸€å€‹å•é¡Œ &amp;ndash; æˆ‘å€‘æ˜¯å¦å¯ä»¥åªç”¨ QA pair ä¾†è¨“ç·´æ›´å¥½çš„ dense embedding modelï¼Œè€Œä¸ç”¨é¡å¤–çš„é è¨“ç·´ï¼Ÿ&lt;/p&gt;
&lt;p&gt;åˆ©ç”¨ç¾åœ¨çš„ standard BERT pre-trained model ä»¥åŠ dual-encoder architectureï¼Œæœ¬æ–‡å°ˆæ³¨æ–¼ç”¨ç›¸å°å°‘é‡çš„ question and passage pair ä¾†è¨“ç·´ dense retriever&lt;/p&gt;
&lt;p&gt;ç¶“éä¸€ç³»åˆ—çš„ ablation studyï¼Œæœ¬æ–‡çš„è§£æ±ºæ–¹æ¡ˆå‡ºå¥‡çš„ç°¡å–®&lt;/p&gt;
&lt;p&gt;embedding å¯ä»¥ç”¨æœ€å¤§åŒ– question å’Œ ç›¸é—œçš„ passage çš„ inner product ä¾†è¨“ç·´&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„ Dense Passage Retrieval (DPR)  éå¸¸å¼·å¤§ï¼Œä¸åƒ…å¤§å¹…å„ªæ–¼ BM25ï¼Œè€Œä¸”å’Œ ORQA ç›¸æ¯”ï¼Œend-to-end QA æº–ç¢ºåº¦ä¹Ÿæœ‰å¤§å¹…æå‡&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„è²¢ç»æœ‰å…©éƒ¨åˆ†ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ä½œè€…è­‰æ˜åœ¨é©ç•¶çš„è¨­ç½®ä¸‹ï¼Œåªéœ€åœ¨ç¾æœ‰ question-passge pair ä¸Š finetune question and passage encoderï¼Œå°±å¯ä»¥å¤§å¹…è¶…é BM25ï¼Œå¯¦é©—çµæœä¹Ÿè­‰æ˜å¯èƒ½ä¸ç”¨é¡å¤–çš„é è¨“ç·´&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½œè€…è­‰æ˜äº†åœ¨ open-domain QA çš„èƒŒæ™¯ä¸‹ï¼Œæ›´é«˜çš„ retrieval accuracy å¯ä»¥è½‰åŒ–ç‚ºæ›´é«˜çš„ end-to-end QA accuracy&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é€éå° top retrived passage ä½¿ç”¨ modern reader modelï¼Œå’Œå¹¾å€‹éå¸¸è¤‡é›œçš„ç³»çµ±ç›¸æ¯”ï¼Œä½œè€…åœ¨ open-retrieval setting ä¸‹çš„å¤šå€‹è³‡æ–™é›†å–å¾—äº†å¯æ¯”è¼ƒæˆ–æ›´å¥½çš„çµæœ&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡ç ”ç©¶çš„ open-domain QA çš„æè¿°å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;p&gt;å…ˆçµ¦å‡ºä¸€å€‹äº‹å¯¦æ€§å•é¡Œï¼Œä¸å±¬æ–¼ç‰¹å®šä¸»é¡Œï¼Œéœ€è¦ä¸€å€‹ç³»çµ±ä½¿ç”¨å¤§é‡å¤šæ¨£åŒ–ä¸»é¡Œçš„ corpus ä¾†å›ç­”&lt;/p&gt;
&lt;p&gt;æ›´å…·é«”åœ°èªªï¼Œä½œè€…å‡è¨­ extractive QA settingï¼Œç­”æ¡ˆåƒ…é™æ–¼ corpus ä¸­çš„ä¸€å€‹æˆ–å¤šå€‹ passage ä¸­å‡ºç¾çš„ç¯„åœ&lt;/p&gt;
&lt;p&gt;å‡è¨­æœ‰ D å€‹ documentsï¼Œå…ˆæŠŠæ¯å€‹ document æ‹†æˆå¤šå€‹ç­‰é•·çš„ text passageï¼Œå¥½åšç‚º basic retrieval unitï¼Œæœ€å¾Œå¾—åˆ° M å€‹ passage in corpus C&lt;/p&gt;
&lt;p&gt;æ¯å€‹ passage å¯ä»¥è¢«è¦–ä½œä¸€å€‹ token åºåˆ—&lt;/p&gt;
&lt;p&gt;å°æ–¼ question qï¼Œå‰‡æ˜¯è¦æ‰¾åˆ°æŸæ®µ passage ä¸­çš„ä¸€ä¸²é€£çºŒçš„ token ä¾†å›ç­”&lt;/p&gt;
&lt;p&gt;å€¼å¾—æ³¨æ„çš„ä¸€é»æ˜¯ï¼Œç‚ºäº†æ¶µè“‹ç›¡å¯èƒ½å»£æ³›çš„æ¦‚å¿µï¼Œcorpus çš„å¤§å°å¯èƒ½æœƒå¾æ•¸ç™¾è¬å€‹æ–‡ä»¶åˆ°æ•¸åå„„å€‹æ–‡ä»¶ä¸ç­‰&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œä»»ä½• open-domain QA system éƒ½éœ€è¦ä¸€å€‹é«˜æ•ˆçš„ retrieverï¼Œå¯ä»¥åœ¨ reader æå–ç­”æ¡ˆå‰é¸æ“‡ä¸€å°çµ„ç›¸é—œçš„æ–‡æœ¬&lt;/p&gt;
&lt;h2 id=&#34;dense-passage-retriever-dpr&#34;&gt;Dense Passage Retriever (DPR)&lt;/h2&gt;
&lt;p&gt;ä½œè€…çš„ç ”ç©¶é‡é»æ˜¯æ”¹é€² open-domain QA çš„ retrieval component&lt;/p&gt;
&lt;p&gt;çµ¦å®š M å€‹ passageï¼ŒDPR çš„ç›®æ¨™æ˜¯æŠŠæ‰€æœ‰ low-dimensional continuous space çš„æ‰€æœ‰ passage éƒ½çµ¦ indexï¼Œä½¿å®ƒå¯ä»¥æœ‰æ•ˆåœ°æª¢æ‰€å’Œè¼¸å…¥å•é¡Œç›¸é—œçš„ top-k passage&lt;/p&gt;
&lt;p&gt;M æœ‰å¯èƒ½éå¸¸å¤§ï¼Œæ¯”å¦‚æœ¬æ–‡æœ‰ 21M å€‹ passageï¼Œè€Œ k é€šå¸¸å¾ˆå°ï¼Œå¯èƒ½åªæœ‰ 20~100 å€‹&lt;/p&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;ç”¨ dense encoder $E_P(ï¼)$ å’Œ $E_Q(ï¼)$ ä¾†åˆ†åˆ¥ encode passage å’Œ questionï¼Œåœ¨è¨ˆç®—å…©è€…çš„ inner product ä¾†è¡¡é‡ç›¸ä¼¼åº¦&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ç¢ºå¯¦å­˜åœ¨æ¸¬é‡ question å’Œ passage ä¹‹é–“æ›´å…·è¡¨ç¾åŠ›çš„æ¨¡å‹å½¢å¼ï¼Œæ¯”å¦‚å¸¶æœ‰ cross-attention çš„ multi-layer networksï¼Œä½†æ˜¯ similarity function éœ€è¦å¯ä»¥åˆ†è§£ï¼Œæ‰å¯ä»¥é å…ˆè¨ˆç®— passage çš„ embedding&lt;/p&gt;
&lt;p&gt;å¤§å¤šæ•¸ decomposable similarity function æ˜¯ Euclidean distance (L2) çš„è®Šæ›&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼Œconsine æ˜¯ unit vector çš„ inner productï¼Œè€Œ Mahalanobis distance ç­‰åŒæ–¼åœ¨ transformed space çš„ L2 distance&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ ablation study è¡¨ç¤ºå…¶ä»–ç›¸ä¼¼å«æ•¸çš„è¡¨ç¾ç›¸ç•¶ï¼Œå› æ­¤é¸æ“‡æ›´ç°¡å–®çš„å…§ç©å‡½æ•¸ï¼Œä¸¦é€éå­¸ç¿’æ›´å¥½çš„ encoder ä¾†æ”¹å–„ dense passage retriever&lt;/p&gt;
&lt;h4 id=&#34;encoder&#34;&gt;Encoder&lt;/h4&gt;
&lt;p&gt;æœ¬æ–‡æ¡ç”¨å…©å€‹ç¨ç«‹çš„ BERTï¼Œä¸¦æŠŠ [CLS] token çš„ representation ç•¶ä½œ output&lt;/p&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;
&lt;p&gt;æ¨ç†éšæ®µï¼Œå°‡ passage encoder $E_P$ æ‡‰ç”¨åœ¨æ‰€æœ‰ passageï¼Œä¸¦ç”¨ FAISS offline index&lt;/p&gt;
&lt;p&gt;FAISS æ˜¯ä¸€å€‹éå¸¸é«˜æ•ˆçš„ open-source libraryï¼Œç”¨åœ¨ similarity search å’Œ clustering of dense vectorsï¼Œå¯ä»¥è¼•é¬†æ‡‰ç”¨åœ¨æ•¸åå„„å€‹å‘é‡ä¸Š&lt;/p&gt;
&lt;p&gt;åœ¨æ¨è«–æ¥æ®µï¼Œè¨ˆç®— q çš„ embeddingï¼Œç„¶å¾Œç”¨ FAISS ä¾†æ‰¾åˆ° top-k passages&lt;/p&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
&lt;p&gt;æ¯ä¸€å€‹ training instance éƒ½åŒ…å«å•é¡Œ qï¼Œé‚„æœ‰ä¸€å€‹æ­£ (ç›¸é—œ) passageï¼Œä»¥åŠ n å€‹è²  (ä¸ç›¸é—œ) passages&lt;/p&gt;
&lt;h4 id=&#34;positive-and-negative-passages&#34;&gt;Positive and negative passages&lt;/h4&gt;
&lt;p&gt;å°æ–¼æª¢ç´¢å•é¡Œï¼Œæ­£ä¾‹é€šå¸¸æ˜ç¢ºå¯ç”¨ï¼Œè€Œåä¾‹å‰‡é€šå¸¸è¦å¾éå¸¸å¤§çš„ pool ä¸­é¸æ“‡&lt;/p&gt;
&lt;p&gt;æ­£ä¾‹å¯èƒ½æœƒåœ¨ QA è³‡æ–™é›†çµ¦å‡ºï¼Œæˆ–æ˜¯å¯ä»¥å¾ç­”æ¡ˆæ‰¾åˆ°ï¼Œå…¶ä»–çš„æ®µè½é è¨­æƒ…æ³ä¸‹éƒ½å¯è¦–ç‚ºä¸ç›¸é—œ&lt;/p&gt;
&lt;p&gt;åœ¨å¯¦è¸ä¸­ï¼Œå¦‚ä½•é¸æ“‡è² ä¾‹å¸¸è¢«å¿½è¦–ï¼Œä½†å°æ–¼å­¸ç¿’ high-quality encoder å¯èƒ½å¾ˆé—œéµ&lt;/p&gt;
&lt;p&gt;è€ƒæ…®ä¸‰ç¨®è² ä¾‹ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;éš¨æ©Ÿ
&lt;ul&gt;
&lt;li&gt;Corpus ä¸­éš¨æ©Ÿé¸æ“‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BM25
&lt;ul&gt;
&lt;li&gt;BM25 è¿”å›çš„ passage ä¸å«ç­”æ¡ˆï¼Œä½†åŒ…å«æœ€å¤š question token&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gold
&lt;ul&gt;
&lt;li&gt;positive å’Œ question é…å°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç¬¬ 2 å’Œ 3 ç¨®æ˜¯åœ¨èªªæ‹¿å…¶ä»–å•é¡Œçš„æ­£ä¾‹ç•¶è² ä¾‹&lt;/p&gt;
&lt;h4 id=&#34;in-batch-negatives&#34;&gt;In-batch negatives&lt;/h4&gt;
&lt;p&gt;æœ‰ B è¨“ç·´å¯¦é«”ï¼Œæ¯å€‹å•é¡Œæœ‰ B-1 å€‹ negative passageï¼Œåªæœ‰ i = j æ™‚ï¼Œ$(q_i, p_j)$ æ˜¯æ­£ä¾‹ï¼Œå…¶ä»–éƒ½æ˜¯è² ä¾‹&lt;/p&gt;
&lt;p&gt;in-batch negative çš„æŠ€å·§å·²è¢«ç”¨åœ¨ full batch setting å’Œ mini-batch setting&lt;/p&gt;
&lt;p&gt;å·²è¢«è­‰æ˜æ˜¯å­¸ç¿’ dual-encoder çš„æœ‰æ•ˆç­–ç•¥ï¼Œå¯ä»¥å¢åŠ è¨“ç·´ç¯„ä¾‹çš„æ•¸é‡&lt;/p&gt;
&lt;h2 id=&#34;experimental-setup&#34;&gt;Experimental Setup&lt;/h2&gt;
&lt;h3 id=&#34;wikipedia-data-pre-processing&#34;&gt;Wikipedia Data Pre-processing&lt;/h3&gt;
&lt;p&gt;ç”¨ DrQA æä¾›çš„é è™•ç†ç¨‹å¼ç¢¼å¾ Wikipedia dump ä¸­æå–æ–‡ç« ä¸­ä¹¾æ·¨çš„æ–‡å­—éƒ¨åˆ†&lt;/p&gt;
&lt;p&gt;æ­¤æ­¥é©Ÿå°‡åˆªé™¤ semi-structured dataï¼Œæ¯”å¦‚è¡¨æ ¼å’Œ info-boxes&lt;/p&gt;
&lt;p&gt;æ¥è‘—å°‡æ¯å¤©æ–‡ç« åˆ†æˆå¤šå€‹ä¸ç›¸äº¤çš„æ–‡å­—å€å¡Šï¼Œæ¯å€‹æ–‡å­—å€å¡Šç”± 100 å€‹å–®å­—çµ„æˆï¼Œä½œç‚º basic retrieval unitï¼Œæœ€å¾Œæœƒæœ‰ 21,015,324 å€‹ passage&lt;/p&gt;
&lt;p&gt;æ¯å€‹ passage ä¹Ÿå¸¶æœ‰æ¨™é¡Œä»¥åŠ [SEP]&lt;/p&gt;
&lt;h3 id=&#34;selection-of-positive-passages&#34;&gt;Selection of positive passages&lt;/h3&gt;
&lt;p&gt;ç”±æ–¼ TREC, WebQuestions å’Œ TriviaQA ä¸­åªæœ‰ question-answer pairï¼Œå› æ­¤ä½¿ç”¨ BM25 æŠŠåŒ…å«ç­”æ¡ˆæœ€å¤šçš„ passage ç•¶ä½œæ­£ä¾‹&lt;/p&gt;
&lt;p&gt;å¦‚æœæª¢ç´¢åˆ°çš„å‰ 100 ç¯‡æ–‡ç« ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œå‰‡è©²å•é¡Œæœƒè¢«ä¸Ÿæ£„&lt;/p&gt;
&lt;h2 id=&#34;experiments-passage-retrieval&#34;&gt;Experiments: Passage Retrieval&lt;/h2&gt;
&lt;h3 id=&#34;main-results&#34;&gt;Main Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DPR/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;SQuAD è¡¨ç¾è¼ƒå·®æœ‰å¯èƒ½æ˜¯å› ç‚º passage å’Œ question å­˜åœ¨é«˜åº¦è©å½™é‡ç–Šï¼Œçµ¦ BM25 å¸¶ä¾†å„ªå‹¢ï¼Œè€Œä¸”è³‡æ–™åƒ…å¾ 500 å¤šç¯‡ wiki æ–‡ç« ä¸­è’é›†ï¼Œå› æ­¤è¨“ç·´ç¯„ä¾‹çš„åˆ†ä½ˆå­˜åœ¨æ¥µå¤§çš„ bias&lt;/p&gt;
&lt;p&gt;ç•¶ç”¨å¤šå€‹è³‡æ–™é›†è¨“ç·´æ™‚ï¼ŒTRECï¼ˆè£¡é¢æœ€å°çš„è³‡æ–™é›†ï¼‰ç²ç›Šæœ€å¤š&lt;/p&gt;
&lt;h3 id=&#34;ablation-study-on-model-training&#34;&gt;Ablation Study on Model Training&lt;/h3&gt;
&lt;h4 id=&#34;sample-efficiency&#34;&gt;Sample efficiency&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DPR/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;in-batch-negative-training&#34;&gt;In-batch negative training&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DPR/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;in-batch negative å¯ä»¥é¡¯è‘—æ”¹å–„çµæœï¼Œé‚„ç°¡å–®ä¸”ç¯€çœè¨˜æ†¶é«”ï¼Œå¯ä»¥é‡è¤‡ä½¿ç”¨ batch ä¸­å·²ç¶“æœ‰çš„è² ä¾‹&lt;/p&gt;
&lt;p&gt;å®ƒæœƒç”¢ç”Ÿæ›´å¤š pairï¼Œå¾è€Œå¢åŠ è¨“ç·´è³‡æ–™çš„æ•¸é‡&lt;/p&gt;
&lt;h4 id=&#34;impact-of-gold-passages&#34;&gt;Impact of gold passages&lt;/h4&gt;
&lt;p&gt;åšäº† distant supervisionï¼Œåªæœ‰å¾ˆå°çš„å½±éŸ¿ï¼Œé™ä½äº† 1 å€‹é»&lt;/p&gt;
&lt;h4 id=&#34;similarity-and-loss&#34;&gt;Similarity and loss&lt;/h4&gt;
&lt;p&gt;L2 å’Œ inner product çš„è¡¨ç¾ç›¸ç•¶ï¼Œå…©å€‹éƒ½æ¯” cosine å¥½&lt;/p&gt;
&lt;p&gt;æœ‰ä¸€ç¨®æµè¡Œçš„ ranking loss å«åš triplet lossï¼Œä½†æ˜¯åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…ç™¼ç¾å®ƒçš„è¡¨ç¾ä¸æœƒå°çµæœç”¢ç”Ÿå¤ªå¤§å½±éŸ¿&lt;/p&gt;
&lt;h4 id=&#34;cross-dataset-generalization&#34;&gt;Cross-dataset generalization&lt;/h4&gt;
&lt;p&gt;ç‚ºäº†æ¸¬è©¦æ³›åŒ–èƒ½åŠ›ï¼Œåªåœ¨ Natural Questions ä¸Šè¨“ç·´ï¼Œåœ¨è¼ƒå°çš„ WebQuestions å’Œ CuratedTREC ä¸Šæ¸¬è©¦ï¼Œç™¼ç¾æ³›åŒ–èƒ½åŠ›å¾ˆå¥½ï¼Œè¼¸çµ¦ SOTA finetune model 3~5 å€‹é»ï¼Œä½†ä»å¤§å¤§å„ªæ–¼ BM25 baseline&lt;/p&gt;
&lt;h2 id=&#34;experiments-question-answering&#34;&gt;Experiments: Question Answering&lt;/h2&gt;
&lt;h3 id=&#34;result&#34;&gt;Result&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DPR/table4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;è­‰æ˜äº† dense retrieval å¯ä»¥ outperform ç”šè‡³å¯èƒ½å–ä»£å‚³çµ±çš„ sparse retrieval&lt;/p&gt;
&lt;p&gt;é›–ç„¶ç°¡å–®çš„ dual-encoder framework å¯ä»¥é”åˆ°å¾ˆæ£’çš„æ•ˆæœï¼Œä½†ä½œè€…é¡¯ç¤ºå‡ºè¦æˆåŠŸè¨“ç·´ä¹Ÿæœ‰ä¸€äº›é—œéµå› ç´ &lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œæ ¹æ“š empirical analysis å’Œ ablation studyï¼Œæ›´è¤‡é›œçš„ model framework æˆ– similarity function ä¸ä¸€å®šèƒ½æä¾›é¡å¤–åƒ¹å€¼&lt;/p&gt;
&lt;p&gt;ç”±æ–¼æª¢ç´¢æ€§èƒ½çš„æé«˜ï¼Œåœ¨å¤šå€‹ open-domain QA benchmarks ä¸Šå–å¾—äº† SOTA&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ORQA è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/orqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 25 Dec 2023 00:00:13 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/orqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.00300&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Latent Retrieval for Weakly Supervised Open Domain Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ€è¿‘çš„å·¥ä½œå¸¸ä¾è³´æ–¼å…©å€‹å‡è¨­ï¼Œä¸€å€‹æ˜¯å° supporting evidence åš strong supervisionï¼Œå¦ä¸€å€‹æ˜¯å‡è¨­ blackbox information retrieval (IR) system å¯ä»¥æ‰¾åˆ°æ‰€æœ‰çš„ evidence candidates&lt;/p&gt;
&lt;p&gt;ä½œè€…èªç‚ºå…©è€…éƒ½ä¸æ˜¯æœ€ç†æƒ³çš„ï¼Œå› ç‚º gold evidence ä¸ç¸½æ˜¯å¯ç”¨ï¼Œè€Œä¸” QA å’Œ IR æœ‰è‘—æ ¹æœ¬ä¸Šçš„ä¸åŒ&lt;/p&gt;
&lt;p&gt;ä½œè€…é¦–æ¬¡è­‰æ˜ï¼Œåœ¨æ²’æœ‰ä»»ä½• IR çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥å¾ question-answer pair ä¸­å…±åŒå­¸ç¿’ retriver å’Œ reader&lt;/p&gt;
&lt;p&gt;åœ¨é€™ç¨® setting ä¸‹ï¼Œä¾†è‡ª Wikipedia çš„ evidence retrieval è¢«è¦–ä½œ latent variable&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ learn from scratch ä¸åˆ‡å¯¦éš›ï¼Œå› æ­¤ç”¨ Inverse Cloze Task (ICT) ä¾†é è¨“ç·´ retriever&lt;/p&gt;
&lt;p&gt;ä½œè€…å°äº”å€‹ QA è³‡æ–™é›†é€²è¡Œè©•ä¼°&lt;/p&gt;
&lt;p&gt;åœ¨æå•è€…å·²ç¶“çŸ¥é“ç­”æ¡ˆçš„æƒ…æ³ä¸‹ï¼Œå‚³çµ±çš„ IR ç³»çµ±ï¼ˆæ¯”å¦‚ BM25ï¼‰å·²è¶³å¤ &lt;/p&gt;
&lt;p&gt;åœ¨ä½¿ç”¨è€…çœŸæ­£å°‹æ±‚ç­”æ¡ˆçš„è³‡æ–™é›†ä¸Šï¼Œä½œè€…é¡¯ç¤ºå‡º learned retrival çš„é‡è¦æ€§ï¼Œåœ¨ exact match ä¸Šæ¯” BM25 å¥½ 19 å€‹é»&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ç”±æ–¼é–±è®€ç†è§£ç³»çµ±çš„ç™¼å±•ï¼Œäººå€‘å° open domain question answering (QA) çš„èˆˆè¶£é‡æ–°ç‡ƒèµ·&lt;/p&gt;
&lt;p&gt;å…¶ä¸­ evidence å¾—å¾ open corpus å–å¾—ï¼Œè€Œä¸æ˜¯ç›´æ¥å¾è¼¸å…¥çµ¦å…¥&lt;/p&gt;
&lt;p&gt;ç¾æœ‰çš„æ–¹æ³•éœ€è¦ blackbox IR system ä¾†å®Œæˆå¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œï¼Œå³ä½¿å®ƒç„¡æ³•å°ä¸‹éŠä»»å‹™é€²è¡Œå¾®èª¿&lt;/p&gt;
&lt;p&gt;åœ¨ DrQA æ¨å»£çš„å¼·ç›£ç£ç’°å¢ƒä¸­ï¼Œä»–å€‘ä¹Ÿå‡è¨­äº†ä¸€å€‹è¨“ç·´åœ¨ question-answer-evidence triple ä¸Šçš„é–±è®€ç†è§£æ¨¡å‹&lt;/p&gt;
&lt;p&gt;åœ¨æŸäº›äººæå‡ºçš„ weakly supervised setting ä¸­ï¼Œä»–å€‘å‡è¨­ IR system æä¾› noisy gold evidence&lt;/p&gt;
&lt;p&gt;é€™äº›æ–¹æ³•åˆ©ç”¨ IR system ä¾†å¤§å¹…æ¸›å°‘æœå°‹ç©ºé–“&lt;/p&gt;
&lt;p&gt;ç„¶è€Œ QA å’Œ IR æœ‰è‘—æ ¹æœ¬æ€§çš„å·®ç•°&lt;/p&gt;
&lt;p&gt;é›–ç„¶ IR é—œå¿ƒçš„æ˜¯ lexical å’Œ semantic matchingï¼Œä½† question çš„å®šç¾©ä¸¦ä¸å…·é«”ï¼Œè€Œä¸”éœ€è¦æ›´å¤š language understandingï¼Œå› ç‚º user åœ¨æ‰¾çš„æ˜¯æœªçŸ¥è³‡è¨Š&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘æ‡‰è©²ç›´æ¥ ç”¨ QA data å­¸ç¿’ retrieveï¼Œè€Œä¸æ˜¯å—é™æ–¼ blackbox IR system&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…ä»‹ç´¹äº†ç¬¬ä¸€å€‹ OpenRetrieval Question Answering system (ORQA) æ¡†æ¶&lt;/p&gt;
&lt;p&gt;ORQA å­¸ç¿’å¾ open corpus retrieve evidenceï¼Œä¸¦ä¸”åªåš question-answer pair çš„ç›£ç£è¨“ç·´&lt;/p&gt;
&lt;p&gt;é›–ç„¶æœ€è¿‘çš„å·¥ä½œåœ¨æ”¹é€² evidence retrieval ä¸Šå–å¾—äº†å·¨å¤§çš„é€²å±•ï¼Œä½†æ˜¯ä»–å€‘ä¾ç„¶åªæ˜¯åœ¨ closed evidence set ä¸‹ rerank&lt;/p&gt;
&lt;p&gt;fully end-to-end çš„æŒ‘æˆ°æ˜¯ï¼Œopen corpus çš„ retrieval å¿…é ˆè¢«è¦–ç‚º latent variableï¼Œè¦ train from scratch æ˜¯ä¸åˆ‡å¯¦éš›çš„&lt;/p&gt;
&lt;p&gt;IR system æä¾›äº†ä¸€å€‹åˆç†ä½†å¯èƒ½éæœ€å¥½çš„èµ·é»&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡çš„ä¸€å€‹é—œéµæ˜¯ï¼Œå¦‚æœç”¨ç„¡ç›£ç£ çš„ ICT å° retriever åšé è¨“ç·´ï¼Œé‚£ end-to-end learning æ˜¯æœ‰å¯èƒ½çš„&lt;/p&gt;
&lt;p&gt;åœ¨ ICT ä¸­ï¼Œä¸€å€‹å¥å­è¢«è¦–ä½œ pseudo-questionï¼Œè€Œå®ƒçš„ context è¢«è¦–ä½œ pseudo-evdience&lt;/p&gt;
&lt;p&gt;çµ¦å®šä¸€å€‹ pseudo-questionï¼Œretriever çš„ç›®æ¨™æ˜¯å¾ batch ä¸­çš„ candidate æ‰¾å‡ºå°æ‡‰çš„ pseudo-evidence&lt;/p&gt;
&lt;p&gt;ICT pretraining æä¾›äº†å¼·å¤§çš„åˆå§‹åŒ–ï¼Œä½¿ ORQA å¯ä»¥ç°¡å–®åœ°å„ªåŒ–&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨äº”å€‹ QA è³‡æ–™é›†ä¸Šé€²è¡Œäº†è©•ä¼°ï¼Œåœ¨å•é¡Œä½œè€…å·²çŸ¥ç­”æ¡ˆçš„è³‡æ–™é›†ä¸Š (SQuADã€TriviaQA)ï¼Œæª¢ç´¢å•é¡Œé¡ä¼¼å‚³çµ±çš„ IRï¼Œä¸¦ä¸” BM25 æ˜¯ SOTA retrieval&lt;/p&gt;
&lt;p&gt;åœ¨å•é¡Œä½œè€…ä¸çŸ¥é“ç­”æ¡ˆçš„è³‡æ–™é›†ä¸Š (Natural Questionsã€WebQuestionsã€CuratedTrec)ï¼Œä½œè€…é¡¯ç¤ºå‡º learned retrieval çš„é‡è¦æ€§ï¼Œæ¯” BM25 åœ¨ exact match ä¸Šå¥½ 6~19 å€‹é»&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h3 id=&#34;task&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;åœ¨ open-domain QA ä¸­ï¼Œ$q$ æ˜¯ question stringï¼Œè€Œ $a$ æ˜¯ answer string&lt;/p&gt;
&lt;p&gt;èˆ‡é–±è®€ç†è§£ä¸åŒï¼Œevidence çš„ä¾†æºæ˜¯ modeling choiceï¼Œè€Œé task definition çš„ä¸€éƒ¨åˆ†&lt;/p&gt;
&lt;h3 id=&#34;formal-definitions&#34;&gt;Formal Definitions&lt;/h3&gt;
&lt;p&gt;Model æŠŠä¸€å€‹ unstructured text corpus åˆ‡æˆ B å¡Šçš„ evidence text&lt;/p&gt;
&lt;p&gt;ä¸€å€‹ answer derivation æ˜¯ä¸€å€‹ pair $(b,s)$&lt;/p&gt;
&lt;p&gt;$1 \le b \le B$ æ˜¯ block çš„ index&lt;/p&gt;
&lt;p&gt;$s$ æ˜¯ block $b$ çš„ span&lt;/p&gt;
&lt;p&gt;scoring function $S(b,s,q)$ ç”¨ä¾†è¨ˆç®— $(b,s)$ å°æ–¼ $q$ çš„åˆ†æ•¸&lt;/p&gt;
&lt;p&gt;ä¸€èˆ¬ä¾†èªª scoring function æœƒè¢«åˆ†è§£æˆ retrieval component $S_{retr}(b,q)$ å’Œ $S_{read}(b,s,q)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S(b,s,q) = S_{retr}(b,q) + S_{read}(b,s,q)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;åœ¨æ¨è«–éšæ®µï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a^* = TEXT(argmax_{b,s} S(b,s,q))$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;open-domain QA çš„ä¸€å€‹ä¸»è¦æŒ‘æˆ°æ˜¯ handling the scale&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ English Wikipedia ä¸Šåšå¯¦é©—ï¼ŒåŒ…å«è¶…é 13M å€‹ blocksï¼Œæ¯å€‹ block éƒ½æœ‰è¶…é 2000 å€‹å¯èƒ½çš„ spans&lt;/p&gt;
&lt;h3 id=&#34;existing-pipelined-models&#34;&gt;Existing Pipelined Models&lt;/h3&gt;
&lt;p&gt;åœ¨ç¾æœ‰çš„ retrieval-based open-domain QA æ¨¡å‹ä¸­ï¼Œblackbox IR system å…ˆé¸æ“‡ä¸€çµ„ closed set of evidence candidates&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œ DrQA ä¹‹å¾Œçš„å¤šæ•¸å·¥ä½œéƒ½ç”¨ TF-IDF ä¾†æŒ‘é¸ candidateï¼Œä¸¦å°ˆæ³¨æ–¼é–±è®€ç†è§£æˆ– reranking çš„éƒ¨åˆ†&lt;/p&gt;
&lt;p&gt;reading component $S_{read}(b,s,q)$ æ˜¯å¾ gold answer å­¸ç¿’çš„&lt;/p&gt;
&lt;p&gt;åœ¨æœ€æ¥è¿‘æˆ‘å€‘çš„æ–¹æ³•çš„å·¥ä½œä¸­ï¼Œreader æ˜¯é€é weak supervision å­¸ç¿’çš„&lt;/p&gt;
&lt;p&gt;retrieval system æœƒå•Ÿç™¼å¼ï¼ˆheuristicallyï¼‰åœ°åˆªé™¤ spurious ambiguitiesï¼Œä¸¦æŠŠæ¸…ç†å¾Œçš„çµæœè¦–ç‚º gold evidence&lt;/p&gt;
&lt;h2 id=&#34;open-retrieval-question-answering-orqa&#34;&gt;Open-Retrieval Question Answering (ORQA)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;$BERT(x_1, [x_2]) = \{CLS: h_{CLS}, 1: h_1, 2: h_2,&amp;hellip;\}$&lt;/p&gt;
&lt;h3 id=&#34;retriever-component&#34;&gt;Retriever component&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$h_q = W_q BERT(q)[CLS]$&lt;/li&gt;
&lt;li&gt;$h_b = W_b BERT(b)[CLS]$&lt;/li&gt;
&lt;li&gt;$S_{retr}(b,q) = h_q^T h_b$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reader-component&#34;&gt;Reader component&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$h_{start} = BERT_R(q,b)[START(s)]$&lt;/li&gt;
&lt;li&gt;$h_{end} = BERT_R(q,b)[END(s)]$&lt;/li&gt;
&lt;li&gt;$S_{read}(b,s,q) = MLP([h_{start};h_{end}])$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inference--learning-challenges&#34;&gt;Inference &amp;amp; Learning Challenges&lt;/h3&gt;
&lt;p&gt;ä¸Šé¢çš„æ¨¡å‹æ¦‚å¿µå¾ˆç°¡å–®&lt;/p&gt;
&lt;p&gt;ä½†æ¨è«–å’Œå­¸ç¿’ä¸Šå…·æœ‰æŒ‘æˆ°æ€§ï¼Œå› ç‚ºï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;open evidence corpus æœ‰å·¨å¤§çš„æœå°‹ç©ºé–“ï¼ˆè¶…é 13M å€‹ blocksï¼‰&lt;/li&gt;
&lt;li&gt;è¦å¦‚ä½•åœ¨ç©ºé–“ navigate æ˜¯ latent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å› æ­¤æ¨™æº–çš„ teacher forcing ä¸èƒ½ç”¨ï¼Œlatent variable ä¹Ÿä¸èƒ½ç”¨ï¼Œå› ç‚ºå­˜åœ¨å¤§é‡ spuriously ambiguous derivationsï¼ˆæ¯”å¦‚ç­”æ¡ˆæ˜¯ &amp;ldquo;seven&amp;rdquo;ï¼Œå¾ˆå¤š evidence éƒ½æœƒæœ‰ &amp;ldquo;seven&amp;rdquo; é€™å€‹å­—çœ¼ï¼‰&lt;/p&gt;
&lt;p&gt;ä½œè€…é€ééç›£ç£é è¨“ç·´ä¾†è‰¯å¥½åœ°åˆå§‹åŒ– retriever ä¾†è§£æ±ºé€™äº›æŒ‘æˆ°&lt;/p&gt;
&lt;p&gt;é è¨“ç·´çš„ retriever ä½¿ä½œè€…èƒ½å¤ ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;pre-encode Wikipedia blocksï¼Œå¾è€Œåœ¨ finetune éšæ®µå¯¦ç¾å‹•æ…‹ä¸”å¿«é€Ÿçš„ top-k retrieval&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½¿ retrieval å¯ä»¥é é›¢ spuriously ambiguities ä¸¦åå‘ supportive evidence&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;inverse-cloze-task&#34;&gt;Inverse Cloze Task&lt;/h2&gt;
&lt;p&gt;ä½œè€…æå‡ºçš„é è¨“ç·´æµç¨‹çš„ç›®æ¨™æ˜¯æƒ³è®“ retriever è§£æ±ºå’Œ evidence retrieval for QA ç›¸ä¼¼çš„ç„¡ç›£ç£ä»»å‹™&lt;/p&gt;
&lt;p&gt;ç›´è¦ºä¸Šï¼Œuseful evidence é€šå¸¸æœƒè¨è«–å•é¡Œä¸­çš„ entities, events å’Œ relations&lt;/p&gt;
&lt;p&gt;é‚„åŒ…å«å•é¡Œä¸­ä¸å­˜åœ¨çš„é¡å¤–è³‡è¨Š (the answer)&lt;/p&gt;
&lt;p&gt;question-evidence pair æ˜¯ setence-context pairï¼Œå¥å­çš„ä¸Šä¸‹æ–‡åœ¨èªæ„ä¸Šæ˜¯ç›¸é—œçš„ï¼Œå¯ä»¥æ¨è«–å¥å­ä¸­ç¼ºå°‘çš„è³‡è¨Š&lt;/p&gt;
&lt;p&gt;æ†‘é€™ç¨®æƒ³æ³•ï¼Œä½œè€…å»ºè­°ä½¿ç”¨ ICT ä¾†é è¨“ç·´ retriever&lt;/p&gt;
&lt;p&gt;åœ¨ standard cloze task ä¸­ï¼Œç›®æ¨™æ˜¯æ ¹æ“šä¸Šä¸‹æ–‡é æ¸¬ masked-out text&lt;/p&gt;
&lt;p&gt;ç›¸åï¼ŒICT è¦é€†å‘é æ¸¬ï¼Œçµ¦å®šä¸€å€‹å¥å­ï¼Œé æ¸¬ä¸Šä¸‹æ–‡&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨é¡ä¼¼ downstream retrieval çš„ discriminative objectiveï¼š&lt;/p&gt;
&lt;p&gt;$P_{ICT}(b|q)=\frac{exp(S_{retr}(b,q))}{\sum_{b&amp;rsquo; \in BATCH} exp(S_{retr}(b&amp;rsquo;,q))}$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;é æ¸¬å“ªå€‹ä¸Šä¸‹æ–‡æ˜¯ query çš„&lt;/p&gt;
&lt;p&gt;ICT æœ‰å€‹é‡é»æ˜¯ï¼Œå®ƒè¦åšçš„ä¸åƒ…åƒ…æ˜¯å–®å­—åŒ¹é…ï¼Œå› ç‚º evidence ä¸­æ²’æœ‰ pseudo-question&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ fig.2 çš„ pseudo-question å®Œå…¨æ²’æœ‰æåŠ zebraï¼Œä½† retriever è¦èƒ½å¤ é¸æ“‡ zebra çš„ context&lt;/p&gt;
&lt;p&gt;èƒ½å¤ å¾éæŒ‡å®šçš„èªè¨€æ¨è«–å‡ºèªæ„æ˜¯ QA å’Œå‚³çµ± IR çš„å·®ç•°&lt;/p&gt;
&lt;p&gt;ç„¶è€Œä½œè€…ä¹Ÿä¸æƒ³é˜»æ­¢ retriever å­¸ç¿’å–®å­—åŒ¹é…ï¼Œå› ç‚º lexical overlap æœ€çµ‚æ˜¯ä¸€å€‹éå¸¸æœ‰ç”¨çš„ç‰¹å¾µ&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œä½œè€…åªåœ¨ 90 % çš„ä¾‹å­ä¸­æŠŠ sentence å¾ context ä¸­ç§»é™¤ï¼Œé¼“å‹µæ¨¡å‹åœ¨éœ€è¦çš„æ™‚å€™å­¸ç¿’æŠ½è±¡è¡¨ç¤ºï¼Œä¹Ÿåœ¨å¯ç”¨æ™‚å­¸ç¿’ low-level word matching features&lt;/p&gt;
&lt;p&gt;ICT é è¨“ç·´å¯¦ç¾å…©å€‹ä¸»è¦ç›®æ¨™ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;å„˜ç®¡é è¨“ç·´çš„å¥å­å’Œå¾®èª¿æ™‚çš„ question ä¸åŒ¹é…ï¼Œä½†ä½œè€…é æœŸ zero-shot evidence retrieval performance è¶³ä»¥å¼•å° latent variable çš„å­¸ç¿’&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pretrained evidence blocks å’Œ downstream evidence blocks é–“æ²’æœ‰é€™ç¨®ä¸åŒ¹é…çš„å•é¡Œ&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å› æ­¤å¯é æœŸ $BERT_{B}(b)$ ç„¡é ˆé€²ä¸€æ­¥è¨“ç·´å³å¯æ­£å¸¸å·¥ä½œ&lt;/p&gt;
&lt;p&gt;åªæœ‰ question encoder éœ€è¦é‡å°ä¸‹éŠè³‡æ–™å¾®èª¿&lt;/p&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;ç”±æ–¼ fixed block encoder å·²ç¶“ç‚º retrieval æä¾›äº†æœ‰ç”¨çš„ representationï¼Œå¯ä»¥é å…ˆè¨ˆç®—æ‰€æœ‰ block çš„ encoding&lt;/p&gt;
&lt;p&gt;å› æ­¤åœ¨å¾®èª¿çš„æ™‚å€™ä¸éœ€è¦å°å¤§é‡ evidence block é‡æ–° encodeï¼Œä¸¦ä¸”å¯ä»¥ä½¿ç”¨æ¯”å¦‚ locality-sensitive hashing ä¹‹é¡çš„ç¾æœ‰å·¥å…·ä¾†å»ºç«‹ç´¢å¼•&lt;/p&gt;
&lt;p&gt;é€é pre-compiled indexï¼Œæ¨ç†éµå¾ª standard beam-search&lt;/p&gt;
&lt;p&gt;æª¢ç´¢ top-k evidence blockï¼Œä¸¦åªè¨ˆç®—é€™ k å€‹ block çš„ reader score&lt;/p&gt;
&lt;h2 id=&#34;learning&#34;&gt;Learning&lt;/h2&gt;
&lt;p&gt;é€™é‚Šå¤ªè¤‡é›œï¼Œå»ºè­°çœ‹åŸæ–‡&lt;/p&gt;
&lt;h2 id=&#34;experimental-setup&#34;&gt;Experimental Setup&lt;/h2&gt;
&lt;h3 id=&#34;open-domain-qa-datasets&#34;&gt;Open Domain QA Datasets&lt;/h3&gt;
&lt;p&gt;å° 5 å€‹ç¾æœ‰ question answering æˆ–é–±è®€ç†è§£è³‡æ–™é›†é€²è¡Œè©•ä¼°&lt;/p&gt;
&lt;p&gt;ä¸¦éæ‰€æœ‰è³‡æ–™é›†çš„åŸå§‹å½¢å¼éƒ½æ˜¯ open-domain QAï¼Œå› æ­¤ä½œè€…éµå¾ª DrQA çš„åšæ³•ï¼Œè½‰æˆ open format&lt;/p&gt;
&lt;p&gt;æ¯å€‹ example éƒ½æœ‰ä¸€å€‹ single question å’Œä¸€ã€Œçµ„ã€ reference answer&lt;/p&gt;
&lt;h4 id=&#34;natural-questions&#34;&gt;Natural Questions&lt;/h4&gt;
&lt;p&gt;åŒ…å«äº†å¾ Google Search çš„ aggregated queries ä¸­çš„ question&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è’é›†é€™å€‹è³‡æ–™é›†çš„ open versionï¼Œä½œè€…åªä¿ç•™ short answer ä¸¦ä¸Ÿæ£„ evidence document&lt;/p&gt;
&lt;p&gt;å…·æœ‰è¨±å¤š token çš„ç­”æ¡ˆé€šå¸¸é¡ä¼¼ extractive snippets è€Œä¸æ˜¯ canonical answerï¼Œå› æ­¤ä½œè€…ä¸Ÿæ£„é•·åº¦è¶…é 5 å€‹ token çš„ç­”æ¡ˆ&lt;/p&gt;
&lt;h4 id=&#34;webquestions&#34;&gt;WebQuestions&lt;/h4&gt;
&lt;p&gt;åŒ…å«å¾ Google Suggest API æŠ½å–çš„å•é¡Œ&lt;/p&gt;
&lt;p&gt;ç­”æ¡ˆæ˜¯æ ¹æ“š Freebase æ¨™è¨»çš„ï¼Œä½†æ˜¯åªä¿ç•™ entities çš„ string representation&lt;/p&gt;
&lt;h4 id=&#34;curatedtrec&#34;&gt;CuratedTrec&lt;/h4&gt;
&lt;p&gt;å•é¡Œä¾†è‡ªçœŸå¯¦æŸ¥è©¢çš„å„ç¨®ä¾†æºï¼Œæ¯”å¦‚ MSNSearch&lt;/p&gt;
&lt;h4 id=&#34;triviaqa&#34;&gt;TriviaQA&lt;/h4&gt;
&lt;p&gt;å¾ç¶²è·¯ä¸ŠæŠ“çš„ question-answer pairs&lt;/p&gt;
&lt;p&gt;ä½œè€…ä½¿ç”¨ unfiltered set ä¸¦æ¨æ£„ distanly supervised evidence&lt;/p&gt;
&lt;h4 id=&#34;squad&#34;&gt;SQuAD&lt;/h4&gt;
&lt;p&gt;è¢«è¨­è¨ˆä¾†ç”¨ä½œé–±è®€ç†è§£ï¼Œè€Œä¸æ˜¯ open-domain QA&lt;/p&gt;
&lt;p&gt;ç­”æ¡ˆç¯„åœæ˜¯å¾ Wikipedia çš„æ®µè½ä¸­é¸æ“‡çš„ï¼Œå•é¡Œç”± annotators ç·¨å¯«ï¼Œannotators è¢«æŒ‡ç¤ºæå‡ºå•é¡Œï¼Œè¦ç”±çµ¦å®šçš„ context ä¸­çš„ span ä¾†å›ç­”&lt;/p&gt;
&lt;h3 id=&#34;dataset-biases&#34;&gt;Dataset Biases&lt;/h3&gt;
&lt;p&gt;åœ¨ Natural Questionsã€WebQuestions å’Œ CuratedTrec ä¸­ï¼Œæå•è€…ä¸çŸ¥é“ç­”æ¡ˆï¼Œåæ˜ äº†çœŸå¯¦çš„å°‹æ±‚å•é¡Œçš„åˆ†ä½ˆ&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯ï¼Œannotators å¿…é ˆå–®ç¨æ‰¾åˆ°æ­£ç¢ºçš„ç­”æ¡ˆï¼Œå› æ­¤éœ€è¦ automatic toolsï¼Œä¸¦å¯èƒ½æœƒå°é€™äº›å·¥å…·çš„çµæœç”¢ç”Ÿ bias&lt;/p&gt;
&lt;p&gt;åœ¨ TriviaQA å’Œ SQuAD ä¸­ï¼Œä¸éœ€è¦ automatic toolsï¼Œå› ç‚º annotators æ˜¯æ ¹æ“šå·²çŸ¥ç­”æ¡ˆå¯«å•é¡Œçš„&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™å¼•å…¥äº†å¦ä¸€çµ„å¯èƒ½æ›´æˆå•é¡Œçš„ biasï¼Œå°±æ˜¯æ’°å¯«å•é¡Œä¸¦éå‡ºæ–¼è³‡è¨Šéœ€æ±‚&lt;/p&gt;
&lt;p&gt;å°è‡´å•é¡Œä¸­æœ‰è¨±å¤šè‡ªç„¶å‡ºç¾çš„å•é¡Œä¸­æ²’æœ‰çš„æç¤º&lt;/p&gt;
&lt;p&gt;é€™åœ¨ SQuAD ä¸­å•é¡Œç‰¹åˆ¥åš´é‡ï¼Œä½¿å•é¡Œå’Œ evidence é–“äººç‚ºåœ°å‡ºç¾å¤§é‡è©å½™é‡ç–Š&lt;/p&gt;
&lt;p&gt;ä½†ä¸Šè¿°é€™äº›åªæ˜¯æƒ³è¡¨é”è³‡æ–™é›†çš„å±¬æ€§ï¼Œè€Œéå¯æ¡å–è¡Œå‹•çš„æ‰¹è©•ï¼Œå› ç‚ºè¦å–å¾—å¤§è¦æ¨¡è³‡æ–™å¿…å®šæœƒé‡åˆ°é€™äº›ç‹€æ³ï¼Œç›®å‰é‚„ä¸æ¸…æ¥šå¦‚ä½•åœ¨åˆç†çš„æˆæœ¬ä¸‹æ”¶é›†å…¬æ­£çš„è³‡æ–™é›†&lt;/p&gt;
&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h3&gt;
&lt;h4 id=&#34;evidence-corpus&#34;&gt;Evidence Corpus&lt;/h4&gt;
&lt;p&gt;corpus è¢«åˆ†æˆæœ€å¤š 288 å€‹å–®å­—çš„ chunkï¼Œä¸¦ä¸”ä¿ç•™ sentence boundaries&lt;/p&gt;
&lt;p&gt;å°è‡´æœ‰è¶…é 13M å€‹ blocks&lt;/p&gt;
&lt;h2 id=&#34;main-results&#34;&gt;Main Results&lt;/h2&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;h4 id=&#34;bm25&#34;&gt;BM25&lt;/h4&gt;
&lt;p&gt;BM25 æ˜¯äº‹å¯¦ä¸Šçš„éç›£ç£æœç´¢æ–¹æ³• SOTA
è¢«è­‰æ˜å°æ–¼å‚³çµ±è³‡è¨Šæª¢ç´¢ä»»å‹™å’Œ QA çš„ evidence retrieval ä»»å‹™éƒ½æ˜¯ robust&lt;/p&gt;
&lt;h4 id=&#34;language-models&#34;&gt;Language Models&lt;/h4&gt;
&lt;p&gt;éç›£ç£çš„ neural retrieval å°æ–¼å‚³çµ± IR ä¾†èªªå¾ˆé›£æ”¹é€²ï¼Œä½†é€™è£¡è¦–ä½œæ¯”è¼ƒçš„ baseline&lt;/p&gt;
&lt;p&gt;ä½œè€…å° LM é€²è¡Œå¯¦é©—ï¼Œä¸¦ä¸”é€™å·²è¢«è­‰æ˜æ˜¯ SOTA unsupervised representation&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘èˆ‡å…©ç¨®å»£æ³›ä½¿ç”¨çš„ 128-dimensional representation é€²è¡Œæ¯”è¼ƒï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NNLM
&lt;ul&gt;
&lt;li&gt;context-independent embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ELMO
&lt;ul&gt;
&lt;li&gt;context-dependent bidirectional LSTM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å°±åƒ ICT ä¸€æ¨£ï¼Œä½¿ç”¨ alternate encoder ä¾†é å…ˆè¨ˆç®— encoded evidence blocks é‚„æœ‰åˆå§‹åŒ–ç¶“é finetune çš„ question encoding&lt;/p&gt;
&lt;p&gt;æ ¹æ“šç¾æœ‰çš„ IR æ–‡ç»ï¼Œé‚„æœ‰ LM æ²’æœ‰é¡¯è‘—å„ªåŒ– retrieval çš„ç›´è¦ºï¼Œä½œè€…ä¸¦ä¸æœŸæœ›é€™äº›æˆç‚ºå¼·å¤§çš„ baselineï¼Œä½†æ˜¯ä»–å€‘è­‰æ˜äº†å°‡æ–‡æœ¬ç·¨ç¢¼ç‚º 128 ç¶­çš„é›£åº¦&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/table5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨æå•è€…å·²ç¶“çŸ¥é“ç­”æ¡ˆçš„è³‡æ–™é›†ä¸­&lt;/p&gt;
&lt;p&gt;è­‰å¯¦å£“ç¸®åˆ° 128ç¶­çš„å‘é‡ç„¡æ³•èˆ‡ BM25 ç²¾ç¢ºè¡¨ç¤º evidence ä¸­æ¯å€‹å–®å­—çš„èƒ½åŠ›ç›¸ç¬¦&lt;/p&gt;
&lt;p&gt;SQuAD çš„ dev å’Œ test é–“çš„é¡¯è‘—ä¸‹é™åæ˜ äº†è³‡æ–™é›†ä¸­çš„æŸå€‹ç‰¹æ€§ - 10 è¬å€‹å•é¡Œåƒ…æºè‡ª 536 å€‹æ–‡ä»¶&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼ŒSQuAD çš„å¥½çš„æª¢ç´¢ç›®æ¨™ï¼Œæœƒå’Œè¨“ç·´ç¯„ä¾‹é«˜åº¦ç›¸é—œï¼Œé•åäº† IID å‡è¨­ï¼Œä½¿å…¶ä¸é©åˆå­¸ç¿’æª¢ç´¢&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œä½œè€…å¼·çƒˆå»ºè­°å° end-to-end open-domain QA models æœ‰èˆˆè¶£çš„äººä¸å†ä½¿ç”¨ SQuAD é€²è¡Œè¨“ç·´å’Œè©•ä¼°&lt;/p&gt;
&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;h3 id=&#34;strongly-supervised-comparison&#34;&gt;Strongly supervised comparison&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ORQA/table6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è­‰å¯¦ä½œè€…çš„ BM25 Baseline æ˜¯ SOTAï¼Œæä¾›äº†å’Œ DrQA çš„æ¯”è¼ƒ&lt;/p&gt;
&lt;p&gt;DrQA çš„ reader æ˜¯ DocReaderï¼Œç”¨ TF-IDF å–å¾— top k documents&lt;/p&gt;
&lt;p&gt;é‚„åŒ…æ‹¬åŸºæ–¼ TF-IDF retrieval çš„ distant supervision&lt;/p&gt;
&lt;p&gt;BERTserini çš„ reader æ˜¯ä¸€å€‹åŸºæ–¼ base BERTï¼ˆé¡ä¼¼ä½œè€…çš„ readerï¼‰ï¼Œä¸¦ç”¨ BM25 æœç´¢ top-k å€‹æ®µè½ï¼ˆåƒä½œè€…çš„ BM25 baselineï¼‰&lt;/p&gt;
&lt;p&gt;ä¸»è¦å€åˆ¥åœ¨ BERTserini ä½¿ç”¨ Wikipedia ä¸­çš„çœŸå¯¦æ®µè½ï¼Œè€Œä¸æ˜¯ä»»æ„ blockï¼Œå¾è€Œç”±æ–¼é•·åº¦ä¸å‡å°è‡´æ›´å¤š evidence blocks&lt;/p&gt;
&lt;p&gt;ç‚ºäº†å’Œé€™äº›å¼·ç›£ç£ç³»çµ±é€²è¡Œæ¯”è¼ƒï¼Œä½œè€…åœ¨ SQuAD ä¸Šé è¨“ç·´ reader&lt;/p&gt;
&lt;h3 id=&#34;masking-rate-in-the-inverse-cloze-task&#34;&gt;Masking Rate in the Inverse Cloze Task&lt;/h3&gt;
&lt;p&gt;pseudo-query åœ¨ 90% çš„æ™‚é–“è£¡éƒ½å¾ evidence block é®è”½&lt;/p&gt;
&lt;p&gt;å¦‚æœç¸½æ˜¯å±è”½ pseudo-queryï¼Œé‚£éº¼ retriever æ°¸é ä¸æœƒçŸ¥é“ n-gram overlap æ˜¯ä¸€å€‹å¼·å¤§çš„ retrieval signalï¼Œå°è‡´æå¤± 10 å€‹é»&lt;/p&gt;
&lt;p&gt;å¦‚æœå¾ä¸å±è”½ï¼Œå•é¡Œå°±æœƒç°¡åŒ–ç‚ºè¨˜æ†¶ï¼Œå°è‡´ä¸èƒ½å¾ˆå¥½åœ°æ¨å»£åˆ°å•é¡Œ&lt;/p&gt;
&lt;h3 id=&#34;example-predictions&#34;&gt;Example Predictions&lt;/h3&gt;
&lt;p&gt;ç™¼ç¾ ORQA åœ¨å…·æœ‰é«˜åº¦è©å½™é‡ç–Šçš„æ–‡æœ¬æ›´åŠ  robust&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯ç”±æ–¼ 128 ç¶­å‘é‡çš„è³‡è¨Šæœ‰é™&lt;/p&gt;
&lt;p&gt;å¾ˆé›£ç²¾ç¢ºåœ°è¡¨ç¤ºæ¥µç‚ºå…·é«”çš„æ¦‚å¿µï¼Œæ¯”å¦‚æº–ç¢ºæ—¥æœŸ&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DrQA è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/drqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 25 Dec 2023 00:00:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/drqa-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1704.00051&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Reading Wikipedia to Answer Open-Domain Questions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä»¥ Wikipedia ç‚ºçŸ¥è­˜ä¾†æºä¾†è§£æ±º open-domain question answering å•é¡Œ&lt;/p&gt;
&lt;p&gt;ä»»ä½•å•é¡Œçš„ç­”æ¡ˆéƒ½æ˜¯ Wikipedia ä¸­çš„ä¸€æ®µæ–‡å­—&lt;/p&gt;
&lt;p&gt;é€™é …æŒ‘æˆ°çµåˆäº†æ–‡ä»¶æª¢ç´¢å’Œç†è§£æ–‡å­—çš„èƒ½åŠ›&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡çš„ä½œæ³•åŸºæ–¼ä¸€å€‹ search componentï¼Œç”± bigram hashing å’Œ TF-IDF matching æ§‹æˆï¼Œä¸¦çµåˆ RNN&lt;/p&gt;
&lt;p&gt;å°å¤šå€‹ QA è³‡æ–™é›†åšçš„å¯¦é©—è¡¨æ˜ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;é€™å…©å€‹ components å°æ–¼ç¾æœ‰çš„å°æ‡‰æ¨¡å¡Šå…·æœ‰é«˜åº¦ç«¶çˆ­åŠ›&lt;/li&gt;
&lt;li&gt;åœ¨ä»–å€‘çš„çµ„åˆä¸Šä½¿ç”¨ distant supervision ååˆ†æœ‰æ•ˆ&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;è¦æŠŠ wikipedia ç•¶ä½œçŸ¥è­˜ä¾†æºï¼Œå›ç­”ä»»ä½•ä¸€å€‹å•é¡Œï¼Œéƒ½å¿…é ˆå…ˆå¾è¶…é 5 ç™¾è¬ç¯‡æ–‡ç« ä¸­æ‰¾å‡ºå°‘æ•¸ç›¸é—œçš„æ–‡ç« ï¼Œä¸¦ä»”ç´°æƒæä»¥æ‰¾å‡ºç­”æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å°‡é€™ç¨±ç‚º machine reading at scale (MRS)&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æŠŠ wikipedia ç•¶ä½œä¸€å€‹æ–‡ç« çš„é›†åˆï¼Œä¸¦ä¸”ä¸ä¾è³´å…§éƒ¨çš„ graph structure&lt;/p&gt;
&lt;p&gt;å› æ­¤è©²æ–¹æ³•æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥å¥—åˆ°è«¸å¦‚æ–°èã€ç¶²è·¯è«–å£‡ç­‰ç­‰çš„è³‡æ–™é›†ä¸Š&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡é–‹ç™¼äº† DrQAï¼Œå¼·å¤§çš„ç¶­åŸºç™¾ç§‘å•ç­”ç³»çµ±ï¼Œç”±ä»¥ä¸‹éƒ¨åˆ†çµ„æˆï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Document Retriever
&lt;ul&gt;
&lt;li&gt;ä¸€å€‹ç”¨ bigram hashing å’Œ TF-IDF matching çš„ module&lt;/li&gt;
&lt;li&gt;ç”¨æ–¼åœ¨çµ¦å®šå•é¡Œçš„æƒ…æ³ä¸‹ï¼Œè¿”å›æœ‰æ•ˆç›¸é—œæ–‡ç« çš„å­é›†&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Document Reader
&lt;ul&gt;
&lt;li&gt;RNNï¼Œç”¨ä¾† detect æ–‡ç« ä¸­ç­”æ¡ˆçš„ä½ç½®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å¯¦é©—è¡¨æ˜ Document Retriever çš„æ€§èƒ½å„ªæ–¼ Wikipedia çš„å…§éƒ¨æœå°‹å¼•æ“ï¼ŒDocument Reader åœ¨ SQuAD ä¸Šé”åˆ° SOTA&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œèˆ‡ single task training ç›¸æ¯”ï¼Œä½œè€…è¡¨æ˜ multitask learning å’Œ distant supervision æœ‰åŠ©æ–¼æé«˜æ¨¡å‹çš„æ€§èƒ½&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;éš¨è‘— Knowledge Base (KB) çš„ç™¼å±•ï¼ŒQA å‡ºç¾äº†è¨±å¤šå‰µæ–°ï¼Œä½†æ˜¯ KB å…·å‚™å›ºæœ‰é™åˆ¶ï¼ˆincompleteness, fixed schemaï¼‰ï¼Œä¿ƒä½¿ç ”ç©¶äººå“¡è½‰å›å¾ raw text ä¸­æå–ç­”æ¡ˆ&lt;/p&gt;
&lt;p&gt;æœ‰äº›å·¥ä½œå˜—è©¦åˆ©ç”¨ multitask learning ä¾†çµ„åˆå¤šå€‹ QA è³‡æ–™é›†ï¼Œç›®æ¨™æ˜¯ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;é€é task transfer ä¾†å¯¦ç¾è·¨è³‡æ–™é›†çš„æ”¹é€²&lt;/li&gt;
&lt;li&gt;æä¾›ä¸€å€‹å–®ä¸€é€šç”¨çš„ç³»çµ±ï¼Œå¯ä»¥å›ç­”ä¸åŒç¨®é¡çš„å•é¡Œï¼Œå› ç‚ºè³‡æ–™ä¾†æºä¸­ä¸å¯é¿å…åœ°å­˜åœ¨ä¸åŒçš„è³‡æ–™åˆ†å¸ƒ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;æœ¬æ–‡çš„å·¥ä½œåœ¨å…ˆ retrive å† read çš„ setting ä¸‹ï¼Œæ²’æœ‰åˆ©ç”¨ KBï¼Œå–å¾—äº†æ­£é¢æˆæœ&lt;/p&gt;
&lt;h2 id=&#34;drqa&#34;&gt;DrQA&lt;/h2&gt;
&lt;p&gt;ç”±å…©å€‹ Components çµ„æˆï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Document Retriever
&lt;ul&gt;
&lt;li&gt;ç”¨ä¾†å°‹æ‰¾ç›¸é—œæ–‡ç« &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Document Reader
&lt;ul&gt;
&lt;li&gt;ç”¨æ–¼å¾å–®ä¸€æ–‡ä»¶æˆ–ä¸€å°éƒ¨åˆ†æ–‡ä»¶æå–ç­”æ¡ˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;document-retriever&#34;&gt;Document Retriever&lt;/h3&gt;
&lt;p&gt;éµå¾ªç¶“å…¸çš„ QA Systemï¼Œå…ˆç”¨é«˜æ•ˆçš„ ï¼ˆéæ©Ÿå™¨å­¸ç¿’ï¼‰çš„ document retrieval system ç¸®å°æœç´¢ç¯„åœï¼Œä¸¦å°ˆæ³¨æ–¼æ¯”è¼ƒå¯èƒ½æœ‰é—œçš„æ–‡ç« &lt;/p&gt;
&lt;p&gt;èˆ‡åŸºæ–¼ ElasticSearch çš„ Wikipedia Search API ç›¸æ¯”ï¼Œç°¡å–®çš„ inverted index lookup å’Œ term vector model scoring è¡¨ç¾çš„ååˆ†å¥½&lt;/p&gt;
&lt;p&gt;æ–‡ç« å’Œå•é¡Œè¢«è¡¨ç¤ºç‚º TF-IDF weighted bag-of-words vectors&lt;/p&gt;
&lt;p&gt;ä½œè€…è€ƒæ…®é€éè€ƒæ…® local word order å’Œ n-gram features ä¾†é€²ä¸€æ­¥æ”¹å–„&lt;/p&gt;
&lt;p&gt;è¡¨ç¾æœ€ä½³çš„ç³»çµ±ç”¨ bigram countsï¼Œä¸¦åˆ©ç”¨ hashing æ˜ å°„åˆ° $2^{24}$ bins ä¾†ä¿æŒ speed å’Œ memory efficiencyï¼Œç”¨çš„æ˜¯ unsigned murmur3 hash&lt;/p&gt;
&lt;p&gt;Document Retriever ä½œç‚ºæ¨¡å‹çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œè¨­å®šç‚ºå°ä»»ä½•å•é¡Œè¿”å› 5 å€‹ç›¸é—œçš„æ–‡ç« &lt;/p&gt;
&lt;p&gt;é€™äº›æ–‡ç« å†äº¤ç”± Document Reader ä¾†è™•ç†&lt;/p&gt;
&lt;h3 id=&#34;document-reader&#34;&gt;Document Reader&lt;/h3&gt;
&lt;h4 id=&#34;paragraph-encoding&#34;&gt;Paragraph encoding&lt;/h4&gt;
&lt;p&gt;$p_i$ æ˜¯æ®µè½ $p$ ä¸­çš„ tokenï¼ŒæœŸæœ› $p_i$ å¯ä»¥è¢« encode æˆå¸¶æœ‰å‘¨åœè³‡è¨Šçš„å‘é‡&lt;/p&gt;
&lt;p&gt;æ¡ç”¨çš„æ˜¯ multi-layer bidirectional LSTM&lt;/p&gt;
&lt;p&gt;ç‰¹å¾µå‘é‡ $p_i$ ç”±ä»¥ä¸‹éƒ¨åˆ†çµ„æˆï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Word embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨ 300 ç¶­çš„ Glove word embeddingï¼Œå›ºå®šå¤§éƒ¨åˆ†é è¨“ç·´çš„ word embeddingï¼Œåª finetune æœ€å¸¸è¦‹çš„ 1000 å€‹ question wordsï¼Œä¾‹å¦‚ what, how, which&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exact match&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ¡ç”¨ä¸‰å€‹ç°¡å–®çš„ binary featuresï¼Œç”¨ä¾†è¡¨ç¤º $p_i$ æ˜¯å¦èˆ‡å•é¡Œä¸­çš„ question word $q$ ç²¾ç¢ºåŒ¹é…ï¼Œç„¡è«–æ˜¯åŸå§‹å½¢å¼ã€å°å¯«ï¼Œé‚„æ˜¯ lemma form&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Token features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä½œè€…æ·»åŠ äº†ä¸€äº› manual feature å¥½åæ‡‰ $p_i$ åœ¨ context ä¸­çš„å±¬æ€§ï¼Œæ¯”å¦‚ part-of-speech (POS)ã€named entity recognition (NER) tags å’Œå®ƒçš„ (normalized) term frequency (TF)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aligned question embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$f_{align}(p_i)=\sum_j a_{i,j}E(q_j)$
&lt;ul&gt;
&lt;li&gt;$E$ æ˜¯ word embedding&lt;/li&gt;
&lt;li&gt;$a_{i,j}$ æ˜¯ attention scoreï¼Œè¨ˆç®— $p_i$ å’Œæ¯å€‹ $q_j$ ä¹‹é–“çš„ similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;question-encoding&#34;&gt;Question encoding&lt;/h4&gt;
&lt;p&gt;é€™å€‹æ¯”è¼ƒç°¡å–®ï¼Œåªæ˜¯åœ¨ word embedding ä¸ŠåŠ ä¸Šä¸€å±¤ RNNï¼Œä¸¦æŠŠ hidden units é‡æ–°çµåˆæˆä¸€å€‹å‘é‡&lt;/p&gt;
&lt;h4 id=&#34;prediction&#34;&gt;Prediction&lt;/h4&gt;
&lt;p&gt;æŠŠ $\{p_1,&amp;hellip;,p_m\}$ å’Œ $q$ ä½œç‚º inputï¼Œä¸¦å€‹åˆ¥å–®ç¨è¨“ç·´å…©å€‹åˆ†é¡å™¨é æ¸¬é–‹é ­å’Œçµå°¾çš„ä½ç½®&lt;/p&gt;
&lt;p&gt;å…·é«”ä¾†èªªï¼Œä½œè€…ç”¨ bilinear term ä¾†è¨ˆç®—æ¯å€‹ $p_i$ å’Œ $q$ ä¹‹é–“çš„ç›¸ä¼¼åº¦ï¼Œä¸¦è¨ˆç®—æ¯å€‹ $p_i$ æ˜¯é–‹é ­çš„æ©Ÿç‡å’Œçµå°¾çš„æ©Ÿç‡&lt;/p&gt;
&lt;p&gt;æœ€å¾Œé¸æ“‡æœ€ä½³ç¯„åœï¼Œå¾ token $i$ åˆ° token $i&#39;$&lt;/p&gt;
&lt;p&gt;$i \le i&amp;rsquo; \le i+15$&lt;/p&gt;
&lt;p&gt;ä¸¦ä¸”ä½¿ $P_{start}(i) \times P_{end}(i&amp;rsquo;)$ æœ€å¤§åŒ–&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡çš„å·¥ä½œä¾è³´ä¸‰ç¨®è³‡æ–™ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wikipedia
&lt;ul&gt;
&lt;li&gt;å°‹æ‰¾ç­”æ¡ˆçš„ä¾†æº&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SQuAD
&lt;ul&gt;
&lt;li&gt;ç”¨ä¾†è¨“ç·´å’Œè©•ä¼°æ¨¡å‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å¦å¤–ä¸‰å€‹ QA è³‡æ–™é›† (WebQuestions, CuratedTREC, WikiMovies)
&lt;ul&gt;
&lt;li&gt;ç”¨ä¾†æ¸¬è©¦æ¨¡å‹åœ¨ Open-domain QA ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸¦è©•ä¼°æ¨¡å‹å¾ multitask learning å’Œ distant supervision ä¸­ç²ç›Šçš„ç¨‹åº¦&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;wikipedia-knowledge-source&#34;&gt;Wikipedia (Knowledge Source)&lt;/h3&gt;
&lt;p&gt;ç”¨ 2016-12-21 dump2 çš„ English Wikipedia&lt;/p&gt;
&lt;p&gt;å°æ–¼æ¯é ï¼Œåªæå–æ–‡æœ¬ï¼Œä¸¦åˆªé™¤çµæ§‹åŒ–çš„è³‡æ–™ï¼ˆlists and figuresï¼‰&lt;/p&gt;
&lt;p&gt;ä¸Ÿæ£„ disambiguation pages, list, index å’Œ outline pagesï¼Œä¿ç•™äº† 5,075,182 ç¯‡æ–‡ç« &lt;/p&gt;
&lt;h3 id=&#34;squad&#34;&gt;SQuAD&lt;/h3&gt;
&lt;p&gt;ä½¿ç”¨å…©å€‹ evaluation metricsï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exact string match (EM)&lt;/li&gt;
&lt;li&gt;F1 score&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç‚ºäº†è©•ä¼° open-domain QA çš„èƒ½åŠ›ï¼Œä½œè€…åªæœ‰ä½¿ç”¨ SQuAD çš„ QA pairsï¼Œè¦æ±‚ç³»çµ±åœ¨ç„¡æ³•å­˜å–ç›¸é—œæ®µè½çš„æƒ…æ³ä¸‹ç™¼ç¾æ­£ç¢ºçš„ answer span&lt;/p&gt;
&lt;p&gt;ä¸åƒæ¨™æº–çš„ SQuAD setting æœƒçµ¦å‡ºç›¸é—œæ®µè½&lt;/p&gt;
&lt;h3 id=&#34;open-domain-qa-evaluation-resources&#34;&gt;Open-domain QA Evaluation Resources&lt;/h3&gt;
&lt;p&gt;SQuAD æ˜¯ç›®å‰å¯ç”¨çš„æœ€å¤§çš„ general purpose QA è³‡æ–™é›†ä¹‹ä¸€&lt;/p&gt;
&lt;p&gt;æ”¶é›†éç¨‹åŒ…æ‹¬å‘æ¯å€‹ human annotator å±•ç¤ºä¸€å€‹æ®µè½ï¼Œä¸¦å¯«ä¸€å€‹å•é¡Œ&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œdistribution éå¸¸ç‰¹å®š&lt;/p&gt;
&lt;p&gt;ä½œè€…å»ºè­°åœ¨å…¶ä»– open-domain QA è³‡æ–™é›†ä¸Šè©•ä¼°ç³»çµ±ï¼Œä»–å€‘ä»¥ä¸åŒçš„æ–¹å¼å»ºæ§‹&lt;/p&gt;
&lt;h4 id=&#34;curatedtrec&#34;&gt;CuratedTREC&lt;/h4&gt;
&lt;p&gt;ä½¿ç”¨å¤§ç‰ˆæœ¬ï¼ŒåŒ…å« TREC 1999, 2000, 2001 å’Œ 2002 çš„ 2180 å€‹å•é¡Œ&lt;/p&gt;
&lt;h4 id=&#34;webquestions&#34;&gt;WebQuestions&lt;/h4&gt;
&lt;p&gt;é€™è³‡æ–™é›†æ—¨åœ¨å›ç­” Freebase KB çš„å•é¡Œ&lt;/p&gt;
&lt;p&gt;é€é Google Suggest API æŠ“å•é¡Œï¼Œå†ç”¨ Amazon Mechanical Turk ä¾†ç²å–ç­”æ¡ˆ&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ entity names æŠŠæ¯å€‹ answer è½‰æˆç­”æ¡ˆï¼Œä»¥ä¾¿ä¸å¼•å…¥ Freebase IDs&lt;/p&gt;
&lt;h4 id=&#34;wikimovies&#34;&gt;WikiMovies&lt;/h4&gt;
&lt;p&gt;åŒ…å«å°é›»å½±é ˜åŸŸçš„ 96k å€‹å•ç­”å°&lt;/p&gt;
&lt;h3 id=&#34;distantly-supervised-data&#34;&gt;Distantly Supervised Data&lt;/h3&gt;
&lt;p&gt;ä¸Šé¢èªªçš„è³‡æ–™é›†é™¤äº† SQuAD éƒ½æ²’æœ‰ç›¸é—œæ®µè½ï¼Œå› æ­¤ä¸èƒ½ç›´æ¥è¨“ç·´ Document Reader&lt;/p&gt;
&lt;p&gt;è¿½éš¨ä¹‹å‰å·²æœ‰çš„åˆ©ç”¨ distant supervision (DS) ä¾†åš relation extraction çš„å·¥ä½œï¼Œä½œè€…ç”¨ä¸€å€‹ç¨‹å¼è‡ªå‹•æŠŠæ®µè½å’Œæ­¤é¡è¨“ç·´ç¯„ä¾‹åšç›¸é—œè¯&lt;/p&gt;
&lt;p&gt;å°æ¯å€‹å•ç­”å°ä½¿ç”¨ä»¥ä¸‹éç¨‹ä¾†å»ºç«‹è¨“ç·´é›†ï¼š&lt;/p&gt;
&lt;p&gt;é¦–å…ˆï¼Œå°å•é¡Œç”¨ Document Retriever æ‰¾åˆ°å‰ 5 å€‹ç›¸é—œçš„æ®µè½&lt;/p&gt;
&lt;p&gt;èˆ‡å·²çŸ¥ç­”æ¡ˆæ²’æœ‰ exact match çš„æ®µè½éƒ½è¢«ç›´æ¥ä¸Ÿæ£„&lt;/p&gt;
&lt;p&gt;çŸ­æ–¼ 25 å€‹å­—å…ƒæˆ–é•·æ–¼ 1500 å€‹å­—å…ƒçš„æ®µè½ä¹Ÿè¢«ä¸Ÿæ£„&lt;/p&gt;
&lt;p&gt;å¦‚æœåœ¨å•é¡Œä¸­æ‰¾åˆ° named entityï¼Œä¸åŒ…å«è©² named entity çš„æ®µè½ä¹Ÿè¢«ä¸Ÿæ£„&lt;/p&gt;
&lt;p&gt;å°æ–¼æ¯å€‹ retrived page çš„æ¯å€‹æ®µè½ï¼Œä½¿ç”¨ question å’Œ 20 token window é–“çš„ unigram  å’Œ bigram overlap ä¾†è¨ˆç®—ç›¸ä¼¼åº¦ï¼Œä¿ç•™é‡ç–Šåº¦æœ€é«˜çš„å‰äº”å€‹æ®µè½&lt;/p&gt;
&lt;p&gt;å°‡æ‰¾åˆ°çš„æ¯å€‹ pair åŠ å…¥åˆ° DS è¨“ç·´é›†ä¸­&lt;/p&gt;
&lt;p&gt;å¤§ç´„ä¸€åŠçš„ DS ç¯„ä¾‹ä¾†è‡ª SQuAD ä»¥å¤–çš„é é¢&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;finding-relevant-articles&#34;&gt;Finding Relevant Articles&lt;/h3&gt;
&lt;p&gt;å…ˆæª¢æŸ¥ Retriever åœ¨æ‰€æœ‰ QA è³‡æ–™é›†ä¸Šçš„æ€§èƒ½&lt;/p&gt;
&lt;p&gt;è¨ˆç®— ration æ˜¯æ ¹æ“šç‰¹å®šçš„å•é¡Œï¼Œè€ƒæ…®é€™äº›å•é¡Œå°æ‡‰çš„æ–‡æœ¬åœ¨å‰ 5 å€‹ç›¸é—œæ–‡ç« ä¸­çš„æ¯”ä¾‹&lt;/p&gt;
&lt;p&gt;çµæœè¡¨æ˜ï¼Œæ¯” Wikipedia Search é‚„æ›´å¥½ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ bigram hashing çš„æƒ…æ³ä¸‹&lt;/p&gt;
&lt;h3 id=&#34;reader-evaluation-on-squad&#34;&gt;Reader Evaluation on SQuAD&lt;/h3&gt;
&lt;h4 id=&#34;implementation-details&#34;&gt;Implementation details&lt;/h4&gt;
&lt;p&gt;ä½¿ç”¨ h=128 çš„ 3 å±¤é›™å‘ LSTMï¼Œä¾†åš paragraph encoding å’Œ question encoding&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ Stanford CoreNLP ä¾†åš tokenization ä¸¦ç”Ÿæˆ lemma, part-of-speech å’Œ named entity tags&lt;/p&gt;
&lt;p&gt;Optimizer ä½¿ç”¨ Adamax&lt;/p&gt;
&lt;p&gt;Dropout rate ç‚º 0.3&lt;/p&gt;
&lt;h4 id=&#34;result-and-analysis&#34;&gt;Result and analysis&lt;/h4&gt;
&lt;p&gt;ä½œè€…çš„ç³»çµ±å¯ä»¥åœ¨ SQuAD ä¸Šé”åˆ° SOTA&lt;/p&gt;
&lt;p&gt;åšäº† ablation studyï¼Œçµæœè¡¨æ˜æ‰€æœ‰åŠŸèƒ½éƒ½æœƒå½±éŸ¿æ€§èƒ½&lt;/p&gt;
&lt;p&gt;æœ‰è¶£çš„æ˜¯ï¼Œå–®ç¨æ²’æœ‰ $f_{alignd}$ æˆ– $f_{exact_match}$ å°æ€§èƒ½ä¸æœƒæœ‰æ¥µå¤§çš„å½±éŸ¿ï¼Œä½†å…©å€‹éƒ½æ²’æœ‰å°±æœƒæ€¥é½ä¸‹é™&lt;/p&gt;
&lt;h3 id=&#34;full-wikipedia-question-answering&#34;&gt;Full Wikipedia Question Answering&lt;/h3&gt;
&lt;p&gt;æ¯”è¼ƒä¸‰å€‹ç‰ˆæœ¬çš„ DrQAï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SQuAD
&lt;ul&gt;
&lt;li&gt;åªåœ¨ SQuAD ä¸Šè¨“ç·´ï¼Œä¸¦ç”¨æ–¼æ‰€æœ‰è©•ä¼°è³‡æ–™é›†&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tune (DS)
&lt;ul&gt;
&lt;li&gt;å…ˆåœ¨ SQuAD ä¸Šé è¨“ç·´ï¼Œåœ¨ç”¨ DS è¨“ç·´é›†å°æ¯å€‹è³‡æ–™é›†é€²è¡Œå¾®èª¿&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multitask (DS)
&lt;ul&gt;
&lt;li&gt;åœ¨ SQuAD å’Œ DS è¨“ç·´é›†ä¸Šè¯åˆè¨“ç·´&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DrQA/table6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;å…©å€‹æ˜é¡¯çš„ angles of attackï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æŠŠå¤šå€‹æ®µè½ç›´æ¥ç´å…¥ Document Reader çš„è¨“ç·´ï¼Œå› ç‚ºä»–ç›®å‰ç¨ç«‹è¨“ç·´æ¯å€‹æ®µè½&lt;/li&gt;
&lt;li&gt;å¯¦ä½œä¸€å€‹ end-to-end training çš„ pipelineï¼Œå¯ä»¥åœ¨ä¸€å€‹æ¨¡å‹ä¸­çµåˆ Document Retriever å’Œ Document Reader&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>LangChain ç­†è¨˜</title>
        <link>https://roykesydon.github.io/Blog/p/langchain-%E7%AD%86%E8%A8%98/</link>
        <pubDate>Mon, 18 Dec 2023 00:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/langchain-%E7%AD%86%E8%A8%98/</guid>
        <description>&lt;h2 id=&#34;ä»‹ç´¹&#34;&gt;ä»‹ç´¹&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ChatGPT çš„ç¼ºé™·&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ²’æœ‰ä¸€å®šæ™‚é–“å¾Œçš„è³‡æ–™ (ç•¶æ™‚)&lt;/li&gt;
&lt;li&gt;æ²’æœ‰è¾¦æ³•é€£çµå¤–éƒ¨ç§äººè³‡æ–™ (e.g. Google Drive)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LangChain çš„å„ªé»&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integration
&lt;ul&gt;
&lt;li&gt;å¯ä»¥é€£çµå¤–éƒ¨è³‡æ–™&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Agency
&lt;ul&gt;
&lt;li&gt;è®“ LLM å¯ä»¥å’Œç’°å¢ƒäº’å‹•ï¼Œåšå‡ºæ±ºç­–&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;components&#34;&gt;Components&lt;/h2&gt;
&lt;h3 id=&#34;çµæ§‹&#34;&gt;çµæ§‹&lt;/h3&gt;
&lt;h4 id=&#34;schema&#34;&gt;Schema&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å°è©±
&lt;ul&gt;
&lt;li&gt;System: å°è©±çš„ context&lt;/li&gt;
&lt;li&gt;Human: ä½ çš„è©¢å•&lt;/li&gt;
&lt;li&gt;AI: AI çš„å›ç­”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;document&#34;&gt;Document&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å„²å­˜ä¸€æ®µæ–‡å­—ä»¥é›† metadata çš„çµæ§‹&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;embedding&#34;&gt;Embedding&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;OpenAIEmbeddings
&lt;ul&gt;
&lt;li&gt;å¯ä»¥æŠŠæ–‡å­—è½‰æ›æˆå‘é‡ï¼Œé è¨­æ˜¯ 1536 ç¶­&lt;/li&gt;
&lt;li&gt;ä½†æ˜¯æœ‰æœ€å¤§é•·åº¦é™åˆ¶&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;æ–‡å­—è™•ç†&#34;&gt;æ–‡å­—è™•ç†&lt;/h3&gt;
&lt;h4 id=&#34;output-parser&#34;&gt;Output Parser&lt;/h4&gt;
&lt;p&gt;è®“æ¨¡å‹é‡ç”Ÿä½ æƒ³è¦çš„æ ¼å¼ï¼Œä¸¦è½‰æ›æˆä½ æƒ³è¦çš„çµæ§‹ï¼Œæ¯”å¦‚ json&lt;/p&gt;
&lt;h3 id=&#34;è³‡æ–™å„²å­˜&#34;&gt;è³‡æ–™å„²å­˜&lt;/h3&gt;
&lt;h4 id=&#34;indexes&#34;&gt;Indexes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Document_loaders&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯ä»¥å¾ä¸åŒçš„ä¾†æºç²å¾—è³‡æ–™&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;text_splitter&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠå¤§é‡çš„æ–‡å­—åˆ‡æˆå¤šå€‹ chunks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;retriever&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨ä¾†æ‰¾åˆ°ç›¸è¿‘çš„æ–‡ä»¶&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VectorStores&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chroma
&lt;ul&gt;
&lt;li&gt;local storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;äº¤äº’&#34;&gt;äº¤äº’&lt;/h3&gt;
&lt;h4 id=&#34;prompttemplate&#34;&gt;PromptTemplate&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨ä¾†å¾€ String Template å¡«å…¥è®Šæ•¸&lt;/li&gt;
&lt;li&gt;æœ‰äº› Component æœƒå’Œé€™å€‹ç”¨æ³•çµ„åˆï¼Œæ‰€ä»¥ä¸é©åˆç›´æ¥æ›æˆ f-string&lt;/li&gt;
&lt;li&gt;FewShotPromptTemplate
&lt;ul&gt;
&lt;li&gt;å¯ä»¥ç”¨è‡ªå·±æº–å‚™çš„ä¸€äº›ä¾‹å­çµåˆ PromptTemplate ä¾†åš Few-shot learning&lt;/li&gt;
&lt;li&gt;å¯ä»¥é€é FAISS ä¾†æ‰¾åˆ°æœ€ç›¸è¿‘çš„ä¾‹å­&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;chain&#34;&gt;Chain&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç”¨åœ¨æœ‰å¤šå€‹æœ‰åºçš„å•é¡Œçš„æƒ…æ³&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;multi-step workflow&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VectorDBQA&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯ä»¥ç”¨åœ¨æœç´¢ local çš„å‘é‡è³‡æ–™åº«&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instruction çµåˆ context çš„æ¨¡å¼ (Summarize)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;æ¨¡å¼&lt;/th&gt;
&lt;th&gt;Stuffing&lt;/th&gt;
&lt;th&gt;Map Reduce&lt;/th&gt;
&lt;th&gt;Refine&lt;/th&gt;
&lt;th&gt;Map-Rerank&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;èªªæ˜&lt;/td&gt;
&lt;td&gt;ç›´æ¥æŠŠ context å’Œ query çµåˆ&lt;/td&gt;
&lt;td&gt;æŠŠ Document åˆ‡æˆå¤šå¡Š ï¼ŒæŠŠæ¯å¡Šäº¤çµ¦ LLMï¼Œè½‰æ›æˆ summaryï¼Œåè¦†å¾é€™äº› summary ç”Ÿ summary&lt;/td&gt;
&lt;td&gt;æ¯æ¬¡åˆ‡ä¸€å°å¡Šï¼Œä¸¦ä¸”å’Œå…ˆå‰çš„çµæœåš summary&lt;/td&gt;
&lt;td&gt;è®“æ¯å€‹ chunk å’Œ query å»ç”Ÿç­”æ¡ˆï¼Œä¸¦è¦æ¨¡å‹å°è‡ªå·±çš„ç­”æ¡ˆè©•åˆ†ï¼Œæœ€å¾Œé¸åˆ†æ•¸é«˜çš„&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;å„ªé»&lt;/td&gt;
&lt;td&gt;åªéœ€ call ä¸€æ¬¡ APIã€æ¶µè“‹æ‰€æœ‰è³‡æ–™&lt;/td&gt;
&lt;td&gt;å¯ä»¥é¤µå…¥æ›´å¤§çš„æ–‡ä»¶ã€å¯ä»¥å¹³è¡Œé‹ç®—&lt;/td&gt;
&lt;td&gt;å¯ä»¥é¤µå…¥æ›´å¤§çš„æ–‡ä»¶&lt;/td&gt;
&lt;td&gt;å°æ–¼ç°¡å–®çš„å•é¡Œå¯èƒ½æ¯”è¼ƒæœ‰æ•ˆ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ç¼ºé»&lt;/td&gt;
&lt;td&gt;å®¹æ˜“é”åˆ° token ä¸Šé™&lt;/td&gt;
&lt;td&gt;call å¤šæ¬¡ APIã€summary çš„éç¨‹ä¸­æœƒæµå¤±è³‡è¨Š&lt;/td&gt;
&lt;td&gt;call å¤šæ¬¡ APIã€summary çš„éç¨‹ä¸­æœƒæµå¤±è³‡è¨Š&lt;/td&gt;
&lt;td&gt;æ²’æœ‰è¾¦æ³•çµåˆå¤šå€‹ Document çš„è³‡è¨Š&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;agent&#34;&gt;Agent&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;æœ‰äº›æ‡‰ç”¨ä¸­ï¼Œä½ å¯èƒ½ä¸çŸ¥é“è©²éµå¾ªä»€éº¼æµç¨‹ä¾†è®“ LLM å®Œæˆä»»å‹™ï¼Œé€™æ™‚å€™ä½ æœƒéœ€è¦è®“ LLM è‡ªè¡Œæ±ºå®šè¦æ¡å–å“ªäº›å‹•ä½œä»¥åŠæ¡å–çš„é †åº&lt;/li&gt;
&lt;li&gt;å¯ä»¥å‹•æ…‹åœ°åˆ©ç”¨ Chain&lt;/li&gt;
&lt;li&gt;verbose=True çš„æ™‚å€™æœƒå°å‡ºæ€è€ƒéç¨‹&lt;/li&gt;
&lt;li&gt;Tools
&lt;ul&gt;
&lt;li&gt;æœ‰æ¯”å¦‚ Google Search ä¹‹é¡çš„ Tool å¯ä»¥çµåˆæ‡‰ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;memory&#34;&gt;Memory&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ConversationChain&lt;/li&gt;
&lt;li&gt;è®“ Chain å’Œ Agent å¯ä»¥ä¿ç•™ä¹‹å‰çš„å°è©±&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>CLIP è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Tue, 21 Nov 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.00020&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç¾æœ‰çš„ SOTA CV system å¯ä»¥ç¶“éè¨“ç·´é æ¸¬ä¸€çµ„å›ºå®šçš„é¡åˆ¥ã€‚
ä½†é€™ç¨®ç›£ç£å¼çš„æ–¹æ³•ä¹Ÿå—é™äº†é€šç”¨æ€§ï¼Œå› ç‚ºéœ€è¦é¡å¤–çš„ labeled data ä¾†æ“´å±•ã€‚&lt;/p&gt;
&lt;p&gt;ç›´æ¥å¾ raw text å­¸ç¿’ image æ˜¯å€‹æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è­‰æ˜äº†ã€Œé æ¸¬å“ªå€‹æ˜¯åœ–ç‰‡çš„ captionã€é€™ç¨®å½¢å¼çš„é è¨“ç·´æ˜¯ä¸€ç¨®é«˜æ•ˆä¸”å¯æ“´å±•çš„æ–¹æ³•ï¼Œå¯ä»¥å¾ internet ä¸Šè’é›†çš„ 4 å„„å°è³‡æ–™å¾é ­å­¸ç¿’åˆ° SOTA image representationã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´å¾Œï¼Œé€éè‡ªç„¶èªè¨€ä¾†å¼•å°ï¼Œå°±å¯ä»¥åœ¨ä¸‹æ¸¸ä»»å‹™åç·š zero-shotã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å° 30 å€‹ä¸åŒçš„ç¾æœ‰é›»è…¦è¦–è¦ºè³‡æ–™é›†é€²è¡Œæ¯”è¼ƒï¼Œå¯ä»¥åœ¨å¤šæ•¸ä»»å‹™å’Œç›£ç£å¼å­¸ç¿’çš„ baseline ç«¶çˆ­ï¼Œè€Œä¸”ç„¡é ˆä»»è³‡æ–™é›†ä¾†åšç‰¹åˆ¥çš„è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚åœ¨ ImageNet ä¸Šåš zero-shot å¯ä»¥å’Œ ResNet-50 å–å¾—ç›¸è¿‘çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-motivating-work&#34;&gt;Introduction and Motivating Work&lt;/h2&gt;
&lt;p&gt;ç›´æ¥å¾åŸå§‹æ–‡æœ¬å­¸ç¿’çš„é è¨“ç·´æ–¹æ³•åœ¨éå»å¹¾å¹´å¾¹åº•æ”¹è®Šäº† NLPã€‚&lt;/p&gt;
&lt;p&gt;Task-agnostic (èˆ‡ä¸‹æ¸¸ä»»å‹™ç„¡é—œ) objectivesï¼Œæ¯”å¦‚ autoregressive å’Œ masked language modelingï¼Œè®“æ¨¡å‹å¾—ä»¥éš¨è‘— compute, model capacity, å’Œ data è¦æ¨¡çš„å¢é•·ï¼Œä½¿èƒ½åŠ›ä¹Ÿé€æ­¥æå‡ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ &amp;ldquo;text-to-text&amp;rdquo; é€™ç¨®è¼¸å…¥è¼¸å‡ºå½¢å¼çš„é è¨“ç·´ï¼Œä½¿æ¨¡å‹è½‰ç§»åˆ°ä¸‹æ¸¸ä»»å‹™çš„æ™‚å€™ï¼Œä¸ç”¨ç‰¹åœ°å®¢è£½åŒ– output headï¼Œæˆ–å°è³‡æ–™é›†åšç‰¹åˆ¥åœ°è™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›çµæœè¡¨æ˜ï¼Œç¾ä»£çš„é è¨“ç·´æ–¹æ³•åœ¨ web-scale çš„æ–‡å­—é›†åˆçš„è¡¨ç¾å·²ç¶“è¶…éäº†ç”¨é«˜å“è³ªçš„äººç‚ºæ¨™è¨˜ NLP è³‡æ–™é›†ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œåœ¨ CV ç­‰é ˜åŸŸï¼Œåœ¨ ImageNet é€™ç¨®äººç‚ºæ¨™è¨˜çš„è³‡æ–™é›†ä¸Šåšé è¨“ç·´å»ä¾ç„¶æ˜¯æ¨™æº–åšæ³•ã€‚&lt;/p&gt;
&lt;p&gt;ç›´æ¥å¾ç¶²è·¯æ–‡æœ¬å­¸ç¿’çš„å¯æ“´å±•é è¨“ç·´æ–¹æ³•æˆ–è¨±èƒ½åœ¨ CV å¸¶ä¾†é¡ä¼¼çš„çªç ´ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥å¾€æœ‰ä¸€äº›å·¥ä½œå˜—è©¦åˆ©ç”¨å¹¾ä¹ç„¡é™é‡çš„åŸå§‹æ–‡æœ¬è€Œä¸æ˜¯æœ‰é™æ•¸é‡çš„ &amp;ldquo;gold-labels&amp;rdquo;ï¼Œ
ä½†æ˜¯é€™äº›æ–¹æ³•éƒ½æœ‰ä¸€äº›å¦¥å”ï¼Œæ¯”å¦‚éƒ½åˆ©ç”¨ softmax ä¾†åŸ·è¡Œé æ¸¬ï¼Œä½¿å…¶æ²’è¾¦æ³•æ‡‰ä»˜æ–°é¡åˆ¥ï¼Œåš´é‡é™åˆ¶äº† zero-shot çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æäº†å¹¾å€‹å¼±ç›£ç£å­¸ç¿’çš„ä¾‹å­ï¼Œä»–å€‘åˆ©ç”¨é¡å¤–çš„è³‡æ–™çµåˆé è¨“ç·´ï¼Œä¾†å¹«å¿™æ”¹å–„ç›£ç£å¼å­¸ç¿’çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;ä¹Ÿæäº†å¹¾å€‹å’Œ CLIP é¡ä¼¼çš„å·¥ä½œ VirTex, ICMLM, ConVIRTï¼Œæƒ³åˆ©ç”¨ Transformerï¼Œå¾ Natural Language ä¸­å­¸ç¿’ image representationã€‚&lt;/p&gt;
&lt;p&gt;é€™äº› weakly supervised model å’Œæœ€è¿‘å¾ NLP å­¸ç¿’ image representation çš„æ–¹æ³•æœ‰ä¸€å€‹é‡å¤§å·®ç•°ï¼Œè¦æ¨¡ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ä¸€äº›ç ”ç©¶ï¼Œæ¯”å¦‚ä¸€äº›å¼±ç›£ç£å­¸ç¿’åœ¨æ•¸ç™¾è¬åˆ°æ•¸åå„„å¼µç…§ç‰‡ä¸Šè¨“ç·´äº†å¤šå€‹ accelerator yearsã€‚ä½†æ˜¯å’Œ CLIP ç›¸ä¼¼çš„ç ”ç©¶åªåœ¨äºŒåè¬å¼µåœ–ç‰‡ä¸Šè¨“ç·´äº†å¹¾å¤©ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å°‡è¦æ¨¡æ‹‰é«˜ï¼Œä»¥ç¸®çŸ­è¦æ¨¡ä¸Šçš„å·®è·ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ internet ä¸Šè’é›†äº† 4 å„„å°åœ–ç‰‡å’Œæ–‡å­—çš„è³‡æ–™ï¼Œåšæˆæ–°çš„è³‡æ–™é›†ï¼Œä¸¦æå‡ºäº† CLIPï¼ŒConVIRT çš„ç°¡åŒ–ç‰ˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ 30 å¹¾å€‹è³‡æ–™é›†ä¸Šæ¸¬è©¦ï¼ŒåŸºæœ¬ä¸Šèƒ½å’Œç›£ç£å¼çš„æ¨¡å‹ç«¶çˆ­ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœç”¨ linear-probeï¼Œæ¯”å…¬é–‹å¯ç”¨çš„ SOTA ImageNet model é‚„æ›´å¥½ã€‚&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/ex1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;natural-language-supervision&#34;&gt;Natural Language Supervision&lt;/h3&gt;
&lt;p&gt;æ ¸å¿ƒæƒ³æ³•æ˜¯åˆ©ç”¨ natural language ä¾†å­¸ç¿’ perceptionã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç¨±é€™ä¸æ˜¯ä¸€å€‹æ–°æƒ³æ³•ï¼Œä½†ä»¥å¾€ç›¸ä¼¼çš„æ–¹æ³•çš„ç”¨èªå¤šæ¨£ï¼Œä»–ä»‹ç´¹äº†å››ç¯‡æ–‡ç« ï¼Œä½†æŠŠå¾æ–‡å­—å’Œåœ–ç‰‡ä¸­å­¸ç¿’ image representation çš„æ–¹æ³•å€‹åˆ¥ç¨±ç‚ºï¼šç„¡ç›£ç£ã€è‡ªç›£ç£ã€å¼±ç›£ç£ã€ç›£ç£å¼ã€‚&lt;/p&gt;
&lt;p&gt;æ“´å±• natural language supervision æ¯”èµ·åœ–åƒåˆ†é¡ç°¡å–®çš„å¤šï¼Œä¸å¿…å®šå¥½é¡åˆ¥ï¼Œå†å»æ¨™è¨»æ¯å¼µç…§ç‰‡çš„é¡åˆ¥ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸” natural language supervision é‚„æœ‰å€‹å„ªå‹¢ï¼Œä»–ä¸åªèƒ½å­¸ç¿’ image representationï¼Œé‚„èƒ½å°‡å…¶å’Œæ–‡å­—ç›¸é—œè¯ï¼Œä½¿å…¶æ›´å¥½åš zero-shot çš„é·ç§»ã€‚&lt;/p&gt;
&lt;h3 id=&#34;creating-a-sufficiently-large-dataset&#34;&gt;Creating a Sufficiently Large Dataset&lt;/h3&gt;
&lt;p&gt;ç¾æœ‰å·¥ä½œä¸»è¦ç”¨ä¸‰å€‹è³‡æ–™é›†:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MS-COCO&lt;/li&gt;
&lt;li&gt;Visual Genome&lt;/li&gt;
&lt;li&gt;YFCC100M&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MS-COCO å’Œ Visual Genome éƒ½æ˜¯é«˜å“è³ªçš„äººç‚ºæ¨™è¨˜è³‡æ–™é›†ï¼Œä½†æ˜¯æŒ‰ç…§ç¾ä»£æ¨™æº–ä¾†çœ‹ï¼Œå®ƒå€‘å¾ˆå°ï¼Œæ¯å€‹è³‡æ–™é›†å¤§ç´„æœ‰ 100,000 å¼µè¨“ç·´ç…§ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸è¼ƒä¹‹ä¸‹ï¼Œä½œè€…èˆ‰äº†ä¸€å€‹æœ€è¿‘çš„ç ”ç©¶ï¼Œç”¨äº† 3.5 Billion å¼µ Instagram ç…§ç‰‡ä½œç‚ºè¨“ç·´è³‡æ–™ã€‚&lt;/p&gt;
&lt;p&gt;YFCC100M æ˜¯ä¸€å€‹å¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒæœ‰ 100 million å¼µç…§ç‰‡ï¼Œä½†æ¯å¼µç…§ç‰‡çš„ metadata è³‡æ–™ç¨€ç–ï¼Œè€Œä¸”è‰¯è ä¸é½Šã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚è¨±å¤šæª”åæ˜¯è‡ªå‹•ç”¢ç”Ÿçš„ï¼Œå¯èƒ½æ˜¯æ™‚é–“ï¼Œæˆ–æ˜¯ç›¸æ©Ÿçš„åƒæ•¸ã€‚&lt;/p&gt;
&lt;p&gt;ç¶“ééæ¿¾ï¼Œä¿ç•™å¸¶æœ‰è‡ªç„¶èªè¨€çš„æ¨™é¡Œæˆ–æè¿°çš„åœ–åƒï¼Œè³‡æ–™é›†ç¸®å°äº† 6 å€ï¼Œåªå‰© 15000 è¬å¼µç…§ç‰‡ï¼Œå’Œ ImageNet çš„å¤§å°ç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;natural language supervision çš„ä¸€å€‹ä¸»è¦å‹•æ©Ÿæ˜¯ç¶²è·¯ä¸Šå…¬é–‹è‘—å¤§é‡é€™ç¨®å½¢å¼çš„ dataã€‚
ç”±æ–¼ç¾æœ‰è³‡æ–™é›†æ²’æœ‰åæ˜ é€™ç¨®å¯èƒ½æ€§ï¼Œå› æ­¤åªè€ƒæ…®é€™äº›è³‡æ–™é›†æœƒä½ä¼°é€™æ–¹é¢ç ”ç©¶çš„æ½›åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥ä½œè€…å»ºç«‹äº†ä¸€å€‹æ–°çš„åŒ…å« 400 million pairs çš„è³‡æ–™é›†ï¼Œå¾ç¶²è·¯ä¸Šå„ç¨®å…¬é–‹çš„ä¾†æºè’é›†çš„ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç›¡å¯èƒ½æ¶µè“‹æ‰€æœ‰çš„ visual conceptsï¼Œä½œè€…åœ¨å»ºæ§‹è³‡æ–™é›†çš„æ™‚å€™æº–å‚™äº† 50 è¬çµ„ç‰¹å®šçš„ queryï¼Œæ¯çµ„ query æœ€å¤šåŒ…å« 20,000 å€‹ pairï¼Œä¾†é€²è¡Œ class balanceã€‚&lt;/p&gt;
&lt;p&gt;ç”¢ç”Ÿçš„è³‡æ–™é›†çš„ç¸½å­—æ•¸å’Œ GPT-2 ç”¨çš„ WebText å·®ä¸å¤šã€‚&lt;/p&gt;
&lt;p&gt;å°‡æ­¤è³‡æ–™é›†ç¨±ç‚º WITï¼Œå…¨åæ˜¯ WebImageTextã€‚&lt;/p&gt;
&lt;h3 id=&#34;selecting-an-efficient-pre-training-method&#34;&gt;Selecting an Efficient Pre-Training Method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æœ€å…ˆé€²çš„ CV System éœ€è¦å¤§é‡çš„è¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…èˆ‰äº†å…©å€‹è¨ˆç®—é‡éƒ½éå¸¸ææ€–çš„æ¨¡å‹ï¼Œè€Œä¸”ä»–å€‘åªèƒ½é æ¸¬ 1000 å€‹ ImageNet çš„é¡åˆ¥ã€‚
å…¶ä¸­ä¸€å€‹èŠ±äº† 19 å€‹ GPU yearsï¼Œå¦ä¸€å€‹èŠ±äº† 33 å€‹ TPUv3 core-yearsã€‚
ä¹çœ‹ä¹‹ä¸‹ï¼Œå¾è‡ªç„¶èªè¨€ä¸­å­¸ç¿’ä¸€çµ„é–‹æ”¾çš„è¦–è¦ºæ¦‚å¿µä¼¼ä¹ä»¤äººç”Ÿç•ã€‚&lt;/p&gt;
&lt;p&gt;ä½†åœ¨ä½œè€…åŠªåŠ›çš„éç¨‹ä¸­ï¼Œä»–å€‘ç™¼ç¾è¨“ç·´æ•ˆç‡æ˜¯æˆåŠŸæ“´å±•è‡ªç„¶èªè¨€ç›£ç£çš„é—œéµï¼Œä¹Ÿæ ¹æ“šè©²æŒ‡æ¨™é¸å®šæœ€çµ‚çš„é è¨“ç·´æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;æœ€åˆçš„æ–¹æ³•å’Œ VirTex ç›¸ä¼¼ï¼Œå¾é ­é–‹å§‹è¨“ç·´ä¸€å€‹ CNNï¼Œå’Œ text transformer ä¾†é æ¸¬ captionã€‚&lt;/p&gt;
&lt;p&gt;Fig.2 å±•ç¤ºçš„ Transformer èªè¨€æ¨¡å‹çš„è¨ˆç®—é‡æ˜¯ ResNet-50 Image encoder çš„å…©å€ã€‚
é æ¸¬ caption æ¯”é æ¸¬ caption ä½†æ¡ç”¨è©è¢‹çš„æ–¹å¼é‚„æ…¢ä¸‰å€ã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£é æ¸¬ caption æ˜¯ä¸€å€‹å›°é›£çš„ä»»å‹™ï¼ŒåŒä¸€å¼µç…§ç‰‡å°æ‡‰çš„ caption å¯èƒ½å‡ºç¾çš„æè¿°ç”šè‡³æœ‰éå¸¸å¤šç¨®ã€‚
æœ€è¿‘åœ¨ Contrastive representation learning æ–¹é¢çš„ç ”ç©¶ç™¼ç¾ contrastive objectives æœ‰ä¸éŒ¯çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ä½œè€…æ¢ç´¢ä¸€ç¨®æ–¹æ³•æ˜¯ï¼Œåªé æ¸¬æ–‡æœ¬å’Œå“ªä¸€å€‹åœ–ç‰‡é…å°ï¼Œè€Œä¸æ˜¯é æ¸¬ç¢ºåˆ‡çš„å–®å­—ã€‚&lt;/p&gt;
&lt;p&gt;å› ç‚ºè³‡æ–™é›†è¶…ç´šå¤§ï¼Œoverfitting çš„å•é¡Œå½±éŸ¿ä¸å¤§ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…ç™¼ç¾å°æ–¼ encoder çš„ representationï¼Œè¦è½‰æ›åˆ° multi-model embedding spaceï¼Œåªéœ€è¦ä½¿ç”¨ linear projection å³å¯ï¼Œä¸éœ€è¦ non-linearï¼Œå…©è€…ä¹‹é–“å·®åˆ¥ä¸å¤§ã€‚&lt;/p&gt;
&lt;p&gt;Data augmentation åªæœ‰ä½¿ç”¨ random cropï¼Œè€Œæ²’æœ‰ä½¿ç”¨å…¶ä»–çš„ã€‚&lt;/p&gt;
&lt;h3 id=&#34;choosing-and-scaling-a-model&#34;&gt;Choosing and Scaling a Model&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# image_encoder - ResNet or Vision Transformer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# text_encoder - CBOW or Text Transformer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# I[n, h, w, c] - minibatch of aligned images&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# T[n, l] - minibatch of aligned texts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# W_i[d_i, d_e] - learned proj of image to embed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# W_t[d_t, d_e] - learned proj of text to embed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# t - learned temperature parameter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# extract feature representations of each modality&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;I_f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image_encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#[n, d_i]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;T_f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text_encoder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#[n, d_t]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# joint multimodal embedding [n, d_e]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;I_e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2_normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;T_e&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2_normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;W_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# scaled pairwise cosine similarities [n, n]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;I_e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T_e&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# symmetric loss function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;arange&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss_t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cross_entropy_loss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logits&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;prompt-engineering-and-ensembling&#34;&gt;Prompt Engineering and Ensembling&lt;/h3&gt;
&lt;p&gt;ä¸€ç¨®å¸¸è¦‹çš„å•é¡Œæ˜¯ polysemyï¼Œä¸€å€‹å–®å­—å¯èƒ½æœ‰å¤šç¨®æ„æ€ï¼Œæ¯”å¦‚ &amp;ldquo;boxer&amp;rdquo; å¯èƒ½æ˜¯ä¸€ç¨®ç‹—ï¼Œæˆ–æ˜¯æ‹³æ“Šæ‰‹ã€‚
å¦‚æœä¸€å¼µåœ–ç‰‡å°æ‡‰ä¸€å€‹å–®å­—å°±æœƒé¢è‡¨é€™å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€ç¨®æ˜¯ distribution gapï¼Œæ¯”å¦‚è¨“ç·´ç”¨å¥å­ï¼Œä½†æ¸¬è©¦ç”¨å–®å­—ã€‚
ç‚ºäº†ç·©è§£é€™å•é¡Œï¼Œä½œè€…ç™¼ç¾ç”¨ prompt template &amp;ldquo;A photo of a {label}.&amp;rdquo; æ¯”ç›´æ¥ç”¨ label å¥½ã€‚&lt;/p&gt;
&lt;p&gt;å…‰ç”¨é€™å€‹ prompt template å°±æé«˜ 1.3 % åœ¨ ImageNet ä¸Šçš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœå¯ä»¥çµ¦å…¶ä»–é¡å¤–è¨Šæ¯æœƒæ›´æœ‰å¹«åŠ©ï¼Œæ¯”å¦‚å°æ–¼å¯µç‰©çš„è³‡æ–™é›†ï¼Œå¯ä»¥ç”¨ &amp;ldquo;A photo of a {label}, a type of pet.&amp;quot;ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ OCR è³‡æ–™é›†ï¼Œä½œè€…ç™¼ç¾åœ¨è¦è­˜åˆ¥çš„æ–‡å­—æˆ–æ•¸å­—å‰å¾ŒåŠ ä¸Šå¼•è™Ÿå¯ä»¥æé«˜æ•ˆèƒ½ã€‚&lt;/p&gt;
&lt;p&gt;å†ä¾†æ˜¯ prompt ensemblingï¼Œä½œè€…ç™¼ç¾ç”¨å¤šå€‹ prompt template ä¾†é æ¸¬ï¼Œç„¶å¾Œç¶œåˆçµæœï¼Œå¯ä»¥æé«˜æ•ˆèƒ½ã€‚
ä½œè€…ç”¨äº† 80 å€‹ templateã€‚åœ¨ ImageNet ä¸Šæ¯”ç”¨å–®ä¸€çš„ prompt template æé«˜ 3.5 % çš„ performanceã€‚&lt;/p&gt;
&lt;p&gt;ç¶œåˆè€ƒæ…® prompt engineering å’Œ prompt ensemblingï¼Œä½œè€…åœ¨ ImageNet ä¸Šçš„æº–ç¢ºåº¦æé«˜å¤§æ¦‚ 5%ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é€™è£¡åˆ—å¹¾å€‹ä½œè€…ç”¨çš„ prompt template:
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;a bad photo of a {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a photo of many {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a sculpture of a {}.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;a photo of the hard to see {}.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analysis-of-zero-shot-clip-performance&#34;&gt;Analysis of Zero-Shot CLIP Performance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å°æ–¼ä¸€èˆ¬çš„ç‰©é«”åˆ†é¡çš„è³‡æ–™é›†ï¼ŒCLIP è¡¨ç¾è¼ƒå¥½ã€‚&lt;/p&gt;
&lt;p&gt;ä¸‹é¢æœ‰äº›è¤‡é›œã€å°ˆé–€ã€æŠ½è±¡çš„ä»»å‹™ï¼ŒCLIP å‰‡è¡¨ç¾çš„å¾ˆå·®ï¼Œæ¯”å¦‚è¨ˆç®—å ´æ™¯ä¸­æœ‰å¤šå°‘ç‰©é«”çš„ ï¼ˆCLEVRCountsï¼‰ã€è¡›æ˜Ÿåœ–åƒåˆ†é¡ï¼ˆEuroSATï¼‰æˆ–æ˜¯ è­˜åˆ¥æœ€è¿‘çš„æ±½è»Šè·é›¢ï¼ˆKITTI Distanceï¼‰&lt;/p&gt;
&lt;p&gt;å°æ–¼é€™ç¨®ç‰¹åˆ¥é›£çš„ä»»å‹™ï¼Œè®“ CLIP åš zero-shot ä¸å¤ªåˆç†ã€‚
å¯èƒ½ç”¨ few-shot çš„æ–¹å¼æœƒæ¯”è¼ƒå¥½ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;BiT æ˜¯ google ç‚º Transfer Learning è¨­è¨ˆçš„é è¨“ç·´æ¨¡å‹ï¼Œåœ¨åˆ†é¡å•é¡Œï¼ŒFew-shot learning ä¸Šæœ‰è‰¯å¥½çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;h3 id=&#34;representation-learning&#34;&gt;Representation Learning&lt;/h3&gt;
&lt;p&gt;é€™ç¯€æ¢è¨å®Œå…¨ä½¿ç”¨ä¸‹æ¸¸ä»»å‹™è³‡æ–™é›†è€Œé Zero-shot æˆ– few-shot çš„æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é¸ç”¨ linear-probe è€Œä¸æ˜¯ finetune ä¾†åšä¸‹æ¸¸ä»»å‹™çš„è©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;å› ç‚ºä»–å€‘çš„é‡é»æ˜¯é–‹ç™¼èˆ‡è³‡æ–™é›†ç„¡é—œçš„é è¨“ç·´æ–¹æ³•ï¼Œfinetune æœ‰å¯èƒ½è®“ä¸€å€‹é è¨“ç·´å­¸ç¿’ representation å¤±æ•—çš„æ¨¡å‹åœ¨å¾®èª¿éç¨‹ä¸­è®Šå¥½ã€‚
è€Œ linear-probe çš„é™åˆ¶å¯ä»¥å‡¸é¡¯é€™äº›å¤±æ•—ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/fig10.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;comparison-to-human-performance&#34;&gt;Comparison to Human Performance&lt;/h2&gt;
&lt;p&gt;å†ä¾†æ˜¯ CLIP å’Œäººé¡ç›¸æ¯”çš„çµæœã€‚
æŒ‘é¸äº†äº”å€‹äººåœ¨å¯µç‰©è³‡æ–™é›†ä¸Šæ¯”è¼ƒçš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/CLIP/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;data-overlap-analysis&#34;&gt;Data Overlap Analysis&lt;/h2&gt;
&lt;p&gt;å¯èƒ½æœƒæœ‰äººè³ªç–‘ï¼ŒCLIP çš„è¡¨ç¾æ˜¯å› ç‚ºè¨“ç·´è³‡æ–™é›†å’Œæ¸¬è©¦è³‡æ–™é›†æœ‰é‡ç–Šã€‚
ä½†ä½œè€…åšäº†ä¸€äº›å¯¦é©—ï¼Œæœ‰äº›è³‡æ–™é›†å®Œå…¨æ²’æœ‰åµæ¸¬åˆ°é‡ç–Šã€‚
å°æœ‰é‡ç–Šçš„åšå¯¦é©—ï¼Œç™¼ç¾æœ‰é‡ç–Šçš„å°æ•ˆæœæå‡å½±éŸ¿å¾ˆå°ã€‚&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;CLIP é›–ç„¶å¯ä»¥å’Œä½œç‚º Baseline çš„ ResNet-50 æ‰“å¹³æ‰‹ï¼Œä½†ç¾åœ¨çš„ SOTA é é«˜æ–¼è©² Baselineã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾å†ç¹¼çºŒåŠ å¤§æ¨¡å‹å’Œè³‡æ–™æ˜¯å¯ä»¥ç¹¼çºŒæå‡æ€§èƒ½çš„ï¼Œä½†ä½œè€…ä¼°è¨ˆè¦é”åˆ°ç¾æœ‰çš„ SOTA éœ€è¦å¢åŠ å¤§æ¦‚ 1000 å€çš„è¨ˆç®—é‡æ‰èƒ½é”åˆ°ï¼Œä½¿ç”¨ç¾æœ‰çš„ç¡¬é«”æ˜¯ä¸å¯è¡Œçš„ã€‚&lt;/p&gt;
&lt;p&gt;CLIP å°ç´°åˆ†é¡ã€æŠ½è±¡æˆ–æ›´é›£çš„ä»»å‹™è¡¨ç¾ä¸å¥½ï¼Œä½œè€…ç›¸ä¿¡é‚„æœ‰è¨±å¤šä»»å‹™æ˜¯ CLIP ç”¨ zero-shot åªèƒ½é”åˆ°äº‚çŒœç­‰ç´šçš„ã€‚&lt;/p&gt;
&lt;p&gt;Zero-Shot çš„ CLIP å¾ˆé›£æ³›åŒ–åˆ° out-of-distribution çš„è³‡æ–™ï¼Œæ¯”å¦‚åœ¨ MNIST ä¸Šåªèƒ½é”åˆ° 88% çš„æº–ç¢ºåº¦ã€‚
ä½œè€…ç™¼ç¾é è¨“ç·´è³‡æ–™å¹¾ä¹æ²’æœ‰é¡ä¼¼ MNIST çš„åœ–ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ CLIP å¯ä»¥éˆæ´»æ‡‰ç”¨å„ç¨® Zero-Shot çš„åˆ†é¡ï¼Œä½†åŸºæœ¬ä¸Šé‚„æ˜¯å¾ä½ çµ¦å®šçš„åˆ†é¡é¸æ“‡ã€‚
å’ŒçœŸæ­£éˆæ´»çš„æ–¹æ³•ï¼ˆç”Ÿæˆ image captionï¼‰ç›¸æ¯”ï¼Œæ˜¯é‡å¤§çš„é™åˆ¶ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹å€¼å¾—å˜—è©¦çš„ç°¡å–®æƒ³æ³•æ˜¯æŠŠ contrastive objective å’Œ generative objectiveï¼Œçµåˆã€‚&lt;/p&gt;
&lt;p&gt;CLIP ä¹Ÿæ²’æœ‰è§£æ±ºæ·±åº¦å­¸ç¿’è³‡æ–™æ•ˆç‡ä½ä¸‹çš„å•é¡Œï¼ŒCLIP è¨“ç·´äº† 32 å€‹ epochï¼Œå¦‚æœæŠŠé è¨“ç·´æœŸé–“çš„ç…§ç‰‡ä»¥ä¸€ç§’ä¸€å¼µä¾†å‘ˆç¾ï¼Œéœ€è¦ 405 å¹´ã€‚
æŠŠ CLIP å’Œ self-supervision æˆ–è€…å’Œ self-training åšçµåˆæ˜¯æœ‰å‰é€”çš„æ–¹å‘ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ä½œè€…å¼·èª¿ Zero-Shot Learningï¼Œä½†æ˜¯ä½œè€…é‚„æ˜¯æœ‰åè¦†æª¢æŸ¥ä¸‹æ¸¸ä»»å‹™æ¸¬è©¦é›†çš„è¡¨ç¾ï¼Œä¾†èª¿æ•´ CLIPã€‚
æ¯æ¬¡éƒ½ç”¨ ImageNet ä¾†ç¢ºèªï¼Œä¸¦ä¸ç®—çœŸæ­£çš„ zero-shot çš„æƒ…æ³ã€‚
å¦‚æœèƒ½å†å‰µä¸€å€‹æ–°çš„è³‡æ–™é›†ï¼Œå°ˆé–€ç”¨ä¾†è©•ä¼° zero-shot é·ç§»çš„èƒ½åŠ›æœƒæ›´æ°ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;çˆ¬ä¸‹ä¾†çš„è³‡æ–™æœ‰å¯èƒ½å¸¶æœ‰ç¤¾æœƒåè¦‹ã€‚&lt;/p&gt;
&lt;p&gt;æœ‰ä¸€äº›è¤‡é›œçš„ä»»å‹™å¾ˆé›£ç”¨æ–‡å­—ä¾†å‚³é”ï¼Œé›–ç„¶å¯¦éš›çš„è¨“ç·´æ¨£æœ¬æœ‰ç”¨ï¼Œä½† CLIP ä¸¦ä¸æœƒé‡å° few-shot æœ€ä½³åŒ–ã€‚æœ‰å€‹é•åç›´è¦ºçš„çµæœï¼Œå¯ä»¥æ³¨æ„åˆ°åœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œfew-shot ä¸è¦‹å¾—æ¯” zero-shot å¥½ã€‚&lt;/p&gt;
&lt;h2 id=&#34;é¡å¤–æ‡‰ç”¨&#34;&gt;é¡å¤–æ‡‰ç”¨&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;åœ–ç‰‡ç”Ÿæˆ
&lt;ul&gt;
&lt;li&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery
&lt;ul&gt;
&lt;li&gt;ç”¨æ–‡å­—å¼•å°ç”Ÿæˆåœ–ç‰‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç‰©ä»¶åµæ¸¬
&lt;ul&gt;
&lt;li&gt;Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation
&lt;ul&gt;
&lt;li&gt;å°‡åŸºç¤é¡åˆ¥å†åšç´°åˆ†é¡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OCR
&lt;ul&gt;
&lt;li&gt;Contrastive Language-Image Forensic Search
&lt;ul&gt;
&lt;li&gt;æœç´¢å½±ç‰‡ä¸­æœ‰æ²’æœ‰æ–‡æœ¬æè¿°çš„ç‰©é«”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ç­†è¨˜&#34;&gt;ç­†è¨˜&lt;/h2&gt;
&lt;p&gt;prompt engineering
prompt ensemble&lt;/p&gt;
</description>
        </item>
        <item>
        <title>MLP-Mixer è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/mlp-mixer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 30 Oct 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mlp-mixer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2107.05407&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PonderNet: Learning to Ponder&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;CNN åœ¨  CV é ˜åŸŸæ˜¯é¦–é¸ï¼Œä½†åŸºæ–¼ attention çš„ç¶²è·¯ä¹Ÿåœ¨è®Šæµè¡Œã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è­‰æ˜é›–ç„¶ convolution å’Œ attention éƒ½å¯ä»¥ç²å¾—è‰¯å¥½çš„ performanceï¼Œä½†çš†éå¿…é ˆã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº† MLP-Mixerï¼Œä¸€ç¨®ç´” MLP çš„æ¶æ§‹ï¼ŒåŒ…å«å…©ç¨® layerï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å°‡ MLP ç¨ç«‹ç”¨åœ¨ patch
&lt;ul&gt;
&lt;li&gt;mixing per-location features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å°‡ MLP ç”¨åœ¨ patches ä¹‹é–“ã€‚
&lt;ul&gt;
&lt;li&gt;mixing spatial information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº†å®Œå…¨åŸºæ–¼ MLP çš„ MLP-Mixer ( åˆç°¡ç¨± Mixer )ï¼Œæœ‰ç«¶çˆ­åŠ›å»æ¦‚å¿µå’ŒæŠ€è¡“ä¸Šéƒ½å¾ˆç°¡å–®ã€‚&lt;/p&gt;
&lt;p&gt;æœ‰å…©ç¨® MLP layerï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Channel-Mixing MLP&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨åœ¨æ¯å€‹ patch ä¸Š&lt;/li&gt;
&lt;li&gt;è®“ä¸åŒ channel ä¹‹é–“çš„è³‡è¨Šäº’ç›¸äº¤æ›&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Token-Mixing MLP&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨åœ¨æ‰€æœ‰ patch ä¸Š&lt;/li&gt;
&lt;li&gt;è®“ç©ºé–“è³‡è¨Šäº’ç›¸äº¤æ›&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é€™å…©è€…äº¤éŒ¯å‡ºç¾ï¼Œè®“å…©å€‹è¼¸å…¥ç¶­åº¦å¯ä»¥äº¤äº’ä½œç”¨&lt;/p&gt;
&lt;p&gt;åœ¨æ¥µç«¯æƒ…æ³ä¸‹ï¼Œæœ¬æ–‡çš„æ¶æ§‹å¯ä»¥çœ‹åšæ˜¯ä¸€å€‹éå¸¸ç‰¹æ®Šçš„ CNNï¼Œä½¿ç”¨ 1*1 å·ç©åš channel mixingï¼Œä¸¦ç”¨ single-channel ä¸” full receptive field çš„ 1D å·ç©åš token mixingã€‚&lt;/p&gt;
&lt;p&gt;åä¹‹å‰‡ä¸ç„¶ï¼Œå› ç‚ºç¶“å…¸çš„ CNN ä¸¦ä¸æ˜¯ Mixer çš„ç‰¹ä¾‹ã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡å¾ˆç°¡å–®ï¼Œä½† Mixer å»å–å¾—æœ‰ç«¶çˆ­åŠ›çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œèˆ‡ ViT ä¸€æ¨£ï¼Œåœ¨ä¸€åˆ‡ç‰¹æœ‰çš„ CNN æ¶æ§‹ä¸‹ç•¥æœ‰æ¬ ç¼ºã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MLP-Mixer/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;mixer-architecture&#34;&gt;Mixer Architecture&lt;/h2&gt;
&lt;p&gt;Mixer çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯æƒ³æŠŠ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;mix features at a given spatial location&lt;/li&gt;
&lt;li&gt;mix features between different spatial locations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;çµ¦åˆ†é–‹ä¾†&lt;/p&gt;
&lt;p&gt;æ‰€æœ‰çš„ patch éƒ½æ˜¯ç”¨åŒä¸€å€‹ projection matrix&lt;/p&gt;
&lt;p&gt;æ³¨æ„ fig1 çš„ MLP2ï¼Œä»–å€‘å…±äº«åŒæ¨£çš„åƒæ•¸ï¼Œé€™æ¨£ç¶å®šåƒæ•¸å¯ä»¥é é˜² C å’Œ S å¢åŠ æ™‚ä½¿æ¶æ§‹å¢é•·éå¿«ã€‚åœ¨ seperable convolution ä¸­ï¼Œä¸åŒé€šé“ä½¿ç”¨ä¸åŒçš„ kernelã€‚ä¸éä½œè€…é€™æ¨£çš„é¸æ“‡ä¸¦ä¸å½±éŸ¿å¯¦éš›çš„ performanceã€‚&lt;/p&gt;
&lt;p&gt;Mixer çš„æ‰€æœ‰ Layer éƒ½å…·å‚™ç›¸åŒçš„è¼¸å…¥å¤§å°ã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨®ã€Œisotropicã€çš„è¨­è¨ˆå’Œ Transformer æˆ– RNN æ¯”è¼ƒåƒã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡å¤§å¤šæ•¸å…·æœ‰ pyramidal structure çš„ CNN ä¸åŒï¼Œå®ƒå€‘åœ¨æ›´æ·±çš„å±¤æœ‰æ›´ä½çš„ resolutionï¼Œä½†æœ‰æ›´å¤š channelã€‚&lt;/p&gt;
&lt;p&gt;ä¸éä¸Šé¢åªæ˜¯æ¢è¨å…¸å‹çš„è¨­è¨ˆï¼Œä¹Ÿå­˜åœ¨ä¾‹å¤–ï¼Œæ¯”å¦‚ isotropic Resnet æˆ– pyramidal ViTã€‚&lt;/p&gt;
&lt;p&gt;é™¤äº† MLPï¼ŒMixer é‚„æœ‰ç”¨åˆ° LayerNorm å’Œ skip connectionã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯ Mixer æ²’æœ‰ç”¨åˆ° positional encodingï¼Œå› ç‚º token-mixing MLP æœ¬èº«å°±å°ä½ç½®æ•æ„Ÿã€‚\&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;Mixer ç”¨ä¸­å¤§å‹è³‡æ–™é›†é è¨“ç·´ï¼Œç„¶å¾Œåœ¨ä¸€ç³»åˆ—ä¸­å°è³‡æ–™é›†ä¸Šè©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;ç›®æ¨™ä¸æ˜¯æ‹¿åˆ° SOTAï¼Œè€Œæ˜¯é¡¯ç¤ºå‡ºèˆ‡ SOTA çš„ CNN å’Œ attention-based model ç›¸æ¯”ï¼ŒMixer æœ‰ç«¶çˆ­åŠ›ã€‚&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;åœ¨ fine-tuning æ™‚ï¼Œç”¨æ¯”é è¨“ç·´æ™‚é‚„è¦é«˜çš„ resolutionã€‚
ç”±æ–¼æ¯å€‹ patch çš„ resolution æ˜¯å›ºå®šçš„ï¼Œé€™æ¨£æœƒå°è‡´æœ‰æ›´å¤šçš„ patchesã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼é€™å€‹å•é¡Œï¼Œæ¡ç”¨ä»¥ä¸‹è§£æ³•ï¼š&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘åŸå…ˆé è¨ˆåƒ S å€‹ patchï¼Œç¾åœ¨æˆ‘å€‘ç”±æ–¼è¼¸å…¥è§£æåº¦æ›´é«˜ï¼Œè€Œä¸” patch å¤§å°ä¸è®Šï¼Œæ‰€ä»¥æˆ‘å€‘å¾—åˆ°ä¸€å€‹æ¯” S é‚„å¤§çš„ $S&amp;rsquo;$ã€‚
æ‰€ä»¥æˆ‘å€‘åœ¨å¾ˆå¤šåœ°æ–¹å°±éœ€è¦æ¯”åŸå…ˆæ¬Šé‡çŸ©é™£ W é‚„æ›´å¤§çš„ $W&#39;$&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘è¦æŠŠ $S&amp;rsquo;$ æ‹†æˆ $K^2$ å€‹é•·åº¦ç‚º $S$ çš„ sequenceï¼ŒK æ˜¯æ•´æ•¸ã€‚&lt;/p&gt;
&lt;p&gt;ä¸¦ä¸”æˆ‘å€‘æŠŠ $W&amp;rsquo;$ çš„ shape æ”¹æˆ $(W.shape[0] * K^2, W.shape[1] * K^2)$&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œæˆ‘å€‘æŠŠ $W&amp;rsquo;$ ä½œç‚º block-diagonal matrix ä¾†åˆå§‹åŒ–ï¼ŒæŠŠ $W$ copy å¥½å¹¾ä»½ï¼Œæ”¾åœ¨ main diagonal ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;æ‰€æœ‰æ¬Šé‡éƒ½ç”¨é¡ä¼¼çš„è™•ç†æ–¹æ³•ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion å…¥é–€</title>
        <link>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</link>
        <pubDate>Mon, 23 Oct 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/diffusion-%E5%85%A5%E9%96%80/</guid>
        <description>&lt;h2 id=&#34;å¤§è‡´æ¦‚å¿µ&#34;&gt;å¤§è‡´æ¦‚å¿µ&lt;/h2&gt;
&lt;p&gt;å±¬æ–¼ç”Ÿæˆå¼ AIï¼Œä¸€é–‹å§‹ç”¨åœ¨ç”Ÿæˆåœ–ç‰‡ï¼Œå¾Œä¾†ä¹Ÿæœ‰æ‡‰ç”¨åˆ°è«¸å¦‚ NLP ç­‰é ˜åŸŸã€‚&lt;/p&gt;
&lt;p&gt;ä¸‹æ–‡ç¨±å‘¼åŸåœ–ç‚º spriteã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡ AutoEncoder æœ‰é»é¡ä¼¼ï¼Œå…ˆå–å¾—ä¸€å¼µ spriteï¼Œéš¨è‘—æ™‚é–“æ¨é€²ï¼Œæ¯æ¬¡éƒ½åœ¨åœ–ç‰‡ä¸ŠåŠ ä¸€å±¤é›œè¨Šï¼Œåè¦†ç–ŠåŠ ï¼Œè¿­ä»£å¤šæ¬¡å¾Œï¼Œå°±æœƒå¾—åˆ°ä¸€å¼µé›£ä»¥çœ‹å‡ºåŸåœ–çš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;å¾ sprite åˆ°åªèƒ½çœ‹å‡ºæ˜¯ä¸€åœ˜é›œè¨Šä¸¦éæ˜¯ä¸€æ­¥åˆ°ä½çš„éç¨‹ã€‚ä¸€é–‹å§‹æ²’æœ‰é›œè¨Šæ™‚å¯ä»¥çœ‹å‡ºåŸæœ¬çš„ spriteï¼Œä¸€å€‹è¿­ä»£å¾Œå¯èƒ½å¯ä»¥å‹‰å¼·çœ‹å‡ºåŸæœ¬çš„ spriteï¼Œå†å¹¾å€‹è¿­ä»£å¾Œå¯èƒ½ä¹Ÿé‚„èƒ½çœ‹å‡ºåŸæœ¬çš„ outlineï¼Œç¶“éè¨±å¤šæ¬¡å¾Œæ‰æœƒè®Šæˆå®Œå…¨è¾¨è­˜ä¸äº†çš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘æœŸæœ›æ¨¡å‹åšçš„äº‹æƒ…å‰‡æ˜¯å¾ gaussian noise é€æ­¥æ¨å› spriteï¼ŒåŒæ¨£ä¸æ˜¯ä¸€æ­¥åˆ°ä½ï¼Œè€Œæ˜¯è®“æ¨¡å‹é æ¸¬ä¸Šä¸€å€‹æ™‚é–“é»çš„é›œè¨Šï¼Œç›¸æ¸›å¾Œå†é€æ­¥æ¨å› spriteï¼Œé€™éç¨‹ç¨±ç‚º denoiseã€‚&lt;/p&gt;
&lt;h2 id=&#34;ddpm&#34;&gt;DDPM&lt;/h2&gt;
&lt;p&gt;å¯¦ç¾ Diffusion å¯èƒ½æœƒæœ‰é» confusingï¼Œå› ç‚ºä»–å¯¦ä½œä¸Šå’Œä¸Šé¢èªªçš„ä¸å¤ªç›¸åŒã€‚
åœ¨è¨“ç·´çš„æ™‚å€™ï¼Œæˆ‘å€‘æœƒæ¡æ¨£ä¸‰å€‹æ±è¥¿ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;è¨“ç·´åœ–ç‰‡ (sprite)&lt;/li&gt;
&lt;li&gt;é›œè¨Š&lt;/li&gt;
&lt;li&gt;æ™‚é–“é» (t)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è¨“ç·´éšæ®µçš„æ™‚å€™ï¼Œæˆ‘å€‘æœƒæŠŠã€ŒåŸå§‹ä¹¾æ·¨çš„åœ–ç‰‡ã€å’Œã€Œé›œè¨Šã€æ ¹æ“šæ™‚é–“é€²è¡Œä¸åŒæ¯”ä¾‹çš„ç›¸åŠ  (æ··åˆ)ï¼Œt è¶Šå¤§ï¼Œé›œè¨Šçš„æ¯”ä¾‹è¶Šå¤§ã€‚&lt;/p&gt;
&lt;p&gt;æ¨¡å‹é æ¸¬çš„ç›®æ¨™æ˜¯å‰é¢ sample å‡ºçš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;é€™èˆ‡å‰é¢èªªçš„æ¦‚å¿µç›¸æ‚–ã€‚æŒ‰ç…§å‰é¢çš„èªªæ³•ï¼Œå°æ–¼æ™‚é–“é» tï¼Œæ‡‰è©²æ˜¯ä»¥ä¸€å¼µåŠ äº† t-1 æ¬¡é›œè¨Šçš„ sprite ä½œç‚ºè¼¸å…¥ï¼Œå†åŠ ä¸Š t æ‰€ sample å‡ºçš„é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;ç¾åœ¨å¯¦ä½œå»æ˜¯åŸå§‹ä¹¾æ·¨çš„ sprite ç›´æ¥æ ¹æ“šæ™‚é–“é»æ··å’ŒæŸå€‹é›œè¨Šã€‚&lt;/p&gt;
&lt;p&gt;é€™èƒŒå¾Œçš„æ•¸å­¸æ¨å°ååˆ†å†—é•·ï¼Œé€™è£¡ä¸æ•˜è¿°ï¼Œä½†éœ€çŸ¥é“å¯¦ä½œå·®ç•°ã€‚&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;åœ¨æ¨è«–éšæ®µçš„æ™‚å€™ï¼Œæ¯æ¬¡ denoise å¾Œéœ€è¦æŠŠåœ–ç‰‡å’Œé¡å¤– sample çš„ noise ç›¸åŠ ã€‚é€™å€‹ noise å’Œå‰é¢çš„ noise ä¸€æ¨£ï¼Œéƒ½æ˜¯å¾ mean=0, std=1 çš„ gaussian distribution ä¸­ sample å‡ºä¾†çš„ã€‚&lt;/p&gt;
&lt;p&gt;ä¸åŠ çš„è©±ä¼¼ä¹é‚„å®¹æ˜“æœ‰ Mode Collapse çš„ç¾è±¡ã€‚
çœ‹åˆ°ä¸€å€‹èªªæ³•æ˜¯ï¼Œæ¨¡å‹å–œæ­¡åƒåœ–ç‰‡åŠ ä¸Šé›œè¨Šçš„åœ–åƒä½œç‚ºåœ–ç‰‡ï¼Œåœ¨åœ–ç‰‡ä¸ŠåŠ ä¸Š noise ä¼¼ä¹æœƒæ›´ç¬¦åˆæ¨¡å‹é æœŸçš„è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;çœ‹äº†æå¼˜æ¯…çš„å½±ç‰‡ï¼Œä¹Ÿæœ‰åŸºæ–¼éš¨æ©Ÿæ€§çš„è§€é»ã€‚&lt;/p&gt;
&lt;p&gt;ç”Ÿæˆå¼ Model ç”Ÿæˆæ–‡ç« æ™‚æ°¸é å–æ©Ÿç‡æœ€å¤§çš„ï¼Œä¸è¦‹å¾—æœ‰æ›´å¥½çš„æ•ˆæœï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ‰ç ”ç©¶æ˜¯è®“ Model é¸æ©Ÿç‡æœ€å¤§çš„ï¼Œçµæœå®¹æ˜“ç”Ÿå‡ºåè¦†è·³é‡çš„æ–‡ç« ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¹Ÿæœ‰æŠŠäººé¡å¯«çš„æ–‡ç« å»é¤µçµ¦ Model çœ‹ï¼Œå¾ä»–çš„è§’åº¦çœ‹äººé¡å¯«çš„ä¸‹ä¸€å€‹å­—çš„æ©Ÿç‡æ˜¯å¤šå°‘ï¼Œç™¼ç¾äººé¡å¯«çš„æ–‡ç« å¾ˆå¸¸å‡ºç¾ä¸€ä¸‹æ©Ÿç‡é«˜ä¸€ä¸‹æ©Ÿç‡ä½çš„å­—ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æŸç¯‡èªéŸ³åˆæˆçš„æ–‡ç« éœ€è¦åœ¨æ¨è«–éšæ®µã€Œå•Ÿç”¨ã€dropout æ‰å¯ä»¥æœ‰å¥½çš„çµæœã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Diffusion ä¹Ÿæœ‰å¯èƒ½æˆåŠŸçš„é»æ˜¯åœ¨æ–¼ä¸¦éã€Œä¸€æ¬¡åˆ°ä½ã€è€Œæ˜¯ã€ŒN æ¬¡åˆ°ä½ã€ã€‚
å¾é€™æ¨£çš„è§’åº¦çœ‹ï¼ŒDiffusion æ˜¯ autoregressive æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;é¡ä¼¼çš„ä½œæ³•ä¹Ÿæœ‰ Mask-Predictï¼Œå¤§è‡´æ¦‚å¿µæ˜¯å¾åŸæœ¬éƒ½æ˜¯ Mask çš„æƒ…å¢ƒé–‹å§‹ï¼Œå°‡ä¸€äº›ä¿¡å¿ƒé«˜çš„é æ¸¬ç•™ä½ï¼Œä¿¡å¿ƒä½çš„ä¿æŒç‚º Maskï¼Œä¸€æ­¥æ­¥é æ¸¬å‡ºæ‰€æœ‰è³‡è¨Šã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>æŸ¥æ‰¾ç›¸ä¼¼å‘é‡</title>
        <link>https://roykesydon.github.io/Blog/p/%E6%9F%A5%E6%89%BE%E7%9B%B8%E4%BC%BC%E5%90%91%E9%87%8F/</link>
        <pubDate>Fri, 06 Oct 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/%E6%9F%A5%E6%89%BE%E7%9B%B8%E4%BC%BC%E5%90%91%E9%87%8F/</guid>
        <description>&lt;h2 id=&#34;æé«˜æŸ¥æ‰¾ç›¸ä¼¼å‘é‡çš„é€Ÿåº¦&#34;&gt;æé«˜æŸ¥æ‰¾ç›¸ä¼¼å‘é‡çš„é€Ÿåº¦&lt;/h2&gt;
&lt;p&gt;ä»»ä½•éæš´åŠ›æœå°‹çš„æœå°‹æ–¹æ³•ï¼Œéƒ½æœƒä¸€å®šç¨‹åº¦ä¸Šçš„é™ä½æœç´¢å“è³ªã€‚
éœ€è¦åœ¨æœç´¢å“è³ªå’Œé€Ÿåº¦é€²è¡Œ trade-offã€‚&lt;/p&gt;
&lt;h3 id=&#34;k-means&#34;&gt;K-Means&lt;/h3&gt;
&lt;p&gt;å¯ç”¨åœ¨å°‡å‘é‡è³‡æ–™åº«åˆ†ç¾¤ï¼Œä»¥ä¾¿ç¸®å°æŸ¥æ‰¾ç›¸ä¼¼å‘é‡çš„ç¯„åœã€‚&lt;/p&gt;
&lt;p&gt;è¿­ä»£è¨ˆç®—ç¾¤å¿ƒï¼Œç›´åˆ°æ”¶æ–‚&lt;/p&gt;
&lt;p&gt;ä¾æ“šé›¢ç¾¤å¿ƒçš„é è¿‘åˆ†é¡&lt;/p&gt;
&lt;h4 id=&#34;å•é¡Œ&#34;&gt;å•é¡Œ&lt;/h4&gt;
&lt;p&gt;ç›¸è¿‘çš„å‘é‡æœ‰å¯èƒ½è¢«åˆ†åˆ°ä¸åŒç¾¤&lt;/p&gt;
&lt;p&gt;å¯ä»¥é€éã€Œç”¨æ›´å¤šé¡ï¼Œä¸¦æœç´¢å¤šå€‹æœ€è¿‘ç¾¤ã€ä¾†ç·©è§£å•é¡Œ&lt;/p&gt;
&lt;p&gt;å¯ä»¥æ‰¾å…¶ä»– ANN (Approximate Nearest Neighbors) æ¼”ç®—æ³•ä¾†é¢å°è©²å•é¡Œ&lt;/p&gt;
&lt;h3 id=&#34;ä½ç½®æ•æ„Ÿå“ˆå¸Œ-locality-sensitive-hashing-lsh&#34;&gt;ä½ç½®æ•æ„Ÿå“ˆå¸Œ (Locality Sensitive Hashing, LSH)&lt;/h3&gt;
&lt;p&gt;è®“è¶Šç›¸ä¼¼çš„å‘é‡è¶Šå®¹æ˜“ç¢°æ’ï¼Œæ‰¾ç›¸ä¼¼å‘é‡å°±åœ¨åŒå€‹ bucket æ‰¾&lt;/p&gt;
&lt;h4 id=&#34;å¯¦ç¾æ–¹æ³•&#34;&gt;å¯¦ç¾æ–¹æ³•&lt;/h4&gt;
&lt;p&gt;æ­¤è™•æŒ‘ä¸€ç¨®æ–¹å¼èˆ‰ä¾‹ï¼Œæ­¤è™•ç”¨éš¨æ©Ÿè¶…å¹³é¢èˆ‰ä¾‹ã€‚&lt;/p&gt;
&lt;p&gt;å¯ä»¥åœ¨ç©ºé–“ä¸­éš¨æ©Ÿç”Ÿæˆå¤šå€‹ (n-1) ç¶­åº¦çš„è¶…å¹³é¢ï¼Œå°‡å…©é‚Šåˆ†é¡ç‚º 0 å’Œ 1ã€‚
è·é›¢è¼ƒé çš„é»å°è¢«åˆ‡å‰²é–‹çš„æ©Ÿç‡æœƒæ¯”è·é›¢è¼ƒè¿‘çš„é»å°é‚„å¤§ã€‚
ç”¨é€™æ¨£çš„æ–¹æ³•ï¼Œæœƒè®“ç›¸è¿‘çš„é»å°ç”Ÿå‡ºçš„ Hash å€¼è¼ƒæ¥è¿‘ã€‚&lt;/p&gt;
&lt;h4 id=&#34;å•é¡Œ-1&#34;&gt;å•é¡Œ&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;æ¥è¿‘çš„å‘é‡æœ‰å¯èƒ½å› ç‚ºæ©Ÿç‡å› ç´ è¢«åˆ†åˆ°ä¸åŒ bucket
&lt;ul&gt;
&lt;li&gt;å°‡å‘é‡åˆ†æ®µï¼Œæ¯æ®µæœ‰åŒ¹é…åˆ°åŒå€‹ bucket å°±è¦–ä½œå€™é¸é …&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;æ¸›å°‘æŸ¥æ‰¾ç›¸ä¼¼å‘é‡çš„è¨˜æ†¶é«”é–‹éŠ·&#34;&gt;æ¸›å°‘æŸ¥æ‰¾ç›¸ä¼¼å‘é‡çš„è¨˜æ†¶é«”é–‹éŠ·&lt;/h2&gt;
&lt;p&gt;å¤§é‡çš„é«˜ç¶­å‘é‡æœƒé€ æˆå¤§é‡çš„è¨˜æ†¶é«”é–‹éŠ·&lt;/p&gt;
&lt;h3 id=&#34;k-means-1&#34;&gt;K-Means&lt;/h3&gt;
&lt;p&gt;æŠŠåŒä¸€ç¾¤çš„å‘é‡éƒ½ç”¨ç¾¤å¿ƒå‘é‡ä»£æ›¿ï¼Œæ˜¯ä¸€ç¨®æœ‰æå£“ç¸®ã€‚&lt;/p&gt;
&lt;h4 id=&#34;å•é¡Œ-2&#34;&gt;å•é¡Œ&lt;/h4&gt;
&lt;p&gt;ä½†é€™æ¨£éœ€è¦å¦å¤–çš„ç©ºé–“ä¾†å­˜å– codebook (å‘é‡å°æ‡‰è¡¨)ï¼Œåœ¨æŸäº›æƒ…æ³ä¸è¦‹å¾—æ¯”åŸæœ¬çš„å‘é‡é‚„çœç©ºé–“ï¼Œç”šè‡³å¯èƒ½èŠ±æ›´å¤šã€‚&lt;/p&gt;
&lt;p&gt;n ç¶­çš„å‘é‡å¯èƒ½éœ€è¦ $2^{\frac{n}{2}}$ çš„ class æ‰å¯ä»¥è¼ƒå¥½çš„åˆ†é¡ (ä¾†æºæœªçŸ¥)&lt;/p&gt;
&lt;p&gt;å¯ä»¥é€éæŠŠé«˜ç¶­å‘é‡åˆ‡å‰²æˆå¤šå€‹ä½ç¶­å­å‘é‡å€‹åˆ¥è™•ç†å†åˆä½µä¾†ç·©è§£è©²å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;è©²æ–¹æ³•ç¨±ç‚º Product Quntization (PQ)&lt;/p&gt;
&lt;h2 id=&#34;å…¶ä»–åšæ³•&#34;&gt;å…¶ä»–åšæ³•&lt;/h2&gt;
&lt;h3 id=&#34;nsw&#34;&gt;NSW&lt;/h3&gt;
&lt;h4 id=&#34;å…­åº¦åˆ†éš”ç†è«–-six-degrees-of-separation&#34;&gt;å…­åº¦åˆ†éš”ç†è«– (Six degrees of separation)&lt;/h4&gt;
&lt;p&gt;å°æ–¼ä¸–ç•Œä¸Šå…©å€‹äº’ä¸ç›¸è­˜çš„äººï¼Œåªéœ€è¦å…­å€‹ä¸­é–“äººå°±å¯ä»¥å»ºç«‹èµ·é€£çµã€‚&lt;/p&gt;
&lt;h4 id=&#34;åšæ³•&#34;&gt;åšæ³•&lt;/h4&gt;
&lt;p&gt;æˆ‘å€‘æƒ³æ‰¾å°æ–¼æŸå€‹ç›®æ¨™å‘é‡è€Œè¨€æœ€ç›¸ä¼¼çš„å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;å…ˆéš¨æ©Ÿæ‰¾ä¸€å€‹é»ï¼Œæ‰¾ä»–çš„ç›¸é„°ç¯€é»èª°å’Œç›®æ¨™å‘é‡æœ€ç›¸è¿‘ï¼Œä¸¦åè¦†æ­¤éç¨‹ï¼Œç›´åˆ°æ‰€æœ‰ç›¸é„°ç¯€é»éƒ½æ²’æœ‰è‡ªå·±é›¢ç›®æ¨™ç›¸è¿‘ã€‚&lt;/p&gt;
&lt;p&gt;å…­åº¦åˆ†éš”ç†è«–è®“æˆ‘å€‘æ¨æ¸¬é€™éç¨‹å¯èƒ½å¾ˆå¿«å°±æœƒçµæŸã€‚&lt;/p&gt;
&lt;h4 id=&#34;å»ºç«‹çµæ§‹&#34;&gt;å»ºç«‹çµæ§‹&lt;/h4&gt;
&lt;p&gt;æˆ‘å€‘å¾—å¹«é€™äº›å‘é‡å»ºç«‹åœ–é—œä¿‚ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Delaunay triangulation algorithm
&lt;ul&gt;
&lt;li&gt;å¯ä»¥ç”¨ä¾†å»ºç«‹åœ–é—œä¿‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ä½†é  Delaunay triangulation algorithmï¼Œæœ‰å¯èƒ½éš¨æ©Ÿçš„å‘é‡å’Œç›®æ¨™å‘é‡è·é›¢å¾ˆé ï¼ŒæŸ¥æ‰¾å¾ˆæ…¢ã€‚&lt;/p&gt;
&lt;p&gt;NSW çš„å¯¦éš›åšæ³•æ˜¯å°‡æ‰€æœ‰å‘é‡éš¨æ©Ÿåœ°æ”¾å›åœ–ä¸­ï¼Œä¸¦å’Œæœ€è¿‘çš„ k å€‹é»é€£æ¥ã€‚&lt;/p&gt;
&lt;p&gt;åªçœ‹è¼ƒçŸ­çš„é€£æ¥ï¼Œæœƒç™¼ç¾å’Œ Delaunay triangulation algorithm ç”¢çš„åœ–ç›¸è¿‘ï¼Œå¯ä»¥é€²è¡Œç´°ç²’åº¦çš„æŸ¥æ‰¾ã€‚
åªçœ‹è¼ƒé•·çš„é€£æ¥ï¼Œå‰‡å¯é”åˆ°å¿«é€Ÿå°èˆªçš„æ•ˆæœã€‚&lt;/p&gt;
&lt;h4 id=&#34;hnsw&#34;&gt;HNSW&lt;/h4&gt;
&lt;p&gt;å»ºç«‹ä¸€å€‹åˆ†å±¤çµæ§‹ï¼Œè¶Šä¸Šå±¤çš„é»è¶Šç¨€ç–ã€é€£ç·šè¶Šé•·ã€‚&lt;/p&gt;
&lt;p&gt;å’Œ NSW ç›¸æ¯”ï¼Œè®“ç²—ç²’åº¦åˆ°ç´°ç²’åº¦çš„å°èˆªéç¨‹æ›´åŠ ç©©å®šã€‚&lt;/p&gt;
&lt;p&gt;ä½†å ç”¨çš„è¨˜æ†¶é«”ç©ºé–“æ›´å¤§ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DETR è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 10 Aug 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.12872&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ä½œè€…æŠŠ object detection è¦–ä½œä¸€å€‹ set prediction å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ç°¡åŒ–äº† pipelineï¼Œæ¶ˆé™¤äº†è¨±å¤š hand-designed componentsï¼Œæ¯”å¦‚ non-maximum suppression å’Œ anchor generationï¼Œé€™äº› component ç”±æˆ‘å€‘å°æ–¼ä»»å‹™çš„å…ˆé©—çŸ¥è­˜æ§‹æˆã€‚&lt;/p&gt;
&lt;p&gt;æå‡ºäº†ä¸€å€‹æ–°çš„ç›®æ¨™å‡½æ•¸ï¼Œé€éäºŒåˆ†åŒ¹é…ï¼ˆbipartite matchingï¼‰é€²è¡Œé æ¸¬ï¼Œä¹Ÿç”¨ Transformer encoder-decoder æ¶æ§‹ã€‚&lt;/p&gt;
&lt;p&gt;çµ¦äºˆä¸€çµ„å›ºå®šçš„ learned object queryï¼ŒDETR å¯ä»¥æ¨ç† objects å’Œ globol image context çš„é—œä¿‚ï¼Œä¸¦ã€Œä¸¦è¡Œã€è¼¸å‡ºä¸€çµ„é æ¸¬é›†ã€‚&lt;/p&gt;
&lt;p&gt;DETR æ¦‚å¿µéå¸¸ç°¡å–®ã€‚&lt;/p&gt;
&lt;p&gt;DETR åœ¨ COCO ä¸Šå’Œ Faster RCNN baseline åœ¨æº–ç¢ºåº¦å’Œ performance ä¸Šç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;DETR å¯ä»¥å¾ˆç°¡å–®åœ°æ¨å»£åˆ° Panoptic Segmentationã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ç›®æ¨™æª¢æ¸¬çš„ç›®æ¨™å°±æ˜¯é›†åˆé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç›®å‰éƒ½ç”¨ä¸€äº›å¾ˆé–“æ¥çš„æ–¹å¼å»åšï¼Œåƒæ˜¯ç”¨ proposals, anchors æˆ– window centersã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯é€™äº›æ–¹æ³•æ€§èƒ½æ˜é¡¯å—é™æ–¼å¾Œè™•ç†æ­¥é©Ÿï¼Œæ¯”å¦‚ non-maximum suppressionï¼Œå› ç‚ºä»–å€‘æœƒç”¢ç”Ÿå¤§é‡å†—é¤˜çš„æ¡†ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç°¡åŒ– pipelineï¼Œä½œè€…æå‡ºäº†ä¸€ç¨® End-to-End çš„æ–¹æ³•ï¼Œä»¥å¾€ä¹Ÿæœ‰ä¸€äº›å˜—è©¦ï¼Œä½†ä»–å€‘è¦ä¸æ·»åŠ äº†å…¶ä»–çš„å…ˆé©—çŸ¥è­˜ï¼Œä¸ç„¶å°±æ˜¯åœ¨å…·æœ‰æŒ‘æˆ°æ€§çš„ benchmark ä¸Šè¡¨ç¾ä¸å¥½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ COCO ä¸Šå’Œ Faster R-CNN çš„æ€§èƒ½ç›¸ç•¶ï¼Œè¡¨ç¾å’Œé€Ÿåº¦éƒ½å·®ä¸å¤šã€‚&lt;/p&gt;
&lt;p&gt;DETR åœ¨å¤§ç‰©é«”è¡¨ç¾å¾ˆå¥½ï¼Œå¯èƒ½æ˜¯æ­¸åŠŸæ–¼ Transformer non-local çš„è¨ˆç®—èƒ½åŠ›ã€‚
é›–ç„¶ DETR åœ¨å°ç‰©é«”ä¸Šè¡¨ç¾å€’ä¸æ€éº¼æ¨£ã€‚&lt;/p&gt;
&lt;p&gt;DETR éœ€è¦è¶…é•·çš„è¨“ç·´æ™‚é–“ï¼Œä½† DETR çš„è¨­è¨ˆç†å¿µå¯ä»¥æ‹“å±•åˆ° Panoptic Segmentationã€‚&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;set-prediction&#34;&gt;Set Prediction&lt;/h3&gt;
&lt;p&gt;æ²’æœ‰è¦ç¯„çš„æ·±åº¦å­¸ç¿’æ¨¡å‹å¯ä»¥ç›´æ¥é æ¸¬é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›ä»»å‹™ä¸­çš„ä¸€å€‹å›°é›£é»æ˜¯é¿å… near-dulicatesï¼ˆç›¸è¿‘çš„é‡è¤‡æª¢æ¸¬æ¡†ï¼‰ ç•¶å‰å¤šæ•¸æª¢æ¸¬å™¨ç”¨ NMS ä¾†è§£æ±ºæ­¤å•é¡Œï¼Œå¦‚æœæ˜¯ direct set prediction å°±ä¸ç”¨å¾Œè™•ç†ã€‚&lt;/p&gt;
&lt;h3 id=&#34;transformers-and-parallel-decoding&#34;&gt;Transformers and Parallel Decoding&lt;/h3&gt;
&lt;p&gt;Transformer åœ¨å„ç¨®åœ°æ–¹è¡¨ç¾å‡ºè‰²ï¼Œä½†æ¨ç†æˆæœ¬ä»¤äººæœ›è€Œç”Ÿç•ã€‚&lt;/p&gt;
&lt;h3 id=&#34;object-detection&#34;&gt;Object detection&lt;/h3&gt;
&lt;p&gt;ç¾åœ¨å¤šæ•¸çš„ç›®æ¨™æª¢æ¸¬æ–¹æ³•æ˜¯åŸºæ–¼ä¸€äº›åˆå§‹çš„çŒœæ¸¬ï¼Œå†å»åšé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚å°æ–¼ two-stage çš„æ–¹æ³•ï¼Œå°±æ˜¯å°æ–¼ proposals å¾€ä¸‹åšé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ single-stageï¼Œåˆå§‹çŒœæ¸¬å°±æ˜¯ anchorsã€‚&lt;/p&gt;
&lt;h4 id=&#34;set-based-loss&#34;&gt;Set-based loss&lt;/h4&gt;
&lt;p&gt;ä»¥å‰çš„ä¸€äº›ä½œæ³•æ¯”å¦‚ Learnable NMS æˆ– relation networks éƒ½å¯ä»¥é€é attention ä¾†è™•ç†ä¸åŒé æ¸¬ä¹‹é–“çš„é—œä¿‚ã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ direct set lossesï¼Œä»–å€‘ä¸éœ€è¦ä»»ä½•å¾Œè™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯é€™äº›æ–¹æ³•å¾€å¾€ç”¨é¡å¤–çš„ hand-crafted context featureï¼Œæ¯”å¦‚ proposal box coordinatesã€‚ä½œè€…å°‹æ‰¾æ¸›å°‘æ¨¡å‹ä¸­å…ˆé©—çŸ¥è­˜çš„æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;h4 id=&#34;recurrent-detectors&#34;&gt;Recurrent detectors&lt;/h4&gt;
&lt;p&gt;ä»¥å¾€æœ‰é¡ä¼¼çš„å·¥ä½œï¼Œä½†ä»–å€‘æ˜¯ç”¨ RNNã€‚&lt;/p&gt;
&lt;h2 id=&#34;the-detr-model&#34;&gt;The DETR model&lt;/h2&gt;
&lt;h4 id=&#34;object-detection-set-prediction-loss&#34;&gt;Object detection set prediction loss&lt;/h4&gt;
&lt;p&gt;DETE æœƒçµ¦ N å€‹å›ºå®šå¤§å°çš„é›†åˆé æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;è¦è§£äºŒåˆ†åœ–åŒ¹é…ï¼Œæœ¬æ–‡ç”¨ scipy çš„ linear_sum_assignment è™•ç†ï¼Œä»–èƒŒå¾Œæ˜¯åŒˆç‰™åˆ©æ¼”ç®—æ³•ã€‚&lt;/p&gt;
&lt;p&gt;å…¶å¯¦é€™ç¨®æ–¹æ³•å’Œ proposals å’Œ anchors æœ‰å·®ä¸å¤šçš„ä½œç”¨ï¼Œå·®åˆ¥åœ¨æ–¼é€™è£¡æœƒæ‰¾ä¸€å°ä¸€çš„åŒ¹é…ï¼Œè€Œä¸ç”¨é‡è¤‡ã€‚&lt;/p&gt;
&lt;p&gt;ç›®æ¨™å‡½æ•¸ï¼š&lt;/p&gt;
&lt;p&gt;$L_{Hungarian}(y, \text{\^{y}}) = \displaystyle\sum^{N}_{i=1} [-log \text{\^{p}} $
$_{\^{\sigma}(i)}(c_i) + \text{1}$
$_\{$
$_{c_i \neq \text{\o}}$
$_\}$
$\mathcal{L}$
$_{\text{box}} (b_i, \text{\^{b}}$
$_{\^{\sigma}}(i))]$&lt;/p&gt;
&lt;p&gt;å‰é¢æ˜¯åˆ†é¡çš„ lossï¼Œå¾Œé¢æ˜¯ bounding box çš„ lossã€‚&lt;/p&gt;
&lt;p&gt;é€™é‚Šæœ‰å…©å€‹æ”¹å‹•ï¼Œç¬¬ä¸€å€‹æ˜¯åˆ†é¡é‚£é‚Šä¸ç”¨ logï¼Œä½¿å€¼å’Œ bounding box çš„ loss æ¯”è¼ƒæ¥è¿‘ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹æ˜¯ bounding box é‚£é‚Šä¸¦ä¸æ˜¯ç”¨æœ€å¸¸è¦‹çš„ L1ï¼Œå› ç‚º L1 å°æ–¼å¤§çš„ç›®æ¨™ loss æ¯”è¼ƒé«˜ï¼Œé€™è£¡é™¤äº† L1 é‚„é¸ç”¨ generalized IoU lossï¼Œå®ƒåœ¨å°ºåº¦ä¸Šèˆ‡ loss ç„¡é—œã€‚&lt;/p&gt;
&lt;h4 id=&#34;detr-architecture&#34;&gt;DETR architecture&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ç”¨ CNN å¾åœ–ç‰‡æŠ½ç‰¹å¾µï¼Œæ‹‰ç›´ï¼Œé¤µçµ¦ Transformer encoder-decoderï¼Œå¾—åˆ°ä¸€çµ„é æ¸¬é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;é€™è£¡ encoder æœ‰åŠ©æ–¼ç‰¹å¾µé–“å½¼æ­¤äº¤äº’ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´çš„æ™‚å€™ï¼Œé æ¸¬çš„æ¡†å’Œ GT åšåŒ¹é…ï¼Œæ²’åŒ¹é…åˆ°çš„å°±æ”¾åˆ° &amp;ldquo;no object&amp;rdquo; classã€‚&lt;/p&gt;
&lt;p&gt;decoder æœƒé¤µå…¥ object queriesï¼Œé€™äº›æ˜¯ learnable positional encodingsã€‚&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablations&#34;&gt;Ablations&lt;/h3&gt;
&lt;h4 id=&#34;number-of-encoder-layers&#34;&gt;Number of encoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…é€éæ”¹è®Š Encoder layer çš„æ•¸é‡ä¾†è©•ä¼° global imagelevel self-attention çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æ¨è«– encoder å¯èƒ½å°æ–¼åˆ¤æ–·åˆ†é–‹å°è±¡å¾ˆé‡è¦ï¼Œåœ– 3 å¯è¦–åŒ–äº†æœ€å¾Œä¸€å€‹ encoder layer çš„ attention mapã€‚&lt;/p&gt;
&lt;p&gt;encoder çœ‹ä¼¼å·²ç¶“åˆ†é›¢äº† instanceï¼Œå¯èƒ½ç°¡åŒ–äº† decoder å°æ–¼ object extraction å’Œ localization çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h4 id=&#34;number-of-decoder-layers&#34;&gt;Number of decoder layers&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig6.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨åœ– 6 åšäº† decoder çš„æ³¨æ„åŠ›å¯è¦–åŒ–ï¼Œå¯ä»¥æ³¨æ„åˆ°è§€å¯Ÿçš„æ³¨æ„åŠ›ç›¸ç•¶å±€éƒ¨ã€‚&lt;/p&gt;
&lt;p&gt;æ¨è«–æ˜¯ encoder ä¸»è¦åˆ†é›¢å¯¦é«”ï¼Œdecoder åªéœ€è¦é—œæ³¨å››è‚¢å³å¯æå–å‡ºå°è±¡çš„é‚Šç•Œå’Œåˆ†é¡ã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/DETR/fig7.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ– 7 æŠŠ 100 å€‹é æ¸¬æ§½ä¸­çš„ 20 å€‹åšå¯è¦–åŒ–ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹é æ¸¬æ¡†ä»£è¡¨ä¸€é»ï¼Œå¯ä»¥æ³¨æ„åˆ°ä¸åŒçš„æ§½ä½æœƒå°ˆæ³¨åœ¨ä¸åŒå€åŸŸã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>BERT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sat, 05 Aug 2023 00:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1810.04805&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ç¾åœ¨å›é ­å¯« BERT è«–æ–‡ç­†è¨˜æ„Ÿè¦ºæœ‰é»æ€ªï¼Œä¹‹å‰å·²ç¶“å¯«éä»€éº¼ RoBERTa ä¹‹é¡çš„ã€‚&lt;/p&gt;
&lt;p&gt;ä¸éç¾åœ¨å› æ‡‰å¯¦é©—å®¤è®€æ›¸æœƒè¦æ±‚ï¼Œé‚„æ˜¯çœ‹ä¸€ä¸‹è«–æ–‡ä¹Ÿå¯«ä¸€ä¸‹ç­†è¨˜ã€‚&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº† BERTï¼Œä¸€ç¨®åŸºæ–¼ Transformer Bidirectional Encoder çš„èªè¨€è¡¨ç¤ºæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;BERT æ—¨åœ¨é€é unlabeled text é€²è¡Œ pretrainã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œåªéœ€è¦ä¸€å€‹é¡å¤–çš„è¼¸å‡ºå±¤å°±å¯ä»¥å°é è¨“ç·´çš„ BERT é€²è¡Œå¾®èª¿ï¼Œåœ¨å„ç¨®ä»»å‹™ä¸Šå–å¾— SOTAã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ã€Œèªè¨€æ¨¡å‹åšé è¨“ç·´ã€å·²è¢«è­‰æ˜å¯ä»¥æœ‰æ•ˆæ”¹å–„å¤šç¨® NLP ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å°‡é è¨“ç·´æ¨¡å‹æ‡‰ç”¨åœ¨ä¸‹æ¸¸ä»»å‹™ï¼Œæœ‰å…©ç¨®ç­–ç•¥ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Feature-based
&lt;ul&gt;
&lt;li&gt;æŠŠ pretrained çš„ representations ä½œç‚ºé¡å¤–çš„ç‰¹å¾µ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tuning
&lt;ul&gt;
&lt;li&gt;æ ¹æ“šç‰¹å®šä»»å‹™å¼•å…¥é¡å¤–åƒæ•¸ï¼Œä¸¦ç°¡å–®åœ°å¾®èª¿æ‰€æœ‰åƒæ•¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é€™å…©ç¨®æ–¹æ³•åœ¨é è¨“ç·´æœŸé–“å…±ç”¨åŒå€‹ objective functionï¼Œä¸¦ç”¨å–®å‘èªè¨€æ¨¡å‹ä¾†å­¸ç¿’ representationã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…èªç‚ºç•¶å‰çš„æŠ€è¡“é™åˆ¶äº†é è¨“ç·´çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œç‰¹åˆ¥æ˜¯åœ¨ Fine-tuning æ–¹æ³•ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦çš„å•é¡Œåœ¨æ–¼èªè¨€æ¨¡å‹æ˜¯å–®å‘çš„ï¼Œé™åˆ¶äº†é è¨“ç·´æœŸé–“å¯ä»¥ä½¿ç”¨çš„æ¶æ§‹çš„é¸æ“‡ã€‚é€™ç¨®å–®å‘çš„æ¶æ§‹å¯èƒ½åœ¨ä¸€äº›ä»»å‹™æœ‰å®³ï¼Œç‰¹åˆ¥æ˜¯å°æ–¼é‚£äº›éœ€è¦å…©å€‹æ–¹å‘çš„ context çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºçš„ BERT æ”¹å–„äº†ç¾æœ‰çš„ Fine-tuning æ–¹æ³•ï¼Œç”¨ Transformer çš„ Bidirectional Encoder ä¾†è¨“ç·´èªè¨€æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;BERT é€éå—åˆ° Cloze taskï¼ˆå¡«ç©ºï¼‰å•Ÿç™¼çš„ masked language model(MLM)ï¼Œä½œç‚ºé è¨“ç·´ç›®æ¨™ã€‚MLM éš¨æ©Ÿåœ°é®è”½ä¸€äº›è¼¸å…¥çš„ä¸€äº› tokenï¼Œç›®æ¨™æ˜¯æ ¹æ“šä¸Šä¸‹æ–‡ä¾†å›æ¨åŸè©ï¼Œä½¿ representation å¯ä»¥èåˆå·¦å³å…©é‚Šçš„ contextã€‚&lt;/p&gt;
&lt;p&gt;é™¤äº† MLMï¼Œä½œè€…é‚„åˆ©ç”¨ next sentence predictionï¼ˆNSPï¼‰ä»»å‹™ä¾†è¨“ç·´ BERTã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è²¢ç»å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BERT è­‰æ˜äº†é›™å‘é è¨“ç·´å° representation çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT å±•ç¾å‡ºé è¨“ç·´çš„ representation æ¸›å°‘äº†è¨±å¤šé‡å° NLP ä»»å‹™ç²¾å¿ƒè¨­è¨ˆæ¶æ§‹çš„éœ€æ±‚ã€‚ BERT æ˜¯ç¬¬ä¸€å€‹åŸºæ–¼ Fine-tuningï¼Œåœ¨å¤§é‡ sentence-level å’Œ token-level ä»»å‹™ä¸Šå–å¾— SOTA çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BERT æ¨é€²äº† 11 å€‹ NLP ä»»å‹™çš„ SOTAã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;unsupervised-feature-based-approaches&#34;&gt;Unsupervised Feature-based Approaches&lt;/h3&gt;
&lt;p&gt;å­¸ç¿’å»£æ³›é©ç”¨çš„ representation of words ä¸€ç›´æ˜¯æ´»èºçš„ç ”ç©¶é ˜åŸŸï¼Œç”šè‡³åœ¨éç¥ç¶“ç¶²è·¯çš„é ˜åŸŸä¹Ÿæ˜¯ã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´çš„ word embeddings èˆ‡å¾é ­è¨“ç·´çš„ embedding ç›¸æ¯”ï¼Œæœ‰é¡¯è‘—æ”¹é€²ã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›æ–¹æ³•é è¢«æ¨å»£åˆ° coarser granularitiesï¼Œåƒæ˜¯ sentence embedding æˆ–æ˜¯ paragraph embeddingã€‚&lt;/p&gt;
&lt;p&gt;æœ‰ç ”ç©¶è­‰æ˜ cloze task æé«˜äº†ç”Ÿæˆæ¨¡å‹çš„ robustnessã€‚&lt;/p&gt;
&lt;h2 id=&#34;bert&#34;&gt;BERT&lt;/h2&gt;
&lt;p&gt;æ¡†æ¶æœ‰å…©æ­¥é©Ÿï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pre-training
&lt;ul&gt;
&lt;li&gt;åœ¨ä¸åŒçš„é è¨“ç·´ä»»å‹™ä¸­ï¼Œç”¨ unlabeled data ä¾† fine-tuneã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Fine-tuning
&lt;ul&gt;
&lt;li&gt;ä½¿ç”¨é è¨“ç·´çš„åƒæ•¸åˆå§‹åŒ–ï¼Œåœ¨åˆ©ç”¨ä¸‹æ¸¸ä»»å‹™çš„ labeled data å°æ‰€æœ‰åƒæ•¸å¾®èª¿ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;BERT çš„ä¸€å€‹ç‰¹é»æ˜¯ä»–å…·å‚™è·¨ä¸åŒä»»å‹™çš„çµ±ä¸€æ¶æ§‹ï¼Œé è¨“ç·´æ¶æ§‹å’Œä¸‹æ¸¸ä»»å‹™æœ€çµ‚æ¶æ§‹å·®ç•°ä¸å¤§ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model Architecture&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æœ¬æ–‡è¡¨ç¤ºæ–¹æ³•&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L: Transformer çš„å±¤æ•¸&lt;/li&gt;
&lt;li&gt;H: hidden size&lt;/li&gt;
&lt;li&gt;A: self-attention heads çš„æ•¸é‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;model size&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BASE: L=12, H=768, A=12, 110M parameters
&lt;ul&gt;
&lt;li&gt;å’Œ GPT ç›¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LARGE: L=24, H=1024, A=16, 340M parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Input/Output Representations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Input representation å¯ä»¥åœ¨ token sequence ä¸­æ˜ç¢ºè¡¨ç¤ºå–®å€‹ sentence å’Œä¸€å° sentenceã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sentence å¯ä»¥æ˜¯é€£çºŒæ–‡æœ¬çš„ä»»æ„ç¯„åœï¼Œè€Œä¸æ˜¯å¯¦éš›çš„å¥å­ã€‚&lt;/li&gt;
&lt;li&gt;sequence æ˜¯è¼¸å…¥çš„ token sequenceï¼Œå¯ä»¥æ˜¯å–®å€‹ sentence æˆ–æ˜¯ä¸€å° sentenceã€‚&lt;/li&gt;
&lt;li&gt;æ¯å€‹ sequence çš„ç¬¬ä¸€å€‹ token å§‹çµ‚æ˜¯ç‰¹æ®Šçš„åˆ†é¡ token &amp;ndash; [CLS]&lt;/li&gt;
&lt;li&gt;å°æ–¼å…©å€‹å¥å­æ”¾åœ¨ä¸€å€‹åºåˆ—çš„æƒ…æ³ï¼Œç”¨ [SEP] éš”é–‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Token Embeddings&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ä½œè€…ä½¿ç”¨ WordPiece embeddingsï¼Œæœ‰ 30000 å€‹è©å½™ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;learned embedding&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å°æ¯å€‹ token æ·»åŠ é€™å€‹æ±è¥¿ï¼Œè¡¨ç¤ºå±¬æ–¼ sentence A é‚„ sentence B&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pre-training-bert&#34;&gt;Pre-training BERT&lt;/h3&gt;
&lt;h4 id=&#34;masked-lm&#34;&gt;Masked LM&lt;/h4&gt;
&lt;p&gt;ç›´è§€ä¸Šï¼Œæœ‰ç†ç”±ç›¸ä¿¡æ·±åº¦çš„é›™å‘æ¨¡å‹æœƒæ¯”å–®åƒä¸²é€£èµ·ä¾†çš„æ·ºå±¤æ¨¡å‹æ›´å¼·å¤§ã€‚&lt;/p&gt;
&lt;p&gt;ä¸å¹¸çš„æ˜¯ standard condition language model åªèƒ½å–®å‘è¨“ç·´ï¼Œå› ç‚ºé›™å‘æœƒå…è¨±æ¯å€‹å–®è©ã€Œé–“æ¥çœ‹åˆ°è‡ªå·±ã€ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è¨“ç·´ deep bidirectional representationsï¼Œæœ¬æ–‡éš¨æ©Ÿé®è”½äº†ä¸€å®šæ¯”ä¾‹çš„ tokensï¼Œä¸¦é æ¸¬é€™äº› tokenï¼Œé€™ç¨®æ–¹æ³•ç¨±ç‚º masked language modelï¼Œæˆ–å¸¸è¢«ç¨±ç‚º cloze taskã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æœƒç”¨ [MASK] åšé è¨“ç·´ï¼Œä½†æœ‰å€‹å•é¡Œæ˜¯ [MASK] åœ¨ fine-tuning æœŸé–“ä¸æœƒå‡ºç¾ï¼Œé€ æˆé è¨“ç·´å’Œå¾®èª¿ä¹‹é–“çš„ mismatchingï¼Œç‚ºäº†ç·©æ¸›é€™ç¨®æƒ…æ³ï¼Œä¸¦ä¸æœƒç¸½æ˜¯ç”¨ [MASK] æ›¿ä»£ masked tokenã€‚&lt;/p&gt;
&lt;p&gt;è¦æ›¿æ› token çš„æ™‚å€™ï¼Œæœ‰ 80% çš„æ™‚é–“æ˜¯ [MASK]ï¼Œ10% æ˜¯éš¨æ©Ÿ tokenï¼Œ10% æ˜¯åŸæœ¬çš„ tokenã€‚&lt;/p&gt;
&lt;h4 id=&#34;next-sentence-prediction-nsp&#34;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;
&lt;p&gt;è¨±å¤šé‡è¦ä¸‹æ¸¸ä»»å‹™ï¼Œæ¯”å¦‚ Question Answering (QA) å’Œ Natural Language Inference (NLI) æ˜¯åŸºæ–¼å…©å€‹å¥å­é–“çš„é—œä¿‚ã€‚&lt;/p&gt;
&lt;p&gt;NSP å°±æ˜¯ç‚ºäº†ç†è§£å¥å­é–“çš„é—œä¿‚è€Œç”¨çš„ã€‚&lt;/p&gt;
&lt;p&gt;æ¯æ¬¡æŒ‘å¥å­ A å’Œ B çš„æ™‚å€™ï¼Œæœ‰ 50% çš„æ©Ÿæœƒ B æ˜¯ A çš„ä¸‹ä¸€å€‹å¥å­ï¼Œæœ‰ 50% æ˜¯éš¨æ©Ÿçš„ã€‚&lt;/p&gt;
&lt;p&gt;é‡å° NSP çš„é è¨“ç·´å° QA å’Œ NLI éƒ½å¾ˆæœ‰ç”¨ã€‚&lt;/p&gt;
&lt;h4 id=&#34;é è¨“ç·´è³‡æ–™&#34;&gt;é è¨“ç·´è³‡æ–™&lt;/h4&gt;
&lt;p&gt;ç”¨ BookCorpus å’Œ English Wikipedia ä¾†è¨“ç·´ BERTã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ English Wikipediaï¼Œåªæå– text passagesï¼Œå¿½ç•¥ lists, tables, headersã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æå–é•·çš„é€£çºŒåºåˆ—ï¼Œç”¨ document-level çš„ corpus è€Œä¸æ˜¯æ‰“äº‚çš„ sentence-level corpus éå¸¸é‡è¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;ablation-studies&#34;&gt;Ablation Studies&lt;/h2&gt;
&lt;h3 id=&#34;effect-of-pre-training-tasks&#34;&gt;Effect of Pre-training Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;No NSP
&lt;ul&gt;
&lt;li&gt;åªæœ‰ MLM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LTR &amp;amp; No NSP
&lt;ul&gt;
&lt;li&gt;Left-to-Right&lt;/li&gt;
&lt;li&gt;åªçœ‹å·¦é‚Šçš„ context&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç™¼ç¾åˆªé™¤ NSP æœƒé¡¯è‘—å‚·å®³å° QNLI ç­‰è³‡æ–™é›†çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;LTR åœ¨æ‰€æœ‰ä»»å‹™ä¸Šéƒ½æ¯” MLM å·®ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶å¯ä»¥åƒ ELMo å–®ç¨è¨“ç·´ LTR å’Œ RTLï¼Œä¸¦ä¸”æŠŠä»–å€‘çµåˆèµ·ä¾†&lt;/p&gt;
&lt;p&gt;ä½†æœ‰ä»¥ä¸‹ç¼ºé»ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ¯”å–®å‘æ¨¡å‹è²´å…©å€&lt;/li&gt;
&lt;li&gt;å° QA ä»»å‹™ä¸ç›´è§€ï¼Œå› ç‚º RTL ç„¡æ³•æ ¹æ“šå•é¡Œçµ¦å‡ºç­”æ¡ˆ&lt;/li&gt;
&lt;li&gt;ä¸å¦‚æ·±åº¦é›™å‘æ¨¡å‹å¼·å¤§ï¼Œå› ç‚ºå…¶å¯ä»¥ç›´æ¥åœ¨æ¯ä¸€å±¤çœ‹åˆ°å·¦å³çš„ context&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-based-approach-with-bert&#34;&gt;Feature-based Approach with BERT&lt;/h3&gt;
&lt;p&gt;ä½œè€…ä¹Ÿç ”ç©¶äº†ç”¨ feature-based çš„æ•ˆæœï¼Œç™¼ç¾å…·å‚™ç«¶çˆ­åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ä»–çš„å¯¦é©—ä¸­ï¼Œç”¨é è¨“ç·´ Transformer çš„ top 4 éš±è—å±¤çš„ token ä¸²è¡—æ•ˆæœæœ€å¥½ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GNN ä»‹ç´¹</title>
        <link>https://roykesydon.github.io/Blog/p/gnn-%E4%BB%8B%E7%B4%B9/</link>
        <pubDate>Fri, 04 Aug 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/gnn-%E4%BB%8B%E7%B4%B9/</guid>
        <description>&lt;h2 id=&#34;åœ–ç°¡ä»‹&#34;&gt;åœ–ç°¡ä»‹&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Graph&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;è¡¨ç¤º Entity (nodes) é–“çš„ relations (edges)&lt;/li&gt;
&lt;li&gt;çµ„æˆ
&lt;ul&gt;
&lt;li&gt;Vertex attributes (V)&lt;/li&gt;
&lt;li&gt;Edge attributes and directions (E)&lt;/li&gt;
&lt;li&gt;Global attributes (U)&lt;/li&gt;
&lt;li&gt;ä¸‹æ–‡ç°¡ç¨± U, V, E&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯ä»¥è¡¨ç¤ºæˆåœ–çš„ç¯„ä¾‹&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Images
&lt;ul&gt;
&lt;li&gt;ç›¸é„° pixel å»ºç„¡å‘é‚Š&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Text
&lt;ul&gt;
&lt;li&gt;è©å’Œä¸‹ä¸€å€‹è©å»ºå–®å‘é‚Š&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Molecules
&lt;ul&gt;
&lt;li&gt;åˆ†å­çš„é€£æ¥è™•å»ºç„¡å‘é‚Š&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Social networks
&lt;ul&gt;
&lt;li&gt;äººå’Œäººä¹‹é–“å»ºç„¡å‘é‚Š&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;å®šç¾©å•é¡Œ&#34;&gt;å®šç¾©å•é¡Œ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ç¨®é¡
&lt;ul&gt;
&lt;li&gt;Graph-level&lt;/li&gt;
&lt;li&gt;Node-level&lt;/li&gt;
&lt;li&gt;Edge-level&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å€‹åˆ¥è¬›çš„æ˜¯åŸºæ–¼ä»€éº¼æ±è¥¿åšåˆ†é¡ï¼Œæ¯”å¦‚å°æ¯å€‹äººï¼ˆnodeï¼‰åˆ†é¡ä¸€å€‹é™£ç‡Ÿï¼Œå°±ç®— Node-level&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;æŒ‘æˆ°&#34;&gt;æŒ‘æˆ°&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;å„²å­˜é‚Šçš„é—œä¿‚
&lt;ul&gt;
&lt;li&gt;é„°æ¥çŸ©é™£
&lt;ul&gt;
&lt;li&gt;åœ¨ç¯€é»å¤šçš„æƒ…æ³ä¸‹ä½”ç”¨ç©ºé–“å¤§ï¼Œè€Œä¸”å¯èƒ½éå¸¸ç¨€ç–&lt;/li&gt;
&lt;li&gt;åŒä¸€å¼µåœ–ï¼Œæ›å€‹é»çš„é †åºå¾Œé„°æ¥çŸ©é™£çœ‹èµ·ä¾†å°±æœƒä¸åŒ
&lt;ul&gt;
&lt;li&gt;é›£ä»¥ä¿è­‰é€™äº›æ±è¥¿é¤µå…¥ç¥ç¶“ç¶²è·¯å¾Œè¼¸å‡ºç›¸åŒ&lt;/li&gt;
&lt;li&gt;å¯ä»¥ç”¨å…©å€‹ listï¼Œä¸€å€‹å„²å­˜é‚Šçš„å‘é‡ï¼Œå¦ä¸€å€‹æ˜¯ Adjacency listï¼Œä¾åºç´€éŒ„é‚Šçš„é—œä¿‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç¨€ç–çŸ©é™£
&lt;ul&gt;
&lt;li&gt;é›£ä»¥ç”¨ GPU é‹ç®—&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;graph-neural-network&#34;&gt;Graph Neural Network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GNN æ˜¯å°åœ–ä¸Šæ‰€æœ‰å±¬æ€§çš„ optimizable transformationï¼Œè€Œä¸”å¯ä»¥ä¿æŒ graph symmetries (permutation invariancesï¼ŒæŠŠ node æ’åºå¾Œçµæœä¸è®Š)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¸‹æ–‡ç”¨çš„ GNN æ˜¯ message passing neural network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;graph-in, graph-out&lt;/li&gt;
&lt;li&gt;ä¸æ”¹è®Šåœ–çš„ connectivity&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;æœ€ç°¡å–®çš„-gnn&#34;&gt;æœ€ç°¡å–®çš„ GNN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;U, V, E å€‹åˆ¥é¤µçµ¦ä¸åŒçš„ MLPï¼Œçµ„æˆä¸€å€‹ GNN çš„ layer
&lt;ul&gt;
&lt;li&gt;MLP å–®ç¨é¤µå…¥æ¯ä¸€å€‹é»ï¼Œä¸è€ƒæ…®é€£æ¥è¨Šæ¯ï¼Œä¿æŒäº† graph symmetries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;é æ¸¬&#34;&gt;é æ¸¬&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å‡å¦‚è¦å°æ¯å€‹é ‚é»åšé æ¸¬ï¼Œæœ€å¾Œå†åŠ å€‹å…¨é€£æ¥å±¤åˆ†é¡&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;pooling&#34;&gt;pooling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å‡å¦‚è¦å°ä¸€å€‹æ²’æœ‰å‘é‡çš„é ‚é»åšé æ¸¬ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ poolingï¼Œè’é›†ç›¸é„°é‚Šå’Œå…¨å±€å‘é‡çš„è³‡è¨Š&lt;/li&gt;
&lt;li&gt;å°æ–¼æ²’æœ‰é ‚é»è³‡è¨Šçš„åœ–ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ pooling layer ç²å–å…¨éƒ¨é»çš„è³‡è¨Šï¼Œå†åšåˆ†é¡&lt;/li&gt;
&lt;li&gt;å°æ–¼æ²’æœ‰é‚Šè³‡è¨Šçš„åœ–ï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥ç”¨ pooling å»å¾ç›¸é„°é»å’Œå…¨å±€å‘é‡ç²å¾—è³‡è¨Š&lt;/li&gt;
&lt;li&gt;å°æ–¼æ²’æœ‰å…¨å±€å‘é‡çš„åœ–ï¼Œæˆ‘å€‘å¯ä»¥ç”¨ pooling å»å¾å…¨éƒ¨çš„é»æˆ–é‚Šç²å¾—è³‡è¨Š&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ç¼ºé™·&#34;&gt;ç¼ºé™·&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ä¸­é–“çš„ layer æ²’æœ‰åˆ©ç”¨åœ–çš„è¨Šæ¯ï¼Œéƒ½æ˜¯å„è‡ªé€²å…¥å„è‡ªçš„ MLP åšè½‰æ›&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;passing-messages&#34;&gt;Passing messages&lt;/h3&gt;
&lt;p&gt;åœ¨åšè½‰æ›å‰ï¼Œå…ˆåšä¸€äº› pooling&lt;/p&gt;
&lt;h4 id=&#34;åŒ¯èšé ‚é»è³‡è¨Š&#34;&gt;åŒ¯èšé ‚é»è³‡è¨Š&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ä¸æ˜¯å–®å–®æŠŠé»å‘é‡é€²è¡Œè½‰æ›ï¼Œè€Œæ˜¯å’Œç›¸é„°çš„é»ä¸€èµ·åš aggregation å¾Œå†åšè½‰æ›
&lt;ul&gt;
&lt;li&gt;å¦‚æœ aggregation æ˜¯åŠ ç¸½ï¼Œå’Œå·ç©æœ‰ä¸€é»åƒï¼Œåªä¸éæ˜¯æ¬Šé‡ä¸€æ¨£çš„ç‰ˆæœ¬&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;åŒ¯èšé ‚é»å’Œé‚Šçš„è³‡è¨Š&#34;&gt;åŒ¯èšé ‚é»å’Œé‚Šçš„è³‡è¨Š&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;å¯ä»¥å…ˆæŠŠé ‚é»åŒ¯èšçµ¦é‚Šï¼Œå†æŠŠé‚ŠåŒ¯èšå›é ‚é»ï¼Œåä¹‹äº¦ç„¶
&lt;ul&gt;
&lt;li&gt;é †åºä¸åŒæœƒå°è‡´ä¸åŒçµæœ&lt;/li&gt;
&lt;li&gt;å…©ç¨®æ–¹æ³•å¯ä»¥ä¸€èµ·åŒæ­¥åšï¼Œäº¤æ›¿æ›´æ–°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;å…¨å±€è³‡è¨Š&#34;&gt;å…¨å±€è³‡è¨Š&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;æ¯æ¬¡ layer åªçœ‹é„°å±…ï¼Œè¦å‚³éåˆ°é çš„é»é ˆè¦èµ°å¾ˆå¤šå±¤&lt;/li&gt;
&lt;li&gt;å°å…¥ master node (context vector)ï¼Œä»–é€£æ¥äº†æ‰€æœ‰çš„é»å’Œé‚Š&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ç›¸é—œä¸»é¡Œ&#34;&gt;ç›¸é—œä¸»é¡Œ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æ¡æ¨£&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;è€ƒé‡åˆ°è¨ˆç®—æ¢¯åº¦å¯èƒ½éœ€è¦å„²å­˜éå¤šçš„ä¸­é–“è³‡è¨Šï¼Œå¯ä»¥è€ƒæ…®æ¡æ¨£ä¸€äº›é»ï¼Œåªåœ¨å­åœ–ä¸Šåšè¨ˆç®—&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Batch&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ¯å€‹é»é„°å±…å„æ•¸ä¸åŒï¼Œä½¿åš batch æˆç‚ºæœ‰æŒ‘æˆ°æ€§çš„å•é¡Œã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inductive Bias&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;graph symmetries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aggregation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç›®å‰æ²’æœ‰ä¸€å€‹æœ€ä½³é¸æ“‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graph Convolutional Network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node æ˜¯æ ¹æ“šé„°å±… node å»åšæŸç¨® aggregateï¼Œäº‹å¾Œå†åšæ›´æ–°&lt;/li&gt;
&lt;li&gt;ç”±æ–¼æ¯æ¬¡éƒ½çœ‹é„°å±…ï¼Œå‡å¦‚æœ‰ k å±¤ï¼Œå¯ä»¥æŠŠåœ–çœ‹åšè§£ n å€‹å­åœ–ï¼Œæ¯å€‹å­åœ–å°±æ˜¯åŸºæ–¼æ¯å€‹é»å»èµ° k æ­¥æ‰€å½¢æˆçš„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graph Attention Network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç”¨ attention æ±ºå®šå…¶å®ƒé»çš„æ¬Šé‡ï¼Œè€Œä¸åƒ GCN ä¸€æ¨£æŠŠé„°å±…åŠ èµ·ä¾†&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>I3D è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/i3d-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 23 Jul 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/i3d-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1705.07750&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç›®å‰çš„å‹•ä½œåˆ†é¡è³‡æ–™é›† (UCF-101 å’Œ HMDB-51) çš„å½±ç‰‡éå¸¸ç¼ºä¹ï¼Œä½¿è¾¨è­˜ã€Œè‰¯å¥½çš„å½±åƒæ¶æ§‹ã€è®Šå¾—å›°é›£ï¼Œ
ä½¿å¤šæ•¸æ–¹æ³•åœ¨ç¾æœ‰çš„å°è¦æ¨¡ benchmark çš„è¡¨ç¾å·®ä¸å¤šã€‚ç‚ºæ­¤æœ¬æ–‡æ ¹æ“šæ–°çš„ Kinetics Human Action Video dataset å° SOTA æ¶æ§‹é€²è¡Œäº†é‡æ–°è©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;Kinetics æœ‰ 400 å€‹äººé¡å‹•ä½œé¡åˆ¥ã€‚æ¯å€‹é¡åˆ¥æœ‰ 400 å€‹ clipã€‚å¾ YouTube ä¸Šç²å–çš„ï¼Œè€Œä¸”æ¯å€‹ clip ä¾†è‡ª unique çš„ youtube å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡åˆ†æäº†ç•¶å‰æ¶æ§‹åœ¨ Kinetics ä¸Šå‹•ä½œåˆ†é¡ä»»å‹™çš„è¡¨ç¾ï¼Œä¹Ÿè©•ä¼° Kinetcis ç”¨ä½œé è¨“ç·´çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºäº†ä¸€ç¨®åŸºæ–¼ 2D ConvNet inflation çš„ Two-Stream Inflated 3D ConvNet (I3D)ã€‚&lt;/p&gt;
&lt;p&gt;I3D çš„æ“´å±•æ–¹æ³•è®“ ImageNet ä¸Šå·²ç¶“å–å¾—æˆåŠŸçš„æ¶æ§‹å¯ä»¥è¢«åˆ©ç”¨åœ¨è§£æ±ºå½±åƒä»»å‹™ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;çµæœè¡¨æ˜ï¼Œç¶“éåœ¨ Kinetics ä¸Šé è¨“ç·´å¾Œï¼ŒI3D åœ¨å‹•ä½œåˆ†é¡æ–¹é¢é¡¯è‘—æé«˜äº† SOTAï¼Œåœ¨ HMDB-51 ä¸Šé”åˆ° 80.9%ï¼Œåœ¨ UCF-101 ä¸Šé”åˆ° 98.0%ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;åœ¨ ImageNet ä¸Šé è¨“ç·´æ¨¡å‹çš„æ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨å½±ç‰‡é ˜åŸŸï¼Œé è¨“ç·´æˆæ•ˆä¸€ç›´æ˜¯ä¸€å€‹æœªçŸ¥çš„å•é¡Œã€‚å› ç‚ºæµè¡Œçš„å‹•ä½œè­˜åˆ¥ benchmark éƒ½éå¸¸å°ï¼Œç´„ç•¥åªæœ‰ 10k å€‹å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;Kinetics æœ‰ 400 å€‹äººé¡å‹•ä½œé¡åˆ¥ã€‚æ¯å€‹é¡åˆ¥æœ‰ 400 å€‹ clipï¼Œè€Œä¸”æ¯å€‹ clip éƒ½ä¾†è‡ªä¸€å€‹ unique çš„ Youtube å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡å¯¦é©—ç­–ç•¥æ˜¯åœ¨ Kinetics ä¸Šé è¨“ç·´ï¼Œå†åœ¨ HMDB-51 å’Œ USC-101 ä¸Šå¾®èª¿ï¼Œçµæœé¡¯ç¤ºå‡ºé è¨“ç·´ç¸½æ˜¯èƒ½æé«˜æ€§èƒ½ï¼Œä½†æå‡å¤šå¯¡å› æ¶æ§‹è€Œç•°ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºæ–°æ¶æ§‹ï¼Œç¨±ç‚ºã€ŒTwo-Stream Inflated 3D ConvNetsã€(I3D)ï¼Œå»ºç«‹åœ¨ SOTA çš„å½±åƒåˆ†é¡æ¶æ§‹ä¸Šï¼Œä¸¦å°‡ filters å’Œ pooling kernel è†¨è„¹æˆ 3Dã€‚&lt;/p&gt;
&lt;p&gt;åŸºæ–¼ Inceptionv1 çš„ I3D åœ¨ Knetics ä¸Šé è¨“ç·´å¾Œï¼Œæ€§èƒ½é è¶…éç•¶å‰çš„ SOTA æ¶æ§‹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬æ–‡çš„æ¨¡å‹ä¸­ï¼Œä¸¦æ²’æœ‰è€ƒæ…®æ›´å¤šç¶“å…¸æ–¹æ³•ï¼Œæ¯”å¦‚ bag-of-visual-words representationï¼Œä½† Kinetics æ˜¯å…¬é–‹çš„ï¼Œå› æ­¤å…¶ä»–äººå¯ä»¥é€²è¡Œå¾ŒçºŒç ”ç©¶ã€‚&lt;/p&gt;
&lt;h2 id=&#34;action-classification-architectures&#34;&gt;Action Classification Architectures&lt;/h2&gt;
&lt;p&gt;ç›®å‰å½±ç‰‡æ¶æ§‹ä¸­çš„ä¸€äº›ä¸»è¦å€åˆ¥å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å·ç©æ˜¯ 2D é‚„ 3D çš„&lt;/li&gt;
&lt;li&gt;æ˜¯å¦åªæ˜¯ RGB å½±ç‰‡ï¼Œé‚„æ˜¯åŒ…å«äº‹å…ˆè¨ˆç®—çš„ optical flow&lt;/li&gt;
&lt;li&gt;å°æ–¼ 2D ConvNetsï¼Œè¨Šæ¯æ˜¯æ€éº¼åœ¨ frame ä¹‹é–“å‚³éçš„
&lt;ul&gt;
&lt;li&gt;é€™éƒ¨åˆ†å¯ä»¥ä½¿ç”¨ temporally-recurrent layersï¼Œæ¯”å¦‚ LSTMï¼Œæˆ–æ˜¯ç”¨éš¨æ™‚é–“çš„ feature aggregation ä¾†å®Œæˆã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œè€ƒæ…®äº†æ¶µè“‹å¤§éƒ¨åˆ†ç¾æœ‰æ¶æ§‹çš„æ¨¡å‹å­é›†ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2D ConvNets
&lt;ul&gt;
&lt;li&gt;é ‚éƒ¨æœ‰ LSTM çš„ ConvNet&lt;/li&gt;
&lt;li&gt;æœ‰å…©ç¨® stream fusion çš„ two-stream networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3D ConvNets
&lt;ul&gt;
&lt;li&gt;C3D&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç”±æ–¼åƒæ•¸ç¶­åº¦è¼ƒé«˜ï¼Œä»¥åŠç¼ºä¹ labeled video dataï¼Œä»¥å‰çš„ 3D ConvNet ç›¸å°è¼ƒæ·ºï¼ˆæœ€å¤š 8 å±¤ï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç™¼ç¾è«¸å¦‚ VGG-16 å’Œ ResNet ç­‰å¾ˆæ·±çš„å½±åƒåˆ†é¡ç¶²è·¯å¯ä»¥è¼•é¬†æ“´å±•æˆ spatio-temporal feature extractorsï¼Œä¸¦ä¸”ä»–å€‘çš„é è¨“ç·´æ¬Šé‡ä¹Ÿå¯ä»¥æä¾›æœ‰åƒ¹å€¼çš„åˆå§‹åŒ–ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ä¹Ÿç™¼ç¾ two-stream çš„ä½œæ³•ä¾ç„¶æœ‰ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;
fig2. K æ˜¯å½±ç‰‡ä¸­çš„ frame çš„ç¸½æ•¸ï¼ŒN æ˜¯ç›¸é„° frames çš„å­é›†åˆã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–æ˜¯æœ¬æ–‡å¯¦é©—çš„äº”ç¨®æ¶æ§‹ï¼Œå‰å››ç¨®æ˜¯ä¹‹å‰çš„åšæ³•ï¼Œæœ€å¾Œä¸€ç¨®æ˜¯æå‡ºçš„æ–°ä½œæ³•ã€‚
ä¸Šåœ–ä¸­é™¤äº† C3D å¤–éƒ½æœ‰ç”¨åˆ° ImageNet é è¨“ç·´çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;æ™‚é–“æ˜¯æ ¹æ“š input çš„ frame æ›ç®—å‡ºä¾†çš„ï¼Œfps æ˜¯ 25ï¼Œé™¤äº† LSTM é‚£å€‹æ¯”è¼ƒç‰¹åˆ¥ï¼Œå› ç‚º LSTM é‚£å€‹æ˜¯æ¯ 5 frame å– 1 frameï¼Œæ‰€ä»¥æ™‚é–“æ˜¯ 5 å€ã€‚&lt;/p&gt;
&lt;h3 id=&#34;ä¹‹å‰çš„åšæ³•&#34;&gt;ä¹‹å‰çš„åšæ³•&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ConvNet+LSTM&lt;/p&gt;
&lt;p&gt;æœ‰ä¸€ç¨®åšæ³•æ˜¯æŠŠæ¯å€‹ frame ç¨ç«‹é¤µçµ¦ 2D Convï¼Œç„¶å¾Œå†æŠŠé æ¸¬åšå½™æ•´ï¼Œç¬¦åˆ bag of words image modeling çš„ç²¾ç¥ï¼Œä½†é€™æ¨£æœƒå¿½ç•¥æ™‚é–“çµæ§‹ä¸Šçš„è³‡è¨Šï¼Œæ¯”å¦‚ç„¡æ³•åˆ¤æ–·é–‹é–€æˆ–é—œé–€ã€‚&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥æœ€å¥½åœ¨å¾Œé¢åŠ ä¸€å€‹ recurrent layerï¼Œæ‰€ä»¥é€™é‚Šå°±ç”¨ Inception-V1 çµåˆ LSTMã€‚&lt;/p&gt;
&lt;p&gt;åŸå§‹çš„å½±ç‰‡ stream æ˜¯ 25 fpsï¼Œé€™é‚Šæ¯ 5 frame æ¡æ¨£ä¸€æ¬¡ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3D ConvNets&lt;/p&gt;
&lt;p&gt;å’Œä¸€èˆ¬çš„å·ç©ç¥ç¶“ç¶²è·¯å·®ä¸å¤šï¼Œåªæ˜¯å…·æœ‰ spatio-temporal filtersã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç”±æ–¼é¡å¤–çš„ kernel ç¶­åº¦ï¼Œç›¸æ¯” 2D Conv æœƒæœ‰æ›´å¤šåƒæ•¸ï¼Œä¹Ÿä½¿ä»–å€‘æ›´é›£è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”é€™æ¨£æœƒç„¡æ³•ç™¼æ® ImageNet é è¨“ç·´çš„å¥½è™•ï¼Œå› æ­¤ä¹‹å‰çš„å·¥ä½œéƒ½å®šç¾©äº†ç›¸å°æ·ºå±¤çš„æ¶æ§‹ï¼Œä¸¦ä¸”å¾é ­è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;benchmark ä¸­çš„è¡¨ç¾å‚™å—æœŸå¾…ï¼Œä½†å’Œ SOTA æ¯”æ²’æœ‰ç«¶çˆ­åŠ›ï¼Œä¹Ÿå› æ­¤æˆç‚ºæœ¬æ–‡å¯¦é©—çš„è‰¯å¥½å€™é¸è€…ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç”¨çš„æ˜¯ C3D çš„å°è®Šé«”ï¼Œå·®ç•°åœ¨æ–¼æ‰€æœ‰å·ç©å±¤å’Œ FC å±¤çš„å¾Œé¢éƒ½ç”¨äº† BNã€‚
è€Œä¸”åœ¨ç¬¬ä¸€å€‹ pooling layer ç”¨çš„ stride æ˜¯ 2ï¼Œå¥½æ¸›å°‘è¨˜æ†¶é«”çš„ä½¿ç”¨ï¼Œæ¯”ç”¨æ›´å¤§çš„ batchï¼Œé€™åœ¨ BN ä¸­éå¸¸é‡è¦ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two-Stream Networks&lt;/p&gt;
&lt;p&gt;Royï¼šé€™è£¡ç”±æ–¼æ¯”è¼ƒè¤‡é›œï¼Œæˆ‘è¦æ”¹æ two-stream çš„åŸå§‹è«–æ–‡ï¼ˆ&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2199&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;ï¼‰èªªæ˜é€™æ±è¥¿æ˜¯ä»€éº¼&lt;/p&gt;
&lt;p&gt;ç°¡è€Œè¨€ä¹‹å°±æ˜¯åˆ†æˆå…©å€‹éƒ¨åˆ†ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ç©ºé–“è³‡è¨Šï¼š&lt;/p&gt;
&lt;p&gt;ç”¨å½±ç‰‡çš„ä¸€å€‹ frameã€€ç¶“éå·ç©ç¥ç¶“ç¶²è·¯é”æˆï¼Œé€™å€‹ frame ç”¨ä¾†æå–å½±åƒä¸­çš„ç‰©ä»¶è³‡è¨Šï¼Œæ¯”å¦‚æ‰“æ’çƒé€™å‹•ä½œå¯èƒ½è¾¨è­˜å‡ºæ’çƒå°±éå¸¸å¥½åˆ¤å®šï¼Œæ‰€ä»¥ç”¨æŸå€‹ frame ä¾†æå–ç©ºé–“è³‡è¨Šã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å‹•ä½œè³‡è¨Šï¼š&lt;/p&gt;
&lt;p&gt;é€™é‚Šç”¨ä¸€é€£ä¸²çš„å…‰æµï¼ˆoptical flowï¼‰åœ–ä¾†é”æˆï¼Œå…‰æµæ˜¯ç‰©é«”ï¼ˆpixelï¼‰åœ¨å…©å€‹ frame é–“çš„ä½ç§»å‘é‡ï¼Œä¼°è¨ˆæ–¹æ³•æœ‰å¾ˆå¤šï¼Œé€™è£¡ä¸ä¸€ä¸€èˆ‰ä¾‹ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/ex-fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–å‡ºè‡ª &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1406.2199&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/a&gt;ï¼Œåœ– c å°±æ˜¯å…‰æµï¼Œå…·æœ‰å…©å€‹æ–¹å‘ï¼ŒæŒ‡å‡ºåƒç´ çš„ä½ç§»ï¼Œåœ– d æ˜¯æ°´å¹³æ–¹å‘çš„è¦–è¦ºåŒ–ï¼Œåœ– e æ˜¯å‚ç›´æ–¹å‘çš„è¦–è¦ºåŒ–ã€‚&lt;/p&gt;
&lt;p&gt;å†æŠŠé€™äº›å…‰æµåœ–é¤µçµ¦å·ç©ç¥ç¶“ç¶²è·¯ï¼Œç”¨ä½œå‹•ä½œè³‡è¨Šçš„åˆ¤åˆ¥ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å€¼å¾—ä¸€æçš„æ˜¯ä»–æ˜¯ late fusionï¼Œè€Œä¸”æ˜¯ç”¨åŠ æ¬Šå¹³å‡ï¼Œä¸æ˜¯åƒä¸€èˆ¬æƒ³çš„æŠŠç‰¹å¾µçµåˆå†åšå…¶ä»–è™•ç†ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-new-two-stream-inflated-3d-convnets&#34;&gt;The New: Two-Stream Inflated 3D ConvNets&lt;/h3&gt;
&lt;p&gt;ä½œè€…æŠŠæˆåŠŸçš„ 2D åˆ†é¡æ¨¡å‹ç°¡å–®åœ°è½‰æ›ç‚º 3D&lt;/p&gt;
&lt;h4 id=&#34;inflating&#34;&gt;Inflating&lt;/h4&gt;
&lt;p&gt;åšæ³•æ˜¯æŠŠæ–¹å½¢çš„ filter æ”¹æˆç«‹æ–¹é«”ï¼ŒæŠŠ N x N çš„ filter æ”¹æˆ N x N x N çš„ filterï¼Œä½†é€™åªæœ‰æ¶æ§‹ä¸Šçš„åƒè€ƒã€‚&lt;/p&gt;
&lt;h4 id=&#34;bootstraping&#34;&gt;Bootstraping&lt;/h4&gt;
&lt;p&gt;æŠŠæ¬Šé‡ä¹Ÿçµ¦è½‰æ›åˆ° 3D æ¶æ§‹çš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…è§€å¯Ÿåˆ°å½±åƒå¯ä»¥é€éåè¦†è¤‡è£½è²¼ä¸Šä¾†ç”Ÿå‡ºä¸€å€‹ã€Œä¸æœƒå‹•çš„ç„¡èŠå½±ç‰‡ã€ï¼Œ
é€éé€™äº›å½±ç‰‡ï¼Œ3D æ¨¡å‹å¯ä»¥é€éé€™ç¨®æ–¹å¼åœ¨ ImageNet ä¸Š implicitly pretrainï¼Œåšæ³•å°±æ˜¯è®“ 3D filter åƒç„¡èŠå½±ç‰‡çš„è¼¸å‡ºå’Œ 2D filter åƒå–®ä¸€ frame çš„è¼¸å‡ºç›¸åŒï¼Œåšæ³•å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘å¯ä»¥æ²¿æ™‚é–“ç¶­åº¦é‡è¤‡ 2D filter N æ¬¡ï¼ŒæŠŠé€™æ¬Šé‡çµ¦ 3D filterï¼ŒåŒæ™‚æŠŠæ¬Šé‡é™¤ä»¥ Nï¼Œé”åˆ°é€™ç¨®æ•ˆæœã€‚&lt;/p&gt;
&lt;h4 id=&#34;pacing-receptive-field-growth-in-space-time-and-network-depth&#34;&gt;Pacing receptive field growth in space, time and network depth&lt;/h4&gt;
&lt;p&gt;ä»¥å¾€åœ¨åœ–ç‰‡ä¸Šå°æ°´å¹³å’Œå‚ç›´è»¸çš„å°å¾…æ˜¯å¹³ç­‰çš„ï¼Œpooling kernel å’Œ stride éƒ½ä¸€æ¨£ã€‚
ä½¿æ„Ÿå—é‡åœ¨å…©å€‹ç¶­åº¦ä¸Šéš¨è‘—æ¨¡å‹è¶Šä¾†è¶Šæ·±ï¼Œæ…¢æ…¢å¹³ç­‰å¢é•·ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯æ™‚é–“è»¸ç”¨å°ç¨±çš„æ„Ÿå—é‡ä¸ä¸€å®šæœ€å¥½ï¼Œè€Œè©²å–æ±ºæ–¼ frame rate å’Œ image dimensinosã€‚
å¦‚æœæ™‚é–“ç›¸å°æ–¼ç©ºé–“å¢é•·å¤ªå¿«ï¼Œå¯èƒ½æœƒæ··æ·†ä¸åŒå°è±¡çš„é‚Šç·£ï¼Œå½±éŸ¿æ—©æœŸçš„ç‰¹å¾µæª¢æ¸¬ã€‚å¦‚æœå¢é•·å¤ªæ…¢ï¼Œå¯èƒ½ç„¡æ³•å¾ˆå¥½åœ°æ•æ‰å ´æ™¯å‹•æ…‹ã€‚&lt;/p&gt;
&lt;p&gt;å¯¦é©—ä¸­ï¼Œè¼¸å…¥å½±ç‰‡çš„ fps æ˜¯ 25ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾åœ¨å‰å…©å€‹ max pooling layer ä¸åœ¨æ™‚é–“è»¸ poolingï¼ˆé€éç”¨ 1 x 3 x 3 çš„ kernelï¼Œä¸¦ä¸”æ™‚é–“è»¸çš„ stride æ˜¯ 1ï¼‰ï¼Œä¸¦åœ¨å…¶ä»– max pooling layer éƒ½ç”¨ symmetric kernels å’Œ stride æ˜¯æœ‰å¹«åŠ©çš„ã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œçš„ average pooling layer æ˜¯ç”¨ 2 x 7 x 7 çš„ kernelã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ 64 frame è¨“ç·´ï¼Œä½†ç”¨æ•´å€‹å½±ç‰‡æ¸¬è©¦ã€‚ï¼ˆaveraging predictions temporallyï¼‰&lt;/p&gt;
&lt;p&gt;æˆ‘æƒ³äº†ä¸€ä¸‹ï¼Œ250 / 64 é™¤ä¸é€²ï¼Œä½†æ˜¯æˆ‘çœ‹ code ç™¼ç¾ä»–å¥½åƒå¯¬é«˜ 224 * 224 çš„ç…§ç‰‡æœƒåœ¨æœ€å¾Œç¶“é Average pool å¾Œè®Šæˆ 1 * 1ï¼Œæ‰€ä»¥ä»–å¯ä»¥ç›´æ¥ç”¨ 1 * 1 * 1 çš„å·ç©æ ¸æŠŠè¼¸å…¥é€šé“æ”¹æˆåˆ†é¡æ•¸ï¼Œå†æŠŠæ™‚é–“è»¸çš„çµæœå¹³å‡ã€‚&lt;/p&gt;
&lt;h4 id=&#34;two-3d-streams&#34;&gt;Two 3D Streams&lt;/h4&gt;
&lt;p&gt;åˆ†åˆ¥è¨“ç·´å…©å€‹ç¶²è·¯ï¼Œä¸¦åœ¨æ¸¬è©¦éšæ®µå°é æ¸¬é€²è¡Œå¹³å‡ã€‚&lt;/p&gt;
&lt;p&gt;é€™é‚Šä½œè€…èªªå…‰æµçš„æ¼”ç®—æ³•æŸç¨®æ„ç¾©ä¸Šæ˜¯ recurrentï¼ˆä¾‹å¦‚ï¼Œå°æ–¼ flow fields é€²è¡Œ iterative optimizationï¼‰ï¼Œæˆ‘ä¸å¤ªæ‡‚é€™é‚Šæ˜¯ä»€éº¼æ„æ€ï¼Œæˆ‘æƒ³ä½œè€…ç”¨çš„å…‰æµæ¼”ç®—æ³•æ‡‰è©²æ˜¯é€éæŸç¨®é¡ä¼¼ EM æ¼”ç®—æ³•é‚£ç¨®ä¸æ–·è¿­ä»£å»é€¼è¿‘æ•¸å€¼çš„æ¼”ç®—æ³•ï¼Œä½†ä½œè€…æåˆ°ã€Œæˆ–è¨±æ˜¯å› ç‚ºç¼ºä¹ recurrenceï¼Œæˆ‘å€‘ç™¼ç¾é›™æµæœ‰åƒ¹å€¼ã€ï¼Œæˆ‘ä¸å¤ªæ‡‚ç‚ºä»€éº¼éœ€è¦ recurrence æ•ˆæœæ‰æœƒå¥½ã€‚&lt;/p&gt;
&lt;p&gt;ä½†çµè«–æ˜¯ two-stream ä¾ç„¶å…·å‚™åƒ¹å€¼ã€‚&lt;/p&gt;
&lt;h4 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h4&gt;
&lt;p&gt;é€™é‚Šè¬›æ»¿è©³ç´°çš„ï¼Œæœ‰èˆˆè¶£å¯ä»¥å»åŸæ–‡çœ‹ã€‚
åªæä¸€ä¸‹å¹¾é»:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å…‰æµæ¼”ç®—æ³•æ˜¯ç”¨ TV-L1ã€‚&lt;/li&gt;
&lt;li&gt;é™¤äº†é¡ä¼¼ C3D çš„ 3D ConvNet éƒ½ç”¨ä½¿ç”¨ ImageNet é è¨“ç·´çš„ Inception-V1 ä½œç‚º base networkã€‚&lt;/li&gt;
&lt;li&gt;å°æ–¼è¼ƒçŸ­çš„å½±ç‰‡ï¼Œæœƒé‡è¤‡å¾ªç’°ä»¥æ»¿è¶³æ¨¡å‹çš„è¼¸å…¥ä»‹é¢&lt;/li&gt;
&lt;li&gt;æ¸¬è©¦æ™‚æœƒåœ¨ä¸­é–“å‰ªè£ 224 x 224&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-kinetics-human-action-video-dataset&#34;&gt;The Kinetics Human Action Video Dataset&lt;/h2&gt;
&lt;p&gt;Kinetics æœ‰ 400 å€‹äººé¡å‹•ä½œé¡åˆ¥ã€‚æ¯å€‹é¡åˆ¥æœ‰ 400 å€‹ clipï¼Œè€Œä¸”æ¯å€‹ clip éƒ½ä¾†è‡ªä¸€å€‹ unique çš„ Youtube å½±ç‰‡ï¼Œå…±æœ‰ 24 è¬å€‹è¨“ç·´å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹ clip éƒ½å¤§ç´„ 10 ç§’ï¼Œè€Œä¸”æ²’æœ‰æœªå‰ªçš„å½±ç‰‡ã€‚&lt;/p&gt;
&lt;p&gt;æ¸¬è©¦é›†æ¯å€‹ class åŒ…å« 100 å€‹ clipã€‚&lt;/p&gt;
&lt;h2 id=&#34;experimental-comparison-of-architectures&#34;&gt;Experimental Comparison of Architectures&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table2and3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;I3D åœ¨æ‰€æœ‰è³‡æ–™é›†ä¸Šéƒ½è¡¨ç¾æœ€å¥½ï¼Œç”šè‡³æ˜¯åœ¨ UCF-101 å’Œ HMDB-51 é€™ç¨®å°è³‡æ–™é›†ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ï¼Œé€™æ„å‘³è‘— ImageNet é è¨“ç·´çš„å¥½è™•æœ‰æˆåŠŸæ“´å±•åˆ° 3D ConvNetã€‚&lt;/p&gt;
&lt;p&gt;å¤šæ•¸æ¨¡å‹åœ¨ UCF ä¸Šéƒ½è¡¨ç¾å¾—æ¯” Kinetics ä¸Šå¥½ï¼Œé¡¯ç¾å‡ºè³‡æ–™é›†çš„é›£åº¦å·®è·ã€‚&lt;/p&gt;
&lt;p&gt;ä½†æ˜¯åœ¨ HMDB è¡¨ç¾å¾—è¼ƒå·®ï¼ŒåŸå› å¯èƒ½æ˜¯ HMDB æ•…æ„å¼„å¾—å¾ˆé›£ï¼Œä½œè€…æœ‰èˆ‰ä¾‹ï¼Œå¾ˆå¤š clip åœ¨å®Œå…¨ç›¸åŒçš„å ´æ™¯æœƒæœ‰ä¸åŒçš„å‹•ä½œã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æœ‰æåˆ°èªª I3D ç‰¹å¾µæ¯”è¼ƒå¥½é·ç§»çš„ä¸€ç¨®è§£é‡‹æ˜¯å®ƒå…·å‚™ high temporal resolutionï¼Œ
I3D åœ¨ 25 fps çš„å½±ç‰‡ä¸­ç”¨ 64 frames åšè¨“ç·´ï¼Œä½¿å®ƒèƒ½æ•æ‰å‹•ä½œçš„ fine-grained æ™‚é–“çµæ§‹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;experimental-evaluation-of-features&#34;&gt;Experimental Evaluation of Features&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/i3d/table4and5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Kinetics ä¸Šåšé è¨“ç·´æ•ˆæœæ˜é¡¯æ¯” ImageNet å¥½ã€‚&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Kinetics ä¸Šçš„é è¨“ç·´å°æ–¼é·ç§»å­¸ç¿’æœ‰æ˜é¡¯å¥½è™•ï¼Œä½†å°æ–¼å…¶ä»–å½±åƒä»»å‹™ï¼Œæ¯”å¦‚å½±åƒèªç¾©åˆ†å‰²æ˜¯å¦æœ‰å¥½è™•ä»å¾…è§€å¯Ÿã€‚&lt;/p&gt;
&lt;p&gt;ç›®å‰å°æ–¼æ¶æ§‹æ²’æœ‰å…¨é¢æ¢ç´¢ï¼Œæ¯”å¦‚æ²’æœ‰æ¡ç”¨ action tubes æˆ–æ˜¯ attention æ©Ÿåˆ¶ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Self-Instruct è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 30 Apr 2023 00:00:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/self-instruct-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2212.10560&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Instruct: Aligning Language Model with Self Generated Instructions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;å¤§å‹ &amp;ldquo;instruction-tuned&amp;rdquo; èªè¨€æ¨¡å‹ (ç¶“éå¾®èª¿å¥½å›æ‡‰ instruction) å·²ç¶“å±•ç¾å‡ºåœ¨æ–°ä»»å‹™ä¸Š zero-shot çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œä»–å€‘åš´é‡ä¾è³´äººå·¥ç·¨å¯«çš„æŒ‡ä»¤ï¼Œåœ¨æ•¸é‡ã€å¤šæ¨£æ€§å’Œå‰µé€ åŠ›ä¸Šéƒ½å—åˆ°äº†é™åˆ¶ï¼Œé˜»ç¤™äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä»‹ç´¹äº† Self-Instruct é€™å€‹æ¡†æ¶ï¼Œå¯ä»¥é€éè‡ªå·±ç”Ÿæˆçš„æŒ‡ä»¤ï¼Œä¾†å¢å¼·é è¨“ç·´æ¨¡å‹éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;å°‡ä½œè€…çš„æ–¹æ³•æ‡‰ç”¨åœ¨ GPT3ï¼Œåœ¨ SuperNaturalInstructions ç²å¾—äº†æ¯”åŸå§‹æ¨¡å‹é«˜ 33% çš„æ”¹é€²ï¼Œèˆ‡ä½¿ç”¨ private user data å’Œ human annotations çš„ $InstructGPT_{001}$ æ€§èƒ½ç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥è©•ä¼°ï¼Œæˆ‘å€‘ç‚ºæ–°ä»»å‹™æ•´ç†ä¸€çµ„å°ˆå®¶ç·¨å¯«çš„æŒ‡ä»¤ï¼Œä¸¦é€šéäººå·¥è©•ä¼°ï¼Œé¡¯ç¤ºå‡ºä½¿ç”¨ Self-Instruction èª¿æ•´ GPT3 çš„æ€§èƒ½å¤§å¤§å„ªæ–¼ä½¿ç”¨ç¾æœ‰å…¬å…±æŒ‡ä»¤è³‡æ–™é›†ï¼Œåªæ¯” $InstructGPT_{001}$ è½å¾Œ 5% çš„å·®è·ã€‚&lt;/p&gt;
&lt;p&gt;Self-Instruct æä¾›ä¸€å€‹å¹¾ä¹ annotation-free çš„æ–¹æ³•ï¼Œalign é è¨“ç·´æ¨¡å‹å’Œ instructionsï¼Œè€Œä¸”ä½œè€…é‡‹å‡ºäº†ä»–å€‘çš„å¤§å‹åˆæˆè³‡æ–™é›†ï¼Œä»¥ä¿ƒé€²æœªä¾†å° instruction tuning çš„ç ”ç©¶ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;æœ€è¿‘çš„ NLP æ–‡ç»è¦‹è­‰äº†ã€Œå»ºæ§‹å¯ä»¥éµå¾ªè‡ªç„¶èªè¨€æŒ‡ä»¤çš„æ¨¡å‹æ–¹é¢ã€çš„å¤§é‡æ´»å‹•ã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›ç™¼å±•ç”±å…©å€‹é—œéµéƒ¨åˆ†çµ„æˆï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å¤§å‹é è¨“ç·´èªè¨€æ¨¡å‹ (LM)&lt;/li&gt;
&lt;li&gt;äººå·¥ç·¨å¯«çš„æŒ‡ä»¤è³‡æ–™&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PromptSource å’Œ SuperNaturalInstructions æ˜¯æœ€è¿‘å…©å€‹è‘—åçš„è³‡æ–™é›†ã€‚
ä»–å€‘é€éå¤§é‡æ‰‹å‹•è¨»é‡‹ä¾†æ”¶é›†æŒ‡ä»¤ï¼Œä»¥å»ºé€  T0 å’Œ T$k$-Instructã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™éç¨‹ä»£åƒ¹é«˜æ˜‚ï¼Œè€Œä¸”ç”±æ–¼å¤§å¤šæ•¸äººå¾€å¾€ç”Ÿæˆçš„éƒ½æ˜¯æµè¡Œçš„ NLP ä»»å‹™ï¼Œä½¿å…¶æœªèƒ½æ¶µè“‹çœŸæ­£å¤šæ¨£çš„ä»»å‹™ï¼Œä¹Ÿä¸èƒ½æ¶µè“‹å„ç¨®æè¿°ä»»å‹™çš„ä¸åŒæ–¹å¼ï¼Œå› æ­¤å¤šæ¨£æ€§å—ä¾·é™ã€‚&lt;/p&gt;
&lt;p&gt;é‘’æ–¼é€™äº›é™åˆ¶ï¼Œæƒ³è¦ç¹¼çºŒæå‡ instruction-tuned models çš„å“è³ªï¼Œéœ€è¦å¹« supervising instruction-tuned models ç™¼å±•æ›¿ä»£æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ä»‹ç´¹äº† Self-Instructï¼Œé€™æ˜¯ä¸€ç¨® semi-automated çš„éç¨‹ï¼Œç”¨æ¨¡å‹è‡ªèº«çš„ instructional signals å° pretrained LM é€²è¡Œ instruction-tuningã€‚&lt;/p&gt;
&lt;p&gt;æ•´å€‹æµç¨‹æ˜¯ä¸€ç¨® iterative bootstrapping algorithmï¼Œå¾æ‰‹å‹•ç·¨å¯«çš„ limited seed set å¼•å°ç”Ÿæˆã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ç¬¬ä¸€éšæ®µï¼Œæ¨¡å‹è¦å¹«æ–°ä»»å‹™ç”ŸæˆæŒ‡ä»¤ã€‚
åˆ©ç”¨ç¾æœ‰çš„æŒ‡ä»¤é›†åˆï¼Œå‰µå»ºæ›´å»£æ³›çš„æŒ‡ä»¤ï¼Œå¥½å®šç¾© (é€šå¸¸æ˜¯æ–°çš„) ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼æ–°ç”Ÿæˆçš„æŒ‡ä»¤é›†ï¼Œæ¡†æ¶ç‚ºä»–å€‘å‰µå»º input-output instancesï¼Œç¨å¾Œå¯ä»¥é€é supervising ç”¨æ–¼ instruction tuningã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œï¼Œé€éå„ç¨®æ‰‹æ®µï¼Œåœ¨ä½å“è³ªå’Œé‡è¤‡çš„æŒ‡ä»¤åŠ åˆ° task pool å‰ï¼ŒæŠŠä»–å€‘ä¿®å‰ªæ‰ã€‚&lt;/p&gt;
&lt;p&gt;å¯ä»¥é‡è¤‡é€™å€‹æµç¨‹éå¸¸å¤šæ¬¡ï¼Œç›´åˆ°ç²å¾—å¤§é‡ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;è©²æ¨¡å‹çš„è·Œä»£éç¨‹ä¸­ç”¢ç”Ÿäº†å¤§ç´„ 52K å€‹æŒ‡ä»¤ï¼Œèˆ‡å¤§ç´„ 85K å€‹ instance inputs å’Œ target outputs é…å° (æœ‰äº›ç›¸åŒçš„æŒ‡ä»¤æœƒå°æ‡‰å¤šç¨®è¼¸å…¥è¼¸å‡º)ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…è§€å¯Ÿåˆ°ç”Ÿæˆçš„è³‡æ–™æä¾›äº†å„ç¨®æœ‰å‰µæ„çš„ä»»å‹™ï¼Œå…¶ä¸­è¶…é 50% çš„ä»»å‹™å’Œ seed instructions çš„ ROUGE-L overlap å°æ–¼ 0.3ã€‚&lt;/p&gt;
&lt;p&gt;åŸºæ–¼ä¸Šè¿°çµæœï¼Œä½œè€…é€šéå¾®èª¿ GPT3 (å’Œç”ŸæˆæŒ‡ä»¤è³‡æ–™æ˜¯åŒå€‹æ¨¡å‹) å»ºæ§‹äº† $GPT3_{SELF-INST}$ã€‚&lt;/p&gt;
&lt;p&gt;SuperNI çš„çµæœè¡¨æ˜ï¼Œ$GPT3_{SELF-INST}$ æ€§èƒ½å¤§å¤§å„ªæ–¼ GPT3 (åŸå§‹æ¨¡å‹)ï¼Œé«˜äº† 33.1%ï¼Œå¹¾ä¹å’Œ $InstructGPT_{001}$ çš„æ€§èƒ½ç›¸ç•¶ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…åœ¨æ–°å‰µå»ºçš„çš„æŒ‡ä»¤é›†ä¸Šé€²è¡Œäººå·¥è©•ä¼°ï¼Œ$GPT3_{SELF-INST}$ é¡¯ç¤ºå‡ºå»£æ³›çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå„ªæ–¼åœ¨å…¶ä»–å…¬é–‹å¯ç”¨æŒ‡ä»¤æ•¸æ“šé›†ä¸Šè¨“ç·´çš„æ¨¡å‹ï¼Œåªæ¯” InstrcutGPT001 è½å¾Œ 5%ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è²¢ç»ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Self-Instructï¼šä¸€ç¨®ç”¨æœ€å°‘çš„äººå·¥æ¨™è¨˜æ•¸æ“šå¼•å°æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„ä½œæ³•&lt;/li&gt;
&lt;li&gt;é€šéå¤§é‡çš„ instruction-tuning å¯¦é©—ï¼Œè­‰æ˜äº†æœ‰æ•ˆæ€§ã€‚&lt;/li&gt;
&lt;li&gt;ç™¼å¸ƒäº†ä¸€å€‹åŒ…å« 52K æŒ‡ä»¤çš„å¤§å‹ç¶œåˆè³‡æ–™é›†ï¼Œé‚„æœ‰ä¸€çµ„æ‰‹å‹•ç·¨å¯«çš„æ–°ä»»å‹™ï¼Œç”¨æ–¼å»ºæ§‹å’Œè©•ä¼°æœªä¾†çš„ instruction-following modelsã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;instruction-following-language-models&#34;&gt;Instruction-following language models&lt;/h3&gt;
&lt;p&gt;ä¸€ç³»åˆ—å·¥ä½œé¡¯ç¤ºï¼Œä½¿ç”¨ annotated &amp;ldquo;instructional&amp;rdquo; dataï¼Œå¯ä»¥ä½¿æ™®é€šèªè¨€æ¨¡å‹éµå¾ªä¸€èˆ¬èªè¨€çš„æŒ‡ä»¤ã€‚&lt;/p&gt;
&lt;p&gt;ä¹Ÿé¡¯ç¤ºå‡º &amp;ldquo;instructional&amp;rdquo; data çš„å¤§å°å’Œå¤šæ¨£æ€§ç›´æ¥å½±éŸ¿æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡çš„å·¥ä½œç›®çš„åœ¨æ¸›å°‘å°äººå·¥è¨»é‡‹è€…çš„ä¾è³´ã€‚&lt;/p&gt;
&lt;h3 id=&#34;language-models-for-data-generation-and-augmentation&#34;&gt;Language models for data generation and augmentation&lt;/h3&gt;
&lt;p&gt;è¨±å¤šå·¥ä½œä¾è³´ç”Ÿæˆå¼ LM ä¾†ç”Ÿæˆæ•¸æ“šæˆ–åš augmentationã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ä½œè€…çš„å·¥ä½œå¯è¢«è¦–ç‚ºä¸€ç¨® augmentationï¼Œä½†å’Œé€™äº›å·¥ä½œçš„å·®åˆ¥åœ¨æ–¼ä¸é™æ–¼ç‰¹å®šä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;Self-Instruct çš„ä¸€å€‹æ˜é¡¯å‹•æ©Ÿæ˜¯å¼•å°å‡ºæ–°çš„ä»»å‹™å®šç¾©ï¼Œè€Œé€™äº›ä»»å‹™å¯èƒ½é‚„æœªè¢« NLP çš„ç ”ç©¶è€…å®šç¾©éã€‚&lt;/p&gt;
&lt;h3 id=&#34;self-training&#34;&gt;Self-training&lt;/h3&gt;
&lt;p&gt;ä¸€ç¨®å…¸å‹çš„ self-training æ¡†æ¶é€éç¶“éè¨“ç·´çš„æ¨¡å‹ï¼Œå¹« unlabeled è³‡æ–™é€²è¡Œ labelï¼Œç„¶å¾Œç”¨é€™äº›è³‡æ–™æ”¹é€²æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ Self-Instruct å’Œ self-training æœ‰ä¸€äº›ç›¸ä¼¼ä¹‹è™•ï¼Œä½†å¤šæ•¸ self-training çš„æ–¹æ³•éƒ½å‡è¨­äº†ä¸€å€‹ç‰¹å®šçš„ç›®æ¨™ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSelf-Instruct å¾é ­é–‹å§‹ç”Ÿå‡ºå„ç¨®ä»»å‹™ã€‚&lt;/p&gt;
&lt;h3 id=&#34;knowledge-distillation&#34;&gt;Knowledge distillation&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;é€™é‚Šæˆ‘æƒ³ä¸å¤ªé€šç‚ºä»€éº¼å¯ä»¥å’Œ Knowledge distillation æ‰¯ä¸Šé—œä¿‚&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Knowledge distillation é€šå¸¸æ¶‰åŠçŸ¥è­˜å¾è¼ƒå¤§æ¨¡å‹åˆ°è¼ƒå°æ¨¡å‹çš„è½‰ç§»&lt;/p&gt;
&lt;p&gt;Self-Instruct ä¹Ÿå¯ä»¥çœ‹åšæ˜¯ Knowledge distillation çš„ä¸€ç¨®å½¢å¼ï¼Œä½†å€åˆ¥å¦‚ä¸‹&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;distillation çš„ä¾†æºå’Œç›®æ¨™æ˜¯ç›¸åŒçš„ï¼Œå³æ¨¡å‹çš„çŸ¥è­˜è¢« distill åˆ°ä»–è‡ªå·±&lt;/li&gt;
&lt;li&gt;distill çš„å…§å®¹ä»¥ instruction task çš„å½¢å¼å‡ºç¾&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;æ¨™è¨˜å¤§è¦æ¨¡æŒ‡ä»¤è³‡æ–™å°äººé¡ä¾†èªªå¯èƒ½å…·æœ‰æŒ‘æˆ°æ€§ï¼Œå› ç‚ºä»–éœ€è¦&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å‰µæ„ï¼Œå¥½æå‡ºæ–°ä»»å‹™&lt;/li&gt;
&lt;li&gt;ç‚ºæ¯å€‹ä»»å‹™ç·¨å¯« labeled instances çš„å°ˆæ¥­çŸ¥è­˜&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;defining-instruction-data&#34;&gt;Defining Instruction Data&lt;/h3&gt;
&lt;p&gt;æˆ‘å€‘è¦ç”Ÿæˆçš„æŒ‡ä»¤è³‡æ–™é›†åŒ…å« {$I_t$}ï¼Œæ¯å€‹æŒ‡ä»¤ç”¨è‡ªç„¶èªè¨€å®šç¾©äº†ä»»å‹™ $t$ã€‚&lt;/p&gt;
&lt;p&gt;æ¯å€‹ä»»å‹™éƒ½æœ‰ä¸€å€‹æˆ–å¤šå€‹ input-output instances ($X_t,Y_t$)ã€‚&lt;/p&gt;
&lt;p&gt;çµ¦å®š task instruction $I_t$ï¼Œé‚„æœ‰ instance xï¼Œæ¨¡å‹ M è¦ç”Ÿå‡º yï¼š&lt;/p&gt;
&lt;p&gt;$M(I_t,x)=y, for (x,y) \in (X_t,Y_t)$&lt;/p&gt;
&lt;p&gt;å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œinstance input å’Œ instruction æ²’æœ‰åš´æ ¼åˆ†ç•Œã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ Instruction:&amp;ldquo;write an essay about school safety&amp;rdquo; x:&amp;quot;&amp;quot;ï¼Œå¯ä»¥è¢«æ”¹ç‚º Instruction:&amp;ldquo;write an essay about the following topic&amp;rdquo; x:&amp;ldquo;school safety&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;automatic-instruction-data-generation&#34;&gt;Automatic Instruction Data Generation&lt;/h3&gt;
&lt;p&gt;ç”ŸæˆæŒ‡ä»¤è³‡æ–™çš„ pipeline åˆ†æˆå››å€‹æ­¥é©Ÿï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æŒ‡ä»¤ç”Ÿæˆ&lt;/li&gt;
&lt;li&gt;è¾¨è­˜æŒ‡ä»¤æ˜¯å¦æ˜¯åˆ†é¡ä»»å‹™&lt;/li&gt;
&lt;li&gt;ç”¨ input-first æˆ– output-first åš instance generation&lt;/li&gt;
&lt;li&gt;éæ¿¾æ‰ä½å“è³ªçš„è³‡æ–™&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;instruction-generation&#34;&gt;Instruction Generation&lt;/h4&gt;
&lt;p&gt;Self-Instruct æ˜¯åŸºæ–¼ä¸€å€‹ç™¼ç¾ï¼Œä¹Ÿå°±æ˜¯å¤§å‹èªè¨€æ¨¡å‹å¯ä»¥é€é context ä¸­çš„ç¾æœ‰æŒ‡ä»¤ï¼Œç”Ÿå‡ºæ–°ç©çš„æŒ‡ä»¤ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºä½œè€…æä¾›äº†ä¸€ç¨®å¾ä¸€å°çµ„äººé¡ç·¨å¯«çš„æŒ‡ä»¤ä¸­ï¼Œä½¿æŒ‡ä»¤è³‡æ–™å¢é•·çš„åšæ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ä»–å€‘ç·¨å¯«çš„ 175 å€‹ä»»å‹™ (æ¯å€‹ä»»å‹™ 1 å€‹ instruction å’Œ 1 å€‹ instance) åˆå§‹åŒ– task poolã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ¯ä¸€å€‹ stepï¼Œä½œè€…å¾è£¡é¢ sample 8 å€‹ instructionsï¼Œä½œç‚º in-context çš„ç¯„ä¾‹ã€‚åœ¨é€™ 8 å€‹æŒ‡ä»¤ä¸­ï¼Œæœ‰ 6 æ¢ä¾†è‡ªäººå·¥ç·¨å¯«çš„ä»»å‹™ï¼Œå¦å¤–å…©æ¢ä¾†è‡ªå‰é¢æ­¥é©Ÿä¸­æ¨¡å‹ç”Ÿæˆçš„ä»»å‹™ï¼Œä»¥ä¿ƒé€²å¤šæ¨£æ€§ã€‚&lt;/p&gt;
&lt;h4 id=&#34;classification-task-identification&#34;&gt;Classification Task Identification&lt;/h4&gt;
&lt;p&gt;å› ç‚ºå°æ–¼åˆ†é¡å’Œéåˆ†é¡çš„ä»»å‹™ï¼Œä½œè€…æœƒæ¡å–å…©ç¨®åšæ³•ï¼Œæ‰€ä»¥ä½œè€…ä½¿ç”¨ä¾†è‡ª seed taks çš„ 12 æ¢åˆ†é¡æŒ‡ä»¤å’Œ 19 æ¢éåˆ†é¡æŒ‡ä»¤ï¼Œè®“ GPT3 é€é few-shot ä¾†åˆ¤åˆ¥ã€‚&lt;/p&gt;
&lt;h4 id=&#34;instance-generation&#34;&gt;Instance Generation&lt;/h4&gt;
&lt;p&gt;çµ¦äºˆæŒ‡ä»¤å’Œä»–å€‘çš„ä»»å‹™é¡åˆ¥ï¼Œä½œè€…ç¨ç«‹åœ°ç‚ºæ¯æ¢æŒ‡ä»¤ç”Ÿæˆ instanceã€‚&lt;/p&gt;
&lt;p&gt;é€™å…·å‚™æŒ‘æˆ°æ€§ï¼ŒåŸå› åœ¨æ–¼ä»–éœ€è¦æ¨¡å‹ç­è§£ç›®æ¨™ä»»å‹™æ˜¯ä»€éº¼ï¼Œæ ¹æ“šæŒ‡ä»¤æ‰¾å‡ºéœ€è¦é‚£äº›é¡å¤–çš„è¼¸å…¥å…§å®¹ï¼Œä¸¦ç”Ÿæˆä»–å€‘ã€‚ (æ¨¡å‹è¦æ ¹æ“š instruction ç”Ÿå‡º instance input)&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ï¼Œåœ¨ prompt ä¸­æ”¾å…¥å…¶ä»–åŒ…å« instruction-input-output çš„ä»»å‹™ç¯„ä¾‹çš„æ™‚å€™ï¼Œæ¨¡å‹å¯ä»¥å¯¦ç¾é€™é»ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨®è‡ªç„¶çš„æ–¹æ³•æ˜¯ Input-first Approachï¼Œå¯ä»¥è¦æ±‚èªè¨€æ¨¡å‹å…ˆæ ¹æ“šæŒ‡ä»¤æå‡º inputï¼Œå†ç”Ÿå‡ºç›¸æ‡‰çš„ outputã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œï¼Œé€™ç¨®æ–¹æ³•åœ¨åˆ†é¡ä»»å‹™ä¸Šï¼Œå¯èƒ½æœƒåå‘æ–¼ç”ŸæˆæŸç¨® labelã€‚æ‰€ä»¥ï¼Œå°æ–¼åˆ†é¡ä»»å‹™ï¼Œä½œè€…æ¡ç”¨ Output-first Approachï¼Œå…ˆç”Ÿæˆå¯èƒ½çš„ labelï¼Œåœ¨æ¯å€‹ label ä¸Šå†ç”Ÿæˆè¼¸å…¥ã€‚&lt;/p&gt;
&lt;h4 id=&#34;filtering-and-postprocessing&#34;&gt;Filtering and Postprocessing&lt;/h4&gt;
&lt;p&gt;ç‚ºäº†é¼“å‹µå¤šæ¨£æ€§ï¼Œåªæœ‰ç•¶æ–°çš„æŒ‡ä»¤å’Œä»»ä½•ç¾æœ‰çš„æŒ‡ä»¤çš„ ROUGE-L overlapping å°æ–¼ 0.7 çš„æ™‚å€™ï¼Œæ‰æœƒè¢«æ·»åŠ åˆ° task poolã€‚&lt;/p&gt;
&lt;p&gt;é‚„æ’é™¤äº†ä¸€äº›åŒ…å«äº†é€šå¸¸ä¸èƒ½è¢« LM è™•ç†çš„é—œéµå­— (e.g. images, pictures, graphs) çš„æŒ‡ä»¤ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç‚ºæ¯å€‹æŒ‡ä»¤ç”Ÿæˆæ–°çš„ instance çš„æ™‚å€™ï¼Œæœƒéæ¿¾æ‰å®Œå…¨ç›¸åŒæˆ–è€…æ˜¯è¼¸å…¥ç›¸åŒä½†è¼¸å‡ºä¸åŒçš„ instanceã€‚&lt;/p&gt;
&lt;h3 id=&#34;finetuning-the-lm-to-follow-instructions&#34;&gt;Finetuning the LM to Follow Instructions&lt;/h3&gt;
&lt;p&gt;åœ¨å‰µå»ºå¤§è¦æ¨¡æŒ‡ä»¤è³‡æ–™å¾Œï¼Œç”¨é€™äº›è³‡æ–™å°åŸå§‹èªè¨€æ¨¡å‹é€²è¡Œ fine-tuneã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºæ­¤ï¼Œå°‡ instruction å’Œ instance input é€£æ¥èµ·ä¾†ï¼Œä½œç‚º promptï¼Œç„¶å¾Œè¨“ç·´æ¨¡å‹é€éæ¨™æº–çš„ç›£ç£å¼å­¸ç¿’é€²è¡Œå¾®èª¿ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è®“æ¨¡å‹å°ä¸åŒçš„æ ¼å¼ robustï¼Œä½¿ç”¨å¤šå€‹æ¨¡æ¿å°‡æŒ‡ä»¤å’Œè¼¸å…¥ encode åœ¨ä¸€èµ·ã€‚&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ï¼ŒæŒ‡ä»¤å¯ä»¥æœ‰æˆ–æ²’æœ‰ Task: å‰å¢œã€è¼¸å…¥å¯ä»¥æœ‰æˆ–æ²’æœ‰ Input: å‰å¢œï¼Œæˆ–æ˜¯ä¸­é–“å¯ä»¥æœ‰ä¸åŒæ•¸é‡çš„æ›è¡Œä¹‹é¡çš„ã€‚&lt;/p&gt;
&lt;h2 id=&#34;self-instruct-data-from-gpt3&#34;&gt;Self-Instruct Data from GPT3&lt;/h2&gt;
&lt;p&gt;ä½œè€…é€é OpenAI API è¨ªå•æœ€å¤§çš„ GPT3 (davinci)&lt;/p&gt;
&lt;h3 id=&#34;statistics&#34;&gt;Statistics&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;diversity&#34;&gt;Diversity&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;quality&#34;&gt;Quality&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;h3 id=&#34;gpt3_self-inst-fine-tuning-gpt3-on-its-own-instruction-data&#34;&gt;$GPT3_{SELF-INST}$: fine-tuning GPT3 on its own instruction data&lt;/h3&gt;
&lt;p&gt;ä½¿ç”¨ç”Ÿå‡ºä¾†çš„æŒ‡ä»¤è³‡æ–™ï¼Œå° GPT3 é€²è¡Œå¾®èª¿ã€‚&lt;/p&gt;
&lt;p&gt;å¾®èª¿æ˜¯é€é OpenAI finetuning API&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines&lt;/h3&gt;
&lt;h4 id=&#34;off-the-shelf-language-models&#34;&gt;Off-the-shelf language models&lt;/h4&gt;
&lt;p&gt;T5-LM å’Œ GPT3 æ˜¯æ™®é€š LM baselines (åªæœ‰ pre-trainingï¼Œæ²’æœ‰é¡å¤– fine-tune)&lt;/p&gt;
&lt;p&gt;é€™äº› baseline å°‡è¡¨æ˜ç¾æˆçš„ LM åœ¨é è¨“ç·´å¾Œï¼Œèƒ½å¤ ç«‹åˆ»è‡ªç„¶åœ°éµå¾ªæŒ‡ä»¤çš„ç¨‹åº¦ã€‚&lt;/p&gt;
&lt;h4 id=&#34;publicly-available-instruction-tuned-models&#34;&gt;Publicly-available instruction-tuned models&lt;/h4&gt;
&lt;p&gt;T0 å’Œ $T_k$-Instruct æ˜¯å…©å€‹ instruction-tuned modelsã€‚&lt;/p&gt;
&lt;p&gt;å…©è€…éƒ½æ˜¯å¾ T5 é€²è¡Œå¾®èª¿çš„ï¼Œå°é€™å…©ç¨®æ¨¡å‹ï¼Œéƒ½ä½¿ç”¨å…·æœ‰ 11B åƒæ•¸çš„æœ€å¤§ç‰ˆæœ¬ã€‚&lt;/p&gt;
&lt;h4 id=&#34;instruction-tuned-gpt3-models&#34;&gt;Instruction-tuned GPT3 models&lt;/h4&gt;
&lt;p&gt;ä½œè€…è©•ä¼°äº† InstructGPTï¼Œå®ƒæ˜¯ OpenAI åŸºæ–¼ GPT3 é–‹ç™¼çš„ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ SuperNI çš„å¯¦é©—ï¼Œåªèˆ‡ text-davinci-001 engine é€²è¡Œæ¯”è¼ƒï¼Œå› ç‚ºæ›´æ–°çš„ engine ç”¨æœ€æ–°çš„ç”¨æˆ¶è³‡æ–™ï¼Œè€Œä¸”å¾ˆå¯èƒ½å·²ç¶“çœ‹é SuperNIã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼æ–°ç·¨å¯«çš„æŒ‡ä»¤ï¼Œè©•ä¼°æ™‚å‰‡åŒ…å«äº† 001ã€002 å’Œ 003ï¼Œä»¥ç¢ºä¿å®Œæ•´æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æ¯”è¼ƒ Self-Instruct åœ¨å…¶ä»–å…¬é–‹å¯ç”¨çš„æŒ‡ä»¤è¨“ç·´é›†è³‡æ–™ï¼Œä½¿ç”¨ PromptSource å’Œ SuperNI çš„è³‡æ–™å¾®èª¿ GPT3ï¼Œé€™äº›è³‡æ–™ç”¨æ–¼è¨“ç·´ T0 å’Œ $T_k$-Instruct æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;åˆ†åˆ¥ç°¡ç¨±ç‚º T0 è¨“ç·´å’Œ SuperNI è¨“ç·´ã€‚&lt;/p&gt;
&lt;h3 id=&#34;experiment-1-zero-shot-generalization-on-superni-benchmark&#34;&gt;Experiment 1: Zero-Shot Generalization on SUPERNI benchmark&lt;/h3&gt;
&lt;p&gt;é¦–å…ˆä»¥ zero-shot çš„æ–¹å¼è©•ä¼°å…¸å‹ NLP ä»»å‹™éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;experiment-2-generalization-to-user-oriented-instructions-on-novel-tasks&#34;&gt;Experiment 2: Generalization to User-oriented Instructions on Novel Tasks&lt;/h3&gt;
&lt;p&gt;ç›¡ç®¡ SuperNI åœ¨ç¾æœ‰çš„ NLP ä»»å‹™å…·æœ‰å…¨é¢æ€§ï¼Œå¤šæ•¸çš„é€™äº›ä»»å‹™æ˜¯åˆæ–¼ç ”ç©¶ç†ç”±æå‡ºçš„ï¼Œè€Œä¸”åå‘åˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ›´å¥½çš„ç²å–æŒ‡ä»¤éµå¾ªæ¨¡å‹çš„å¯¦ç”¨åƒ¹å€¼ï¼Œä½œè€…ä¸­çš„ä¸€éƒ¨åˆ†äººç­–åŠƒäº†ä¸€çµ„é¢å‘ç”¨æˆ¶æ‡‰ç”¨çš„æ–°æŒ‡ä»¤é›†ã€‚&lt;/p&gt;
&lt;p&gt;ä»–å€‘å…ˆé‡å° Large LM å¯èƒ½å¯ä»¥æ‡‰ç”¨åˆ°çš„é ˜åŸŸé€²è¡Œ brainstormï¼Œä¸¦ä¸”åˆ¶å®šèˆ‡æ¯å€‹é ˜åŸŸç›¸é—œçš„ instruction å’Œ instanceã€‚&lt;/p&gt;
&lt;p&gt;ç¸½å…±å‰µå»ºäº† 252 æ¢æŒ‡ä»¤ï¼Œæ¯æ¢æŒ‡ä»¤æœ‰ 1 å€‹ instanceã€‚&lt;/p&gt;
&lt;h4 id=&#34;human-evaluation-setup&#34;&gt;Human evaluation setup&lt;/h4&gt;
&lt;p&gt;è©•ä¼°æ¨¡å‹åœ¨é€™äº›ä¸åŒä»»å‹™çš„æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾æ¥µå…·æŒ‘æˆ°æ€§ï¼Œå› ç‚ºä¸åŒçš„ä»»å‹™éœ€è¦ä¸åŒçš„å°ˆæ¥­çŸ¥è­˜ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†ç²å¾—æ›´å¿ å¯¦çš„è©•åƒ¹ï¼Œä½œè€…è«‹äº† instructions çš„ä½œè€…å°æ¨¡å‹çš„é æ¸¬çµæœé€²è¡Œè©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;å¯¦æ–½ä¸€å€‹ four-level rating systemï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rating A
&lt;ul&gt;
&lt;li&gt;å›è¦†æœ‰æ•ˆä¸”ä»¤äººæ»¿æ„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating B
&lt;ul&gt;
&lt;li&gt;å›è¦†å¯æ¥å—ï¼Œä½†å­˜åœ¨å¯ä»¥æ”¹é€²çš„åœ°æ–¹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating C
&lt;ul&gt;
&lt;li&gt;å›è¦†ç›¸é—œï¼Œä½†åœ¨å…§å®¹ä¸Šæœ‰é‡å¤§éŒ¯èª¤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rating D
&lt;ul&gt;
&lt;li&gt;å›è¦†ä¸ç›¸é—œæˆ–ç„¡æ•ˆï¼ŒåŒ…å«é‡è¤‡è¼¸å…¥çš„éƒ¨åˆ†ï¼Œå®Œå…¨ç„¡é—œçš„è¼¸å‡ºã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;results-1&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/self-instruct/fig5.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å¦‚æœæŠŠ Rating B ä»¥ä¸Šè¦–ç‚ºæœ‰æ•ˆï¼Œ$GPT_{SELF-INST}$ åªå’Œ $InstructGPT_{001}$ ç›¸å·® 5%&lt;/p&gt;
&lt;h2 id=&#34;discussion-and-limitation&#34;&gt;Discussion and Limitation&lt;/h2&gt;
&lt;h3 id=&#34;why-does-self-instruct-work&#34;&gt;Why does SELF-INSTRUCT work?&lt;/h3&gt;
&lt;p&gt;å€¼å¾—åæ€çš„æ˜¯ï¼Œåœ¨æœ€è¿‘æˆåŠŸçš„ instruction-tuning LMs ä¸­ï¼Œé«˜å“è³ªçš„ human feedback æ‰®æ¼”çš„è§’è‰²ã€‚&lt;/p&gt;
&lt;p&gt;é€™è£¡æœ‰å…©å€‹æ¥µç«¯çš„å‡è¨­ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Human feedback æ˜¯ instruction-tuning ä¸­å¿…è¦ä¸”ä¸å¯æˆ–ç¼ºçš„è§’è‰²ï¼Œå› ç‚º LM éœ€è¦äº†è§£åœ¨é è¨“ç·´éç¨‹ä¸­æ²’å®Œå…¨äº†è§£åˆ°çš„å•é¡Œã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Human feedback æ˜¯ instruction-tuning ä¸€å€‹å¯é¸çš„æ–¹å‘ï¼Œå› ç‚º LM åœ¨é è¨“ç·´å°±å·²ç¶“å¾ˆç†Ÿæ‚‰æŒ‡ä»¤äº†ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;é›–ç„¶ç¾å¯¦å¯èƒ½ä»‹æ–¼é€™å…©å€‹æ¥µç«¯ä¹‹é–“ï¼Œä½œè€…æ¨æ¸¬å¯èƒ½æ›´å‚¾å‘æ–¼ç¬¬äºŒç¨®å‡è¨­ï¼Œå°¤å…¶æ˜¯å°æ–¼è¼ƒå¤§çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;ç¬¬äºŒç¨®ï¼Œä¹Ÿæ˜¯äººé¡ç›´è¦ºï¼Œæ˜¯ Self- Instruct çš„é—œéµå‹•æ©Ÿï¼Œè€Œä¸”ä¹Ÿå¾æˆåŠŸçš„çµæœç²å¾—æ”¯æŒã€‚&lt;/p&gt;
&lt;h3 id=&#34;broader-impact&#34;&gt;Broader Impact&lt;/h3&gt;
&lt;p&gt;é™¤äº†æœ¬æ–‡çš„ç›´æ¥é—œæ³¨é»å¤–ï¼Œä½œè€…ç›¸ä¿¡ Self-Instruct å¯èƒ½æœ‰åŠ©æ–¼æ­éœ²å„ç¨® instruction tuning æ¨¡å‹ &amp;ldquo;å¹•å¾Œ&amp;rdquo; ç™¼ç”Ÿçš„äº‹æƒ…ã€‚&lt;/p&gt;
&lt;p&gt;ä¸å¹¸çš„æ˜¯ï¼Œç”±æ–¼ä»–å€‘çš„è³‡æ–™é›†å°šæœªç™¼å¸ƒï¼Œé€™ç¨®æ¥­ç•Œæ¨¡å‹ä»è™•æ–¼ API ç‰†ä¹‹å¾Œã€‚&lt;/p&gt;
&lt;p&gt;äººå€‘å°å…¶çµæ§‹ä»¥åŠç‚ºä½•èƒ½å±•ç¾ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›çŸ¥ä¹‹ç”šå°‘ã€‚&lt;/p&gt;
&lt;h3 id=&#34;limitations-of-self-instruct&#34;&gt;Limitations of Self-Instruct&lt;/h3&gt;
&lt;h4 id=&#34;tail-phenomena&#34;&gt;Tail phenomena&lt;/h4&gt;
&lt;p&gt;Self-Instruct ä¾è³´æ–¼ LMï¼Œç¹¼æ‰¿ LM çš„æ‰€æœ‰é™åˆ¶ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ç ”ç©¶é¡¯ç¤ºå‡º tail phenomena å° LM çš„æˆåŠŸæ§‹æˆåš´å³»çš„æŒ‘æˆ°ã€‚&lt;/p&gt;
&lt;p&gt;æ›å¥è©±èªªï¼ŒLM çš„æœ€å¤§æ”¶ç›Šå‡ºç¾æ–¼èªè¨€ä¸­æœ€é »ç¹å‡ºç¾çš„éƒ¨åˆ† (èªè¨€åˆ†ä½ˆçš„é ­éƒ¨)ï¼Œè€Œä½é »ç‡å‡ºç¾çš„ä¸Šä¸‹æ–‡ä¸­ç²å¾—çš„æ”¶ç›Šæœ€å°ã€‚&lt;/p&gt;
&lt;p&gt;åŒæ¨£çš„ï¼Œåœ¨é€™é …å·¥ä½œèƒŒæ™¯ä¸‹ï¼Œå¦‚æœ Self-Instruct å¤§éƒ¨åˆ†çš„æ”¶ç›Šåå‘é è¨“ç·´ corpus ä¸­é »ç¹å‡ºç¾çš„ä»»å‹™æˆ–æŒ‡ä»¤ï¼Œé‚£ä¹Ÿä¸ä»¤äººæ„Ÿåˆ°æ„å¤–ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œè©²æ–¹æ³•åœ¨ä¸å¸¸è¦‹å’Œæœ‰å‰µæ„çš„æŒ‡ä»¤ä¸‹ï¼Œå¯èƒ½æœƒé¡¯ç¾å‡ºè„†å¼±æ€§ã€‚&lt;/p&gt;
&lt;h4 id=&#34;dependence-on-large-models&#34;&gt;Dependence on large models&lt;/h4&gt;
&lt;p&gt;å› ç‚º Self-Instruct ä¾è³´æ–¼å¾ LM ä¸­æå–åˆçš„ inductive biasï¼Œå› æ­¤å®ƒå¯èƒ½é©åˆ larger modelã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœé€™æ˜¯å°çš„ï¼Œé€™æœƒå°é‚£äº›æ²’æœ‰å¤§é‡è¨ˆç®—è³‡æºçš„äººé€ æˆé˜»ç¤™ã€‚&lt;/p&gt;
&lt;h4 id=&#34;reinforcing-lm-biases&#34;&gt;Reinforcing LM biases&lt;/h4&gt;
&lt;p&gt;ä½œè€…æ“”å¿ƒé€™ç¨®è¿­ä»£ä½œæ³•å¯èƒ½æœƒç”¢ç”Ÿæ„æ–™ä¹‹å¤–çš„çµæœï¼Œæ¯”å¦‚å°‡æœ‰å•é¡Œçš„ç¤¾æœƒåè¦‹æ”¾å¤§ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Relative Position ä»‹ç´¹ &#43; è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/relative-position-%E4%BB%8B%E7%B4%B9--%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Mon, 24 Apr 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/relative-position-%E4%BB%8B%E7%B4%B9--%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;h2 id=&#34;èªªæ˜&#34;&gt;èªªæ˜&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡å¯«æ–¼ &lt;a class=&#34;link&#34; href=&#34;https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Swin Transformer è«–æ–‡é–±è®€&lt;/a&gt; ä¹‹å¾Œï¼Œç•¶æ™‚å° Relatvie position çš„ç†è§£ä¸å¤ æ¸…æ¥šï¼Œæœ¬æ–‡å°‡æœƒåšè§£é‡‹ï¼Œä¸¦é™„ä¸ŠåŸè«–æ–‡çš„ç­†è¨˜ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸‹å°‡æœƒå…ˆç”¨é•·åº¦ç‚º 3 çš„åºåˆ—ä½œç‚ºç¤ºç¯„ã€‚&lt;/p&gt;
&lt;h3 id=&#34;absolute-position-encodings&#34;&gt;Absolute Position Encodings&lt;/h3&gt;
&lt;p&gt;Absolute Position Encodings çš„åšæ³•æ˜¯æŠŠç”¨æŸç¨®æ–¹å¼ç”Ÿæˆæˆ–å¯å­¸ç¿’çš„å‘é‡åŠ åœ¨è¼¸å…¥ï¼Œç¬¬ä¸€å€‹ä½ç½®ç”¨ $w_1$ï¼Œç¬¬äºŒå€‹ä½ç½®ç”¨ $w_2$ï¼Œç¬¬ä¸‰å€‹ä½ç½®ç”¨ $w_3$ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/abs-pos.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;relative-position-encodings&#34;&gt;Relative Position Encodings&lt;/h3&gt;
&lt;p&gt;Relative Position Encodings é¡§åæ€ç¾©ï¼Œå°±æ˜¯æ”¹ç”¨ç›¸å°ä½ç½®ä¾†åšé€™äº›å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–ä¸­ï¼ŒPosition Encoding çš„éƒ¨åˆ†å¾ 3 å€‹å‘é‡è®Šæˆ 3*3 å€‹å‘é‡ï¼Œå› ç‚ºç¾åœ¨æœƒä»¥æ¯å€‹ token ç‚ºåŸºæº–ï¼Œç”Ÿå‡º 3 å€‹ç›¸å°ä½ç½®å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘ä»¥ $w_0$ï¼Œä»£è¡¨è™•æ–¼åŸé»ï¼Œ$w_x$ ä»£è¡¨å¾€å³ $x$ æ ¼ï¼Œ$w_{-x}$ ä»£è¡¨å¾€å·¦ $x$ æ ¼ï¼Œå…¶ä¸­ $x$ æ˜¯æ­£æ•´æ•¸ã€‚&lt;/p&gt;
&lt;p&gt;ç¬¬ä¸€å€‹ row æœ‰ $w_0$ã€$w_1$ã€$w_2$ï¼Œæ„æ€æ˜¯ä»¥ç¬¬ 0 å€‹å‘é‡ (I) ç‚ºåŸºæº–ï¼Œä»–çš„ä½ç½®æ˜¯ $w_0$ï¼Œå°ç¬¬ 0 å€‹å‘é‡ä¾†èªªï¼Œç¬¬ 1 å€‹å‘é‡ (like) æ˜¯ $w_1$ï¼Œç¬¬ 2 å€‹å‘é‡ (cat) æ˜¯ $w_2$ã€‚&lt;/p&gt;
&lt;p&gt;è¼ªæµä»¥ $n$ å€‹ token ç‚ºåŸºæº–ï¼Œå°±æœƒç”Ÿå‡º n*n å€‹ç›¸å°ä½ç½®å‘é‡ï¼Œè€Œä¸æ˜¯åŸå…ˆçš„ n å€‹çµ•å°ä½ç½®å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;å…¶ä¸­ $w_i$ å’Œ $w_j$ å¦‚æœ $i=j$ï¼Œä»–å€‘æœƒå…±ç”¨åŒæ¨£çš„ weightï¼Œä¸Šåœ–æ˜¯ä»¥ç›¸åŒé¡è‰²è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœåºåˆ—é•·åº¦æ˜¯ $n$ï¼Œå°±æœƒæœ‰ $2n-1$ å€‹å‘é‡è¦å­¸ã€‚&lt;/p&gt;
&lt;p&gt;n*n é€™å€‹æ•¸é‡ä½¿å…¶é©åˆåŠ å…¥åˆ° self-attentionï¼ŒåŸå§‹è«–æ–‡çš„åŠ å…¥æ–¹å¼å¯ä»¥åƒè€ƒä¸‹æ–¹è«–æ–‡ç­†è¨˜ï¼Œé€™é‚Šæ™šé»æœƒä»‹ç´¹å¾ŒçºŒè¡ç”Ÿçš„ç°¡åŒ–ç‰ˆã€‚&lt;/p&gt;
&lt;h3 id=&#34;swin-transformer-å¦‚ä½•å°å…¥-relative-position-encodings&#34;&gt;Swin Transformer å¦‚ä½•å°å…¥ Relative Position Encodings&lt;/h3&gt;
&lt;p&gt;Swin Transformer æ˜¯å€Ÿé‘’è¨±å¤š CNN æ¶æ§‹ï¼Œç‚ºäº† CV è€Œç¶“éä¿®æ”¹çš„ vision transformerã€‚&lt;/p&gt;
&lt;p&gt;å…¶ä¸­ä¸€å€‹é‡é»æ˜¯ï¼Œä»–æœƒåœ¨ä¸€å°å€å¡Šçš„ç‰¹å¾µåœ–ä¸Šåš self-attentionï¼Œè€Œä¸”æ˜¯ç”¨ Relative Position Encodingsã€‚&lt;/p&gt;
&lt;p&gt;å’Œå‰›å‰›çš„å·®åˆ¥åœ¨æ–¼ï¼Œç¾åœ¨è¦åœ¨äºŒç¶­ç©ºé–“åš Relative Position Encodingsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å‡è¨­æœ‰ä¸€å¼µ 2*2 çš„ feature mapï¼Œæˆ‘å€‘å…ˆè¨­å®šå¥½ feature map å„å€‹ token çš„çµ•å°ä½ç½®åº§æ¨™ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶å¾Œæˆ‘å€‘è¼ªæµæŠŠ feature map çš„æ¯ä¸€å€‹ token ä½œç‚ºåŸºæº–é»ï¼ŒæŠŠ feature map çš„æ¯å€‹ token çš„åº§æ¨™æ¸›å»åŸºæº–é»çš„åº§æ¨™ï¼Œå°±å¯ä»¥å¾—åˆ°ç›¸å°ä½ç½®åº§æ¨™ã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœæˆ‘å€‘æŠŠå››å€‹ç›¸å°ä½ç½®åº§æ¨™å„åˆ¥æ”¤å¹³ (æŒ‰ç…§å·¦ä¸Š -&amp;gt; å³ä¸Š -&amp;gt; å·¦ä¸‹ -&amp;gt; å³ä¸‹çš„é †åº)ï¼Œä¸¦ä¸”å¾ä¸Šåˆ°ä¸‹æ’å¥½ï¼Œä»–æœƒçœ‹èµ·ä¾†å¦‚ä¸‹åœ–ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æ­¤æ™‚æˆ‘å€‘å¹¾ä¹å®Œæˆäº†ç›¸å°ä½ç½®çš„è¡¨ï¼Œå’Œå‰›å‰›åºåˆ—ä¸€æ¨£ç”Ÿå‡ºäº† n*n å€‹ç›¸å°ä½ç½®ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘æ¥ä¸‹ä¾†è¦åšçš„äº‹æƒ…æ˜¯æŠŠé€™å€‹è¡¨çµ¦ç·¨è™Ÿï¼ŒæŠŠ (0, 0) éƒ½ç·¨æˆæŸå€‹æ•¸å­—ï¼ŒæŠŠ (1, 0) éƒ½ç·¨æˆæŸå€‹æ•¸å­—ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ­¤ä¹‹å‰ï¼Œå…ˆè€ƒæ…®ç¸½å…±æœƒæœ‰å¹¾ç¨®å¯èƒ½çš„ç›¸å°åº§æ¨™ï¼Œå°æ–¼é‚Šé•· $M$ çš„ feature map (é€™è£¡ M=2)ï¼Œå› ç‚ºå…©è»¸å¯èƒ½çš„æ•¸å­—çš†æœ‰ (2M-1) ç¨®ï¼Œå…±æœƒæœ‰ (2M-1)*(2M-1) ç¨®å¯èƒ½æ€§ï¼Œé€™è£¡ç­‰æ–¼ 9ã€‚&lt;/p&gt;
&lt;p&gt;æ‰€ä»¥æˆ‘å€‘ç­‰ç­‰æœƒæŠŠæ‰€æœ‰åº§æ¨™ç·¨ç‚º 0~8ã€‚&lt;/p&gt;
&lt;p&gt;æƒ³å¾åº§æ¨™ç”Ÿå‡ºç·¨è™Ÿ 0~8 å¯ä»¥è€ƒæ…®æŠŠåº§æ¨™å…©è»¸çš„æ•¸å­—ç›¸åŠ ï¼Œä½†ç”±æ–¼æœ‰è² æ•¸çš„å­˜åœ¨ï¼Œè¦å…ˆæŠŠå…©è»¸çš„æ•¸å­—éƒ½è®Šæˆéè² æ•´æ•¸ï¼Œæ‰€ä»¥å…ˆæŠŠå…©è»¸çš„åº§æ¨™éƒ½å„åˆ¥åŠ  M-1ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;æ­¤æ™‚å¦‚æœç›¸åŠ ï¼Œæœƒä½¿ (2, 1) å’Œ (1, 2) éƒ½å°æ‡‰åˆ°æ•¸å­— 3ï¼Œæ‰€ä»¥æˆ‘å€‘å…ˆæŠŠ row åº§æ¨™ä¹˜ä¸Š 2M-1 å†ç›¸åŠ ï¼Œæ­¤æ™‚å°±å¯ä»¥ç²å¾—ä¸€å€‹ n*n çš„ index table ï¼Œå°æ‡‰ä¸€çµ„ç›¸å°ä½ç½®å‘é‡ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/rel-pos-2d-4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Swin Transformer æ˜¯ç”¨ç°¡åŒ–ç‰ˆçš„ä½œæ³•ä¾†å¼•å…¥ç›¸å°ä½ç½®ï¼Œå…¬å¼å¦‚ä¸‹&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Attention(Q,K,V)=SoftMax(QK^T/\sqrt{d}+B)V$
&lt;ul&gt;
&lt;li&gt;$B$ æ˜¯ relative position biasï¼Œ$B \in R^{M^2 * M^2}$&lt;/li&gt;
&lt;li&gt;$a_{ij}$ æ˜¯ç´”é‡ï¼Œä¸æ˜¯å‘é‡ï¼Œå’ŒåŸå§‹è«–æ–‡ä¸åŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è«–æ–‡å‡ºè™•&#34;&gt;è«–æ–‡å‡ºè™•&lt;/h2&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1803.02155.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Self-Attention with Relative Position Representations&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ä¾è³´æ–¼ attention æ©Ÿåˆ¶çš„ Transformer åœ¨æ©Ÿå™¨ç¿»è­¯æ–¹é¢å–å¾— SOTAï¼Œä½†åœ¨çµæ§‹ä¸­æ²’æœ‰ç›¸å°æˆ–çµ•å°çš„ä½ç½®è³‡è¨Šï¼Œä»–éœ€è¦åœ¨è¼¸å…¥ä¸­æ·»åŠ çµ•å°ä½ç½®çš„è³‡è¨Šã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤æœ¬æ–‡æå‡ºä¸€ç¨®æ›¿ä»£æ–¹æ¡ˆï¼Œæ‹“å±• self-attention ï¼Œè€ƒæ…®ç›¸å°ä½ç½®çš„è¡¨ç¤ºï¼Œä¸¦åœ¨ä¸€äº›ä»»å‹™ä¸­ç²å¾—æ›´å¥½çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;å€¼å¾—ä¸€é¡Œçš„äº‹ï¼Œä½œè€…è§€å¯Ÿåˆ°çµåˆç›¸å°å’Œçµ•å°ä½ç½®ä¸æœƒé€²ä¸€æ­¥æé«˜ç¿»è­¯å“è³ªã€‚&lt;/p&gt;
&lt;p&gt;è©²æ©Ÿåˆ¶å¯ä»¥æ‹“å±•åˆ°ä»»æ„ graph-labeled çš„è¼¸å…¥&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Non-recurrent models ä¸ä¸€å®šæŒ‰é †åºè€ƒæ…®è¼¸å…¥å…ƒç´ ï¼Œå› æ­¤å¯èƒ½éœ€è¦æ˜ç¢ºçš„ position encoding æ‰æ‰èƒ½ç”¨åºåˆ—é †åºã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨®å¸¸è¦‹çš„æ–¹æ³•æ˜¯ä½¿ç”¨èˆ‡è¼¸å…¥å…ƒç´ çµåˆçš„ position encodingï¼Œä»¥å‘æ¨¡å‹å‚³é”ä½ç½®è³‡è¨Šã€‚&lt;/p&gt;
&lt;p&gt;å¯ä»¥æ˜¯ deterministic functionï¼Œæˆ–æ˜¯ learned representationsã€‚&lt;/p&gt;
&lt;p&gt;CNN å¯ä»¥æ•æ‰ kernel çš„ç›¸å°ä½ç½®è³‡è¨Šï¼Œä½†è¢«è­‰æ˜ä»å—ç›Šæ–¼ position encodingã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼æ—¢ä¸ä½¿ç”¨å·ç©ä¹Ÿä¸ä½¿ç”¨éæ­¸çš„ Transformerï¼Œçµåˆä½ç½®ä¿¡æ¯çš„ representation æ˜¯ä¸€å€‹ç‰¹åˆ¥é‡è¦çš„è€ƒæ…®å› ç´ ï¼Œå› ç‚ºè©²æ¨¡å‹åœ¨å…¶ä»–æ–¹é¢å°åºåˆ—æ’åºå®Œå…¨ä¸è®Šã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€ç¨®å°‡ç›¸å°ä½ç½®åˆä½µåˆ° Transformer çš„ self-attention çš„åšæ³•ï¼Œå³ä½¿å®Œå…¨æ›æ‰çµ•å°ä½ç½®ç·¨ç¢¼ï¼Œä¹Ÿä½¿å…©å€‹æ©Ÿå™¨ç¿»è­¯ä»»å‹™çš„å“è³ªæœ‰é¡¯è‘—æé«˜ã€‚&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;åŸå§‹ self-attention&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$z_i=\displaystyle\sum_{j=1}^n\alpha_{ij}(x_jW^V)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\alpha_{ij}=\frac{\text{exp } e_{ij}}{\sum_{k=1}^n\text{exp } e_{ik}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$e_{ij}=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;proposed-architecture&#34;&gt;Proposed Architecture&lt;/h2&gt;
&lt;h3 id=&#34;relation-aware-self-attention&#34;&gt;Relation-aware Self-Attention&lt;/h3&gt;
&lt;p&gt;æœ‰å…©å€‹è¦å¼•å…¥ relative position çš„åœ°æ–¹ï¼Œè€Œä¸”éƒ½æ˜¯å‘é‡&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$z_i = \displaystyle\sum_{j=1}^n \alpha_{ij}(x_jW^V+a_{ij}^V)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$e_{ij}=\frac{x_iW^Q(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;relative-position-representations&#34;&gt;Relative Position Representations&lt;/h3&gt;
&lt;p&gt;å¯ä»¥å¼•å…¥ clipï¼ŒæŠŠç·šæ€§åºåˆ—ä¸­ï¼Œé«˜æ–¼é•·åº¦ k çš„ä¿®å‰ªæˆæœ€å¤§å€¼&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a_{ij}^K=w_{clip(j-i,k)}^K$&lt;/li&gt;
&lt;li&gt;$a_{ij}^V=w_{clip(j-i,k)}^V$&lt;/li&gt;
&lt;li&gt;$clip(x,k)=max(-k,min(k,x))$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;model-variations&#34;&gt;Model Variations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;clipping çš„å¯¦é©—
&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;V å’Œ K çš„ ablation study
&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/relative-position/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Swin Transformer è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Fri, 14 Apr 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.14030&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€å€‹æ–°çš„ vision Transformerï¼Œç¨±ä½œ Swin Transformerï¼Œå¯ä»¥è¢«ç”¨ä½œ computer vision ä¸­çš„ general-purpose backboneã€‚&lt;/p&gt;
&lt;p&gt;æŠŠ Transformer å¾ language ç§»åˆ° vision å…·å‚™æŒ‘æˆ°æ€§ï¼Œæ¯”å¦‚åŒä¸€å€‹ visual entity åœ¨å¤§å°ä¸Šå…·å‚™å¾ˆå¤§çš„ varianceã€‚é‚„æœ‰ high resolution ä¸‹ pixel å’Œ word çš„æ•¸é‡å·®ç•°å¤ªå¤§ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è§£æ±ºé€™äº›å·®ç•°ï¼Œä½œè€…æå‡º hierachical Transformerï¼Œç”¨ shifted windows ä¾†ç®—å‡º representationã€‚&lt;/p&gt;
&lt;p&gt;shifted windowing é€éæŠŠ self-attention é™åˆ¶åœ¨ non-overlapping çš„ local window å’Œå…è¨± cross-windows connection ä¾†æé«˜æ•ˆç‡ã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨® hierarchical architecture å¯ä»¥éˆæ´»åœ°åœ¨å„ç¨® scale ä¸‹æ“´å±• modelï¼Œé‚„å¯ä»¥å°åœ–åƒå¤§å°æœ‰ç·šæ€§çš„è¨ˆç®—æ™‚é–“è¤‡é›œåº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ViT æŠŠåœ–ç‰‡æ‰“æˆ patchï¼Œæ¯å€‹ patch æ˜¯ 16*16ï¼Œfeature maps ç”± single low resolution çš„è¼¸å…¥ç”Ÿæˆï¼Œè€Œä¸”ç”±æ–¼è‡ªæ³¨æ„åŠ›å§‹çµ‚éƒ½æ˜¯åœ¨å…¨å±€ä¸Šè¨ˆç®—çš„ (patch å’Œ patch é–“åšè‡ªæ³¨æ„åŠ›)ï¼Œæ‰€ä»¥æ™‚é–“è¤‡é›œåº¦æ˜¯ quadratic computation complexityã€‚&lt;/p&gt;
&lt;p&gt;Swin Transformer å¾å° patch é–‹å§‹ï¼Œä¸¦åœ¨æ›´æ·±çš„ Transformer layers åˆä½µç›¸é„°çš„ patchesã€‚&lt;/p&gt;
&lt;p&gt;æœ‰äº†é€™äº› hierarchical feature mapsï¼Œå¯ä»¥ç”¨åœ¨åƒæ˜¯ FPN æˆ–æ˜¯ U-Netã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹ Swin Transformer çš„é—œéµè¨­è¨ˆå› ç´ æ˜¯ shifted windowã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;é€é bridge ä¸åŒ layer çš„ windows ä¾†æä¾›ä»–å€‘é€£æ¥ã€‚&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;overall-architecture&#34;&gt;Overall Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Patch Merging
&lt;ul&gt;
&lt;li&gt;åŸæœ¬ç‰¹å¾µåœ–æ˜¯ H * W * C&lt;/li&gt;
&lt;li&gt;ä»¥ä¸Šä¸‹ stride=2 è¡Œèµ°ï¼Œæœƒå¾—åˆ°å››å¼µ H/2 * W/2 * C&lt;/li&gt;
&lt;li&gt;concatenate èµ·ä¾†ï¼Œè®Šæˆ H/2 * W/2 * 4C&lt;/li&gt;
&lt;li&gt;åš linearï¼Œè®Šæˆ H/2 * W/2 * 2C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;swin-transformer-block&#34;&gt;Swin Transformer block&lt;/h4&gt;
&lt;p&gt;Swin Transformer æ˜¯é€éæŠŠ Transformer block ä¸­çš„ multi-head self attention(MSA) æ›æˆåŸºæ–¼ shifted windows çš„ module æ§‹æˆã€‚&lt;/p&gt;
&lt;h3 id=&#34;shifted-window-based-self-attention&#34;&gt;Shifted Window based Self-Attention&lt;/h3&gt;
&lt;p&gt;æ¨™æº–çš„ Transformer æ¶æ§‹æœƒç®— global self-attentionï¼Œè¨ˆç®—æ‰€æœ‰ token é–“å½¼æ­¤çš„é—œä¿‚ï¼Œå°è‡´ quadratic complexityï¼Œä½¿å…¶ä¸é©ç”¨æ–¼éœ€è¦å¤§é‡ token çš„è¨±å¤š CV å•é¡Œ&lt;/p&gt;
&lt;h4 id=&#34;self-attention-in-non-overlapped-windows&#34;&gt;Self-attention in non-overlapped windows&lt;/h4&gt;
&lt;p&gt;åŸä¾†çš„åœ–ç‰‡æœƒä»¥ non-overlapping çš„æ–¹å¼åˆ‡å‰²ã€‚&lt;/p&gt;
&lt;p&gt;å‡è¨­æ¯å€‹ windows æœ‰ M * M å€‹ patchesï¼Œç„¶å¾Œä¸€å¼µåœ–åƒæœ‰ h * w å¡Š patchesï¼Œè¨ˆç®—è¤‡é›œåº¦å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega(MSA)=4hwC^2+2(hw)^2C$&lt;/li&gt;
&lt;li&gt;$\Omega(W-MSA)=4hwC^2+2M^2hwC$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;shifted-window-partitioning-in-successive-blocks&#34;&gt;Shifted window partitioning in successive blocks&lt;/h4&gt;
&lt;p&gt;window-based self-attention module ç¼ºä¹äº† windows é–“å½¼æ­¤çš„é€£æ¥ï¼Œæœƒé™åˆ¶æ¨¡å‹èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºäº†ä¸€ç¨® shifted window çš„æ–¹æ³•ï¼Œä¿æŒ non-overlapping windows çš„é«˜æ•ˆè¨ˆç®—ï¼ŒåŒæ™‚å¼•å…¥ windows é–“çš„é€£æ¥ã€‚&lt;/p&gt;
&lt;p&gt;å†å…©å€‹é€£çºŒçš„ windows é–“ï¼Œæœƒç§»å‹• $(âŒŠ \frac{M}{2} âŒ‹, âŒŠ \frac{M}{2} âŒ‹)$&lt;/p&gt;
&lt;h4 id=&#34;efficient-batch-computation-for-shifted-configuration&#34;&gt;Efficient batch computation for shifted configuration&lt;/h4&gt;
&lt;p&gt;shifted window æœ‰å€‹å•é¡Œæ˜¯ï¼Œæœƒå°è‡´æ›´å¤šçš„ windowsï¼Œå¾ $âŒˆ \frac{h}{M} âŒ‰ * âŒˆ \frac{w}{M} âŒ‰$ åˆ° $(âŒˆ \frac{h}{M} âŒ‰+1) * (âŒˆ \frac{w}{M} âŒ‰+1)$ï¼Œè€Œä¸”æœ‰äº› window æœƒå°æ–¼ M*Mã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¨£æœƒå°è‡´ç„¡æ³•æŠŠé€™äº›çµ¦å£“æˆä¸€å€‹ batch å¿«é€Ÿè¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä¸€ç¨® naive çš„è§£æ³•å°±æ˜¯ç›´æ¥åœ¨å¤–é¢åŠ  zero paddingï¼Œä½†æœƒå¢åŠ è¨ˆç®—é‡ï¼Œç•¶ windows æ•¸é‡è¼ƒå°‘æ™‚ï¼Œè¨ˆç®—é‡æœƒè®Šå¾ˆå¯è§€ (å¾ 2 * 2 å€‹ windows è®Šæˆ 3 * 3 å€‹ windowsï¼Œå¢åŠ äº† 2.25 å€)&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºå¦å¤–ä¸€ç¨®å·§å¦™çš„åšæ³•ï¼ŒæŠŠä¸€äº›éƒ¨åˆ†æŒªç§»ã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/fig4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä½†ç¾åœ¨æœ‰äº› window è£¡æœ‰å¤šå€‹ä¸è©²ç›¸äº’åš attention çš„éƒ¨åˆ†ï¼Œæ‰€ä»¥è¦ç”¨ mask çš„æ–¹å¼è¨ˆç®—ã€‚&lt;/p&gt;
&lt;p&gt;ä¸åŒ windowsï¼Œåš self-attention å¾Œï¼ŒæŠŠä¸ç›¸å¹²çš„éƒ¨åˆ†åšçš„ attention æ¸›å»ä¸€å€‹å¾ˆå¤§çš„æ•¸å€¼ï¼Œæœ€å¾Œå†é softmaxã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/mask.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–ä¾†è‡ªä½œè€…åœ¨ github æä¾›çš„å¯è¦–åŒ–&lt;/p&gt;
&lt;p&gt;æœ€å¾Œå†æŠŠå®ƒæŒªå›åŸæœ¬çš„ä½ç½®ã€‚&lt;/p&gt;
&lt;h4 id=&#34;relative-position-bias&#34;&gt;Relative position bias&lt;/h4&gt;
&lt;p&gt;åƒè€ƒé€™å€‹: &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_37541097/article/details/121119988&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/qq_37541097/article/details/121119988&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;architecture-variants&#34;&gt;Architecture Variants&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;window size é è¨­æ˜¯ M = 7&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;query dimension of each head æ˜¯ d = 32&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;expansion layer of each MLP is $\alpha$ = 4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;C æ˜¯ first stage çš„ hidden layers çš„ channel numbers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-T&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 6, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 0.25 å€&lt;/li&gt;
&lt;li&gt;complexity æ¥è¿‘ ResNet-50&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-S&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 96&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 0.5 å€&lt;/li&gt;
&lt;li&gt;complexity æ¥è¿‘ ResNet-101&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-B&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 128&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Swin-L&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C = 192&lt;/li&gt;
&lt;li&gt;layer numbers = {2, 2, 18, 2}&lt;/li&gt;
&lt;li&gt;å¤§å°å’Œè¨ˆç®—é‡æ˜¯ Base çš„å¤§ç´„ 2 å€&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;image-classification-on-imagenet-1k&#34;&gt;Image Classification on ImageNet-1K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table1.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;object-detection-on-coco&#34;&gt;Object Detection on COCO&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table2.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;semantic-segmentation-on-ade20k&#34;&gt;Semantic Segmentation on ADE20K&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table3.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;Ablation Study&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/Swin-Transformer/table4.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;åŸºæ–¼ self-attention çš„ shifted window æ˜¯ Swin Transformer é—œéµéƒ¨åˆ†ï¼Œè¢«é¡¯ç¤ºå‡ºä»–åœ¨ CV é ˜åŸŸæœ‰æ•ˆç‡ä¸”æœ‰æ•ˆï¼Œä¸¦æœŸæœ›æœªä¾†æŠŠå®ƒæ‡‰ç”¨åœ¨ NLPã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>GIT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 29 Mar 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2205.14100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GIT: A Generative Image-to-text Transformer for Vision and Language&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•   
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;è¨­è¨ˆäº†ä¸€å€‹ Generative Image-to-text Transformerï¼Œçµ±ä¸€ vision-language tasksï¼Œåƒæ˜¯ image/video captioning æˆ–æ˜¯å•ç­”ã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶ generative models åœ¨é è¨“ç·´å’Œå¾®èª¿çš„æ™‚å€™æ˜¯åŒæ¨£çš„ç¶²è·¯æ¶æ§‹ï¼Œç¾æœ‰çš„å·¥ä½œé€šå¸¸éƒ½åŒ…å«è¤‡é›œçš„æ¶æ§‹ (uni/multi-modal encoder/decoder)ï¼Œ
è€Œä¸”ä¾è³´æ–¼å¤–éƒ¨æ¨¡çµ„ï¼Œæ¯”å¦‚ç‰©ä»¶åµæ¸¬æˆ– optical character recognition (OCR)ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ GITï¼Œæˆ‘å€‘ç°¡åŒ–ç‚º single language modeling task ä¸‹çš„ä¸€å€‹ image encoder å’Œä¸€å€‹ text decoderã€‚&lt;/p&gt;
&lt;p&gt;æ“´å¤§äº†é è¨“ç·´è³‡æ–™å’Œæ¨¡å‹å¤§å°ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨è¨±å¤šå…·æœ‰æŒ‘æˆ°æ€§çš„ benchmarks ä¸Šå–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚é¦–æ¬¡åœ¨ TextCpas ä¸Šè¶…è¶Šäººé¡çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;æå‡ºäº†ä¸€ç¨® generation-based image classification and scene text recognition çš„æ–°æ–¹æ¡ˆã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;è¿‘å¹´ä¾†åœ¨ vision-languageï¼ˆVLï¼‰é è¨“ç·´æ–¹é¢å–å¾—äº†å·¨å¤§é€²å±•ï¼Œç‰¹åˆ¥æ˜¯åŸºæ–¼ image-text pairs çš„å¤§è¦æ¨¡æ•¸æ“šï¼Œä¾‹å¦‚ CLIPã€Florence å’Œ SimVLMã€‚&lt;/p&gt;
&lt;p&gt;å­¸ç¿’åˆ°çš„ representation å¾ˆå¥½çš„æé«˜äº†ä¸‹æ¸¸ä»»å‹™çš„æ€§èƒ½ï¼Œæ¯”å¦‚ image captioningã€visual question answering å’Œ image-text retrievalã€‚&lt;/p&gt;
&lt;p&gt;åœ¨é è¨“ç·´éç¨‹ä¸­ï¼ŒMasked Language Modeling (MLM) å’Œ Image-Text Matching (ITM) è¢«å»£æ³›ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œé€™äº› loss å’Œä¸‹æ¸¸ä»»å‹™ä¸åŒï¼Œå¿…é ˆåš task-specific adaptationã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼Œ image captioning è¦ç§»é™¤ ITMï¼ŒVQA éœ€è¦é¡å¤–éš¨æ©Ÿåˆå§‹çš„ MLPã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ¸›å°‘é€™ç¨®å·®ç•°ï¼Œæœ€è¿‘çš„ç ”ç©¶è©¦åœ–ç‚ºé è¨“ç·´æ¨¡å‹è¨­è¨ˆ unified generative models ä¾†é è¨“ç·´ï¼Œå› ç‚ºå¤§å¤šæ•¸ VL çš„å•é¡Œå¯ä»¥è½‰åŒ–ç‚ºç”Ÿæˆå•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›æ–¹æ³•é€šå¸¸åˆ©ç”¨ multi-modal encoder å’Œ text decoderï¼Œä¸¦ç²¾å¿ƒè¨­è¨ˆ text input å’Œ text targetã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æ¨å‹•é€™æ–¹å‘çš„ç ”ç©¶ï¼Œä½œè€…è¨­è¨ˆäº†ä¸€å€‹ç°¡å–®çš„ Generative Image-to-text Transformerï¼Œç¨±ä½œ GITï¼ŒåªåŒ…å«ä¸€å€‹ image encoder å’Œ text decoderã€‚&lt;/p&gt;
&lt;p&gt;é è¨“ç·´ä»»å‹™åªæ˜¯æŠŠè¼¸å…¥çš„åœ–åƒæ˜ å°„åˆ°ç›¸é—œè¯çš„æ–‡å­—æè¿°ã€‚&lt;/p&gt;
&lt;p&gt;ç›¡ç®¡ä»–å¾ˆç°¡å–®ï¼Œä½†é‚„æ˜¯åœ¨çœ¾å¤šå…·æœ‰æŒ‘æˆ°æ€§çš„ benchmark å–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;image encoder æ˜¯ Swin-like vision transformerï¼Œåœ¨å¤§é‡çš„ image-text pairs ä¸Šåš pretrainï¼ŒåŸºæ–¼ contrastive taskã€‚&lt;/p&gt;
&lt;p&gt;é€™æ¶ˆé™¤äº†ç¾æœ‰è¨±å¤šæ–¹æ³•ä¸­å° object detector çš„ä¾è³´ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†å°‡å…¶æ“´å±•åˆ°å½±ç‰‡é ˜åŸŸï¼Œæˆ‘å€‘æŠŠå¤šå€‹ frame çš„ç‰¹å¾µ concatenateï¼Œä½œç‚º video è¡¨ç¤ºã€‚&lt;/p&gt;
&lt;p&gt;text decoder æ˜¯ä¸€å€‹ç”¨ä¾†é æ¸¬ç›¸é—œè¯æ–‡å­—çš„ transformerã€‚&lt;/p&gt;
&lt;p&gt;æ•´å€‹ç¶²è·¯éƒ½æ˜¯åŸºæ–¼ language modeling task ä¾†è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ VQAï¼Œinput question è¢«çœ‹ä½œ text prefixï¼Œä¸¦ä»¥ auto-regressive çš„æ–¹æ³•ç”Ÿå‡ºç­”æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†ä¸€ç¨® generation-based çš„ ImageNet classification æ–°æ–¹æ¡ˆï¼Œé æ¸¬æ¨™ç±¤ç›´æ¥æ ¹æ“šä½œè€…çš„ç”Ÿæˆæ¨¡å‹ï¼Œè€Œä¸ç”¨é å…ˆå®šç¾©è©å½™è¡¨ã€‚&lt;/p&gt;
&lt;p&gt;æˆ‘å€‘çš„ä½œæ³•å¾ˆç°¡å–®ï¼Œä½†åœ¨æ“´å¤§é è¨“ç·´è³‡æ–™å’Œæ¨¡å‹å¤§å°å¾Œï¼Œæˆæœé©šäººã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦è²¢ç»å¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘å±•ç¤ºäº† GITï¼Œåƒ…ç”±ä¸€å€‹ image encoder å’Œä¸€å€‹ text decoder çµ„æˆï¼Œé€é language modeling taskï¼Œåœ¨ 0.8 billion image-text pairs ä¸Š pretrainã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;åœ¨ image/video captioning å’Œ QA ä¸Šï¼Œæ²’æœ‰åŸºæ–¼ object detectorsï¼Œobject tags å’Œ OCRï¼Œå°±åœ¨å¤šå€‹ä»»å‹™ä¸Šå–å¾— SOTAã€‚è­‰æ˜ç°¡å–®çš„ç¶²è·¯æ¶æ§‹ä¹Ÿå¯ä»¥é€é scaling å–å¾—å¼·å¤§çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘è­‰æ˜ GIT é›–ç„¶ pretrain åœ¨ image-text pairsï¼Œä¹Ÿèƒ½åœ¨ video tasks ä¸Šå–å¾— SOTAï¼Œä¸éœ€è¦ video dedicated encodersã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘æå‡ºäº†ä¸€ç¨®æ–°çš„ generation-based image classification æ–¹æ¡ˆï¼Œåœ¨ ImageNet-1K ä¸Šï¼Œå–å¾—ä¸éŒ¯çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/table1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ VL pre-training ä¸­ï¼Œå¤š multi-task pre-training è¢«å»£æ³›ä½¿ç”¨ï¼Œè³¦äºˆç¶²è·¯å¤šç¨®æˆ–å¢å¼·çš„èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;æ¯”å¦‚ï¼ŒMLM å’Œ ITM æ˜¯å»£æ³›æ¡ç”¨çš„é è¨“ç·´ä»»å‹™ï¼Œæœ€è¿‘ä¹Ÿæœ‰ç ”ç©¶åŠ å…¥ image-text contrastive lossã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼å¤šæ•¸ VL ä»»å‹™éƒ½å¯ä»¥è¡¨ç¤ºæˆ text generation taskï¼Œæ‰€ä»¥å¯ä»¥è¨“ç·´ä¸€å€‹ç”Ÿæˆæ¨¡å‹ä¾†æ”¯æŒå„ç¨®ä¸‹æ¸¸ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;è¼¸å…¥å’Œè¼¸å‡ºæ–‡æœ¬é€šå¸¸éƒ½æœƒç¶“éç²¾å¿ƒè¨­è¨ˆï¼Œä»¥é è¨“ç·´é€™æ¨£çš„ç”Ÿæˆæ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ image representationï¼ŒFaster RCNN è¢«å¤§å¤šæ•¸ç¾æœ‰æ–¹æ³•ç”¨ä¾†æå–å€åŸŸç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;åŒæ™‚ï¼Œä¹Ÿå¾ˆå®¹æ˜“ä»¥ end-to-end çš„æ–¹æ³•è¨“ç·´æ•´å€‹ç¶²è·¯ã€‚&lt;/p&gt;
&lt;p&gt;é™¤äº† feature mapï¼Œobject tagsï¼Œä¹Ÿå¾ˆå¸¸è¢«ç”¨ä¾†æ–¹ä¾¿ transformer ç†è§£ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ¥æ˜¯ novel objectsã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼èˆ‡å ´æ™¯æ–‡æœ¬ç›¸é—œçš„ä»»å‹™ï¼Œèª¿ç”¨ OCR ä»¥ç”Ÿæˆå ´æ™¯æ–‡æœ¬ä½œç‚ºé™„åŠ ç¶²è·¯è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼ text predictionï¼Œå¸¸ç”¨ transformer networkï¼Œçµåˆ cross-attention module ä¾†èåˆ image tokensã€‚&lt;/p&gt;
&lt;p&gt;æˆ–è€…åªæ˜¯å–®ç´” concatenate text tokens å’Œ image tokensï¼Œç„¶å¾Œç”¨ self-attentionã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å€‘æœ‰ 9 å€‹ä¸åŒçš„ benchmarkï¼Œ3 ç¨®ä¸åŒæ¨¡å‹å¤§å°å’Œ 3 ç¨®ä¸åŒé è¨“ç·´è³‡æ–™è¦æ¨¡ã€‚&lt;/p&gt;
&lt;h2 id=&#34;generative-image-to-text-transformer&#34;&gt;Generative Image-to-text Transformer&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;network-architecture&#34;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;image encoder åŸºæ–¼ contrastive pre-trained modelã€‚&lt;/p&gt;
&lt;p&gt;è¼¸å…¥æ˜¯åŸå§‹åœ–åƒï¼Œè¼¸å‡ºæ˜¯ compact 2D feature mapï¼Œè¢« flatten æˆ list of featuresã€‚&lt;/p&gt;
&lt;p&gt;é€éä¸€å€‹é¡å¤–çš„ linear layer å’Œä¸€å€‹ layernorm layerï¼Œimage features è¢« project åˆ° D dimensionsï¼Œä¹Ÿå°±æ˜¯ text encoder çš„ inputã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä½¿ç”¨åš contrastive tasks pretraining çš„ image encoderï¼Œå› ç‚ºæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜é€™ç¨® image encoder æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨å¾Œé¢çš„ç« ç¯€ï¼Œé‚„è§€å¯Ÿåˆ° VL performence æ˜é¡¯åœ°éš¨è‘—æ›´å¼·çš„ image encoder è€Œæœ‰æ‰€æå‡ã€‚
é€™å’Œ object detection-based çš„æ–¹æ³•è§€å¯Ÿåˆ°çš„çµæœä¸€è‡´ã€‚&lt;/p&gt;
&lt;p&gt;CoCa çš„ concurrent work çµ±ä¸€äº† contrastive task å’Œ the generation taskï¼Œä½œç‚ºä¸€å€‹é è¨“ç·´éšæ®µã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„æ–¹æ³•ç›¸ç•¶æ–¼æ˜¯æŒ‰é †åºåˆ†é›¢å…©å€‹ä»»å‹™:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨ contrastive task è¨“ç·´ image encoder&lt;/li&gt;
&lt;li&gt;ç”¨ generation task pretrain image encoder å’Œ text decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;text decoder æ˜¯ä¸€å€‹ç”¨æ–¼é æ¸¬æ–‡æœ¬æè¿°çš„ transformer moduleï¼Œç”±å¤šå€‹ transformer block çµ„æˆï¼Œæ¯å€‹ transformer block ç”±ä¸€å€‹ self-attention layer å’Œ feed-forward layer çµ„æˆã€‚&lt;/p&gt;
&lt;p&gt;text è¢« tokenize å’Œ embed åˆ° D dimensionsï¼Œä¸¦æ·»åŠ  positional encoding å’Œ layernorm layerã€‚&lt;/p&gt;
&lt;p&gt;image features å’Œ text embeddings è¢« concatenate èµ·ä¾†ä½œç‚º transformer module çš„è¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;text ä»¥ [BOS] é–‹å§‹ï¼Œä¸¦ä»¥ auto regressive çš„æ–¹å¼ decodeï¼Œç›´åˆ° [EOS] æˆ– maximum stepsã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;attention mask æ ¹æ“šä¸Šåœ–è¨­è¨ˆï¼Œä½¿çš„ text token åªèƒ½ä¾è³´æ–¼å‰é¢çš„ text token å’Œ image tokenï¼Œè€Œ image token å¯ä»¥äº’ç›¸åš attentionã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œ unidirectional attention mask ä¸åŒï¼Œunidirectional attention mask ä¸¦éæ¯å€‹ image token éƒ½å¯ä»¥ä¾è³´æ–¼å…¶ä»–çš„ Image tokenã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…å¾ˆå¥½åœ°åˆå§‹åŒ– image encoderï¼Œå»éš¨æ©Ÿåˆå§‹åŒ– text decoderã€‚&lt;/p&gt;
&lt;p&gt;é€™ç¨®è¨­è¨ˆå‹•æ©Ÿæ˜¯åŸºæ–¼[MiniVLM: A Smaller and Faster Vision-Language Model]ï¼Œè©²ç ”ç©¶éš¨æ©Ÿåˆå§‹åŒ–é¡¯ç¤ºå‡ºèˆ‡ BERT åˆå§‹åŒ–ç›¸ä¼¼åœ°æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;åŸå› å¯èƒ½åœ¨æ–¼ BERT åœ°åˆå§‹åŒ–ç„¡æ³•ç†è§£åœ–åƒä¿¡è™Ÿï¼Œé€™å°æ–¼ VL ä»»å‹™è‡³é—œé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;[Flamingo: a Visual Language Model for Few-Shot Learning] æ¡ç”¨äº†é¡ä¼¼çš„ image encoder + text decoderï¼Œä½†æ˜¯ä»–å€‘çš„ decoder ç¶“é pretrainï¼Œä¸¦ä¸”æœ‰ freezeï¼Œå¥½ä¿ç•™å¤§å‹èªè¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚&lt;/p&gt;
&lt;p&gt;GIT çš„æ‰€æœ‰åƒæ•¸éƒ½æœƒæ›´æ–°ï¼Œä»¥æ›´å¥½åœ°é©æ‡‰ VL çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€ç¨®æ¶æ§‹æ˜¯ cross-attention-based çš„ decoderï¼Œç”¨æ–¼ incorporate image signalsï¼Œè€Œä¸æ˜¯ concatenation å†ç”¨ self-attentionã€‚&lt;/p&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œlarge-scale çš„ pre-trainingï¼Œself-attention-based æœƒæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå°è¦æ¨¡çš„å‰‡æ˜¯ cross-attention-basedã€‚&lt;/p&gt;
&lt;p&gt;ä¸€å€‹åˆç†çš„è§£é‡‹æ˜¯ï¼Œç¶“éå……åˆ†è¨“ç·´ï¼Œdecoder å¯ä»¥å¾ˆå¥½åœ°è™•ç†åœ–åƒå’Œæ–‡æœ¬ï¼Œè€Œä¸” image token å¯ä»¥ç‚ºäº† text generation æ›´å¥½åœ°æ›´æ–°ã€‚&lt;/p&gt;
&lt;p&gt;è€Œ cross-attention è®“ image token æ²’è¾¦æ³• attend å½¼æ­¤ã€‚&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h3&gt;
&lt;p&gt;è¨“ç·´æ¡ç”¨ language modeling (LM) lossã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/for1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$I$ æ˜¯ image&lt;/li&gt;
&lt;li&gt;$y_i,i \in $ { $ 1,&amp;hellip;,N $ } æ˜¯æ–‡å­— tokenï¼Œ$y_0$ æ˜¯ [BOS]ï¼Œ$y_{N+1}$ æ˜¯ [EOS]&lt;/li&gt;
&lt;li&gt;CE æ˜¯æœ‰ 0.1 label smoothing çš„ cross-entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å¦ä¸€ç¨®é¸æ“‡æ˜¯ MLMï¼Œåœ¨æ¯å€‹ epoch ä¸­é æ¸¬ 15% çš„è¼¸å…¥ tokenï¼Œè¦é æ¸¬æ‰€æœ‰ token è‡³å°‘éœ€è¦ 1 / 0.15 = 6.7 å€‹ epochsï¼Œå°æ–¼ LMï¼Œæ¯å€‹ epoch éƒ½å¯ä»¥é æ¸¬æ‰€æœ‰ tokenï¼Œå°æ–¼å¤§è¦æ¨¡é è¨“ç·´è³‡æ–™ä¾†èªªæ•ˆç‡æ›´é«˜ã€‚&lt;/p&gt;
&lt;p&gt;ablation studies é¡¯ç¤ºå‡º LM å¯ä»¥åœ¨æœ‰é™çš„ epoch å…§å¯¦ç¾æ›´å¥½çš„æ€§èƒ½ã€‚
åœ¨å¤§è¦æ¨¡è¨“ç·´ä¸­ï¼Œç”±æ–¼è¨ˆç®—è³‡è¨Šçš„é™åˆ¶ï¼Œåªæœ‰å…©å€‹ epochï¼Œæ‰€ä»¥é¸æ“‡ LMã€‚
èˆ‡æ­¤åŒæ™‚ï¼Œå¤§éƒ¨åˆ†æœ€è¿‘çš„ large-scale language model ä¹Ÿæ˜¯åŸºæ–¼ LMã€‚&lt;/p&gt;
&lt;p&gt;å¦‚æœæ²’æœ‰åœ–åƒè¼¸å…¥ï¼Œè©²æ¨¡å‹å°‡ç°¡åŒ–ç‚º decoder-only çš„èªè¨€æ¨¡å‹ï¼Œæ¶æ§‹é¡ä¼¼æ–¼ GPT-3ã€‚&lt;/p&gt;
&lt;p&gt;å› æ­¤ï¼Œé€™ç¨®è¨­è¨ˆé‚„å¯ä»¥åˆ©ç”¨ text-only çš„è³‡æ–™ä¾†æå‡ scaled-up decoder çš„èƒ½åŠ›ï¼ŒæŠŠé€™ä¿ç•™çµ¦æœªä¾†çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;p&gt;å°æ–¼ image captioningï¼Œç”±æ–¼è¨“ç·´æ•¸æ“šæ ¼å¼å’Œé è¨“ç·´ç›¸åŒï¼Œæ‰€ä»¥ç”¨åŒæ¨£çš„ LM task ä¾†å¾®èª¿ GITã€‚
å°æ–¼ visual question answeringï¼Œå•é¡Œå’Œ GT åœ¨å¾®èª¿çš„æ™‚å€™è¢«çœ‹åš special captionï¼Œä½† LM loss åƒ…ç”¨æ–¼ç­”æ¡ˆå’Œ [EOS]ã€‚&lt;/p&gt;
&lt;p&gt;æ¨ç†éç¨‹ä¸­ï¼Œquestion è¢«ç•¶ä½œ caption çš„ prefixï¼Œå®Œæˆçš„éƒ¨åˆ†æ˜¯é æ¸¬ã€‚&lt;/p&gt;
&lt;p&gt;VQAv2 ç¾æœ‰çš„å·¥ä½œæ”¶é›†å€™é¸ç­”æ¡ˆï¼Œå†é‡æ§‹æˆåˆ†é¡å•é¡Œï¼Œé æ¸¬ä¸€æ¬¡ã€‚
ä½œè€…çš„å·¥ä½œæœ‰æ›´å¤šæŒ‘æˆ°ï¼Œå› ç‚ºæ˜¯ç”Ÿæˆå¼çš„ï¼Œéœ€è¦ç”Ÿå‡ºè‡³å°‘å…©å€‹æ­£ç¢ºçš„ tokenï¼Œç­”æ¡ˆå’Œ [EOS]ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œè€ƒæ…®åˆ°è‡ªç”±å½¢å¼ç­”æ¡ˆçš„å¥½è™•ï¼Œä½œè€…é¸æ“‡äº†ç”Ÿæˆæ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ç”Ÿæˆæ¨¡å‹çš„é›£åº¦ï¼ŒVQAv2 æ¯”ç¾æœ‰çš„åˆ¤åˆ¥å·¥ä½œç•¥å·®ã€‚&lt;/p&gt;
&lt;p&gt;å°æ–¼å’Œ scene-text related VQA ä»»å‹™ï¼Œç¾æœ‰æ–¹æ³•é€šå¸¸åˆ©ç”¨ OCR ç”Ÿæˆ 5 å€‹ scene text ä¸¦ç”¨ dynamic pointer network æ±ºå®šç•¶å‰è¼¸å‡ºæ‡‰è©²æ˜¯ OCR é‚„æ˜¯ general textã€‚&lt;/p&gt;
&lt;p&gt;ä½†ç”±æ–¼ä½œè€…çš„æ–¹æ³•ä¸ä¾è³´æ–¼ OCRï¼Œå› æ­¤ä¹Ÿä¸ä¾è³´æ–¼ dynamic pointer networkã€‚&lt;/p&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œä½œè€…ç™¼ç¾æ¨¡å‹é€éå¤§è¦æ¨¡é è¨“ç·´è³‡æ–™å­¸æœƒå¦‚ä½•é–±è®€å ´æ™¯æ–‡æœ¬ï¼Œä¸¦ä¸”ä½œè€…çš„æ¨¡å‹ä¸æ˜¯å°ˆé–€ç‚ºäº†å½±ç‰‡é ˜åŸŸè¨­è¨ˆçš„ï¼Œä½†å¯ä»¥é€éç°¡å–®çš„æ¶æ§‹æ›´æ”¹å°±å–å¾—å…·æœ‰ç«¶çˆ­åŠ›æˆ–ç”šè‡³ SOTA çš„æˆæœï¼Œä¹Ÿå°±æ˜¯ä½œè€…å¯ä»¥å¾æ¯å€‹å½±ç‰‡æ¡æ¨£å¤šå€‹ frameï¼Œä¸¦é€é image encoder ç¨ç«‹åœ°ç‚ºæ¯å€‹ frame ç·¨ç¢¼ã€‚
æœ€å¾Œæ·»åŠ ä¸€å€‹ learnable temporal embedding (åˆå§‹åŒ–ç‚º 0)ï¼Œä¸¦ concatenate sampled frames çš„ç‰¹å¾µã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…é‚„ç”¨æ–¼åœ–ç‰‡åˆ†é¡ï¼ŒæŠŠ class name ç”¨æ–¼ captionã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œç¾æœ‰å·¥ä½œä¸ä¸€æ¨£ï¼Œç¾æœ‰å·¥ä½œé€šå¸¸å…ˆå®šç¾©è©å½™è¡¨ï¼Œä¸¦ç”¨ç·šæ€§å±¤é æ¸¬æ¯å€‹é¡åˆ¥çš„å¯èƒ½æ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç•¶æ–°æ•¸æ“šå’Œæ–°é¡åˆ¥è¢«æ·»åŠ åˆ°ç¾æœ‰æ•¸æ“šçš„æ™‚å€™ï¼Œé€™ç¨®æ–°ä¸€ä»£çš„æ–¹æ¡ˆæ˜¯æœ‰ç›Šçš„ï¼Œå› ç‚ºé€™æ¨£å¯ä»¥åœ¨ä¸å¼•å…¥æ–°åƒæ•¸çš„æƒ…æ³ä¸‹å°æ–°æ•¸æ“šé€²è¡Œè¨“ç·´ã€‚&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;setting&#34;&gt;Setting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æ”¶é›† 0.8B çš„ image-text pairs ä¾†é è¨“ç·´ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image encoder æ˜¯æ ¹æ“š  pre-trained contrastive model åˆå§‹åŒ–çš„ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hidden dimension (D) = 768&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;text decoder æœ‰ 6 å€‹ randomly-initialized transformer blocks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å…±æœ‰ 0.7b çš„åƒæ•¸&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image decoder å’Œ text encoder çš„ learning rate å„åˆ¥æ˜¯ 1e-5 å’Œ 5e-5ï¼Œéƒ½ cosine decay åˆ° 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ¨è«–éšæ®µ beam size æ˜¯ 4ï¼Œlength penalty æ˜¯ 0.6ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supplementary materials å±•ç¤ºäº†å°æ¨¡å‹è®Šé«” (GITB and GITL) å’Œæ›´å¤§æ¨¡å‹ (GIT2) çš„çµæœ&lt;/p&gt;
&lt;h3 id=&#34;results-on-image-classification&#34;&gt;Results on Image Classification&lt;/h3&gt;
&lt;p&gt;è¼¸å‡ºå¿…é ˆèˆ‡é¡åˆ¥åç¨±å®Œå…¨åŒ¹é…ï¼Œç”šè‡³è€ƒæ…®å¤šæˆ–å°‘çš„ç©ºæ ¼ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼ä¸çŸ¥é“è©å½™è¡¨ï¼Œç²¾ç¢ºåŒ¹è¢«æº–ç¢ºåº¦åªæœ‰ 1.93%ï¼Œå¦‚æœé æ¸¬åŒ…å« GT å°±å°ï¼Œé‚£æœ‰ 40.88%ã€‚&lt;/p&gt;
&lt;p&gt;é€šéå¾®èª¿æ¯å€‹é¡åˆ¥åªæœ‰ 1 shot æˆ– 5 shotï¼Œæº–ç¢ºåº¦æœƒé¡¯è‘—æé«˜ï¼Œ
è¡¨æ˜åªç”¨å°‘é‡è¨“ç·´æ¨£æœ¬ï¼Œä¹Ÿå¯ä»¥è¼•é¬†é©æ‡‰ä¸‹æ¸¸ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;èˆ‡ Flamingo ç›¸æ¯”ï¼ŒGIT å¯¦ç¾æ›´é«˜çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;p&gt;Flamingo åœ¨æ²’æœ‰åƒæ•¸æ›´æ–°çš„æƒ…æ³ä¸‹é€²è¡Œå°æ¨£æœ¬å­¸ç¿’ï¼Œä½†éœ€è¦é¡å¤–çš„ç¶²è·¯è¼¸å…¥ï¼Œå¯èƒ½æœƒå¢åŠ æ¨ç†æˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGIT é€éä¸€æ¬¡ lightweight fine-tuningï¼Œæ¨ç†éç¨‹ä¸­ä¸éœ€è¦é€™äº› training shotã€‚&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;
&lt;h4 id=&#34;model-and-data-scaling&#34;&gt;Model and data scaling&lt;/h4&gt;
&lt;p&gt;å°æ–¼ç¶²è·¯æ¶æ§‹ï¼Œä½œè€…çš„æ¨¡å‹è¢«ç¨±ä½œ Hugeï¼ŒæŠŠ image encoder æ›æˆ CLIP çš„ ViT-B/16 å’Œ ViT-L/14 çš„å‰‡æ˜¯ Base å’Œ Largeã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/fig4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;å¯ä»¥çœ‹å‡ºè¼ƒå¤§çš„ image encoder å¸¶ä¾†çš„å¥½è™•ï¼Œä½†æ ¹æ“šå¯¦é©—ï¼Œ
ä½œè€…ç™¼ç¾å¾ˆé›£æœ‰æ•ˆåœ°æ“´å±• text decoderï¼ŒåŸå› å¯èƒ½æ˜¯ LM å¾ˆé›£ç”¨ limited amount of text ä¾†è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹å¯èƒ½çš„åŸå› æ˜¯ image encoder è² è²¬ object recognitionï¼Œè€Œ decoder è² è²¬ä»¥ NLP çš„æ–¹æ³•çµ„ç¹” object termsã€‚
å¾Œä¸€é …ä»»å‹™å¯èƒ½å¾ˆå®¹æ˜“ï¼Œå› ç‚ºå¤§å¤šæ•¸æè¿°éƒ½éµå¾ªç›¸ä¼¼çš„æ¨¡å¼ï¼Œæ¯”å¦‚ Object + verb + subjectï¼Œæ‰€ä»¥åªè¦ä¸€å€‹ small decoderï¼Œè¼ƒå¤§çš„ decoder å¯èƒ½æœƒå¢åŠ å­¸ç¿’é›£åº¦ã€‚&lt;/p&gt;
&lt;p&gt;Flamingo çš„ç ”ç©¶é¡¯ç¤ºæ›´å¤§çš„ Decoder å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†æ˜¯ä»–å€‘çš„ decoder æœ‰ pretrain éï¼Œè€Œä¸”åœ¨ VL é è¨“ç·´çš„æ™‚å€™ frozenï¼Œé¿é–‹äº†å¦‚ä½•æœ‰æ•ˆè¨“ç·´ decoder çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;LEMON çš„ transformer å¯ä»¥æ“´å±•åˆ° 32 å±¤ï¼Œå¯èƒ½æ˜¯å› ç‚ºä»–å€‘ä½¿ç”¨ MLM è€Œä¸æ˜¯ LMï¼Œå¾Œè€…å¯èƒ½æ›´åŠ å›°é›£ã€‚&lt;/p&gt;
&lt;h4 id=&#34;scene-text-in-pre-training-data&#34;&gt;Scene text in pre-training data&lt;/h4&gt;
&lt;p&gt;ç‚ºäº†ç­è§£ scene text comprehension çš„èƒ½åŠ›ï¼Œä½œè€…æª¢æŸ¥äº† pretrain data æœ‰å¤šå°‘ image-text pairs æœ‰ scene textã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç”¨ Microsoft Azure OCR API4 å°ä¸€äº›è³‡æ–™åš OCRï¼Œç„¶å¾ŒæŠŠ OCR çµæœå’Œ associated text åšæ¯”å°ï¼Œåªæœ‰åŒ…å«é•·åº¦è¶…é 5 å€‹å­—å…ƒçš„ OCR çµæœæ‰æœƒç®—æ¯”å°ã€‚
æœ‰ 15% çš„ CC12M å’Œ 31% çš„ä¸‹è¼‰åœ–åƒ(500K) åŒ…å« scene text æè¿°ã€‚
ç”±æ–¼ä»»å‹™æ˜¯è¨“ç·´é æ¸¬ textï¼Œç¶²è·¯é€æ¼¸å­¸æœƒé–±è®€ scene textã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;h3 id=&#34;limitations&#34;&gt;Limitations&lt;/h3&gt;
&lt;p&gt;æ ¹æ“šå¯¦é©—ï¼Œç›®å‰ä¸æ¸…æ¥šå¦‚ä½•æ§åˆ¶ç”Ÿæˆçš„ caption ä»¥åŠå¦‚ä½•åœ¨ä¸æ›´æ–°åƒæ•¸çš„æƒ…æ³ä¸‹åŸ·è¡Œ in-context learningï¼ŒæŠŠé€™ç•™çµ¦æœªä¾†çš„å·¥ä½œã€‚&lt;/p&gt;
&lt;h3 id=&#34;societal-impact&#34;&gt;Societal impact&lt;/h3&gt;
&lt;p&gt;è©²æ¨¡å‹åœ¨å¤§è¦æ¨¡æ•¸æ“šé›†ä¸Šé è¨“ç·´ï¼Œä¸èƒ½ä¿è­‰æ•¸æ“šä¸å« toxic languageï¼Œå¯èƒ½æœƒ poison outputã€‚&lt;/p&gt;
&lt;h2 id=&#34;å…¶ä»–&#34;&gt;å…¶ä»–&lt;/h2&gt;
&lt;h3 id=&#34;a3-network&#34;&gt;A.3 Network&lt;/h3&gt;
&lt;p&gt;è¬›è¶…åƒæ•¸&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/microsoft-GIT/model.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>RoBERTa è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/roberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 22 Mar 2023 01:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/roberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1907.11692&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â•
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ç™¼ç¾ BERT è¨“ç·´ä¸è¶³ï¼Œä¸¦ä¸”ä½œè€…çš„æ¨¡å‹åœ¨ 4/9 çš„ GLUE ä»»å‹™, RACE å’Œ SQuAD å–å¾— SOTAã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;è‡ªç›£ç£çš„è¨“ç·´æ–¹æ³•å¸¶ä¾†äº†é¡¯è‘—çš„æ€§èƒ½æå‡ï¼Œä½†è¦ç¢ºå®šé€™ä¸€å †æ–¹æ³•ä¸­çš„å“ªäº›æ–¹é¢è²¢ç»æœ€å¤§ï¼Œå…·å‚™æŒ‘æˆ°æ€§ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´çš„è¨ˆç®—é‡æ˜¯æ˜‚è²´çš„ï¼Œä½¿ fine-tune å—é™ï¼Œè€Œä¸”é€šå¸¸éƒ½æ˜¯ç”¨ä¸åŒå¤§å°çš„ private training dataï¼Œä½¿è©•ä¼°æ¨¡å‹æ›´åŠ å›°é›£ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æå‡ºäº†å° BERT é è¨“ç·´çš„ replication studyï¼ŒåŒ…æ‹¬å°è¶…åƒæ•¸çš„èª¿æ•´ï¼Œä»¥åŠå°è¨“ç·´é›†å¤§å°çš„ä»”ç´°è©•ä¼°ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ BERT è¨“ç·´ä¸è¶³ï¼Œä¸¦æå‡ºäº†ä¸€ç¨®æ”¹é€²æ–¹æ³•ï¼Œç¨±ç‚º RoBERTaï¼Œå¯ä»¥é”åˆ°æˆ–è¶…éæ‰€æœ‰ post-BERT çš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä¿®æ”¹å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;è¨“ç·´æ¨¡å‹çš„æ™‚é–“æ›´é•·ï¼Œbatch æ›´å¤§ï¼Œç”¨æ›´å¤š data&lt;/li&gt;
&lt;li&gt;ç§»é™¤ next sentence prediction objective&lt;/li&gt;
&lt;li&gt;è¨“ç·´æ›´é•·çš„åºåˆ—&lt;/li&gt;
&lt;li&gt;å‹•æ…‹åœ°æ”¹è®Šç”¨æ–¼è¨“ç·´è³‡æ–™çš„ masking pattern&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è²¢ç»:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æå‡ºä¸€çµ„é‡è¦çš„ BERT è¨­è¨ˆé¸æ“‡å’Œè¨“ç·´ç­–ç•¥&lt;/li&gt;
&lt;li&gt;ä½¿ç”¨äº†æ–°çš„ datasetï¼Œå«åš CCNEWSï¼Œä¸¦è­‰æ˜ç”¨æ›´å¤šçš„è³‡æ–™ä¾†é è¨“ç·´ï¼Œå¯ä»¥æé«˜ä¸‹æ¸¸ä»»å‹™çš„è¡¨ç¾&lt;/li&gt;
&lt;li&gt;è¨“ç·´è¡¨æ˜ï¼Œåœ¨æ­£ç¢ºçš„è¨­è¨ˆé¸æ“‡ä¸‹ï¼Œpretrained masked language model å’Œå…¶ä»–æœ€è¿‘çš„æ–¹æ³•æ¯”ï¼Œå…·æœ‰ç«¶çˆ­åŠ›&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;å° BERT åšå›é¡§&lt;/p&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L layers&lt;/li&gt;
&lt;li&gt;A self-attention heads&lt;/li&gt;
&lt;li&gt;H hidden dimension&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;training-objectives&#34;&gt;Training Objectives&lt;/h3&gt;
&lt;p&gt;é è¨“ç·´çš„æ™‚å€™ï¼ŒBERT æœ‰å…©å€‹ç›®æ¨™: masked language modeling å’Œ next sentence prediction&lt;/p&gt;
&lt;h4 id=&#34;masked-language-model-mlm&#34;&gt;Masked Language Model (MLM)&lt;/h4&gt;
&lt;p&gt;BERT éš¨æ©Ÿé¸æ“‡ 15% çš„ token é€²è¡Œå¯èƒ½çš„æ›¿æ›&lt;/p&gt;
&lt;p&gt;80% æ›æˆ [MASK]ï¼Œ10% ä¿æŒä¸è®Šï¼Œ10% è¢«é¸ç‚ºä¸€å€‹éš¨ä¾¿çš„ vocabulary token&lt;/p&gt;
&lt;h4 id=&#34;next-sentence-prediction-nsp&#34;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;
&lt;p&gt;åˆ†é¡ç¬¬äºŒå¥æ˜¯ä¸æ˜¯ä¸‹ä¸€å¥ï¼Œæ˜¯äºŒå…ƒåˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;æ­£ä¾‹ç”±æå–é€£çºŒçš„å¥å­ç”¢ç”Ÿï¼Œè² ä¾‹ç”±ä¸åŒçš„ç‰‡æ®µé…å°ç”¢ç”Ÿã€‚&lt;/p&gt;
&lt;p&gt;æ­£ä¾‹å’Œè² ä¾‹ä»¥ç›¸ç­‰æ©Ÿç‡ç”¢ç”Ÿã€‚&lt;/p&gt;
&lt;h4 id=&#34;optimization&#34;&gt;Optimization&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;li&gt;$\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = 1e-6&lt;/li&gt;
&lt;li&gt;$L_2$ weight decay of 0.01&lt;/li&gt;
&lt;li&gt;Learning rate å‰ 10,000 step warm up åˆ° 1e-4ï¼Œç„¶å¾Œ linear decay&lt;/li&gt;
&lt;li&gt;å…¨éƒ¨çš„ layer å’Œ attention weight éƒ½ dropout 0.1&lt;/li&gt;
&lt;li&gt;GELU æ¿€æ´»å‡½æ•¸&lt;/li&gt;
&lt;li&gt;1,000,000 æ¬¡ updateï¼Œbatch size 256ï¼Œåºåˆ—é•·åº¦ 512&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data&#34;&gt;Data&lt;/h4&gt;
&lt;p&gt;BERT åœ¨ BookCorpus å’Œ English Wikipedia æ··å’Œçš„è³‡æ–™é›†ä¸Šè¨“ç·´ï¼Œå…±æœ‰ 16GB çš„æœªå£“ç¸®æ–‡æœ¬&lt;/p&gt;
&lt;h2 id=&#34;experimental-setup&#34;&gt;Experimental Setup&lt;/h2&gt;
&lt;p&gt;æè¿°å°æ–¼ BERT çš„ replication study çš„å¯¦é©—è¨­ç½®&lt;/p&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;p&gt;ä½œè€…ç”¨ FAIRSEQ é‡æ–°å¯¦ç¾äº† BERTã€‚&lt;/p&gt;
&lt;p&gt;ä¸»è¦éµå¾ª [Background-Optimization] ä¸­çš„ BERT åŸå§‹è¶…åƒæ•¸ï¼Œä½† peak learning rate å’Œ warmup step é™¤å¤–ï¼Œä»–å€‘é‡å°æ¯å€‹è¨­ç½®å–®ç¨èª¿æ•´ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾è¨“ç·´å° Adam epsilon éå¸¸æ•æ„Ÿã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾è¨­ç½® $\beta_2$ = 0.98ï¼Œåœ¨å¤§ batch size çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥æé«˜è¨“ç·´æ™‚çš„ç©©å®šæ€§ã€‚&lt;/p&gt;
&lt;p&gt;ç”¨æœ€å¤š 512 å€‹ token é è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä¸æœƒéš¨æ©Ÿæ³¨å…¥çŸ­åºåˆ—ï¼Œä¹Ÿä¸æœƒç‚ºå‰ 90% çš„æ›´æ–°ç¸®çŸ­è¼¸å…¥çš„é•·åº¦ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åªè¨“ç·´ full-length çš„ sequencesã€‚&lt;/p&gt;
&lt;h3 id=&#34;data-1&#34;&gt;Data&lt;/h3&gt;
&lt;p&gt;BERT-style çš„é è¨“ç·´ä»°è³´å¤§é‡æ–‡æœ¬ã€‚&lt;/p&gt;
&lt;p&gt;å·²æœ‰ç ”ç©¶è­‰æ˜å¢åŠ æ•¸æ“šé‡å¯ä»¥æé«˜ end-task çš„æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;å·²æœ‰ä¸€äº›ç ”ç©¶ï¼Œç”¨æ¯”åŸå§‹ BERT æ›´å¤šæ¨£æ›´å¤§çš„æ•¸æ“šé›†ï¼Œä½†ä¸æ˜¯æ‰€æœ‰çš„æ•¸æ“šé›†éƒ½æœ‰å…¬é–‹ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶ç”¨äº†äº”å€‹ä¸åŒå¤§å°å’Œé ˜åŸŸçš„è‹±æ–‡æ–‡æœ¬ï¼Œå…±æœ‰è¶…é 160 GB çš„æœªå£“ç¸®æ–‡æœ¬ã€‚&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨ä»¥ä¸‹æ•¸æ“šé›†:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BookCorpus + English Wikipedia
&lt;ul&gt;
&lt;li&gt;BERT åŸæœ¬ä½¿ç”¨çš„ã€‚&lt;/li&gt;
&lt;li&gt;16 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CC-News
&lt;ul&gt;
&lt;li&gt;ä½œè€…å¾ CommonCrawl News dataset çš„è‹±æ–‡éƒ¨åˆ†ä¸­è’é›†ï¼ŒåŒ…å«äº† 2016 å¹´ 9 æœˆåˆ° 2019 å¹´ 2 æœˆçš„ 6300 è¬ç¯‡è‹±æ–‡æ–°èã€‚&lt;/li&gt;
&lt;li&gt;éæ¿¾å¾Œæœ‰ 76 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;OpenWebText
&lt;ul&gt;
&lt;li&gt;WebText çš„é–‹æºé‡å»ºç‰ˆï¼Œå¾ Reddit ä¸Šè‡³å°‘æœ‰ 3 å€‹ upvotes çš„ shared URLs æå–å‡ºçš„ Web å…§å®¹ã€‚&lt;/li&gt;
&lt;li&gt;38 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stories
&lt;ul&gt;
&lt;li&gt;åŒ…å« CommonCrawl data çš„ä¸€å€‹å­é›†åˆï¼Œç¶“ééæ¿¾ï¼Œä»¥åŒ¹é… story-like style of Winograd schemas&lt;/li&gt;
&lt;li&gt;31 GB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;ä½¿ç”¨ä»¥ä¸‹ä¸‰å€‹ benchmarks è©•ä¼°é è¨“ç·´æ¨¡å‹&lt;/p&gt;
&lt;h4 id=&#34;glue&#34;&gt;GLUE&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The General Language Understanding Evaluation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”¨æ–¼è©•ä¼°è‡ªç„¶èªè¨€ç†è§£çš„ 9 å€‹æ•¸æ“šé›†çš„é›†åˆï¼Œä»»å‹™è¢«å®šç¾©ç‚º single-sentence åˆ†é¡æˆ– sentence-pair åˆ†é¡ä»»å‹™ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;finetune çš„æµç¨‹éµå¾ªåŸå§‹ BERT paper&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;squad&#34;&gt;SQuAD&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Stanford Question Answering Dataset&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æä¾›ä¸€æ®µ context ä»¥åŠä¸€å€‹å•é¡Œ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å…·æœ‰å…©å€‹ç‰ˆæœ¬ V1.1 å’Œ V2.0&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;V1.1
&lt;ul&gt;
&lt;li&gt;context ç¸½æ˜¯åŒ…å«ä¸€å€‹ç­”æ¡ˆ&lt;/li&gt;
&lt;li&gt;è©•ä¼° V1.1 çš„æ™‚å€™ï¼Œä½œè€…æ¡ç”¨å’Œ BERT ç›¸åŒçš„ span prediction method&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V2.0
&lt;ul&gt;
&lt;li&gt;ä¸€äº›å•é¡Œåœ¨æä¾›çš„ context ä¸­æ²’æœ‰å›ç­”ï¼Œä½¿ä»»å‹™æ›´æœ‰æŒ‘æˆ°æ€§&lt;/li&gt;
&lt;li&gt;è©•ä¼° V2.0 çš„æ™‚å€™ï¼Œä½œè€…æœƒç”¨ä¸€å€‹é¡å¤–çš„äºŒå…ƒåˆ†é¡å™¨é æ¸¬å•é¡Œæ˜¯å¦å¯ä»¥å›ç­”ï¼Œåœ¨è©•ä¼°çš„æ™‚å€™ï¼Œåªé æ¸¬è¢«åˆ†é¡ç‚ºå¯å›ç­”çš„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;race&#34;&gt;RACE&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The ReAding Comprehension from Examinations&lt;/li&gt;
&lt;li&gt;å¤§å‹é–±è®€ç†è§£æ•¸æ“šé›†ï¼Œæœ‰è¶…é 28,000 ç¯‡æ–‡ç«  ä»¥åŠå°‡è¿‘ 100,000 å€‹å•é¡Œ&lt;/li&gt;
&lt;li&gt;å¾ä¸­åœ‹çš„è‹±æ–‡è€ƒè©¦è’é›†çš„ï¼Œé€™äº›è€ƒè©¦æ˜¯ç‚ºåœ‹ä¸­ç”Ÿå’Œé«˜ä¸­ç”Ÿè¨­è¨ˆçš„&lt;/li&gt;
&lt;li&gt;æ¯ç¯‡æ–‡ç« éƒ½èˆ‡å¤šå€‹å•é¡Œç›¸é—œè¯&lt;/li&gt;
&lt;li&gt;å°æ¯å€‹å•é¡Œï¼Œè¦å¾å››å€‹é¸é …ä¸­é¸å‡ºä¸€å€‹å°çš„&lt;/li&gt;
&lt;li&gt;context æ¯”èµ·å…¶ä»–é–±è®€ç†è§£çš„æ•¸æ“šé›†è¦é•·ï¼Œè€Œä¸”è¦æ¨ç†çš„å•é¡Œæ¯”ä¾‹å¾ˆå¤§&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training-procedure-analysis&#34;&gt;Training Procedure Analysis&lt;/h2&gt;
&lt;p&gt;æ¢è¨å“ªäº›é¸æ“‡å°æˆåŠŸé è¨“ç·´ BERT å¾ˆé‡è¦ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æŠŠæ¶æ§‹å›ºå®šï¼Œä¹Ÿå°±æ˜¯è¨“ç·´å’Œ$BERT_{BASE}$ (L=12, H=768, A=12, 110M params)ä¸€æ¨£æ¶æ§‹çš„ BERT models&lt;/p&gt;
&lt;h3 id=&#34;static-vs-dynamic-masking&#34;&gt;Static vs. Dynamic Masking&lt;/h3&gt;
&lt;p&gt;BERT åœ¨ preprocessing çš„æ™‚å€™è™•ç† maskingï¼Œç”¢ç”Ÿå–®å€‹ static maskã€‚
ä½œè€…ç‚ºäº†é¿å…åœ¨æ¯å€‹ epoch éƒ½å°æ¯å€‹ instance ç”¨ç›¸åŒçš„ maskï¼Œå°‡æ•¸æ“šè¤‡è£½äº† 10 æ¬¡ï¼Œåœ¨ 40 å€‹ epochs è£¡ï¼Œä»¥ 10 ç¨®ä¸åŒçš„æ–¹å¼ maskã€‚æ‰€ä»¥ä¸€æ¬¡è¨“ç·´éç¨‹ä¸­ï¼Œç›¸åŒçš„ mask æœƒå‡ºç¾å››æ¬¡ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æœƒä»¥ä¸Šè¿°ç­–ç•¥å’Œ Dynamic masking é€²è¡Œæ¯”è¼ƒï¼ŒDynamic masking æ˜¯åœ¨æ¯æ¬¡é¤µ model å‰ï¼Œæ‰ç”Ÿæˆ maskã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ Dynamic Masking ç›¸æ¯” staticï¼Œè¦ä¸æ˜¯å·®ä¸å¤šï¼Œå°±æ˜¯ç•¥å¥½ï¼ŒåŸºæ–¼çµæœå’Œæ•ˆç‡çš„å„ªå‹¢è€ƒé‡ï¼Œå…¶ä»–å¯¦é©—ä¸­éƒ½ç”¨ dynamic maskingã€‚&lt;/p&gt;
&lt;h3 id=&#34;model-input-format-and-next-sentence-prediction&#34;&gt;Model Input Format and Next Sentence Prediction&lt;/h3&gt;
&lt;p&gt;åŸå§‹çš„ BERT é è¨“ç·´ä¸­ï¼Œå…©å€‹å¥å­è¦ä¸æ˜¯åŒä¸€å€‹æ–‡ä»¶çš„é€£çºŒå¥å­(p = 0.5)ï¼Œä¸ç„¶å°±æ˜¯ä¸åŒçš„ document åšæ¡æ¨£&lt;/p&gt;
&lt;p&gt;ä»¥å¾€æœ‰ç ”ç©¶æŒ‡å‡ºç§»é™¤ NSP æœƒæå®³æ€§èƒ½ï¼Œä½†ä¹Ÿæœ‰ç ”ç©¶è³ªç–‘å¿…è¦æ€§ï¼Œæ‰€ä»¥æœ¬æ–‡æ¯”è¼ƒäº†å¹¾ç¨®æ›¿ä»£è¨“ç·´æ ¼å¼ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SEGMENT-PAIR+NSP
&lt;ul&gt;
&lt;li&gt;æœ€åŸå§‹çš„æ–¹æ³•ï¼Œæ¯å€‹ segment å¯ä»¥æœ‰å¤šå€‹è‡ªç„¶å¥å­&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SENTENCE-PAIR+NSP
&lt;ul&gt;
&lt;li&gt;åªåŒ…å«ä¸€å°å¥å­ï¼Œç”±æ–¼è¼¸å…¥æ˜é¡¯å°‘æ–¼ 512 tokenï¼Œæ‰€ä»¥æœƒå¢åŠ  batch size è®“ token ç¸½æ•¸å’Œå‰è€…å·®ä¸å¤š&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FULL-SENTENCES
&lt;ul&gt;
&lt;li&gt;åŒ…å«å¾ä¸€å€‹æˆ–å¤šå€‹æ–‡ä»¶ä¸­é€£çºŒæ¡æ¨£çš„å®Œæ•´å¥å­ï¼Œå¯èƒ½æœƒè·¨è¶Šæ–‡ä»¶é‚Šç•Œï¼Œåœ¨æ–‡ä»¶é‚Šç•Œé–“æœƒåŠ å€‹é¡å¤–çš„åˆ†éš”ç¬¦&lt;/li&gt;
&lt;li&gt;ç§»é™¤äº† NSP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DOC-SENTENCES
&lt;ul&gt;
&lt;li&gt;å’Œ FULL-SENTENCES å·®ä¸å¤šï¼Œä½†ä¸èƒ½è·¨è¶Š documentï¼Œåœ¨ document å°¾å·´çš„éƒ¨åˆ†æœƒå®¹æ˜“å°‘æ–¼ 512ï¼Œæ‰€ä»¥æœƒå‹•æ…‹å¢åŠ  batch sizeï¼Œè®“ token ç¸½æ•¸å’Œ FULL-SENTENCES å·®ä¸å¤š&lt;/li&gt;
&lt;li&gt;ç§»é™¤äº† NSP&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ç™¼ç¾ DOC-SENTENCES æ˜¯æœ€æ£’çš„ï¼Œä½†ç”±æ–¼ DOC-SENTENCES æœƒè®“ batch sizes å¤§å°å¯è®Šï¼Œæ‰€ä»¥å…¶ä»–å¯¦é©—æœƒç”¨ FULL-SENTENCESï¼Œæ¯”è¼ƒå¥½å’Œå…¶ä»–ç›¸é—œå·¥ä½œæ¯”è¼ƒã€‚&lt;/p&gt;
&lt;h3 id=&#34;training-with-large-batches&#34;&gt;Training with large batches&lt;/h3&gt;
&lt;p&gt;æ ¹æ“šéå»ç¥ç¶“ç¶²è·¯æ©Ÿå™¨ç¿»è­¯çš„å·¥ä½œï¼Œç•¶ learning rate é©ç•¶å¢åŠ çš„æ™‚å€™ï¼Œç”¨éå¸¸å¤§çš„çš„ mini-bathces å¯ä»¥æé«˜ optimization çš„é€Ÿåº¦å’Œ end-task æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;æœ€è¿‘çš„ç ”ç©¶ä¹Ÿé¡¯ç¤º BERT é©ç”¨æ–¼ large batch trainingã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;text-encoding&#34;&gt;Text Encoding&lt;/h3&gt;
&lt;p&gt;Byte-Pair Encoding (BPE) æ˜¯ä¸€ç¨®ä»‹æ–¼å­—ç¬¦ç´šåˆ¥å’Œè©ç´šåˆ¥è¡¨ç¤ºä¹‹é–“çš„æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒå…è¨±è™•ç†è‡ªç„¶èªè¨€èªæ–™åº«ä¸­å¸¸è¦‹çš„å¤§è©å½™é‡ã€‚&lt;/p&gt;
&lt;p&gt;BPE ä¸ä¾è³´æ–¼å®Œæ•´çš„å–®è©ï¼Œè€Œæ˜¯ä¾é  subwords unitsï¼Œé€šéå°è¨“ç·´èªæ–™é€²è¡Œçµ±è¨ˆåˆ†æä¾†æå–é€™äº› subwords unitsã€‚&lt;/p&gt;
&lt;p&gt;BPE è©å½™è¡¨çš„å¤§å°é€šå¸¸åœ¨ 10K-100K çš„ subword unitsã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ &amp;ldquo;Language Models are Unsupervised Multitask Learners&amp;rdquo; æ–‡ä¸­ï¼Œæåˆ°äº†ä¸€ç¨®å·§å¦™çš„ BPE å¯¦ç¾ï¼Œä¸æ˜¯ç”¨ unicode charactersï¼Œè€Œæ˜¯ç”¨ bytes ä½œç‚º base subword unitsã€‚å¯ä»¥ç”Ÿå‡º 50K å¤§å°çš„è©å½™è¡¨ï¼Œè€Œä¸”ä¸ç”¨å¼•å…¥ä»»ä½•çš„ &amp;ldquo;unknown&amp;rdquo;ã€‚&lt;/p&gt;
&lt;p&gt;åŸå§‹çš„ BERT ç”¨ character-level BPE vocabularyï¼Œå¤§å°ç‚º 30Kã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡è€ƒæ…®ç”¨ 50K byte-level BPE vocabularyï¼Œè€Œä¸å°è¼¸å…¥åšé¡å¤–çš„ preprocessing æˆ– tokenizationï¼Œ&amp;ldquo;Language Models are Unsupervised Multitask Learners&amp;rdquo; çš„ç ”ç©¶é¡¯ç¤ºé€™äº› Encoding çš„æ–¹æ³•åœ¨æœ€çµ‚æ•ˆèƒ½ä¸Šä¸¦ç„¡å¤ªå¤§å·®åˆ¥ï¼Œåªåœ¨æŸäº›ä»»å‹™ä¸Š end-task performance è¡¨ç¾ç¨å·®ã€‚&lt;/p&gt;
&lt;p&gt;ä½†ä½œè€…ç›¸ä¿¡ universal encoding scheme çš„å„ªå‹¢è¶…éäº†è¼•å¾®çš„æ€§èƒ½ä¸‹é™ï¼Œå…¶ä»–å¯¦é©—ä¹Ÿæœƒç”¨é€™ç¨®é‚Šç¢¼æ–¹å¼ã€‚&lt;/p&gt;
&lt;h2 id=&#34;roberta&#34;&gt;RoBERTa&lt;/h2&gt;
&lt;p&gt;æ•´ç†ä¸Šé¢èªªçš„æ”¹é€²ã€‚&lt;/p&gt;
&lt;p&gt;RoBERTa ç”¨ä»¥ä¸‹é…ç½®:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;dynamic masking&lt;/li&gt;
&lt;li&gt;FULL-SENTENCES without NSP loss&lt;/li&gt;
&lt;li&gt;large mini-batches&lt;/li&gt;
&lt;li&gt;larger byte-level BPE&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;æ­¤å¤–ï¼Œé‚„èª¿æŸ¥äº†å…©å€‹ä¹‹å‰çš„å·¥ä½œæ²’å¼·èª¿çš„é‡è¦å› ç´ :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨æ–¼é è¨“ç·´çš„ data&lt;/li&gt;
&lt;li&gt;è¨“ç·´é data çš„æ¬¡æ•¸&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç‚ºäº†æŠŠé€™äº›å› ç´ çš„é‡è¦æ€§å’Œå…¶ä»–æ¨¡å‹é¸æ“‡åˆ†éš”é–‹ï¼Œå…ˆæŒ‰ç…§ $BERT_{LARGE}$ (L = 24, H = 1024, A = 16, 355M parameters) è¨“ç·´ RoBERTaã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…åœ¨ BOOKCORPUS plus WIKIPEDIA dataset é€²è¡Œäº† 100K step çš„é è¨“ç·´ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ§åˆ¶ training data çš„æƒ…æ³ä¸‹ï¼Œ RoBERTa æ¯” $BERT_{LARGE}$ çš„çµæœæœ‰å¤§å¹…åº¦çš„æ”¹é€²ï¼Œé‡ç”³äº†å‰é¢è¨­è¨ˆé¸æ“‡çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;æ¥ä¸‹ä¾†ï¼Œçµåˆä¹‹å‰èªªçš„é¡å¤– datasetï¼Œä¸¦ç”¨ç›¸åŒçš„æ­¥æ•¸(100K) è¨“ç·´ RoBERTaï¼Œè§€å¯Ÿåˆ°ä¸‹æ¸¸ä»»å‹™çš„æ€§èƒ½é€²ä¸€æ­¥æé«˜ï¼Œé©—è­‰äº†æ•¸æ“šå¤§å°å’Œå¤šæ¨£æ€§çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œï¼Œå° RoBERTa åšæ›´é•·æ™‚é–“çš„é è¨“ç·´ï¼Œå°‡æ­¥æ•¸æé«˜åˆ° 300K å’Œ 500Kï¼Œå†æ¬¡è§€å¯Ÿåˆ°ä¸‹æ¸¸ä»»å‹™æ€§èƒ½é¡¯è‘—æå‡ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ä¹Ÿæ³¨æ„åˆ°ï¼Œå³ä½¿æ˜¯ä»–å€‘è¨“ç·´æ™‚é–“æœ€é•·çš„æ¨¡å‹ï¼Œä¹Ÿä¸æœƒ overfit ä»–å€‘çš„æ•¸æ“šã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡çš„å…¶ä»–éƒ¨åˆ†åœ¨ä¸‰å€‹ benchmark è©•ä¼°å¥½å£: GLUEã€SQuaD å’Œ RACE&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table4.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;glue-results&#34;&gt;GLUE Results&lt;/h3&gt;
&lt;p&gt;é›–ç„¶å¾ˆå¤š GLUE æ’è¡Œæ¦œçš„æäº¤éƒ½æ˜¯ depend on multi-task finetuningï¼Œä½†ä½œè€…çš„ submission æ˜¯ depends only on single-task finetuningã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œå°æ–¼ RTEã€STS å’Œ MRPCï¼Œå¾ MNLI çš„æ¨¡å‹å¾®èª¿æœƒæ¯” baseline çš„ RoBERTa æœ‰å¹«åŠ©è¨±å¤šã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç¬¬ä¸€å€‹è¨­ç½® (single-task, dev) ä¸­ï¼ŒRoBERTa åœ¨æ‰€æœ‰ 9 å€‹ GLUE ä»»å‹™ dev set ä¸Šéƒ½å–å¾—äº†æœ€å…ˆé€²çš„çµæœã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç¬¬äºŒå€‹è¨­ç½® (ensembles, test) ä¸­ï¼Œä½œè€…å°‡ RoBERTa æäº¤åˆ° GLUE æ’è¡Œæ¦œï¼Œä¸¦åœ¨ 9 å€‹ä»»å‹™ä¸­çš„ 4 å€‹ä¸Šå–å¾—äº† SOTA å’Œè¿„ä»Šç‚ºæ­¢çš„æœ€é«˜å¹³å‡åˆ†ã€‚&lt;/p&gt;
&lt;p&gt;é€™ä»¤äººèˆˆå¥®çš„åœ°æ–¹åœ¨æ–¼ï¼Œèˆ‡å¤šæ•¸ top submissions ä¸åŒï¼ŒRoBERTa ä¸æ˜¯ depend on multi-tasking finetuning&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/RoBERTa/table5.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;åœ¨é è¨“ç·´ BERT æ¨¡å‹æ™‚ï¼Œä½œè€…ä»”ç´°è©•ä¼°äº†è¨±å¤šè¨­è¨ˆæ±ºç­–ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾ï¼Œé€šéå°æ¨¡å‹é€²è¡Œæ›´é•·æ™‚é–“çš„è¨“ç·´ã€ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡è™•ç†æ›´å¤šçš„æ•¸æ“šã€å»é™¤ NSPã€è¨“ç·´æ›´é•·çš„åºåˆ—ã€dynamic maskingï¼Œå¯ä»¥é¡¯è‘—æé«˜æ€§èƒ½ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æ”¹é€²çš„é è¨“ç·´ç¨‹åºï¼Œæˆ‘å€‘ç¨±ä¹‹ç‚º RoBERTaï¼Œåœ¨ GLUEã€RACE å’Œ SQuAD ä¸Šå¯¦ç¾äº† SOTAï¼Œè€Œç„¡éœ€ç‚º GLUE é€²è¡Œå¤šä»»å‹™å¾®èª¿æˆ–ç‚º SQuAD æä¾›é¡å¤–çš„æ•¸æ“šã€‚&lt;/p&gt;
&lt;p&gt;é€™äº›çµæœèªªæ˜äº†é€™äº›ä»¥å‰è¢«å¿½è¦–çš„è¨­è¨ˆæ±ºç­–çš„é‡è¦æ€§ï¼Œä¸¦è¡¨æ˜ BERT çš„é è¨“ç·´ç›®æ¨™èˆ‡æœ€è¿‘æå‡ºçš„æ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ä»ç„¶å…·æœ‰ç«¶çˆ­åŠ›ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PatentSBERTa è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/patentsberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Wed, 15 Mar 2023 15:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/patentsberta-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2103.11933&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PatentSBERTa: A Deep NLP based Hybrid Model for Patent Distance and Classification using Augmented SBERT&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;æœ¬ç ”ç©¶æä¾›äº†ä¸€å€‹è¨ˆç®—  patent-to-patent (p2p) technological similarity çš„æœ‰æ•ˆæ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;ä¸¦æå‡ºä¸€å€‹ hybrid frameworkï¼Œç”¨æ–¼æŠŠ p2p ç›¸ä¼¼æ€§çš„çµæœæ‡‰ç”¨æ–¼ semantic search å’Œ automated patent classificationã€‚&lt;/p&gt;
&lt;p&gt;æŠŠ Sentence-BERT (SBERT) ç”¨åœ¨ claims ä¸Šä¾†ä½œ embeddingsã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†é€²ä¸€æ­¥æå‡ embedding çš„å“è³ªï¼Œä½¿ç”¨åŸºæ–¼ SBERT å’Œ RoBERT çš„ transformer modelï¼Œç„¶å¾Œå†ç”¨ augmented approach åœ¨  in-domain supervised patent claims data(ç›¸å°æ–¼ out-domain) ä¾† fine-tune SBERTã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ KNN(Nearest Neighbors) ä¾†æ ¹æ“š p2p similarity åˆ†é¡æ¨¡å‹ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;å‚³çµ±ä¸Šçš„ p2p ç›¸ä¼¼åº¦æ˜¯åŸºæ–¼é—œéµå­—ã€æŠ€è¡“é¡åˆ¥ç­‰ metadata æ±ºå®šçš„ï¼Œä½†è¿‘æœŸ semantic-based çš„æ–¹æ³•ä¹Ÿè¶Šä¾†è¶Šå—æ­¡è¿ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç›®å‰é‡åˆ°çš„å•é¡Œ
&lt;ol&gt;
&lt;li&gt;BERT ç”¨ä¾†è¨ˆç®— p2p ç›¸ä¼¼æ€§çš„æˆæœ¬å¾ˆé«˜&lt;/li&gt;
&lt;li&gt;åŸºæ–¼ generic text çš„ pre-trained model åœ¨é‡åˆ°ç‰¹å®šé ˜åŸŸçš„å°ˆæ¥­è¡“èªæ™‚å¯èƒ½æœƒé‡åˆ°ä¾·é™ã€‚&lt;/li&gt;
&lt;li&gt;åœ¨å°ˆåˆ©åš multi-label classification (MLC) æ˜¯å€‹æŒ‘æˆ°&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;è²¢ç»
&lt;ol&gt;
&lt;li&gt;æä¾›ä¸€å€‹å¿«é€Ÿé«˜æ•ˆçš„æ¡†æ¶ï¼Œåˆ©ç”¨ Transformer æ¶æ§‹è¨ˆç®— p2p ç›¸ä¼¼åº¦&lt;/li&gt;
&lt;li&gt;é€é augmented SBERTï¼Œå°‡ transformer model fine-tune åˆ° domain-specific language&lt;/li&gt;
&lt;li&gt;æå‡ºä¸€å€‹åŸºæ–¼ Transformer å’Œ å‚³çµ± ML æ¨¡å‹çš„æ··å’Œæ¶æ§‹ï¼Œå¯ä»¥æ‰“æ•— multi-label å’Œ multi-class çš„å°ˆåˆ©åˆ†é¡ SOTA æ¨¡å‹&lt;/li&gt;
&lt;li&gt;ç”¨ç°¡å–®çš„ KNN é€²è¡Œå°ˆåˆ©åˆ†é¡ï¼Œæä¾›äº†ä¸€ç¨®ç°¡å–®çš„æ–¹æ³•ä¾†æª¢æŸ¥ã€ç†è§£å’Œè§£é‡‹æ¨¡å‹çš„é æ¸¬çµæœ&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;h3 id=&#34;dataset-description&#34;&gt;Dataset Description&lt;/h3&gt;
&lt;p&gt;æœ¬ç ”ç©¶ä½¿ç”¨ PatentsView datasetï¼ŒPatentsView å¹³å°å»ºç«‹åœ¨ä¸€å€‹å®šæœŸæ›´æ–°çš„ database ä¸Šã€‚&lt;/p&gt;
&lt;p&gt;dataset å·²ç”¨æ–¼ä¹‹å‰é¡ä¼¼çš„ç ”ç©¶ï¼Œæ¯”å¦‚ DeepPatentã€PatentBERTã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶ä½¿ç”¨äº† 2013-2017 çš„æ‰€æœ‰å°ˆåˆ©ï¼Œé€™äº›å°ˆåˆ©è‡³å°‘è¦åœ¨ BigQuery ä¸Šæœ‰ä¸€æ¢ claimã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶çš„ record æœ‰ 1,492,294 é …å°ˆåˆ©ï¼Œä¸¦ç”¨ 8% ä½œç‚ºæ¸¬è©¦é›†ã€‚&lt;/p&gt;
&lt;p&gt;æ­¤å¤–ï¼Œæœ¬ç ”ç©¶åˆªé™¤äº†æœ‰é‡è¤‡å°ˆåˆ© ID å’Œ claim text çš„ recordã€‚&lt;/p&gt;
&lt;h3 id=&#34;textual-data-patent-claims&#34;&gt;Textual Data: Patent Claims&lt;/h3&gt;
&lt;p&gt;æœ¬ç ”ç©¶ä½¿ç”¨ claim ä½œç‚ºè¼¸å…¥ã€‚&lt;/p&gt;
&lt;p&gt;claim è¢«èªç‚ºæ˜¯æº–å‚™å°ˆåˆ©æ–‡ä»¶çš„åˆå§‹æ¡†æ¶ï¼Œå…¶ä»–æ–‡ä»¶éƒ½æ˜¯æ ¹æ“š claim æº–å‚™çš„ï¼Œ
å› æ­¤ï¼Œclaim æ¯”å…¶ä»–æ–‡ä»¶åŒ…å«æ›´å…¨é¢å’Œæº–ç¢ºçš„è¨Šæ¯ã€‚&lt;/p&gt;
&lt;p&gt;claim å…·æœ‰å±¤æ¬¡çµæ§‹ï¼Œfirst claim è¢«è¦–ç‚ºè©²æ¶æ§‹çš„ä¸»å¹¹ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬ç ”ç©¶åƒ…ä½¿ç”¨ first claimï¼Œä½†åœ¨ä»¥å¾Œçš„ç ”ç©¶ä¸­ï¼Œå¸Œæœ›æ ¹æ“š tree structure çµ„åˆæ‰€æœ‰ claimï¼Œä¸¦è¨ˆç®— semantic similarityï¼Œä¸¦åšå¤šæ¨™ç±¤åˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç ”ç©¶æ¨£æœ¬ä¸­ï¼Œ claim å¹³å‡æœ‰ 17 å€‹ã€‚&lt;/p&gt;
&lt;p&gt;claim çš„å¹³å‡é•·åº¦æ˜¯ 162ï¼Œæœ¬ç ”ç©¶ä¸­ï¼ŒBERT çš„ max_seq_length æ˜¯ 510ã€‚&lt;/p&gt;
&lt;h3 id=&#34;patent-classification-cpc-classes&#34;&gt;Patent Classification: CPC Classes&lt;/h3&gt;
&lt;p&gt;CPCç³»çµ±å’ŒIPCï¼ˆåœ‹éš›å°ˆåˆ©åˆ†é¡ï¼‰ç³»çµ±æ˜¯æœ€å¸¸ç”¨çš„å…©ç¨®åˆ†é¡ç³»çµ±ï¼ŒCPC æ˜¯ IPC ç³»çµ±çš„æ›´å…·é«”å’Œè©³ç´°çš„ç‰ˆæœ¬ã€‚&lt;/p&gt;
&lt;p&gt;CPC å…·æœ‰ç”¨æ–¼åˆ†é¡çš„å±¤æ¬¡çµæ§‹ï¼ŒåŒ…æ‹¬ Sectionã€Classã€Subclass å’Œ Groupï¼Œ
åœ¨å­é¡ç´šåˆ¥ï¼ŒCPC æœ‰ 667 å€‹æ¨™ç±¤ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨æ•¸æ“šé›†ä¸­æˆ‘å€‘æœ‰ 663 å€‹æ¨™ç±¤ï¼Œå…¶ä¸­ 159 å€‹åœ¨æ•¸æ“šé›†ä¸­çš„æ¨£æœ¬å°‘æ–¼ 350 å€‹ï¼Œé€™ç¨®æ¨™ç±¤åˆ†ä½ˆå°è‡´äº† KNN ä¸å¥½è™•ç†ï¼Œä¸€èˆ¬ä¾†èªªï¼Œéš¨è‘— instance æ•¸é‡çš„å¢åŠ ï¼Œæˆ‘å€‘å¯ä»¥æé«˜æ¨¡å‹çš„æº–ç¢ºæ€§ã€‚&lt;/p&gt;
&lt;h2 id=&#34;method-and-experimental-setup&#34;&gt;Method and experimental setup&lt;/h2&gt;
&lt;p&gt;Pretrained Language Models (LMs) åœ¨ NLP ä¸­è®Šå¾—ååˆ†æµè¡Œã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ pairwise sentence semantic similarityï¼ŒSBERT å’Œ BERT æ˜¯å…©ç¨®å…·æœ‰é¡¯è‘—ä¸åŒæ•ˆæœçš„æ–¹æ³•ã€‚&lt;/p&gt;
&lt;p&gt;BERT é€šå¸¸å¯ä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨å¯¦éš›æ‡‰ç”¨ä¸Šä¾†èªªå¤ªæ…¢äº†ã€‚&lt;/p&gt;
&lt;p&gt;SBERT åœ¨å¯¦éš›æ‡‰ç”¨ä¸Šè¡¨ç¾é‚„è¡Œï¼Œä½†éœ€è¦ in-domain training data ä¸¦ä¸” finetuneã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/PatentSBERTa/Bi_vs_Cross-Encoder.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/PatentSBERTa/approach.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–æ˜¯ Augmented SBERT In-domain approachã€‚&lt;/p&gt;
&lt;p&gt;in-domain sentence pairs é€é cross-encoder ä¾†æ¨™è¨˜ï¼Œå‡è¨­æœ‰ n å€‹ in-domain sentencesï¼Œæœƒæœ‰ $C_2^n$ çµ„å¯èƒ½çš„çµ„åˆã€‚&lt;/p&gt;
&lt;p&gt;ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„çµ„åˆä¸¦ä¸æœƒæé«˜æ€§èƒ½ï¼Œæ‰€ä»¥è¦æœ‰æ­£ç¢ºçš„æ¡æ¨£ç­–ç•¥ï¼Œæ‰å¯æå‡æ€§èƒ½çš„åŒæ™‚ä¹Ÿæ¸›å°‘è¨ˆç®—é–‹éŠ·ã€‚&lt;/p&gt;
&lt;p&gt;ä¸Šåœ–é‚£ç¨®çµåˆ cross-encoder å’Œ bi-encoder çš„ä½œæ³•è¢«ç¨±ç‚º Augmented SBERT (AugSBERT)ï¼Œ
æ¶‰åŠä»¥ä¸‹ä¸‰å€‹æ­¥é©Ÿ:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨è³‡æ–™é›† Fine-tune RoBERTa ä»¥ç”Ÿå‡º cross-encoder&lt;/li&gt;
&lt;li&gt;ç”¨ cross-encoder ä¾†æŠŠæœªæ¨™è¨˜çš„è³‡æ–™æ¨™è¨˜ï¼ŒåŒæ™‚åŸºæ–¼æŸç¨®ç‰¹å®šçš„æ¡æ¨£ç­–ç•¥ï¼Œå¾ 652,653 ç¨®å¯èƒ½çš„çµ„åˆä¸­æŒ‘é¸ 3432 çµ„&lt;/li&gt;
&lt;li&gt;æŠŠè³‡æ–™é›† + é¡å¤–çš„ 3432 çµ„è³‡æ–™ä¸€èµ·æ‹¿ä¾†è¨“ç·´ SBERT&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;p2p-similarity-and-semantic-search&#34;&gt;P2P similarity and semantic search&lt;/h3&gt;
&lt;p&gt;Patent Semantic Search (PSS) æ˜¯å°ˆåˆ©åˆ†æçš„åŸºç¤éƒ¨åˆ†ã€‚&lt;/p&gt;
&lt;p&gt;Transformer æ¨¡å‹ç­‰èªç¾©ç›¸ä¼¼æ€§çš„è§£æ³•æ˜¯ä¸€ç¨®æ–°è§£æ³•ï¼Œå¯ä»¥ç”¨ä¾†è§£æ±ºåŸºæ–¼é—œéµå­—çš„æœå°‹æ–¹æ³•ä¸­ï¼Œ query terms å’Œå°ˆåˆ©å…§å®¹ä¸åŒ¹é…çš„å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†è©•ä¼°æ¨¡å‹çš„æº–ç¢ºæ€§ï¼Œæœªä¾†çš„ç ”ç©¶ä¸­ï¼Œä½œè€…å¸Œæœ›é€šé Mean Reciprocal Rank (MRR) ä¾†è©•ä¼°åˆ†é¡çµæœã€‚&lt;/p&gt;
&lt;h3 id=&#34;cpc-prediction&#34;&gt;CPC Prediction&lt;/h3&gt;
&lt;p&gt;Top-N æº–ç¢ºåº¦ç­‰æ–¼ GT èˆ‡é æ¸¬æœ‰æœ€é«˜æ¦‚ç‡çš„ä»»ä½• N å€‹é æ¸¬åŒ¹é…çš„é »ç‡ï¼Œ
æ‰€ä»¥ Top-5 å°±æ˜¯æœ€é«˜çš„äº”å€‹åˆ†é¡ä¸­ä¸€å€‹å°±æœ‰ä¸­ã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;æœ¬æ–‡ä½¿ç”¨  augmented SBERT  ç²å¾— SOTA çš„å°ˆåˆ©æ–‡æœ¬ embeddingã€‚&lt;/p&gt;
&lt;p&gt;ä»‹ç´¹äº†ä¸€ç¨® augmented çš„æ–¹æ³•ï¼ŒæŠŠ SBERT å¾®èª¿åˆ°é©åˆ patent claims çš„ domainã€‚&lt;/p&gt;
&lt;p&gt;SBERT çš„ä¸€å€‹ä¸»è¦å„ªé»æ˜¯å¯ä»¥æœ‰æ•ˆç‡åœ°ç²å¾— embedding distanceï¼Œä½¿æˆ‘å€‘èƒ½å¤ ç‚ºå¤§çš„å°ˆåˆ©è³‡æ–™é›†å»ºæ§‹ p2p similarityã€‚&lt;/p&gt;
&lt;p&gt;é›–ç„¶åŸºæ–¼æ–‡æœ¬çš„ p2p similarity çš„æœ‰ç”¨æ€§å·²ç¶“åœ¨å„ç¨®æ‡‰ç”¨æ–¹é¢å¾—åˆ°è­‰æ˜ï¼Œä½†æœ¬æ–‡é€²ä¸€æ­¥è­‰æ˜ä½œè€…çš„ transformer-based p2p similarity å¯ä»¥è¢«ç”¨åœ¨ SOTA çš„å°ˆåˆ©åˆ†é¡ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”ä½¿ç”¨ç°¡å–®çš„ KNN æ–¹æ³•ï¼Œæª¢æŸ¥ä»–å€‘å¯ä»¥ä½¿æ¨¡å‹æ±ºç­–å…·å‚™ understandable å’Œ explainableã€‚&lt;/p&gt;
&lt;h2 id=&#34;limitations--future-research&#34;&gt;Limitations &amp;amp; Future Research&lt;/h2&gt;
&lt;p&gt;æœªä¾†å¸Œæœ›ç”¨ Annoy(Approximate Nearest Neighbor Oh Yeah!) ä¾†æ¸¬è©¦æ›´å¤§æ¨£æœ¬çš„æ¨¡å‹ä¸¦æ¯”è¼ƒçµæœã€‚&lt;/p&gt;
&lt;p&gt;Annoy(Approximate Nearest Neighbor Oh Yeah!) æ˜¯æƒ³å°‹æ‰¾è¿‘ä¼¼ç›¸ä¼¼è€Œä¸æ˜¯ç²¾ç¢ºç›¸ä¼¼çš„å¥å­ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Actor-Critic</title>
        <link>https://roykesydon.github.io/Blog/p/actor-critic/</link>
        <pubDate>Tue, 14 Mar 2023 16:21:23 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/actor-critic/</guid>
        <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;çµåˆ policy-based å’Œ value-based&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A3C
&lt;ul&gt;
&lt;li&gt;Actor-Critic æœ€çŸ¥åçš„æ–¹æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Advantage Actor-Critic æ˜¯ A2C&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;advantage-actor-critic&#34;&gt;Advantage Actor-Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Review: Policy gradient&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(\displaystyle\sum_{t^{&amp;rsquo;}=t}^{T_n}\gamma^{t^{&amp;rsquo;}-t}r_{t^{&amp;rsquo;}}^n-b)\triangledown log p_{\theta}(a_t^n|s_t^n)$
&lt;ul&gt;
&lt;li&gt;$G_t^n=\displaystyle\sum_{t^{&amp;rsquo;}=t}^{T_n}\gamma^{t^{&amp;rsquo;}-t}r_{t^{&amp;rsquo;}}^n-b$
&lt;ul&gt;
&lt;li&gt;G very unstableï¼Œå› ç‚ºçµ¦åŒæ¨£çš„ state ä½œåŒæ¨£çš„ action ä¸ä¸€å®šæœƒå¾—åˆ°åŒæ¨£çš„çµæœï¼ŒG æ˜¯å€‹ random variable&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æƒ³è¦æ”¹ç²å¾—æœŸæœ›å€¼ï¼Œå–ä»£æ‰ sample çš„å€¼(G çš„éƒ¨åˆ†)ï¼Œå¯ä»¥ç”¨ Q-Learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E[G_t^n]=Q^{\pi_\theta}(s_t^n,a_t^n)$
&lt;ul&gt;
&lt;li&gt;Q function é€™æ¨£å®šç¾©&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ‰€ä»¥æˆ‘å€‘å¯ä»¥æŠŠ G çš„éƒ¨åˆ†æ”¹ç”¨ Q æ›¿æ›æ‰ï¼Œå°±å¯ä»¥æŠŠ Actor å’Œ Critic çµåˆèµ·ä¾†&lt;/li&gt;
&lt;li&gt;baseline çš„éƒ¨åˆ†ä¹Ÿå¯ä»¥ç”¨ value function æ›¿æ›æ‰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä½†ç”¨ $Q^{\pi}(s_t^n,a_t^n)-V^{\pi}(s_t^n)$ è¦ä¸€æ¬¡ estimate å…©å€‹ network&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å¯ä»¥æŠŠ Q ä»¥ V ä¾†è¡¨ç¤ºï¼Œé‚£åªéœ€è¦ä¼°æ¸¬ V
&lt;ul&gt;
&lt;li&gt;$Q^{\pi}(s_t^n,a_t^n)=E[r_t^n+V^{\pi}(s_{t+1}^n)]$&lt;/li&gt;
&lt;li&gt;é›–ç„¶æœ‰éš¨æ©Ÿæ€§(ç²å¾—çš„ reward å’Œè·³åˆ°ä»€éº¼ state ä¸ä¸€å®š)ï¼Œä½†å…ˆä¸ç®¡æœŸæœ›å€¼ $Q^{\pi}(s_t^n,a_t^n)=r_t^n+V^{\pi}(s_{t+1}^n)$&lt;/li&gt;
&lt;li&gt;ç¾åœ¨é›–ç„¶å¤šå€‹ä¸€å€‹ rï¼Œæœ‰ä¸€äº› varianceï¼Œä½†ä¹Ÿæ¯” G å¥½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(r_t^n+V^{\pi}(s_{t+1}^n)-V^{\pi}(s_t^n))\triangledown log p_{\theta}(a_t^n|s_t^n)$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/actor-critic/A2C.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;tips&#34;&gt;Tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;actor $\pi(s)$ å’Œ critic $V^{\pi}(s)$ çš„æ¬Šé‡å¯ä»¥å…±äº«&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å‰é¢å¹¾å€‹ layer å¯ä»¥ share&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å° $\pi$ çš„ output ä¸‹ constrainï¼Œè®“ä»–çš„ entropy ä¸è¦å¤ªå°ï¼Œé”åˆ° exploration çš„æ•ˆæœ&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;asynchronous-advantage-actor-critic&#34;&gt;Asynchronous Advantage Actor-Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ä¸€é–‹å§‹æœ‰å€‹ global networkï¼Œé–‹ä¸€å † workerï¼Œæ¯æ¬¡å·¥ä½œå‰ï¼ŒæŠŠ global network çš„åƒæ•¸ copy éå»&lt;/li&gt;
&lt;li&gt;å€‹åˆ¥å»å’Œç’°å¢ƒä½œäº’å‹•ï¼Œæ›´æ–°çš„æ¢¯åº¦æ–½åŠ åœ¨ global network ä¸Š&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pathwise-derivative-policy-gradient&#34;&gt;Pathwise Derivative Policy Gradient&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;å¯ä»¥ç•¶ä½œæ˜¯ Q-Learning è§£ continuous action çš„ä¸€ç¨®æ–¹æ³•&lt;/li&gt;
&lt;li&gt;è¨“ç·´ä¸€å€‹ actorï¼Œç›®æ¨™æ˜¯ç”Ÿå‡ºçš„ a é¤µçµ¦ Q å¾Œï¼Œå¯ä»¥è®“ Q function çš„è¼¸å‡ºè¶Šå¤§è¶Šå¥½
&lt;ul&gt;
&lt;li&gt;åªæœƒèª¿ actor çš„åƒæ•¸ï¼Œæœƒ fix Q çš„&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å°±æ˜¯å€‹ GAN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/actor-critic/pathwise.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;åœ¨æ¯å€‹ episode&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å°æ–¼æ¯å€‹ time step t
&lt;ul&gt;
&lt;li&gt;âš ï¸çµ¦ state $s_t$ï¼Œæ ¹æ“š $\pi$ åŸ·è¡Œ action $a_t$ (epsilon greedy)âš ï¸&lt;/li&gt;
&lt;li&gt;ç²å¾— reward $r_t$ï¼Œåˆ°é” $s_{t+1}$&lt;/li&gt;
&lt;li&gt;æŠŠ {$s_t,a_t,r_t,s_{t+1}$} å­˜åˆ° buffer&lt;/li&gt;
&lt;li&gt;å¾ buffer sample {$s_t,a_t,r_t,s_{t+1}$}(é€šå¸¸æ˜¯ä¸€å€‹ batch)&lt;/li&gt;
&lt;li&gt;âš ï¸Target $y=r_i+\hat{Q}(s_{i+1},\hat{\pi}(s_{i+1}))$âš ï¸&lt;/li&gt;
&lt;li&gt;Update Q çš„åƒæ•¸ï¼Œå¥½è®“ $Q(s_i,a_i)$ æ›´æ¥è¿‘ y(regression)&lt;/li&gt;
&lt;li&gt;âš ï¸Update $\pi$ çš„åƒæ•¸ï¼Œè®“ $Q(s_i,\pi(s_i))$ æœ€å¤§åŒ–âš ï¸&lt;/li&gt;
&lt;li&gt;æ¯ C æ­¥ reset $\hat{Q}=Q$&lt;/li&gt;
&lt;li&gt;âš ï¸æ¯ C æ­¥ reset $\hat{\pi}=\pi$âš ï¸&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;âš ï¸ æ˜¯å’Œ Q-Learning ä¸ä¸€æ¨£çš„åœ°æ–¹&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Sentence-BERT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/sentence-bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Sun, 12 Mar 2023 10:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/sentence-bert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1908.10084&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;BERT å’Œ RoBERTa åœ¨ semantic textual similarity (STS) ä¸Šå¤ªèŠ±æ™‚é–“ï¼Œå› ç‚ºä»–éœ€è¦å°‡å…©å€‹å¥å­éƒ½è¼¸å…¥ç¶²è·¯ï¼Œä¸¦ä¸”å…©å…©æ¯”å°ã€‚&lt;/p&gt;
&lt;p&gt;Sentence-BERT(SBERT) å°é è¨“ç·´çš„ BERT ä½œäº†ä¸€äº›ä¿®æ”¹ï¼Œé€é siamese å’Œ triplet network çš„çµæ§‹ä¾†ç”Ÿå‡ºæœ‰æ„ç¾©çš„ embeddingsï¼Œä½¿å…¶æœ€å¾Œå¯ä»¥é€é cosine-similarity æ¯”è¼ƒç›¸ä¼¼åº¦ã€‚&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;SBERT ä½¿ BERT å¯ä»¥ç”¨æ–¼æŸäº›è¿„ä»Šç‚ºæ­¢ä¸é©ç”¨æ–¼ BERT çš„ä»»å‹™ï¼Œæ¯”å¦‚ large-scale semantic similarity comparisonã€clustering é‚„æœ‰ information retrieval via semantic searchã€‚&lt;/p&gt;
&lt;p&gt;ä»¥å¾€çš„ç›¸é—œç ”ç©¶æ˜¯æŠŠå–®å€‹å¥å­è¼¸å…¥ BERTï¼Œæœ€å¾Œ average BERT output layerï¼Œæˆ–æ˜¯ä½¿ç”¨ç¬¬ä¸€å€‹ outputï¼Œä½†é€™æ¨£æœƒç”¢ç”Ÿç³Ÿç³•çš„ sentence embeddingsã€‚&lt;/p&gt;
&lt;p&gt;SentEval æ˜¯ä¸€å€‹ evaluation toolkit for sentence embeddings&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;BERT é€éè¼¸å…¥å…©å€‹å¥å­ï¼Œä»¥ [SEP] éš”é–‹ï¼Œå¯ä»¥åœ¨ STS å–å¾— SOTAã€‚&lt;/p&gt;
&lt;p&gt;ä½†é€™æ¨£ç„¡æ³•è¨ˆç®—ç¨ç«‹çš„ sentence embeddingï¼Œæ‰€ä»¥éå¾€çš„ç ”ç©¶äººå“¡æŠŠå–®å€‹å¥å­è¼¸å…¥ BERTï¼Œæœ€å¾Œ average BERT output layerï¼Œæˆ–æ˜¯ä½¿ç”¨ç¬¬ä¸€å€‹ outputã€‚&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;SBERT åœ¨ BERT / RoBERTa çš„è¼¸å‡ºä¸­æ·»åŠ äº† poolingï¼Œä½œè€…å˜—è©¦äº†ä¸‰ç¨®ç­–ç•¥ï¼ŒCLS-token çš„è¼¸å‡ºã€æ‰€ä»¥è¼¸å‡ºå‘é‡çš„å¹³å‡ã€max-over-time of the output vectorsï¼Œé»˜èªæ˜¯ MEANã€‚&lt;/p&gt;
&lt;p&gt;å¯¦é©—ä»¥ä¸‹çµæ§‹å’Œç›®æ¨™å‡½æ•¸:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Classification Objective Function&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/COF-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/COF-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regression Objective Function&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç”¨ mean squared-error loss&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/ROF.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Triplet Objective Function&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/SBERT/TOF.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-details&#34;&gt;Training Details&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset
&lt;ul&gt;
&lt;li&gt;SNLI çµåˆ Multi-Genre NLI
&lt;ul&gt;
&lt;li&gt;SNLI: 570,000 å€‹ å¥å­ pairï¼Œæœ‰ä¸‰é¡ï¼Œcontradiction, eintailment, and neutral&lt;/li&gt;
&lt;li&gt;MultiNLI: 430,000 å€‹å¥å­ pair&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3-way softmax Classification Objective Function&lt;/li&gt;
&lt;li&gt;1-epoch&lt;/li&gt;
&lt;li&gt;batch-size: 16&lt;/li&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;li&gt;lr: 2e-5&lt;/li&gt;
&lt;li&gt;warm-up: è¶…é 10% of the training data&lt;/li&gt;
&lt;li&gt;é»˜èª pooling ç­–ç•¥: MEAN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;å­¸ç¿’ä¸€å€‹è¤‡é›œçš„å›æ­¸å‡½æ•¸åˆ†æ STS å¸¸æ˜¯ SOTAï¼Œä½†æ˜¯ç”±æ–¼ä»–æ˜¯ pair-wiseï¼Œé‡åˆ° combinatorial explosionï¼Œä¸å¥½æ‹“å±•ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡ç”¨ cosine-similarity æ¯”è¼ƒå…©å€‹ embeddings çš„ç›¸ä¼¼åº¦ï¼Œä¹Ÿç”¨ negative Manhatten å’Œ negative Euclidean distancesï¼Œä½†å¾—åˆ°å·®ä¸å¤šçš„çµæœã€‚&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;ç”¨ BERT ç”Ÿå‡ºçš„ embeddings ä¸é©åˆå¸¸è¦‹çš„ç›¸ä¼¼åº¦æ¸¬é‡æ–¹æ³•ï¼Œæ¯”å¦‚ cosine-similarityã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡º SBERT æ”¹é€²ï¼Œåœ¨ siamese / triplet ç¶²è·¯æ¶æ§‹ä¸­å¾®èª¿ BERTã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ RoBERTa æ›¿æ›æ‰ BERT ä¸¦æ²’æœ‰ä»€éº¼é¡¯è‘—æ”¹é€²ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>PatentBERT è«–æ–‡é–±è®€</title>
        <link>https://roykesydon.github.io/Blog/p/patentbert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</link>
        <pubDate>Thu, 02 Mar 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/patentbert-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1906.02124&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;æŠŠ fine-tune BERT æ‡‰ç”¨åœ¨å°ˆåˆ©åˆ†é¡ä¸Šï¼Œç•¶æ‡‰ç”¨æ–¼è¶…é 200 è¬ä»¶å°ˆåˆ©çš„è³‡æ–™é›†æ™‚ï¼Œè©²æ–¹æ³•è¶…è¶Šäº†çµåˆ word-embedding çš„ CNN çš„ SOTA ä½œæ³•ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;è²¢ç»:
&lt;ol&gt;
&lt;li&gt;ä¸€å€‹ç”¨é è¨“ç·´çš„ BERT å» fine-tune çš„ SOTA æ–¹æ³•&lt;/li&gt;
&lt;li&gt;ä¸€å€‹å«åš USPTO-3M çš„å¤§å‹è³‡æ–™é›†ï¼Œå±¬æ–¼ CPC subclass levelï¼Œä¸¦æä¾› SQL èªå¥è®“å¾ŒçºŒçš„ç ”ç©¶è€…ä½¿ç”¨&lt;/li&gt;
&lt;li&gt;èˆ‡å‚³çµ±è§€å¿µç›¸åï¼Œåªéœ€è¦ claim å°±è¶³ä»¥å®Œæˆåˆ†é¡ä»»å‹™&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;å°ˆåˆ©åˆ†é¡æ˜¯ä¸€å€‹ multi-label çš„åˆ†é¡ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ç”±æ–¼æ¨™ç±¤çš„æ•¸é‡å¯èƒ½å¾ˆå¤§ï¼Œæ‰€ä»¥æ˜¯å€‹å…·æœ‰æŒ‘æˆ°æ€§çš„ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…æº–å‚™äº†ä¸€å€‹åŸºæ–¼ CPC çš„æ–°è³‡æ–™é›†ï¼Œæœ‰è¶…éä¸‰ç™¾è¬é …ç¾åœ‹å°ˆåˆ©ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPC
&lt;ul&gt;
&lt;li&gt;Cooperative Patent Classification&lt;/li&gt;
&lt;li&gt;æ˜¯ IPC æ›´å…·é«”å’Œè©³ç´°çš„ç‰ˆæœ¬&lt;/li&gt;
&lt;li&gt;å¯é è¦‹å°‡å–ä»£ IPC æˆç‚ºæ–°çš„æ¨™æº–
&lt;ul&gt;
&lt;li&gt;åªæ˜¯ç”±æ–¼ CLEP-IP ç«¶è³½ï¼Œå¤§éƒ¨åˆ†è«–æ–‡éƒ½åŸºæ–¼ IPC
&lt;ul&gt;
&lt;li&gt;è³‡æ–™é›†åŒ…å« 1978 åˆ° 2009 æäº¤çš„å°ˆåˆ©&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IPC
&lt;ul&gt;
&lt;li&gt;International Patent Classification&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;æ­¤å¤–ï¼Œä½œè€…çš„ dataset åŸºæ–¼ patent claims&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patent claims
&lt;ul&gt;
&lt;li&gt;é‡è¦æ€§åœ¨éå¾€è¢«ä½ä¼°&lt;/li&gt;
&lt;li&gt;åœ¨èµ·è‰å°ˆåˆ©ç”³è«‹æ™‚ï¼Œå°ˆåˆ©æ¥­è€…æœƒå…ˆèµ·è‰ patent claims&lt;/li&gt;
&lt;li&gt;å°ˆåˆ©æ–‡ä»¶çš„å…¶é¤˜éƒ¨åˆ†ç”± claim åšå»¶ä¼¸&lt;/li&gt;
&lt;li&gt;åœ¨å°ˆåˆ©æ³•ä¸­ï¼Œclaims å®šç¾©äº†å°ˆåˆ©ç™¼æ˜çš„ç•Œç·šï¼Œç¢ºå®šäº†å°ˆåˆ©æ¬Šç¯„åœ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç‚ºä½¿æ¨¡å‹æ›´ç°¡å–®ï¼Œåªé—œæ³¨ patent claimsï¼Œä¸¦ä¸”åƒ…ç”¨ç¬¬ä¸€é … claimã€‚&lt;/p&gt;
&lt;h1 id=&#34;ç›¸é—œå·¥ä½œ&#34;&gt;ç›¸é—œå·¥ä½œ&lt;/h1&gt;
&lt;p&gt;éå¾€æœ‰äº›ç ”ç©¶åªé¡¯ç¤ºäº† precisionï¼Œä½†æ²’æœ‰ F1 value æˆ– recallï¼Œé›£ä»¥å…¬å¹³æ¯”è¼ƒã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ DeepPatent&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;éå¾€è³‡æ–™åŸºæ–¼ CLEF-IP æˆ– patent officesã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…ç™¼ç¾åœ¨ BigQuery ç”¨ Google Patents Public Datasets æ›´å®¹æ˜“ã€‚&lt;/p&gt;
&lt;p&gt;è€Œä¸”å¯ç”¨ SQL statementsï¼Œä½œè€…èªç‚ºæ¯”å…±äº«å‚³çµ±è³‡æ–™é›†æ›´å¥½ï¼ŒåŸå› å¦‚ä¸‹:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Seperation of concerns
&lt;ul&gt;
&lt;li&gt;å¦‚æœè³‡æ–™åŒ…å«å‰è™•ç†æˆ–å¾Œè™•ç†ï¼Œå…¶ä»–ç ”ç©¶äººå“¡éœ€è¦ä¸åŒæ“ä½œæ™‚æœƒå¾ˆé ­ç—›ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Clarity and flexibility
&lt;ul&gt;
&lt;li&gt;SQL statement ç²¾ç¢ºä¸”å®¹æ˜“æ ¹æ“šä¸åŒæ¢ä»¶é€²è¡Œä¿®æ”¹ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åœ¨å’Œ DeepPatent æ¯”è¼ƒçš„æ™‚å€™ï¼Œå¯ä»¥çš„è©±ï¼Œæœƒç”¨ USPTO2M é€²è¡Œæ¸¬è©¦ï¼Œå¦‚æœä¸è¡Œï¼Œæ‰æœƒåˆä½µä¾†è‡ª USPTO-3M çš„è³‡æ–™ï¼Œæ¯”å¦‚ USPTO-2M æ²’æœ‰ claims çš„æƒ…æ³ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº†æ¯”è¼ƒ claim å¦‚ä½•å½±éŸ¿æ€§èƒ½ï¼Œå°‡åˆä½µå…©å€‹è³‡æ–™é›†ã€‚&lt;/p&gt;
&lt;h1 id=&#34;method--experimental-setup&#34;&gt;Method &amp;amp; Experimental Setup&lt;/h1&gt;
&lt;p&gt;ç”¨ BERT-Base å°±å¯ä»¥æ‰“æ•— DeepPatentã€‚&lt;/p&gt;
&lt;p&gt;éµå¾ª BERT Project ä¸­çµ¦çš„ fine-tune ç¯„ä¾‹ã€‚&lt;/p&gt;
&lt;p&gt;ç‚ºäº† multilabelï¼Œç”¨ sigmoid cross entropy with logits function è€Œä¸æ˜¯ç”¨ softmaxã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/PatentBERT/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;å°ˆåˆ©åˆ†é¡ä½œç‚ºå…·æœ‰æŒ‘æˆ°æ€§çš„ä»»å‹™ï¼Œå¹¾åå¹´ä¾†ä¸€ç›´æ²’æœ‰ä»¤äººæ»¿æ„çš„è¡¨ç¾ã€‚&lt;/p&gt;
&lt;p&gt;æœ¬æ–‡æå‡ºä¸€å€‹åŸºæ–¼ fine-tune BERT çš„æ–¹æ³•ï¼Œæ€§èƒ½å„ªæ–¼ DeepPatentã€‚&lt;/p&gt;
&lt;p&gt;ä¸¦ä¸”çµæœè¡¨æ˜åªç”¨ patent claim å°±å¯ä»¥å®Œæˆåˆ†é¡ä»»å‹™ã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Q-learning</title>
        <link>https://roykesydon.github.io/Blog/p/q-learning/</link>
        <pubDate>Mon, 20 Feb 2023 16:21:23 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/q-learning/</guid>
        <description>&lt;h2 id=&#34;rl-æ–¹æ³•&#34;&gt;RL æ–¹æ³•&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Policy-based
&lt;ul&gt;
&lt;li&gt;learn åšäº‹çš„ actor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Value-based
&lt;ul&gt;
&lt;li&gt;ä¸ç›´æ¥ learn policyï¼Œè€Œæ˜¯ Learn criticï¼Œè² è²¬æ‰¹è©•&lt;/li&gt;
&lt;li&gt;Q-learning å±¬æ–¼é€™ç¨®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;critic&#34;&gt;Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ä¸ç›´æ¥æ±ºå®š action&lt;/li&gt;
&lt;li&gt;çµ¦äºˆ actor $\pi$ï¼Œè©•ä¼° actor $\pi$ æœ‰å¤šå¥½&lt;/li&gt;
&lt;li&gt;critic çš„ output ä¾è³´æ–¼ actor çš„è¡¨ç¾&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;state-value-function&#34;&gt;State Value Function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;State value function $V^{\pi}(s)$
&lt;ul&gt;
&lt;li&gt;ç”¨ actor $\pi$ï¼Œçœ‹åˆ° s å¾Œç©åˆ°çµæŸï¼Œcumulated reward expectation æ˜¯å¤šå°‘&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;è©•ä¼°æ–¹æ³•&#34;&gt;è©•ä¼°æ–¹æ³•&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Monte-Carlo(MC) based approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;critic çœ‹ $\pi$ ç©éŠæˆ²&lt;/li&gt;
&lt;li&gt;è¨“ç·´ä¸€å€‹ networkï¼Œçœ‹åˆ°ä¸åŒçš„ state ï¼Œè¼¸å‡º cumulated reward(ç›´åˆ°éŠæˆ²çµæŸï¼Œä»¥ä¸‹ç¨±ç‚º $G_a$)ï¼Œè§£ regression å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal-difference(TD) approach&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC çš„æ–¹æ³•è‡³å°‘è¦ç©åˆ°éŠæˆ²çµæŸæ‰å¯ä»¥ update networkï¼Œä½†æœ‰äº›éŠæˆ²è¶…é•·
&lt;ul&gt;
&lt;li&gt;TD åªéœ€è¦ {$s_t,a_t,r_t,s_{t+1}$}&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$V^{\pi}(s_t)=V^{\pi}(s_{t+1})+r_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MS v.s. TD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC
&lt;ul&gt;
&lt;li&gt;Larger variance
&lt;ul&gt;
&lt;li&gt;æ¯æ¬¡çš„è¼¸å‡ºå·®ç•°å¾ˆå¤§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TD
&lt;ul&gt;
&lt;li&gt;smaller variance
&lt;ul&gt;
&lt;li&gt;ç›¸è¼ƒ $G_a$ è¼ƒå°ï¼Œå› ç‚ºé€™é‚Šçš„ random variable æ˜¯ rï¼Œä½† $G_a$ æ˜¯ç”±å¾ˆå¤š r çµ„åˆè€Œæˆ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;V å¯èƒ½ä¼°å¾—ä¸æº–ç¢º
&lt;ul&gt;
&lt;li&gt;é‚£ learn å‡ºä¾†çš„çµæœè‡ªç„¶ä¹Ÿä¸å‡†&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;è¼ƒå¸¸è¦‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;another-critic&#34;&gt;Another Critic&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;State-action value function $Q^\pi(s,a)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åˆå« Q function&lt;/li&gt;
&lt;li&gt;ç•¶ç”¨ actor $\pi$ æ™‚ï¼Œåœ¨ state s æ¡å– a é€™å€‹ action å¾Œçš„ cumulated reward expectation
&lt;ul&gt;
&lt;li&gt;æœ‰ä¸€å€‹è¦æ³¨æ„çš„åœ°æ–¹æ˜¯ï¼Œactor çœ‹åˆ° s ä¸ä¸€å®šæœƒæ¡å– a&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/q-function.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/how-to-use-q.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åªè¦æœ‰ Q functionï¼Œå°±å¯ä»¥æ‰¾åˆ°&amp;quot;æ›´å¥½çš„&amp;quot; policyï¼Œå†æ›¿æ›æ‰åŸæœ¬çš„ policy
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;æ›´å¥½çš„&amp;quot;å®šç¾©
&lt;ul&gt;
&lt;li&gt;$V^{\pi^{&amp;rsquo;}} \ge V^{\pi}(s), \text{for all state s}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\pi^{&amp;rsquo;}(s)=arg \underset{a}{max}Q^{\pi}(s,a)$
&lt;ul&gt;
&lt;li&gt;$\pi^{&amp;rsquo;}$ æ²’æœ‰å¤šé¤˜çš„åƒæ•¸ï¼Œå°±å–®ç´”é  Q function æ¨å‡ºä¾†&lt;/li&gt;
&lt;li&gt;é€™é‚Šå¦‚æœ a æ˜¯ continuous çš„æœƒæœ‰å•é¡Œï¼Œç­‰ç­‰è§£æ±º&lt;/li&gt;
&lt;li&gt;é€™æ¨£å°±å¯ä»¥é”åˆ°&amp;quot;æ›´å¥½çš„&amp;quot;policyï¼Œä¸éå°±ä¸åˆ—è­‰æ˜äº†&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basic-tip&#34;&gt;Basic Tip&lt;/h2&gt;
&lt;h3 id=&#34;target-network&#34;&gt;Target network&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨ training çš„æ™‚å€™ï¼ŒæŠŠå…¶ä¸­ä¸€å€‹ Q å›ºå®šä½ï¼Œä¸ç„¶è¦å­¸çš„ target æ˜¯ä¸å›ºå®šçš„ï¼Œæœƒä¸å¥½ train&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/target-network.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;exploration&#34;&gt;Exploration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;policy å®Œå…¨ depend on Q function&lt;/li&gt;
&lt;li&gt;å¦‚æœ action ç¸½æ˜¯å›ºå®šï¼Œé€™ä¸æ˜¯å¥½çš„ data collection æ–¹æ³•ï¼Œè¦åœ¨ s æ¡å– a éï¼Œæ‰æ¯”è¼ƒå¥½ä¼°è¨ˆ Q(s, a)ï¼Œå¦‚æœ Q function æ˜¯ table å°±æ ¹æœ¬ä¸å¯èƒ½ä¼°å‡ºä¾†ï¼Œnetwork ä¹Ÿæœƒæœ‰ä¸€æ¨£çš„å•é¡Œï¼Œåªæ˜¯æ²’é‚£éº¼åš´é‡ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;è§£æ³•&#34;&gt;è§£æ³•&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Epsilon Greedy
&lt;ul&gt;
&lt;li&gt;$a=\begin{cases}
arg \underset{a}{max}Q(s,a), &amp;amp; \text{with probability } 1-\varepsilon \\
random, &amp;amp; otherwise
\end{cases}$&lt;/li&gt;
&lt;li&gt;é€šå¸¸ $\varepsilon$ æœƒéš¨æ™‚é–“éæ¸›ï¼Œå› ç‚ºä½ ä¸€é–‹å§‹ train çš„æ™‚å€™ä¸çŸ¥é“æ€éº¼æ¯”è¼ƒå¥½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Boltzmann Exploration
&lt;ul&gt;
&lt;li&gt;$P(a|s)=\frac{exp(Q(s,a))}{\sum_a exp(Q(s,a))}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;replay-buffer&#34;&gt;Replay Buffer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠä¸€å †çš„ {$s_t,a_t,r_t,s_{t+1}$} å­˜æ”¾åœ¨ä¸€å€‹ buffer&lt;/li&gt;
&lt;li&gt;{$s_t,a_t,r_t,s_{t+1}$} ç°¡ç¨±ç‚º exp&lt;/li&gt;
&lt;li&gt;è£¡é¢çš„ exp å¯èƒ½ä¾†è‡ªæ–¼ä¸åŒçš„ policy&lt;/li&gt;
&lt;li&gt;åœ¨ buffer è£æ»¿çš„æ™‚å€™æ‰æŠŠèˆŠçš„è³‡æ–™ä¸Ÿæ‰&lt;/li&gt;
&lt;li&gt;æ¯æ¬¡å¾ buffer éš¨æ©ŸæŒ‘ä¸€å€‹ batch å‡ºä¾†ï¼Œupdate Q function&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;å¥½è™•&#34;&gt;å¥½è™•&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;è·Ÿç’°å¢ƒä½œäº’å‹•å¾ˆèŠ±æ™‚é–“ï¼Œé€™æ¨£å¯ä»¥æ¸›å°‘è·Ÿç’°å¢ƒä½œäº’å‹•çš„æ¬¡æ•¸&lt;/li&gt;
&lt;li&gt;æœ¬ä¾†å°±å¸Œæœ› batch è£¡çš„ data è¶Š diverse è¶Šå¥½ï¼Œä¸æœƒå¸Œæœ› batch è£¡çš„ data éƒ½æ˜¯åŒæ€§è³ªçš„&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;issue&#34;&gt;issue&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;æˆ‘å€‘è¦è§€å¯Ÿ $\pi$ çš„ valueï¼Œæ··é›œäº†ä¸€äº›ä¸æ˜¯ $\pi$ çš„ exp åˆ°åº•æœ‰æ²’æœ‰é—œä¿‚?
&lt;ul&gt;
&lt;li&gt;ç†è«–ä¸Šæ²’å•é¡Œï¼Œä½†æè€å¸«æ²’è§£é‡‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;typical-q-learning-æ¼”ç®—æ³•&#34;&gt;Typical Q-learning æ¼”ç®—æ³•&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;åˆå§‹åŒ– Q-fucntion Qï¼Œtarget Q-function $\hat{Q}=Q$&lt;/li&gt;
&lt;li&gt;åœ¨æ¯å€‹ episode
&lt;ul&gt;
&lt;li&gt;å°æ–¼æ¯å€‹ time step t
&lt;ul&gt;
&lt;li&gt;çµ¦ state $s_t$ï¼Œæ ¹æ“š Q åŸ·è¡Œ action $a_t$ (epsilon greedy)&lt;/li&gt;
&lt;li&gt;ç²å¾— reward $r_t$ï¼Œåˆ°é” $s_{t+1}$&lt;/li&gt;
&lt;li&gt;æŠŠ {$s_t,a_t,r_t,s_{t+1}$} å­˜åˆ° buffer&lt;/li&gt;
&lt;li&gt;å¾ buffer sample {$s_t,a_t,r_t,s_{t+1}$}(é€šå¸¸æ˜¯ä¸€å€‹ batch)&lt;/li&gt;
&lt;li&gt;Target $y=r_i+\underset{a}{max}\hat{Q}(s_{i+1},a)$&lt;/li&gt;
&lt;li&gt;Update Q çš„åƒæ•¸ï¼Œå¥½è®“ $Q(s_i,a_i)$ æ›´æ¥è¿‘ y(regression)&lt;/li&gt;
&lt;li&gt;æ¯ C æ­¥ reset $\hat{Q}=Q$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;adveanced-tip&#34;&gt;Adveanced Tip&lt;/h2&gt;
&lt;h3 id=&#34;double-dqn&#34;&gt;Double DQN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q Value å¾€å¾€è¢«é«˜ä¼°
&lt;ul&gt;
&lt;li&gt;æˆ‘å€‘çš„ç›®çš„æ˜¯è¦è®“ $Q(s_t, a_t)$ å’Œ $r_t+\underset{a}{max}Q(s_{t+1},a)$ è¶Šæ¥è¿‘è¶Šå¥½(å¾Œè€…å°±æ˜¯ target)&lt;/li&gt;
&lt;li&gt;target å¸¸å¸¸ä¸å°å¿ƒè¨­å¤ªé«˜ï¼Œå› ç‚ºå¦‚æœæœ‰ action è¢«é«˜ä¼°äº†ï¼Œå°±æœƒé¸é‚£å€‹ç•¶ target&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Double DQN: å…©å€‹å‡½å¼ $Q$ å’Œ $Q^{&amp;rsquo;}$
&lt;ul&gt;
&lt;li&gt;æŠŠ target æ›æˆ  $r_t+Q^{&amp;rsquo;}(s_{t+1},arg \underset{a}{max}Q(s_{t+1},a))$&lt;/li&gt;
&lt;li&gt;é¸ action äº¤çµ¦ $Q$ï¼Œå¯¦éš›ç®—äº¤çµ¦ $Q^{&amp;rsquo;}$
&lt;ul&gt;
&lt;li&gt;å¦‚æœ $Q$ é¸äº†é«˜ä¼°çš„ actionï¼Œ$Q^{&amp;rsquo;}$ æœ‰å¯èƒ½ä¿®æ­£å›ä¾†&lt;/li&gt;
&lt;li&gt;å¦‚æœ $Q^{&amp;rsquo;}$ é«˜ä¼°ï¼Œ$Q$ ä¸ä¸€å®šæœƒé¸åˆ°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$Q^{&amp;rsquo;}$ æ˜¯ target network(å›ºå®šä¸å‹•)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dueling-dqn&#34;&gt;Dueling DQN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;æ”¹è®Š network æ¶æ§‹&lt;/li&gt;
&lt;li&gt;åˆ†æˆå…©æ¢ path
&lt;ul&gt;
&lt;li&gt;ç¬¬ä¸€æ¢ç®— scalar&lt;/li&gt;
&lt;li&gt;ç¬¬äºŒæ¢ç®— vectorï¼Œæ¯å€‹ action éƒ½æœ‰å€‹ value&lt;/li&gt;
&lt;li&gt;æŠŠ scalar åŠ åˆ°æ¯ä¸€å€‹ç¶­åº¦&lt;/li&gt;
&lt;li&gt;åªæ›´æ”¹åˆ° V(s) çš„æ™‚å€™ï¼Œæœƒå…¨éƒ¨çš„ action éƒ½æ”¹åˆ°ï¼Œå¯èƒ½æœƒæ˜¯ä¸€å€‹æ¯”è¼ƒæœ‰æ•ˆç‡çš„æ–¹å¼ï¼Œä¸ç”¨ sample æ‰€æœ‰çš„ action
&lt;ul&gt;
&lt;li&gt;ä½†æœ‰å¯èƒ½æ¨¡å‹ä¸ç®¡ V(s)ï¼Œç›´æ¥è¨­ 0ï¼Œåªæ”¹ A&lt;/li&gt;
&lt;li&gt;æ‰€ä»¥æœƒå° A ä¸‹ constrainï¼Œè®“ network å‚¾å‘æ–¼æ”¹ V
&lt;ul&gt;
&lt;li&gt;æ¯”å¦‚åŒå€‹ state ä¸‹çš„æ‰€æœ‰ action è¦ç”Ÿå‡º A(s,a) ç¸½å’Œç‚º 0
&lt;ul&gt;
&lt;li&gt;åœ¨ A çš„è¼¸å‡ºåŠ å€‹ normalization å³å¯è¾¦åˆ°ï¼Œé€™å€‹ normalization å°±æ˜¯æŠŠæ¯å€‹ç¶­åº¦éƒ½æ¸›æ‰å¹³å‡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/q-learning/dueling-dqn.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;prioritized-replay&#34;&gt;Prioritized Replay&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;åŸæœ¬æ˜¯ uniform çš„å¾ buffer sample data&lt;/li&gt;
&lt;li&gt;æ”¹è®“ ã€Œæœ‰æ›´å¤§çš„ TD errorã€çš„ data æœ‰æ›´é«˜çš„æ©Ÿç‡è¢« sample
&lt;ul&gt;
&lt;li&gt;TD error å°±æ˜¯ $Q(s_t, a_t)$ å’Œ target çš„å·®è·&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;å¯¦éš›åœ¨åšçš„æ™‚å€™æœ‰é¡å¤–çš„ç´°ç¯€ï¼Œä¸æœƒåªæ”¹ sampling çš„ processï¼Œé‚„è¦æ”¹ update åƒæ•¸çš„æ–¹æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-step&#34;&gt;Multi-step&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Balance between MC å’Œ TD&lt;/li&gt;
&lt;li&gt;TD åªéœ€è¦å­˜ {$s_t,a_t,r_t,s_{t+1}$}&lt;/li&gt;
&lt;li&gt;æ”¹å­˜ {$s_t,a_t,r_t,&amp;hellip;,s_{t+N},a_{t+N},r_{t+N}, s_{t+N+1}$}&lt;/li&gt;
&lt;li&gt;æˆ‘å€‘çš„ç›®çš„æ˜¯è¦è®“ $Q(s_t, a_t)$ å’Œ $\displaystyle\sum_{t^{&amp;rsquo;}=t}^{t+N} r_{t^{&amp;rsquo;}}+\hat{Q}(s_{t+N+1},a_{t+N+1})$ è¶Šæ¥è¿‘è¶Šå¥½(å¾Œè€…å°±æ˜¯ target)
&lt;ul&gt;
&lt;li&gt;$a_{t+N+1}=arg\underset{a}{max}\hat{Q}(s_{t+N+1},a)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;åŒæ™‚æœ‰ MC å’Œ TD çš„å¥½è™•å’Œå£è™•
&lt;ul&gt;
&lt;li&gt;ä¼°æ¸¬çš„å½±éŸ¿æ¯”è¼ƒè¼•å¾®&lt;/li&gt;
&lt;li&gt;r æ¯”è¼ƒå¤šé …ï¼Œvariance æ¯”è¼ƒå¤§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;noisy-net&#34;&gt;Noisy Net&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;improve exploration&lt;/li&gt;
&lt;li&gt;Noise on Action
&lt;ul&gt;
&lt;li&gt;Epsilon Greedy(ä¹‹å‰çš„å›é¡§)
&lt;ul&gt;
&lt;li&gt;$f_X(x) = \begin{cases}
arg \underset{a}{max}Q(s,a), &amp;amp; \text{with probability }1-\varepsilon \\
random, &amp;amp; ,otherwise
\end{cases}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;çµ¦åŒæ¨£çš„ stateï¼Œæ¡å–çš„ action ä¸ä¸€å®šä¸€æ¨£&lt;/li&gt;
&lt;li&gt;æ²’æœ‰çœŸå¯¦çš„ policy æœƒé€™æ¨£é‹ä½œ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Noise on Parameters
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$a = arg \underset{a}{max}\tilde{Q}(s,a)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨æ¯å€‹ episode å‰›é–‹å§‹çš„æ™‚å€™ï¼Œåœ¨ Q-function çš„åƒæ•¸ä¸Šé¢åŠ ä¸Š gaussian noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;çµ¦åŒæ¨£çš„ stateï¼Œæ¡å–åŒæ¨£çš„ action&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å«åš state-dependent exploration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;explore in a consistent way&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;distributional-q-function&#34;&gt;Distributional Q-function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q-function ç”Ÿå‡ºçš„æ±è¥¿æ˜¯ cumulated reward çš„æœŸæœ›å€¼
&lt;ul&gt;
&lt;li&gt;æ‰€ä»¥æˆ‘å€‘æ˜¯åœ¨å° distribution å– meanï¼Œä½†ä¸åŒçš„ distribution ä¹Ÿå¯èƒ½æœ‰åŒæ¨£çš„ mean&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æƒ³åšçš„äº‹æƒ…æ˜¯ model distribution&lt;/li&gt;
&lt;li&gt;å¦‚æœæœ‰åšé€™å€‹ï¼Œå°±æ¯”è¼ƒä¸æœƒæœ‰ over estimate reward çš„çµæœï¼Œåè€Œå®¹æ˜“ under estimateï¼Œä½¿ double æ¯”è¼ƒæ²’ç”¨
&lt;ul&gt;
&lt;li&gt;output çš„ range ä¸å¯èƒ½ç„¡é™å¯¬ï¼Œè¶…éé‚Šç•Œçš„ reward æœƒè¢«ä¸Ÿæ‰&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rainbow&#34;&gt;Rainbow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ç¶œåˆä¸€å †æ–¹æ³•&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;continuous-actions&#34;&gt;Continuous actions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Q learning ä¸å®¹æ˜“è™•ç† continuous action&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;sample n å€‹å¯èƒ½çš„ aï¼Œéƒ½ä¸Ÿ Q function çœ‹èª°æœ€å¤§&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;gradient descent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠ a ç•¶ä½œ parameterï¼Œè¦æ‰¾ä¸€çµ„ a å» maximize Q function
&lt;ul&gt;
&lt;li&gt;é‹ç®—é‡å¤§ï¼Œè¦ iterative çš„ update a&lt;/li&gt;
&lt;li&gt;ä¸ä¸€å®šå¯ä»¥æ‰¾åˆ° global çš„æœ€ä½³è§£&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç‰¹åˆ¥è¨­è¨ˆ Q networkï¼Œè®“è§£ optimization çš„å•é¡Œè®Šå®¹æ˜“&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ç¯„ä¾‹
&lt;ul&gt;
&lt;li&gt;Q network è¼¸å‡º $\mu(s)$ã€$\Sigma(s)$ã€$V(s)$ï¼Œå€‹åˆ¥æ˜¯ vectorã€matrixã€scalar&lt;/li&gt;
&lt;li&gt;a æ˜¯ continuous çš„ Actionï¼Œæ˜¯ä¸€å€‹ vectorï¼Œæ¯å€‹ç¶­åº¦éƒ½æ˜¯å¯¦æ•¸&lt;/li&gt;
&lt;li&gt;$\Sigma(s)$ æ˜¯ positive definite çš„ï¼Œå¯¦ä½œçš„æ™‚å€™æœƒæŠŠ $\Sigma$ å’Œå®ƒçš„ transpose ç›¸ä¹˜&lt;/li&gt;
&lt;li&gt;$Q(s,a)=-(a-\mu(s))^T\Sigma(s)(a-\mu(s))+V(s)$&lt;/li&gt;
&lt;li&gt;$(a-\mu(s))^T\Sigma(s)(a-\mu(s))$ é€™é …å¿…ç‚ºæ­£ï¼Œæ‰€ä»¥ $a=\mu(s)$ çš„æ™‚å€™å°±æ˜¯æœ€ä½³è§£&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¸è¦ç”¨ Q-learning&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Proximal Policy Optimization(PPO)</title>
        <link>https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/</link>
        <pubDate>Mon, 20 Feb 2023 12:35:56 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/proximal-policy-optimizationppo/</guid>
        <description>&lt;h1 id=&#34;onoff-policy&#34;&gt;On/Off-policy&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;On-policy
&lt;ul&gt;
&lt;li&gt;å­¸ç¿’çš„ agent å’Œèˆ‡ç’°å¢ƒäº’å‹•çš„ agent æ˜¯åŒä¸€å€‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Off-policy
&lt;ul&gt;
&lt;li&gt;å­¸ç¿’çš„ agent å’Œèˆ‡ç’°å¢ƒäº’å‹•çš„ agent æ˜¯ä¸åŒå€‹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;æƒ³å¾-on-policy-è½‰-off-policy&#34;&gt;æƒ³å¾ On-policy è½‰ Off-policy&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;On-policy æ¯æ¬¡éƒ½è¦é‡æ–°è’é›†è³‡æ–™ï¼Œå¾ˆèŠ±æ™‚é–“&lt;/li&gt;
&lt;li&gt;ç”±å¦ä¸€å€‹ $\pi_{\theta^{&amp;rsquo;}}$ å» train $\theta$ï¼Œ$\theta^{&amp;rsquo;}$æ˜¯å›ºå®šçš„ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥ re-use sample data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æ˜¯ä¸€å€‹ general çš„æƒ³æ³•ï¼Œä¸é™æ–¼ RL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$E_{x \text{\textasciitilde} p}[f(x)]\approx \frac{1}{N}\displaystyle\sum_{i=1}^N f(x^i)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x^i$ is sampled from p(x)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æˆ‘å€‘é‡åˆ°çš„å•é¡Œæ˜¯æ²’è¾¦æ³•å¾ p ä¾† sample dataï¼Œåªèƒ½é€é q(x) å» sample $x^i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å¯ä»¥æŠŠä¸Šå¼æ”¹å¯«æˆ $E_{x \text{\textasciitilde} p}[f(x)]=E_{x \text{\textasciitilde} q}[f(x)\frac{p(x)}{q(x)}]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;issue&#34;&gt;Issue&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;é›–ç„¶ç†è«–ä¸Š q å¯ä»¥ä»»æ„é¸ï¼Œåªè¦ä¸è¦ q(x) æ˜¯ 0 çš„æ™‚å€™ p(x) ä¸æ˜¯ 0ï¼Œå¯¦ä½œä¸Š p å’Œ q ä¸èƒ½å·®å¤ªå¤šï¼Œä¸ç„¶æœƒæœ‰å•é¡Œ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;é€™å…©é …çš„ Variance ä¸ä¸€æ¨£ï¼Œå¦‚æœ p é™¤ä»¥ q å·®è·å¾ˆå¤§ï¼Œå³é‚Šçš„ Variance æœƒå¾ˆå¤§ï¼Œå¦‚æœ sample ä¸å¤ å¤šæ¬¡å°±æœƒæœ‰å•é¡Œ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/ppo/importance-sample-issue.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è½‰æ›&#34;&gt;è½‰æ›&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;åŸæœ¬&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}=E_{\tau \text{\textasciitilde}p_{\theta}(\tau)}[R(\tau)\triangledown log p_{\theta} (\tau)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ”¹ç‚º&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}=E_{\tau \text{\textasciitilde}p_{\theta^{&amp;rsquo;}}(\tau)}[\frac{p_{\theta}(\tau)}{p_{\theta^{&amp;rsquo;}}(\tau)}R(\tau)\triangledown log p_{\theta} (\tau)]$&lt;/li&gt;
&lt;li&gt;å¾ $\theta^{&amp;rsquo;}$ sample è³‡æ–™&lt;/li&gt;
&lt;li&gt;æ›´æ–° $\theta$ å¤šæ¬¡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;advantage-function&#34;&gt;Advantage function&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;åŸæœ¬&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta}}[A^{\theta}(s_t,a_t)\triangledown log p_\theta(a_t^n|s_t^n)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ”¹ç‚º&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{P_\theta(s_t,a_t)}{P_{\theta^{&amp;rsquo;}}(s_t,a_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)\triangledown log p_\theta(a_t^n|s_t^n)]$&lt;/li&gt;
&lt;li&gt;è¦æ³¨æ„ Advantage çš„çµæœè¦ç”± $\theta^{&amp;rsquo;}$ å¾—å‡ºï¼Œæ˜¯ $\theta^{&amp;rsquo;}$åœ¨å’Œç’°å¢ƒäº’å‹•&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ–°çš„ objective function&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J^{\theta^{&amp;rsquo;}}(\theta)=E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{&amp;rsquo;}}(a_t|s_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ppo&#34;&gt;PPO&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;ç¢ºä¿ $\theta$ å’Œ $\theta^{&amp;rsquo;}$ ä¸æœƒå·®å¤ªå¤š&lt;/li&gt;
&lt;li&gt;$J_{PPO}^{\theta^{&amp;rsquo;}}(\theta)=J^{\theta^{&amp;rsquo;}}(\theta)-\beta KL(\theta, \theta^{&amp;rsquo;})$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;å‰èº«-trpo&#34;&gt;å‰èº« TRPO&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Trust Region Policy Optimization&lt;/li&gt;
&lt;li&gt;$J_{TRPO}^{\theta^{&amp;rsquo;}}(\theta)=E_{(s_t,a_t)\text{\textasciitilde}\pi_{\theta^{&amp;rsquo;}}}[\frac{p_\theta(a_t|s_t)}{p_{\theta^{&amp;rsquo;}}(a_t|s_t)}A^{\theta^{&amp;rsquo;}}(s_t,a_t)], KL(\theta, \theta^{&amp;rsquo;})&amp;lt;\delta$&lt;/li&gt;
&lt;li&gt;constrain å¾ˆé›£è™•ç†&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kl-divergence&#34;&gt;KL divergence&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;é€™é‚Šä¸æ˜¯ $\theta$ å’Œ $\theta^{&amp;rsquo;}$ åƒæ•¸ä¸Šçš„è·é›¢ï¼Œè€Œæ˜¯ behavior çš„è·é›¢
&lt;ul&gt;
&lt;li&gt;åƒæ•¸ä¸Šçš„è·é›¢æ˜¯æŒ‡é€™å…©å€‹åƒæ•¸æœ‰å¤šåƒ&lt;/li&gt;
&lt;li&gt;æ˜¯çµ¦åŒæ¨£çš„ state ç”Ÿå‡º action çš„ distribution è¦åƒ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;åˆå§‹åƒæ•¸ $\theta^0$&lt;/li&gt;
&lt;li&gt;æ¯å€‹ iteration
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ç”¨ $\theta^k$ å’Œç’°å¢ƒäº’å‹•ï¼Œè’é›†{$s_t,a_t$}ï¼Œä¸¦è¨ˆç®— advantage $A^{\theta^k}(s_t,a_t)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;æ‰¾å‡º theta æœ€ä½³åŒ– $J_{PPO}(\theta)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta KL(\theta, \theta^{k})$&lt;/li&gt;
&lt;li&gt;å¯ä»¥æ›´æ–°å¾ˆå¤šæ¬¡&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å‹•æ…‹èª¿æ•´ $\beta$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adaptive KL Penalty&lt;/li&gt;
&lt;li&gt;è¨­å¯æ¥å—çš„ KL æ•¸å€¼ç¯„åœ&lt;/li&gt;
&lt;li&gt;if $KL(\theta,\theta^k)&amp;gt;KL_{max},\text{increase} \beta$&lt;/li&gt;
&lt;li&gt;if $KL(\theta,\theta^k)&amp;lt;KL_{min},\text{decrease} \beta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ppo2&#34;&gt;PPO2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta KL(\theta, \theta^{k})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PPO2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$J_{PPO2}^{\theta^{k}}(\theta)\approx \displaystyle\sum_{(s_t,a_t)}min(\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}A^{\theta^k}(s_t,a_t), \\
clip(\frac{p_{\theta}(a_t|s_t)}{p_{\theta^k}(a_t|s_t)}, 1-\varepsilon, 1+\varepsilon)A^{\theta^k}(s_t,a_t))$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/ppo/ppo2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Policy Gradient</title>
        <link>https://roykesydon.github.io/Blog/p/policy-gradient/</link>
        <pubDate>Sun, 19 Feb 2023 17:16:14 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/policy-gradient/</guid>
        <description>&lt;h1 id=&#34;basic-components&#34;&gt;Basic Components&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Actor
&lt;ul&gt;
&lt;li&gt;Policy $\pi$ is a network with parameter $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Env&lt;/li&gt;
&lt;li&gt;Reward Function&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;trajectory&#34;&gt;Trajectory&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/drl/policy-gradient/aer.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åœ¨ä¸€å ´éŠæˆ²ï¼ŒæŠŠ env è¼¸å‡ºçš„ s å’Œ actor è¼¸å‡ºçš„ a ä¸²èµ·ä¾†ï¼Œæ˜¯ä¸€å€‹ Trajectory&lt;/li&gt;
&lt;li&gt;Trajectory $\tau$ = {$s_1,a_1,s_2,a_2,&amp;hellip;,s_T,a_T$}&lt;/li&gt;
&lt;li&gt;$p_{\theta}(\tau)=p(s_1)\displaystyle\prod_{t=1}^Tp_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;update&#34;&gt;Update&lt;/h1&gt;
&lt;p&gt;$\theta \leftarrow \theta + \eta \triangledown \overline{R}_{\theta}$&lt;/p&gt;
&lt;p&gt;$\triangledown \overline{R_{\theta}} = \displaystyle\sum_{\tau} R(\tau) \triangledown p_{\theta} (\tau) \\
=\frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}R(\tau^n)\triangledown log p_{\theta} (a_t^n|s_t^n)$&lt;/p&gt;
&lt;h1 id=&#34;å¯¦ä½œ&#34;&gt;å¯¦ä½œ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;å¸¸è¦‹å…¬å¼
&lt;ul&gt;
&lt;li&gt;$\triangledown f(x)=f(x)\triangledown logf(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;ç”¨ç•¶å‰æ¨¡å‹è’é›†ä¸€å † Trajectory&lt;/li&gt;
&lt;li&gt;æ›´æ–°æ¨¡å‹&lt;/li&gt;
&lt;li&gt;å›åˆ°ç¬¬ä¸€æ­¥&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;ç´°ç¯€
&lt;ul&gt;
&lt;li&gt;åšä¸€å€‹åˆ†é¡å•é¡Œï¼ŒæŠŠ state ç•¶ä½œåˆ†é¡å™¨çš„ Inputï¼ŒæŠŠ action ç•¶ä½œåˆ†é¡å™¨çš„ ground truth ä½œè¨“ç·´&lt;/li&gt;
&lt;li&gt;åœ¨å¯¦ä½œåˆ†é¡å•é¡Œçš„æ™‚å€™ï¼Œobjective function éƒ½æœƒå¯«æˆ minimize cross entropyï¼Œå°±æ˜¯ maximize log likelihood&lt;/li&gt;
&lt;li&gt;RL å’Œä¸€èˆ¬åˆ†é¡çš„å€åˆ¥æ˜¯ï¼Œè¦è¨˜å¾—åœ¨ loss å‰é¢ä¹˜ä¸Š $R(\tau^n)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;tip&#34;&gt;Tip&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Add a Baseline
&lt;ul&gt;
&lt;li&gt;$R(\tau^n)$ æœ‰å¯èƒ½æ°¸é éƒ½ç‚ºæ­£
&lt;ul&gt;
&lt;li&gt;æ­¤æ™‚ç­‰æ–¼å‘Šè¨´ Model èªªï¼Œä»Šå¤©ä¸ç®¡æ˜¯ä»€éº¼ actionï¼Œéƒ½è¦æé«˜å®ƒçš„æ©Ÿç‡ã€‚ä¸ä¸€å®šæœƒæœ‰å•é¡Œï¼Œå› ç‚ºé›–ç„¶éƒ½æ˜¯æ­£çš„ï¼Œä½†æ­£çš„é‡æœ‰å¤§æœ‰å°ï¼Œå¯èƒ½æŸäº› action ä¸Šå‡çš„å¹…åº¦æœƒæ›´å¤§ã€‚å› ç‚ºæˆ‘å€‘æ˜¯åœ¨åš samplingï¼Œä¸ä¸€å®šæœƒ sample åˆ°æŸäº› actionï¼Œæœ¬ä¾†æƒ³çš„æƒ…æ³æ˜¯æ‰€æœ‰çš„ trajectory éƒ½æœƒå‡ºç¾æ‰æ²’å•é¡Œã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;è§£æ³•: å¸Œæœ› reward ä¸è¦ç¸½æ˜¯æ­£çš„
&lt;ul&gt;
&lt;li&gt;$\triangledown \overline{R_{\theta}}\approx \frac{1}{N}\displaystyle\sum_{n=1}^{N}\displaystyle\sum_{t=1}^{T_n}(R(\tau^n)-b)\triangledown log p_{\theta}(a_t^n|s_t^n)$&lt;/li&gt;
&lt;li&gt;$b \approx E[R(\tau)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Assign Suitable Credit
&lt;ul&gt;
&lt;li&gt;åŸæœ¬æ•´å ´éŠæˆ²çš„æ‰€æœ‰ action éƒ½æœƒä¹˜ä¸Š $R(\tau)$ï¼Œä½†é€™ä¸å¤ªå…¬å¹³ï¼Œå› ç‚ºå°±ç®—çµæœæ˜¯å¥½çš„ï¼Œä¸ä»£è¡¨æ‰€æœ‰ action éƒ½æ˜¯å°çš„ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨ç†æƒ³çš„æƒ…æ³ä¸‹ï¼Œå¦‚æœ sample å¤ å¤šï¼Œå°±å¯ä»¥è§£æ±ºé€™å•é¡Œã€‚&lt;/li&gt;
&lt;li&gt;è§£æ³•
&lt;ol&gt;
&lt;li&gt;åªè¨ˆç®—å¾é€™å€‹ action å¾Œçš„ reward ç¸½å’Œ
&lt;ul&gt;
&lt;li&gt;å› ç‚ºå‰é¢çš„ reward å’Œä½ åšäº†ä»€éº¼æ²’é—œä¿‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;æ¥çºŒè§£æ³• 1ï¼ŒæŠŠæ¯”è¼ƒæœªä¾†çš„ reward åš discount
&lt;ul&gt;
&lt;li&gt;ä¹˜æŸå€‹å°æ–¼ 1 çš„ $\gamma^{t^{&amp;rsquo;}-t}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;advantage-function&#34;&gt;Advantage function&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;base å¯ä»¥æ˜¯ state-dependentï¼Œå¯ä»¥æ ¹æ“š network å¾—å‡ºï¼Œä»¥å¾Œå†èªª&lt;/li&gt;
&lt;li&gt;$(Reward-b)$ å¯ä»¥åˆèµ·ä¾†çœ‹åš Advantage function $A^{\theta}(s_t,a_t)$
&lt;ul&gt;
&lt;li&gt;é€™é‚Š Reward ä¸ç®¡ä½ æ˜¯ä»€éº¼å½¢å¼ï¼Œæœ‰æ²’æœ‰ discountã€‚&lt;/li&gt;
&lt;li&gt;å®ƒçš„æ„ç¾©æ˜¯ï¼Œé€™å€‹ action ç›¸è¼ƒæ–¼å…¶ä»–çš„ action æœ‰å¤šå¥½ï¼Œè€Œä¸æ˜¯çµ•å°å¥½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;é€™å€‹ A é€šå¸¸å¯ä»¥ç”±æŸå€‹é¡ç¥ç¶“ç¶²è·¯ä¼°è¨ˆï¼Œé‚£å€‹é¡ç¥ç¶“ç¶²è·¯å«åš criticï¼Œä»¥å¾Œè¬› Actor-Critic çš„æ™‚å€™å†èªª&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>MAE è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</link>
        <pubDate>Wed, 15 Feb 2023 16:08:46 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/mae-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2111.06377&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;é€™ç¯‡è«–æ–‡é¡¯ç¤ºå‡º MAE æ˜¯ CV ä¸­çš„ scalable self-supervised learnersã€‚&lt;/p&gt;
&lt;p&gt;MAE çš„æ–¹æ³•å¾ˆç°¡å–®&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;éš¨æ©Ÿè“‹ä½è¼¸å…¥å½±åƒçš„ä¸€äº› patch&lt;/li&gt;
&lt;li&gt;é‡å»º missing pixels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;å…·å‚™å…©å€‹æ ¸å¿ƒè¨­è¨ˆ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;éå°ç¨±çš„ encoder-decoder æ¶æ§‹ï¼Œencoder åªä½œç”¨æ–¼å¯è¦‹çš„ patch å­é›†åˆ(æ²’æœ‰ mask tokens)ï¼Œlightweight decoder å‰‡æ ¹æ“š latent representation å’Œ make tokens ä¾†é‡å»ºåœ–ç‰‡ã€‚&lt;/li&gt;
&lt;li&gt;ç•¶é®ä½é«˜æ¯”ä¾‹(æ¯”å¦‚ 75%)çš„å½±åƒæ™‚ï¼Œæœƒå¾—åˆ°ä¸€å€‹ nontrivial å’Œ meaningful çš„ self-supervisory task&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;çµåˆé€™å…©é»è¨­è¨ˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¨“ç·´å¤§æ¨¡å‹ã€‚
ä»¥ ViT-Huge ç”¨ ImageNet-1K è¨“ç·´(è¨“ç·´é›†ä¸€ç™¾å¤šè¬å¼µç…§ç‰‡)å¯é”åˆ° 87.8% çš„æº–ç¢ºåº¦ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/intro.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/valid-example-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;åœ¨ CV ä¸­ï¼Œå¸¸éœ€è¦å¤§é‡ labeled imagesã€‚
NLP ä¸­ï¼Œè‡ªç›£ç£é è¨“ç·´è™•ç†äº†éœ€è¦å¤§é‡æ¨™è¨»è³‡æ–™çš„å•é¡Œã€‚
masked autoencoders æ˜¯ä¸€ç¨®æ›´ general çš„ denoising autoencoders çš„å½¢å¼ã€‚
BERT éå¸¸æˆåŠŸï¼Œautoencoding methods åœ¨ CV çš„ç ”ç©¶å»è½å¾Œ NLPï¼Œä½œè€…æ€è€ƒæ˜¯ä»€éº¼è®“ masked autoencoding åœ¨ CV å’Œ NLP ç”¢ç”Ÿä¸åŒã€‚
æœ‰ä»¥ä¸‹è§€é»&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç›´åˆ°å‰é™£å­ï¼ŒCV ä¸­çš„ CNN æ˜¯ä¸»æµï¼Œä½†å·ç©å±¤ä¸å¥½å¼•å…¥ mask tokens æˆ– positional embedding é€™äº› indicatorã€‚ä½†é€™äº›å¯ä»¥é€é ViT ä¾†è§£æ±ºï¼Œä¸æ‡‰æˆç‚ºå•é¡Œã€‚&lt;/li&gt;
&lt;li&gt;èªè¨€å’Œè¦–è¦ºçš„ Information density ä¸åŒï¼Œèªè¨€æ˜¯ highly semantic å’Œ information-denseï¼Œä½¿å¡«å­—æœ¬èº«ä¸æ˜¯å¾ˆç°¡å–®çš„äº‹æƒ…ï¼Œä½†å½±åƒå«æœ‰å¤§é‡å†—é¤˜çš„è¨Šæ¯ï¼Œç¼ºå¤±çš„éƒ¨åˆ†æ¯”è¼ƒå¥½å¾ç›¸é„°çš„ patch é‡å»ºï¼Œæ¯”å¦‚ç›´æ¥æ’å€¼ï¼Œæ‰€ä»¥ä½œè€…ç”¨ä¸€ç¨®ç°¡å–®çš„ç­–ç•¥ï¼Œéš¨æ©Ÿ mask å¾ˆå¤§ä¸€éƒ¨åˆ†çš„ patchï¼Œå‰µé€ ä¸€å€‹å…·æœ‰æŒ‘æˆ°æ€§çš„è‡ªç›£ç£ä»»å‹™ï¼Œå¼·è¿«æ¨¡å‹é—œæ³¨ global çš„è³‡è¨Šã€‚&lt;/li&gt;
&lt;li&gt;é—œæ–¼ decoderï¼ŒCV é‚„åŸ pixelï¼Œpixel å±¬æ–¼ lower semantic levelï¼ŒNLP é‚„åŸ wordï¼Œword çš„ semantic information è¼ƒé«˜ã€‚ä½œè€…ç™¼ç¾ï¼Œé›–ç„¶åœ¨ BERT ä¸­ï¼Œå¯ä»¥ç”¨ç°¡å–®çš„ decoder é‚„åŸ(ä¸€å€‹ MLP)ï¼Œä½† CV ä¸­ decoder çš„è¨­è¨ˆå°±å¾ˆé‡è¦ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åŸºæ–¼ä»¥ä¸Šè§€é»ï¼Œä½œè€…æå‡º MAEï¼Œéš¨æ©Ÿé®ä½å¤§é‡çš„ patchï¼Œä¸¦åœ¨ pixel space é‡å»ºå¤±å»çš„ patchã€‚è€Œä¸”æ˜¯éå°ç¨± encoder-decoder æ¶æ§‹ï¼Œencoder åªæœƒçœ‹åˆ°å¯è¦‹çš„ patchï¼Œä½† docoder é™¤äº† latent representationï¼Œé‚„æœƒçœ‹åˆ° mask tokensã€‚é€™ç¨®è¨­è¨ˆåœ¨éå¸¸é«˜çš„æ©è“‹ç‡(æ¯”å¦‚ 75%)ä¸‹ä¸ä½†å¯ä»¥æé«˜æº–ç¢ºåº¦ï¼Œé‚„å¯ä»¥è®“ encoder åªè™•ç†è¼ƒå°‘æ¯”ä¾‹(æ¯”å¦‚ 25%)çš„ patchï¼Œå°‡è¨“ç·´æ™‚é–“æ¸›å°‘ 3 å€æˆ–æ›´å¤šï¼Œä½¿ MAE å¯ä»¥è¼•é¬†æ“´å±•æˆæ›´å¤§çš„æ¨¡å‹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨é€™æ¨£çš„æ¶æ§‹ä¸‹ï¼Œç”¨ MAE çš„ pre-trainingï¼Œå¯ä»¥è¨“ç·´éå¸¸åƒ data çš„æ¨¡å‹ï¼Œæ¯”å¦‚ ViT-Large/-Hugeï¼Œè€Œåªä½¿ç”¨ ImageNet-1Kã€‚&lt;/p&gt;
&lt;p&gt;ç”¨ ImageNet-1K åœ¨ vanilla ViT-Huge ä¸Š fine-tune å¯é”åˆ° 87.8% æº–ç¢ºåº¦ï¼Œæ¯”ä»¥å¾€åªä½¿ç”¨ ImageNet-1K çš„çµæœéƒ½é«˜ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ obejct detectionã€instance segmentationã€semantic segmentation ä¸Šåš transfer learning éƒ½é”åˆ°ä¸éŒ¯çš„æ•ˆæœï¼Œå¯ä»¥æ‰“æ•—ç”¨ç›£ç£å¼é è¨“ç·´æ¨¡å‹çš„å°æ‰‹ã€‚&lt;/p&gt;
&lt;h1 id=&#34;ç›¸é—œå·¥ä½œ&#34;&gt;ç›¸é—œå·¥ä½œ&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Autoencoding
&lt;ul&gt;
&lt;li&gt;MAE æ˜¯ä¸€ç¨® denoising autoencoding çš„å½¢å¼ï¼Œä½†å’Œ DAE é‚„æ˜¯å·®åˆ¥å¾ˆå¤§ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Masked image encoding
&lt;ul&gt;
&lt;li&gt;iGPTã€ViTã€BEiT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Masking&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å’Œ ViT ä¸€æ¨£ï¼ŒæŠŠåœ–ç‰‡åˆ‡æˆå¤šå€‹ patchï¼Œå°æ–¼ patch å‡å‹»éš¨æ©Ÿåœ°æ¡æ¨£ä¿ç•™ï¼Œå‰©ä¸‹åœ°é®ä½&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE encoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViT&lt;/li&gt;
&lt;li&gt;ä¹Ÿæœ‰ positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MAE decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer block&lt;/li&gt;
&lt;li&gt;è¼¸å…¥
&lt;ul&gt;
&lt;li&gt;encoded visible patches&lt;/li&gt;
&lt;li&gt;mask tokens
&lt;ul&gt;
&lt;li&gt;shared, learned vector&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;éƒ½æœƒåŠ å…¥ positional embedding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ç”¨ç›¸è¼ƒ encoder è¼•é‡çš„è§£ç¢¼å™¨ï¼Œæ‰€æœ‰çš„ patch ç”±é€™å€‹è¼•é‡çš„ decoder è™•ç†ï¼Œæ¸›å°‘é è¨“ç·´æ™‚é–“&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reconstruction target&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;decoder çš„æœ€å¾Œä¸€å±¤æ˜¯ linear projectionï¼Œä¹‹å¾Œå† reshape æˆä½ è¦çš„  patch&lt;/li&gt;
&lt;li&gt;loss function
&lt;ul&gt;
&lt;li&gt;mean squared error(MSE)&lt;/li&gt;
&lt;li&gt;åªç®— masked patched çš„ MSEï¼Œåƒ BERT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple implementation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;å…ˆå–å¾—ä¸€ç³»åˆ— token(patch åš linear projection + positional embedding)&lt;/li&gt;
&lt;li&gt;randomly shuffleï¼Œæ ¹æ“šæ¯”ä¾‹ç§»é™¤å°¾ç«¯ä¸€éƒ¨ä»½&lt;/li&gt;
&lt;li&gt;encoding å¾Œï¼Œå°¾ç«¯æ¥ä¸Š mask tokensï¼Œä¸¦ä¸” unshuffle&lt;/li&gt;
&lt;li&gt;åŠ ä¸Š positional embedding å¾Œï¼Œçµ¦ decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;imagenet-experiments&#34;&gt;ImageNet Experiments&lt;/h1&gt;
&lt;p&gt;åœ¨ ImageNet-1K ä¸Šåšè‡ªç›£ç£çš„é è¨“ç·´ï¼Œç„¶å¾Œåš&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;end-to-end fine-tuning
&lt;ul&gt;
&lt;li&gt;æ‰€æœ‰åƒæ•¸éƒ½å¯æ”¹&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;linear probing
&lt;ul&gt;
&lt;li&gt;åªæ”¹æœ€å¾Œä¸€å±¤ç·šæ€§å±¤&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/vit-mae.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/ratio-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;optimal masking ratio æ„å¤–åœ°é«˜ï¼Œç›¸æ¯” BERT åªæœ‰ 15%&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/MAE/fine-tune-blocks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;è¨è«–å’Œçµè«–&#34;&gt;è¨è«–å’Œçµè«–&lt;/h1&gt;
&lt;p&gt;åœ¨ CV å¯¦ç”¨çš„é è¨“ç·´åšæ³•ä¸»æµæ˜¯ç›£ç£å¼çš„ï¼ŒCV ä¸­è‡ªç›£ç£çš„åšæ³•å¯èƒ½æ­£è·Ÿè‘— NLP çš„è»Œè·¡èµ°ã€‚&lt;/p&gt;
&lt;p&gt;è¦ä»”ç´°è™•ç†åœ–åƒå’Œèªè¨€çš„å€åˆ¥ï¼Œä½œè€…å»é™¤åœ–ç‰‡ä¸­å¾ˆå¯èƒ½ä¸æ§‹æˆ semantic segment çš„éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ç§»é™¤æŸå€‹ objectã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ViT è«–æ–‡</title>
        <link>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</link>
        <pubDate>Sun, 12 Feb 2023 00:27:55 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/vit-%E8%AB%96%E6%96%87/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2010.11929&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;åœ¨ CV é ˜åŸŸ transformer è¡¨ç¾æœ‰é™ï¼Œç›®å‰ attention å¸¸å¸¸æ˜¯å’Œå·ç©ç¥ç¶“ç¶²è·¯ä¸€èµ·ç”¨ï¼Œæˆ–æ˜¯ç”¨ä¾†æŠŠä¸€äº›å·ç©å±¤æ›æˆ self-attentionï¼Œä½†æ•´é«”æ¶æ§‹ä¸è®Šã€‚é€™ç¯‡è«–æ–‡æƒ³å±•ç¾ä¸€å€‹ç´” Transformer å¯ä»¥ç›´æ¥åœ¨å½±åƒåˆ†é¡ä¸Šè¡¨ç¾å¾ˆå¥½ã€‚å¦‚æœç”¨å¤§é‡è³‡æ–™ä½œé è¨“ç·´ï¼Œå†é·ç§»åˆ°ä¸­å°å‹çš„è³‡æ–™é›†ï¼Œå¯ä»¥å’Œ SOTA çš„ CNN è¡¨ç¾å¾—ä¸€æ¨£å¥½ï¼Œé‚„éœ€è¦è¼ƒå°‘çš„è¨“ç·´è³‡æºä½œè¨“ç·´ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;self-attention-based æ¶æ§‹ï¼Œç‰¹åˆ¥æ˜¯ Transformerï¼Œå·²ç¶“æ˜¯ NLP çš„é‡è¦é¸æ“‡ã€‚ä¸»æµçš„ä½œæ³•æ˜¯åœ¨å¤§å‹æ–‡å­—è³‡æ–™é›†ä¸Šä½œè¨“ç·´ï¼Œå†é‡å°å°å‹ä»»å‹™è³‡æ–™é›†ä½œ fine-tuneã€‚ç”±æ–¼ Transformer çš„è¨ˆç®—æ•ˆç‡é«˜ï¼Œé‚„æœ‰å¯æ“´å±•æ€§ï¼Œå¯ä»¥ train ä¸€äº›å¾ˆå¤§çš„ modelï¼Œéš¨è‘— model å’Œè³‡æ–™é›†å¢å¤§ï¼Œç›®å‰é‚„æ²’çœ‹å‡ºé£½å’Œçš„ç¾è±¡ã€‚&lt;/p&gt;
&lt;p&gt;ç„¶è€Œåœ¨ CVï¼ŒCNN é‚„æ˜¯ä¸»æµï¼Œä¸€äº›å·¥ä½œå˜—è©¦ç”¨ self-attention çµåˆ CNN-like çš„æ¶æ§‹ï¼Œæ¯”å¦‚æŠŠ feature map ç•¶ transformer çš„è¼¸å…¥ï¼Œå› ç‚ºåŸå§‹ pixel å¤ªå¤šï¼Œæˆ–ç”šè‡³æŠŠå·ç©å±¤å…¨æ›æˆ self-attentionï¼Œé›–ç„¶å¾Œè€…ç†è«–ä¸Šæ•ˆç‡å¾ˆé«˜(åŸè«–æ–‡ä¸­æœ‰å¦å¤– cite å…©ç¯‡ä½œæ³•)ï¼Œä½†å› ç‚ºä»–å€‘åšæ³•ç‰¹æ®Šï¼Œåœ¨ç¾ä»£ç¡¬é«”ä¸Šå¾ˆé›£åŠ é€Ÿï¼Œæ‰€ä»¥ç„¡æ³•å¾ˆæœ‰æ•ˆåœ°æ“´å±•ã€‚åœ¨ large-scale çš„å½±åƒè­˜åˆ¥ä¸Šï¼Œ ResNet-like çš„æ¶æ§‹é‚„æ˜¯ SOTAã€‚&lt;/p&gt;
&lt;p&gt;è©²å¯¦é©—ç›´æ¥æŠŠä¸€å€‹æ¨™æº–çš„ Transformer ä½œç”¨æ–¼åœ–ç‰‡ä¸Šï¼Œåªä½œæœ€å°‘çš„ä¿®æ”¹ã€‚æŠŠå½±åƒåˆ†æˆå¤šå€‹ patchï¼Œä¸¦æŠŠå®ƒå€‘è®Šæˆä¸€ç³»åˆ—çš„ linear embeddingï¼Œç•¶ä½œ NLP ä¸­çš„ tokens(words) ä¾†è™•ç†ã€‚&lt;/p&gt;
&lt;p&gt;ç•¶åœ¨ä¸­å‹å¤§å°çš„è³‡æ–™é›†(e.g. ImageNet)ä¸Šè¨“ç·´ï¼Œå¦‚æœæ²’æœ‰ strong regularizationï¼ŒViT æœƒç•¥è¼¸åŒç­‰å¤§å°çš„ ResNets&lt;/p&gt;
&lt;p&gt;é€™ç¯‡è«–æ–‡åœ¨æ›´å¤§çš„è³‡æ–™é›†(14M-300M çš„å½±åƒ)ä¸Šè¨“ç·´ï¼Œå°±æ‰“æ•—äº† inductive biasã€‚åœ¨å¤§é‡è³‡æ–™ä¸Šä½œé è¨“ç·´å°±å¾ˆè®šã€‚&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;å¤§å‹çš„ Transformer-based æ¨¡å‹å¸¸å¸¸æ˜¯å…ˆåœ¨å¤§è³‡æ–™é›†ä¸Šé è¨“ç·´ç„¶å¾Œæ ¹æ“šä»»å‹™ fine-tuneï¼Œæ¯”å¦‚ BERT å’Œ GPTã€‚&lt;/p&gt;
&lt;p&gt;è¦æŠŠ self-attention ç”¨åœ¨ CV ä¸Šï¼Œæœ€ç°¡å–®çš„åšæ³•å°±æ˜¯æŠŠæ¯å€‹ Pixel ç•¶ä¸€å€‹å…ƒç´ ï¼Œä½† self-attention æ˜¯å¹³æ–¹è¤‡é›œåº¦ï¼Œåœ¨ç¾å¯¦çš„åœ–ç‰‡å¾ˆé›£æ‡‰ç”¨ã€‚ä¸€å€‹æ‡‰ç”¨ Transformer çš„åšæ³•æ˜¯åªæŠŠ self-attention ç”¨åœ¨ local neighborhoodï¼Œå¦å¤–ä¸€å€‹æ˜¯ç”¨ Sparse Transformerï¼Œé‚„æœ‰ä¸€å †ç‰¹æ®Šçš„æ–¹æ³•ï¼Œé›–ç„¶è¡¨ç¾ä¸éŒ¯ï¼Œä½†è¦ç”¨ç¡¬é«”åŠ é€Ÿèµ·ä¾†ä¸å®¹æ˜“ã€‚&lt;/p&gt;
&lt;p&gt;å¦ä¸€å€‹æœ‰é—œçš„æ¨¡å‹æ˜¯ iGPTï¼Œåœ¨ reduce image resolution å’Œ color space å¾ŒæŠŠ transformer æ‡‰ç”¨åœ¨ image pixels ä¸Šã€‚å®ƒç”¨éç›£ç£å¼è¨“ç·´å¾Œï¼Œå† fine-tune æˆ–åš linear probing(åªæ›´æ–°æœ€å¾Œçš„ linear layer) åˆ†é¡ä»»å‹™ï¼Œè¡¨ç¾å¾ˆå¥½ã€‚&lt;/p&gt;
&lt;p&gt;å·²ç¶“æœ‰é¡ä¼¼çš„å·¥ä½œäº†ï¼ŒæŠ½å– patches of size 2 * 2ï¼Œæœ€å¾Œå†æ¥ full self-attentionï¼ŒåŸºæœ¬ä¸Šå’Œ ViT éå¸¸åƒï¼Œé€™ç¯‡è«–æ–‡é€²ä¸€æ­¥è­‰æ˜äº†ä½œå¤§è¦æ¨¡çš„é è¨“ç·´å¯ä»¥è®“ Transformer å’Œ SOTA çš„ CNN ç›¸æ¯”ï¼Œè€Œä¸” ViT å› ç‚º patch æ¯”è¼ƒå¤§ï¼Œå¯ä»¥è™•ç† medium-resolution çš„åœ–ç‰‡ã€‚é€™å•é¡Œæ˜¯å¯é æœŸçš„ï¼Œå› ç‚º Transformer ç¼ºå°‘äº†ä¸€äº› inductive biasesã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inductive biases
&lt;ul&gt;
&lt;li&gt;ä¸€äº›å‡è¨­&lt;/li&gt;
&lt;li&gt;æ¯”å¦‚ CNN å¸¸æœ‰å››å€‹å‡è¨­
&lt;ul&gt;
&lt;li&gt;locality&lt;/li&gt;
&lt;li&gt;translation invariance with pooling layers
&lt;ul&gt;
&lt;li&gt;å¹³ç§»ä¸è®Šæ€§&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;translation equivariance
&lt;ul&gt;
&lt;li&gt;f(g(x)) = g(f(x))&lt;/li&gt;
&lt;li&gt;å·ç©å’Œå¹³ç§»çš„å…ˆå¾Œé †åºæ²’å·®&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;æ¨¡å‹ç›¡å¯èƒ½é¡ä¼¼åŸå§‹ Transformerï¼Œé€™æ¨£å¯ä»¥æŠŠä¸€äº› NLP ä¸ŠæˆåŠŸçš„ Transformer æ¶æ§‹æ‹¿ä¾†ç”¨ï¼Œé‚„å¯ä»¥ç”¨ä¸€äº›å¾ˆæœ‰æ•ˆç‡çš„ implementation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-process.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;embedding ç¶­åº¦æ˜¯ 768 = 16 * 16 * 3
position embedding çš„åšæ³•æ˜¯ standard learnable 1D positional embeddingsï¼Œå°±æ˜¯ BERT çš„åšæ³•ï¼Œç°¡å–®ä¾†èªªå°±æ˜¯ç”Ÿå‡ºä¸€å¼µå¯ä»¥è¨“ç·´çš„è¡¨ï¼Œ(åºåˆ—é•·åº¦, embedding size)ï¼Œä½œè€…ä¹Ÿæœ‰å˜—è©¦å…¶ä»–æ–¹æ³•ï¼Œä½†ç™¼ç¾æˆæ•ˆå·®ä¸å¤šï¼Œæ¯”å¦‚ 2D positional embeddingï¼Œæ¦‚å¿µå°±æ˜¯å¾ç”Ÿå‡º(åºåˆ—é•·åº¦, embedding size)è®Šæˆç”Ÿå‡º 2 å€‹(sqrt(åºåˆ—é•·åº¦), embedding size)ã€‚&lt;/p&gt;
&lt;p&gt;[class] çš„æ¦‚å¿µæ˜¯ NLP å‡ºä¾†çš„ï¼ŒResNet-like çš„æ¶æ§‹å¸¸è¦‹çš„åšæ³•ä¹Ÿæœ‰é€šé globally average-pooling (GAP)ä¾†ç”Ÿå‡ºå‘é‡ï¼Œå†æ¥ä¸Šåˆ†é¡å™¨åšé æ¸¬ã€‚å¯¦é©—ç™¼ç¾ç›´æ¥åœ¨ transformer çš„è¼¸å‡ºåš GAP å’Œ [class] éƒ½å¯ä»¥é”åˆ°ä¸éŒ¯çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-gap.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/deep-learning/ViT/ViT-acc.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;æ‹¿æ¨™æº–çš„ Transformer ä¾†ä½œ Image recognitionï¼Œå’Œä»¥å¾€ç”¨ self-attention åœ¨ CV çš„æ–¹æ³•ä¸ä¸€æ¨£ï¼Œé™¤äº†ä¸€é–‹å§‹çš„ initial patch extractionï¼Œæ²’æœ‰å¼•å…¥å…¶ä»–å½±åƒç‰¹æœ‰çš„ inductive biasesã€‚ç›´æ¥æŠŠåœ–ç‰‡ç•¶æˆæ˜¯ä¸€ç³»åˆ—çš„ patchï¼Œç„¶å¾Œç›´æ¥ç”¨ Transformer encoder ç•¶ä¸€èˆ¬ NLP ä»»å‹™è™•ç†ã€‚åœ¨å¾ˆå¤šå½±åƒåˆ†é¡è¨“ç·´é›†ä¸Šè¡¨ç¾å¾—æ›´å¥½é‚„åœ¨ pre-train ä¸Šç›¸å°ä¾¿å®œã€‚&lt;/p&gt;
&lt;p&gt;é‚„æœ‰ä¸€äº›å€¼å¾—æŒ‘æˆ°çš„åœ°æ–¹ï¼Œæ¯”å¦‚æŠŠ ViT æ‡‰ç”¨åœ¨å…¶ä»– CV ä»»å‹™ï¼Œæ¯”å¦‚ detection å’Œ segmentationã€‚å¦ä¸€å€‹æŒ‘æˆ°æ˜¯æ¢ç´¢è‡ªç›£ç£é è¨“ç·´çš„æ–¹æ³•ã€‚é€™ç¯‡è«–æ–‡å…¶å¯¦æœ‰å¯¦é©—è‡ªç›£ç£ï¼Œè¡¨ç¾ OKï¼Œä½†å’Œç›£ç£å¼é‚„æ˜¯æœ‰å¾ˆå¤§çš„è½å·®ã€‚æ“´å¤§ ViT å¯èƒ½æœ‰æ›´å¥½çš„çµæœã€‚&lt;/p&gt;
</description>
        </item>
        <item>
        <title>InstructGPT</title>
        <link>https://roykesydon.github.io/Blog/p/instructgpt/</link>
        <pubDate>Fri, 27 Jan 2023 17:39:12 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/instructgpt/</guid>
        <description>&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2203.02155&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;æŠŠèªè¨€æ¨¡å‹è®Šå¤§ä¸ä»£è¡¨ä»–å€‘æœƒæ›´å¥½åœ°éµå¾ªç”¨æˆ¶çš„æ„åœ–ã€‚&lt;/p&gt;
&lt;p&gt;å¤§çš„èªè¨€æ¨¡å‹æœ‰å¯èƒ½æœƒç”Ÿæˆ untruthful, toxic, not helpful çš„ç­”æ¡ˆã€‚&lt;/p&gt;
&lt;p&gt;è©²è«–æ–‡é€é fine-tuning with human feedback ä¾†è§£æ±ºé€™å•é¡Œã€‚&lt;/p&gt;
&lt;p&gt;ä¸€é–‹å§‹æº–å‚™ä¸€ç³»åˆ—äººå·¥æ¨™è¨»çš„ promptsï¼Œç„¶å¾Œç”¨é€™ dataset å° GPT-3 åš fine-tuneã€‚&lt;/p&gt;
&lt;p&gt;æ¥ä¸‹ä¾†å†è’é›†ä¸€å€‹ datasetï¼Œå­˜æ”¾ rankings of model outputsï¼Œç”±äººå·¥åˆ¤æ–·è¼¸å‡ºå¥½å£ï¼Œå†ç”¨ RL æŠŠå‰›å‰› fine-tune éçš„ model ç¹¼çºŒ fine-tuneã€‚&lt;/p&gt;
&lt;p&gt;æœ€å¾Œæœ‰ 1.3B åƒæ•¸çš„ InstructGPT è¡¨ç¾çš„çµæœæ¯” 175B åƒæ•¸çš„ GPT-3 é‚„å¥½ã€‚&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Large language models(LMs) å¯ä»¥é€é &amp;ldquo;prompt&amp;rdquo; ä¾†åŸ·è¡Œå„ç¨® NLP ä»»å‹™ã€‚&lt;/p&gt;
&lt;p&gt;ä½†é€™äº›æ¨¡å‹ä¹Ÿå¸¸æœ‰ä¸€äº›éç›®çš„æ€§çš„è¡Œç‚ºï¼Œè«¸å¦‚æé€ äº‹å¯¦ç­‰ç­‰ã€‚&lt;/p&gt;
&lt;p&gt;åŸå› æ˜¯å‡ºåœ¨ç›®æ¨™å‡½æ•¸ä¸Šï¼Œå¤šæ•¸ LMs çš„ç›®æ¨™å‡½æ•¸æ˜¯æ ¹æ“šç¶²è·¯ä¸Šçš„æ–‡æœ¬ç”Ÿå‡ºä¸‹ä¸€å€‹å­—è©ã€‚&lt;/p&gt;
&lt;p&gt;é€™å’Œã€Œæ ¹æ“šä½¿ç”¨è€…æŒ‡ä»¤ç”Ÿå‡ºå®‰å…¨ä¸”æœ‰å¹«åŠ©çš„ç­”æ¡ˆä¸åŒã€ã€‚&lt;/p&gt;
&lt;p&gt;ä¸Šè¿°çš„å·®ç•°ä½¿èªè¨€æ¨¡å‹çš„ç›®æ¨™æ˜¯ misalignedã€‚&lt;/p&gt;
&lt;p&gt;ä½œè€…çš„ç›®æ¨™æ˜¯ç”Ÿå‡º helpfulã€ honest(æ²’æœ‰èª¤å°æ€§è³‡è¨Š)ã€harmless çš„ modelã€‚&lt;/p&gt;
&lt;p&gt;å…·é«”ä½œæ³•ï¼Œä½¿ç”¨ reinforcement learning from human feedback(RLHF)ã€‚&lt;/p&gt;
&lt;h2 id=&#34;è¨“ç·´æ­¥é©Ÿ&#34;&gt;è¨“ç·´æ­¥é©Ÿ&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-train-step.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;çµæœ&#34;&gt;çµæœ&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Labelers æ˜é¡¯åå¥½ InstructGPT çš„ç­”æ¡ˆï¼Œå‹é GPT-3 çš„ç­”æ¡ˆ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT çš„ç­”æ¡ˆåœ¨ truthfulness å‹é GPT-3 çš„ç­”æ¡ˆ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;InstructGPT çš„ç­”æ¡ˆåœ¨ toxicity ä¸Šå°å‹ GPT-3 çš„ç­”æ¡ˆï¼Œä½†åœ¨ bias ä¸Šæ²’æœ‰&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;methods&#34;&gt;Methods&lt;/h1&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;æ¨™è¨»äººå“¡å¯«å¾ˆå¤š prompts&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plain:
&lt;ul&gt;
&lt;li&gt;éš¨ä¾¿å¯«ä»»æ„ä»»å‹™&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Few-shot:
&lt;ul&gt;
&lt;li&gt;æƒ³å€‹ instructionï¼Œä¸¦å¯« multiple query/response pairs for that instruction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;User-based:
&lt;ul&gt;
&lt;li&gt;æ ¹æ“šä¸€äº›ç”³è«‹ä½¿ç”¨ OpenAI API çš„ç”¨æˆ¶ï¼Œæå‡ºæœ‰é—œçš„ prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ç„¶å¾Œæ ¹æ“šé€™å€‹è¨“ç·´åˆæ­¥æ¨¡å‹ï¼Œä¸¦æŠŠé€™å€‹åˆæ­¥æ¨¡å‹æ”¾åˆ°ä»–å€‘çš„ Playground çµ¦ç”¨æˆ¶ä½¿ç”¨ã€‚&lt;/p&gt;
&lt;p&gt;å†æŠŠç”¨æˆ¶å•çš„å•é¡Œè’é›†å›ä¾†ï¼Œä¸¦åšç¯©é¸ã€‚&lt;/p&gt;
&lt;p&gt;è¨“ç·´ SFT çš„æ¨¡å‹ç”¨ 13k training prompts&lt;/p&gt;
&lt;p&gt;è¨“ç·´ RM çš„æ¨¡å‹ç”¨ 33k training prompts&lt;/p&gt;
&lt;p&gt;è¨“ç·´ PPO çš„æ¨¡å‹ç”¨ 31k training prompts&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Supervised fine-tuning(SFT)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;æ‹¿ GPT-3 å»è¨“ç·´ 16 å€‹ epochs&lt;/li&gt;
&lt;li&gt;è·‘ä¸€å€‹ epoch å°±ç™¼ç¾ overfittingï¼Œä½†ç™¼ç¾è¨“ç·´æ›´å¤š epoches å°å¾Œé¢çš„ RM æœ‰ç”¨ï¼Œè€Œä¸”é€™å€‹ model ä¹Ÿåªæ˜¯éæ¸¡ç”¢å“&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reward modeling(RM)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;æŠŠ SFT å¾Œé¢çš„ unembedding layer å»é™¤æ‰ï¼Œæ¥ä¸Šç·šæ€§å±¤ï¼Œæœ€å¾Œè¼¸å‡ºä¸€å€‹ scalar reward&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ç”¨ 6B RMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;é€™æ¨¡å‹æœƒåƒ prompt å’Œ response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;äººå·¥æ¨™è¨˜çš„æ˜¯æ’åºï¼Œä¸æ˜¯åˆ†æ•¸&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å°æ¯å€‹ prompt ç”Ÿå‡º 9 å€‹ç­”æ¡ˆ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;åŸæœ¬æ˜¯ 4 å€‹ï¼Œä½†æ’ 9 å€‹èŠ±çš„æ™‚é–“å¯èƒ½ä¸æœƒåˆ° 4 å€‹çš„å…©å€ï¼Œå› ç‚ºä¸»è¦å¿ƒåŠ›æœƒèŠ±åœ¨è®€ promptã€‚ä½†æ¨™è¨»è¨Šæ¯æœƒå¤šå¾ˆå¤šï¼Œå› ç‚ºéƒ½æ˜¯å…©å…©æ¯”è¼ƒã€‚&lt;/li&gt;
&lt;li&gt;è€Œä¸”åœ¨ loss ä¸­æœ€å¤šåªè¦ä¸Ÿå…¥ RM 9 æ¬¡ï¼Œå› ç‚ºå¯ä»¥é‡ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pairwise Ranking Loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å°ä¸€å€‹ prompt(å‡è¨­æ˜¯ x)ï¼Œå–å‡ºä¸€å°å›è¦†(å‡è¨­æ˜¯ $y_w$ å’Œ $y_l$)ï¼Œç®—å‡º RM(x, $y_w$) å’Œ RM(x, $y_l$)ï¼Œå‡è¨­ $y_w$ æ¯” $y_l$ æ’åºé«˜ï¼Œè®“ RM(x, $y_w$) - RM(x, $y_l$) çš„æ•¸å€¼è¶Šå¤§è¶Šå¥½&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-reward-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement learning(RL)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PPO&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-rl-loss.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\beta$ é‚£é …æ˜¯ KL divergence&lt;/li&gt;
&lt;li&gt;$\gamma$ é‚£é …æ˜¯ä¸æƒ³è¦è®“é€™ model å¤ªå°ˆæ³¨åœ¨å¾®èª¿çš„ä»»å‹™ï¼Œè€Œå¤±å»åŸæœ¬åœ¨å…¶ä»– NLP ä»»å‹™ä¹Ÿè¡¨ç¾å¾ˆå¥½çš„åŠŸèƒ½ã€‚
&lt;ul&gt;
&lt;li&gt;$D_{pretrain}$ æ˜¯ pretraining distribution&lt;/li&gt;
&lt;li&gt;å¦‚æœ $\gamma$ ç‚º 0ï¼Œåœ¨è©²å¯¦é©—ä¸­å«åš PPOï¼Œå¦å‰‡ï¼Œç¨±ç‚º PPO-ptx&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/instruct-gpt-result.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Bayesian Optimization</title>
        <link>https://roykesydon.github.io/Blog/p/bayesian-optimization/</link>
        <pubDate>Thu, 26 Jan 2023 01:36:53 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/bayesian-optimization/</guid>
        <description>&lt;h1 id=&#34;ä»‹ç´¹&#34;&gt;ä»‹ç´¹&lt;/h1&gt;
&lt;p&gt;ä¸€ç¨®ç”¨æ–¼è‡ªå‹•åŒ–æ‰¾è¶…åƒæ•¸çš„æ–¹æ³•ï¼Œç”¨åœ¨æ¡æ¨£æ˜‚è²´è€Œä¸”æ˜¯é»‘ç›’å­çš„æƒ…æ³&lt;/p&gt;
&lt;h1 id=&#34;æµç¨‹&#34;&gt;æµç¨‹&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;å–æ¨£ä¸€äº›è³‡æ–™é»&lt;/li&gt;
&lt;li&gt;ç”Ÿå‡ºä¸€å€‹ Surrogate Model(å¯æ¡ç”¨ Gaussian Process)&lt;/li&gt;
&lt;li&gt;åè¦†åšä»¥ä¸‹äº‹æƒ…
&lt;ul&gt;
&lt;li&gt;ç”¨ Acquisition Function æŒ‘é¸ä¸‹ä¸€å€‹è¦æ¡æ¨£çš„é»&lt;/li&gt;
&lt;li&gt;é‡æ–°è©•ä¼° Surrogate Model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;gaussian-process&#34;&gt;Gaussian Process&lt;/h2&gt;
&lt;p&gt;æœ€çµ‚çš„ prediction æ˜¯ä¸€å€‹ distribution è€Œä¸æ˜¯å–®ä¸€å€‹æ•¸å­—
ç”Ÿæˆæ–¹æ³•éœ€å€ŸåŠ© kernel functionï¼Œå¸¸ç”¨ RBF(Radial Basis Function)&lt;/p&gt;
&lt;p&gt;$K(x, x^{&amp;rsquo;}|\tau)=\sigma^2exp(-\frac{1}{2}(\frac{x-x^{&amp;rsquo;}}{l})^2)$&lt;/p&gt;
&lt;p&gt;$\sigma$ å’Œ $l$ æ˜¯å…©å€‹å¯ä»¥èª¿æ•´çš„è¶…åƒæ•¸&lt;/p&gt;
&lt;h2 id=&#34;acquisition-function&#34;&gt;Acquisition Function&lt;/h2&gt;
&lt;p&gt;å¯ç”¨è¶…åƒæ•¸ä¾†èª¿ç¯€ exploitation å’Œ exploitation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UCB(Upper confidence bound)&lt;/li&gt;
&lt;li&gt;PI(probability of improvement)&lt;/li&gt;
&lt;li&gt;EI(Expected improvement)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>GPT ä¸‰éƒ¨æ›²</title>
        <link>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</link>
        <pubDate>Thu, 19 Jan 2023 01:50:07 +0800</pubDate>
        
        <guid>https://roykesydon.github.io/Blog/p/gpt-%E4%B8%89%E9%83%A8%E6%9B%B2/</guid>
        <description>&lt;p&gt;GPT æœ¬è³ªä¸Šå°±æ˜¯ Transformer çš„ decoder&lt;/p&gt;
&lt;h1 id=&#34;gpt-1&#34;&gt;GPT-1&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ç”¨ semi-supervisedï¼Œå¾Œä¾†è¢«æ­¸ç‚º self-supervised&lt;/p&gt;
&lt;h2 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h2&gt;
&lt;p&gt;$L_1(U)=\sum_i logP(u_i|u_{i-k},&amp;hellip;,u_{i-1};\theta)$&lt;/p&gt;
&lt;p&gt;$U= \{ u_1,&amp;hellip;,u_n \}$&lt;/p&gt;
&lt;p&gt;$U$ æ˜¯ä¸€ç³»åˆ—æœªæ¨™è¨˜çš„æ–‡æœ¬ token&lt;/p&gt;
&lt;p&gt;$k$ æ˜¯çª—å£å¤§å°&lt;/p&gt;
&lt;h3 id=&#34;æ¨¡å‹å¤§è‡´æ¶æ§‹&#34;&gt;æ¨¡å‹å¤§è‡´æ¶æ§‹&lt;/h3&gt;
&lt;p&gt;$h_0=UW_e+W_p$&lt;/p&gt;
&lt;p&gt;$h_1=transformer \_ block(h_{i-1})\forall i \in[1,n]$&lt;/p&gt;
&lt;p&gt;$P(u)=softmax(h_nW^T_e)$&lt;/p&gt;
&lt;p&gt;$U=\{u_{-k},&amp;hellip;,u_{-1}\}$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h2&gt;
&lt;p&gt;$P(y|x^1,&amp;hellip;,x^m)=softmax(h^m_lW_y)$&lt;/p&gt;
&lt;p&gt;$L2(C)=\sum_{(x,y)}log P(y|x^1,&amp;hellip;,x^m)$&lt;/p&gt;
&lt;p&gt;$L_3(C)=L_2(C)+\lambda*L_1(C)$&lt;/p&gt;
&lt;p&gt;$C$ æ˜¯ labeled çš„è³‡æ–™é›†ï¼Œå¾®èª¿åŸºæœ¬ä¸Šå°±æ˜¯åœ¨å¾Œé¢åŠ ä¸Šç·šæ€§å±¤&lt;/p&gt;
&lt;p&gt;ä½œè€…æœ€å¤§åŒ– likelihood çš„æ™‚å€™æ˜¯ç”¨ $L_3$ è€Œéå–®ç´”çš„ $L_2$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;å¾®èª¿æ‡‰ç”¨ç¯„ä¾‹&#34;&gt;å¾®èª¿æ‡‰ç”¨ç¯„ä¾‹&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-1-tasks.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;è³‡æ–™é›†&#34;&gt;è³‡æ–™é›†&lt;/h2&gt;
&lt;p&gt;ç”¨ BooksCorpus è¨“ç·´å‡ºä¾†çš„&lt;/p&gt;
&lt;p&gt;æœ‰è¶…é 7000 æœ¬æœªå‡ºç‰ˆçš„æ›¸&lt;/p&gt;
&lt;h2 id=&#34;æ¨¡å‹çµæ§‹&#34;&gt;æ¨¡å‹çµæ§‹&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;12 å±¤ transformer çš„ decoder&lt;/li&gt;
&lt;li&gt;768 ç¶­ word embedding&lt;/li&gt;
&lt;li&gt;12 å€‹ attention heads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;å’Œ-bert-base-æ¯”è¼ƒ&#34;&gt;å’Œ BERT BASE æ¯”è¼ƒ&lt;/h2&gt;
&lt;p&gt;BERT è«–æ–‡æ¯”è¼ƒæ™šå‡ºï¼Œä½† BASE çš„æ¨¡å‹æ¶æ§‹å’Œ GPT æœ‰ç›¸ä¼¼ä¹‹è™•ï¼Œ&lt;/p&gt;
&lt;p&gt;BASE æ˜¯ 12 å±¤çš„ decoderï¼Œword embedding å’Œ attention head çš„ç¶­åº¦æˆ–æ•¸é‡å’Œ GPT-1 ç›¸åŒ&lt;/p&gt;
&lt;h1 id=&#34;gpt-2&#34;&gt;GPT-2&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://paperswithcode.com/paper/language-models-are-unsupervised-multitask&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Unsupervised Multitask Learner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GPT-2 é™¤äº†ç”¨æ›´å¤§çš„çš„æ¨¡å‹å’Œæ›´å¤§çš„è³‡æ–™é›†ï¼ŒæŠŠé‡é»æ”¾åœ¨ zero-shot ä¸Šï¼Œé›–ç„¶åœ¨ GPT-1 çš„è«–æ–‡å°±æœ‰æé zero-shot&lt;/p&gt;
&lt;h2 id=&#34;è³‡æ–™é›†-1&#34;&gt;è³‡æ–™é›†&lt;/h2&gt;
&lt;p&gt;é€™æ¬¡åšäº†ä¸€å€‹å«åš WebText çš„è³‡æ–™é›†ï¼Œæœ‰ç™¾è¬ç´šåˆ¥çš„ç¶²é &lt;/p&gt;
&lt;h3 id=&#34;common-crawl&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;å¤§å‹çˆ¬èŸ²å°ˆæ¡ˆï¼Œæœ‰å¤§é‡ç¶²é è³‡æ–™ï¼Œä½†å……æ–¥äº†åƒåœ¾è¨Šæ¯&lt;/p&gt;
&lt;h3 id=&#34;webtext&#34;&gt;WebText&lt;/h3&gt;
&lt;p&gt;WebText çš„è³‡æ–™ä¾†æºæ˜¯ reddit ä¸Šçš„å¤–éƒ¨é€£çµï¼Œåªè¦æœ‰è‡³å°‘ä¸‰å€‹ karmaï¼Œå°±æœƒè¢«æ¡ç´ï¼Œç”±æ­¤å–å¾—å“è³ªè¼ƒå¥½çš„ç¶²é è³‡æ–™ã€‚é€éé€™ç¨®æ–¹æ³•ï¼Œå–å¾—äº† 4500 è¬å€‹é€£çµã€‚ä¸¦ç”¨Dragnet (Peters &amp;amp; Lecocq, 2013) and Newspaper content extractors æŠŠæ–‡å­—è¨Šæ¯å¾ HTML ä¸­æŠ“å‡ºä¾†&lt;/p&gt;
&lt;h2 id=&#34;æ¶æ§‹&#34;&gt;æ¶æ§‹&lt;/h2&gt;
&lt;p&gt;å’ŒåŸæœ¬å·®ä¸å¤šï¼Œè®Šæˆæœ‰ 1.5B åƒæ•¸çš„ Transformer decoder&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;zero-shot&#34;&gt;zero-shot&lt;/h2&gt;
&lt;p&gt;ä¸éœ€è¦ä¸‹æ¸¸ä»»å‹™çš„æ¨™è¨˜è³‡æ–™&lt;/p&gt;
&lt;p&gt;æ”¹æŠŠä»»å‹™è¼¸å…¥é€²æ¨¡å‹&lt;/p&gt;
&lt;h3 id=&#34;ç›®å‰å•é¡Œ&#34;&gt;ç›®å‰å•é¡Œ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ç¾åœ¨çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸å¤ªå¥½&lt;/li&gt;
&lt;li&gt;Multitask learning
åœ¨ NLP ä¸Šä¸å¤ªå¸¸ç”¨ï¼ŒNLP ç¾åœ¨ä¸»æµé‚„æ˜¯åœ¨é è¨“ç·´æ¨¡å‹ä¸Šåšå¾®èª¿ä»¥æ‡‰å°ä¸‹æ¸¸ä»»å‹™
&lt;ul&gt;
&lt;li&gt;å°æ¯å€‹ä¸‹æ¸¸ä»»å‹™éƒ½å¾—é‡æ–°è¨“ç·´æ¨¡å‹&lt;/li&gt;
&lt;li&gt;å¾—è’é›† labeled è³‡æ–™&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;çµæœ&#34;&gt;çµæœ&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-2-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;gpt-3&#34;&gt;GPT-3&lt;/h1&gt;
&lt;p&gt;paper: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;æ‘˜è¦&#34;&gt;æ‘˜è¦&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;æœ‰ 175B çš„åƒæ•¸ï¼Œç”±æ–¼æ¨¡å‹æ¥µå¤§ï¼Œè¦åœ¨å­ä»»å‹™å¾®èª¿æœƒæˆæœ¬å¾ˆå¤§ï¼Œæ‰€ä»¥ä¸åšä»»ä½•æ¢¯åº¦æ›´æ–°&lt;/li&gt;
&lt;li&gt;åœ¨å¾ˆå¤š NLP ä»»å‹™æœ‰å‚‘å‡ºçš„æˆæœ&lt;/li&gt;
&lt;li&gt;å¯ä»¥ç”Ÿå‡ºäººé¡é›£ä»¥å€åˆ†çš„æ–°èæ–‡ç« &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ç›®å‰æœ‰çš„å•é¡Œ&#34;&gt;ç›®å‰æœ‰çš„å•é¡Œ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;è¦åœ¨å­ä»»å‹™å¾®èª¿ï¼Œéœ€è¦è³‡æ–™é›†&lt;/li&gt;
&lt;li&gt;å¾®èª¿å¾Œåœ¨æœ‰äº›å­ä»»å‹™ä¸Šè¡¨ç¾å¥½ä¸ä»£è¡¨ä½ é è¨“ç·´æ¨¡å‹ä¸€å®šæ³›åŒ–èƒ½åŠ›é«˜&lt;/li&gt;
&lt;li&gt;äººé¡ä¸éœ€è¦å¤§é‡ labeled è³‡æ–™å»å®Œæˆå°ä»»å‹™&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;è©•ä¼°æ–¹å¼&#34;&gt;è©•ä¼°æ–¹å¼&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;åˆ†ç‚ºä¸‰ç¨®ï¼Œfew / one / zero-shot learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;æ¶æ§‹-1&#34;&gt;æ¶æ§‹&lt;/h2&gt;
&lt;p&gt;åŸºæœ¬ä¸Š GPT-3 å’Œ GPT-2 æ¶æ§‹ä¸€æ¨£&lt;/p&gt;
&lt;h3 id=&#34;ç›¸åŒ&#34;&gt;ç›¸åŒ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;modified initialization&lt;/li&gt;
&lt;li&gt;pre-normalization&lt;/li&gt;
&lt;li&gt;reversible tokenization described therein&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ä¸åŒ&#34;&gt;ä¸åŒ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;æŠŠ Sparse Transformer çš„ä¸€äº›ä¿®æ”¹æ‹¿éä¾†ç”¨&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-models.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;GPT-3 Small æ˜¯ GPT-1 çš„å¤§å°
GPT-3 Medium æ˜¯ BERT Large çš„å¤§å°
GPT-3 XL å’Œ GPT-2 ç›¸è¿‘ï¼Œæ¯”è¼ƒæ·ºä¹Ÿæ¯”è¼ƒå¯¬&lt;/p&gt;
&lt;h4 id=&#34;batch-size-å¤§å°&#34;&gt;Batch Size å¤§å°&lt;/h4&gt;
&lt;p&gt;æ¨¡å‹å°çš„æ™‚å€™éœ€è¦å°ä¸€é»ï¼Œé€éé€™ç¨®é¡å¤–çš„ noise ä¾†é¿å… overfitting(ä¸ç¢ºå®šæ˜¯ä¸æ˜¯çŒœæƒ³)&lt;/p&gt;
&lt;h2 id=&#34;è³‡æ–™é›†-2&#34;&gt;è³‡æ–™é›†&lt;/h2&gt;
&lt;h3 id=&#34;common-crawl-1&#34;&gt;Common Crawl&lt;/h3&gt;
&lt;p&gt;æ¶æ§‹æ¯” GPT-2 å¤§å¾ˆå¤šï¼Œæ‰€ä»¥å›é ­è€ƒæ…®é€™å€‹è³‡æ–™é›†&lt;/p&gt;
&lt;h4 id=&#34;ä¸‰æ­¥é©Ÿ&#34;&gt;ä¸‰æ­¥é©Ÿ&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;å…ˆéæ¿¾ï¼Œé€é reddit é‚£å€‹é«˜å“è³ªçš„è³‡æ–™é›†ï¼Œä¾†è¨“ç·´ä¸€å€‹æ¨¡å‹åˆ†é¡é«˜å“è³ªå’Œä½å“è³ªçš„ç¶²é ã€‚&lt;/li&gt;
&lt;li&gt;é€é LSH æ¼”ç®—æ³•æŠŠç›¸ä¼¼çš„æ–‡æœ¬éæ¿¾æ‰&lt;/li&gt;
&lt;li&gt;æŠŠä¸€äº›å·²çŸ¥é«˜å“è³ªçš„è³‡æ–™é›†ä¹ŸåŠ é€²ä¾†&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-dataset.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;é€™æ˜¯ä¸€å€‹ Batch è£¡æœ‰ 60% ä¾†è‡ª Common Crawl(filtered) çš„æ„æ€
Wikipedia é›–ç„¶ç¸½é‡æ¯”è¼ƒå°‘ï¼Œä½†ä¹Ÿæœ‰ 3% çš„æ¡æ¨£ç‡&lt;/p&gt;
&lt;h2 id=&#34;çµæœ-1&#34;&gt;çµæœ&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;è¨ˆç®—é‡æŒ‡æ•¸å¢é•·ï¼Œloss å»æ˜¯ç·šæ€§çš„å¾€ä¸‹é™&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roykesydon.github.io/Blog/Blog/images/gpt/gpt-3-result-2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;paper è£¡æœ‰å¾ˆå¤šä»»å‹™çš„å¯¦é©—çµæœï¼Œé€™é‚Šå°±ä¸é™„ä¸Šäº†&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;åœ¨æ–‡æœ¬ç”Ÿæˆä¸Šé‚„æ˜¯æ¯”è¼ƒå¼±ï¼Œç”Ÿå¾ˆé•·çš„æ±è¥¿ï¼Œå¯èƒ½æœƒé‡è¤‡è‡ªå·±èªªéçš„è©±ã€å¤±å»é€£è²«æ€§ã€è‡ªç›¸çŸ›ç›¾ç­‰ç­‰&lt;/p&gt;
&lt;p&gt;åœ¨æœ‰äº›é›™å‘æ€§çš„ä»»å‹™ä¸Šå¯èƒ½è¡¨ç¾æ›´å·®&lt;/p&gt;
&lt;h2 id=&#34;å½±éŸ¿&#34;&gt;å½±éŸ¿&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;å¯èƒ½è¢«ç”¨ä¾†æ•£å¸ƒä¸å¯¦æ¶ˆæ¯ã€åƒåœ¾éƒµä»¶ç­‰ç­‰&lt;/li&gt;
&lt;li&gt;åè¦‹&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;çµè«–&#34;&gt;çµè«–&lt;/h2&gt;
&lt;p&gt;åœ¨å¾ˆå¤š NLP ä»»å‹™å¯ä»¥åšåˆ°æ¥è¿‘ SOTA å¾®èª¿æ¨¡å‹çš„æˆæœ&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
