<!DOCTYPE html>
<html lang="en-us" dir="ltr">

<head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='paper: Learning Transferable Visual Models From Natural Language Supervision
Abstract 現有的 SOTA CV system 可以經過訓練預測一組固定的類別。 但這種監督式的方法也受限了通用性，因為需要額外的 labeled data 來擴展。
直接從 raw text 學習 image 是個有前途的替代方案。
本文證明了「預測哪個是圖片的 caption」這種形式的預訓練是一種高效且可擴展的方法，可以從 internet 上蒐集的 4 億對資料從頭學習到 SOTA image representation。
預訓練後，透過自然語言來引導，就可以在下游任務十線 zero-shot。
本文對 30 個不同的現有電腦視覺資料集進行比較，可以在多數任務和監督式學習的 baseline 競爭，而且無須任資料集來做特別的訓練。
例如在 ImageNet 上做 zero-shot 可以和 ResNet-50 取得相近的準確度。
Introduction and Motivating Work 直接從原始文本學習的預訓練方法在過去幾年徹底改變了 NLP。
Task-agnostic (與下游任務無關) objectives，比如 autoregressive 和 masked language modeling，讓模型得以隨著 compute, model capacity, 和 data 規模的增長，使能力也逐步提升。
在 &amp;ldquo;text-to-text&amp;rdquo; 這種輸入輸出形式的預訓練，使模型轉移到下游任務的時候，不用特地客製化 output head，或對資料集做特別地處理。'>
<title>CLIP 論文閱讀</title>

<link rel='canonical' href='https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/'>

<link rel="stylesheet" href="/Blog/scss/style.min.4caf1027c282d15ee13c4653037ea9841d5139b7a63d429484073ffbe5907eea.css"><meta property='og:title' content='CLIP 論文閱讀'>
<meta property='og:description' content='paper: Learning Transferable Visual Models From Natural Language Supervision
Abstract 現有的 SOTA CV system 可以經過訓練預測一組固定的類別。 但這種監督式的方法也受限了通用性，因為需要額外的 labeled data 來擴展。
直接從 raw text 學習 image 是個有前途的替代方案。
本文證明了「預測哪個是圖片的 caption」這種形式的預訓練是一種高效且可擴展的方法，可以從 internet 上蒐集的 4 億對資料從頭學習到 SOTA image representation。
預訓練後，透過自然語言來引導，就可以在下游任務十線 zero-shot。
本文對 30 個不同的現有電腦視覺資料集進行比較，可以在多數任務和監督式學習的 baseline 競爭，而且無須任資料集來做特別的訓練。
例如在 ImageNet 上做 zero-shot 可以和 ResNet-50 取得相近的準確度。
Introduction and Motivating Work 直接從原始文本學習的預訓練方法在過去幾年徹底改變了 NLP。
Task-agnostic (與下游任務無關) objectives，比如 autoregressive 和 masked language modeling，讓模型得以隨著 compute, model capacity, 和 data 規模的增長，使能力也逐步提升。
在 &amp;ldquo;text-to-text&amp;rdquo; 這種輸入輸出形式的預訓練，使模型轉移到下游任務的時候，不用特地客製化 output head，或對資料集做特別地處理。'>
<meta property='og:url' content='https://roykesydon.github.io/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/'>
<meta property='og:site_name' content='Roykesydon'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='deep-learning' /><meta property='article:tag' content='machine-learning' /><meta property='article:tag' content='zero-shot-learning' /><meta property='article:tag' content='multimodal' /><meta property='article:tag' content='computer-vision' /><meta property='article:published_time' content='2023-11-21T01:08:46&#43;08:00'/><meta property='article:modified_time' content='2023-11-21T01:08:46&#43;08:00'/>
<meta name="twitter:title" content="CLIP 論文閱讀">
<meta name="twitter:description" content="paper: Learning Transferable Visual Models From Natural Language Supervision
Abstract 現有的 SOTA CV system 可以經過訓練預測一組固定的類別。 但這種監督式的方法也受限了通用性，因為需要額外的 labeled data 來擴展。
直接從 raw text 學習 image 是個有前途的替代方案。
本文證明了「預測哪個是圖片的 caption」這種形式的預訓練是一種高效且可擴展的方法，可以從 internet 上蒐集的 4 億對資料從頭學習到 SOTA image representation。
預訓練後，透過自然語言來引導，就可以在下游任務十線 zero-shot。
本文對 30 個不同的現有電腦視覺資料集進行比較，可以在多數任務和監督式學習的 baseline 競爭，而且無須任資料集來做特別的訓練。
例如在 ImageNet 上做 zero-shot 可以和 ResNet-50 取得相近的準確度。
Introduction and Motivating Work 直接從原始文本學習的預訓練方法在過去幾年徹底改變了 NLP。
Task-agnostic (與下游任務無關) objectives，比如 autoregressive 和 masked language modeling，讓模型得以隨著 compute, model capacity, 和 data 規模的增長，使能力也逐步提升。
在 &amp;ldquo;text-to-text&amp;rdquo; 這種輸入輸出形式的預訓練，使模型轉移到下游任務的時候，不用特地客製化 output head，或對資料集做特別地處理。">
    <link rel="shortcut icon" href="/Blog/images/favicon/logo.ico" />

</head>

<body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            localStorage.setItem(colorSchemeKey, "dark");
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <div class="avatar-container-custom">
                    <div class="front-card">
                        <a href="/Blog">
                        
                            
                            
                            
                                
                                <img src="/Blog/avatar_hucf7329276f6da91af484672e99c97ec6_973602_300x0_resize_q75_box.jpg" width="300"
                                    height="300" class="site-logo" loading="lazy" alt="Avatar">
                            
                        
                        </a>
                    </div>
                    <div class="back-card">
                        <a href="/Blog">
                        
                            
                        
                            
                            <img src="/Blog/avatar2_huacea5d2323c926714fd8ae1ac932701a_458315_300x0_resize_box_3.png" width="300"
                                height="300" class="site-logo" loading="lazy" alt="Avatar">
                        
                        </a>
                    </div>
                </div>
                
                    
                    <span class="emoji">⭐<span class="emoji-image-container"><span class="emoji-image-bg"></span><img class="emoji-image" src="/Blog/images/avatar/StarRabbit.jpg"></img></span></span>
                    
                
            </figure>
            
        
        
        <h1 class="glitch" data-text="Roykesydon" style=""><a href="/Blog">Roykesydon</a></h1>
        <div class="cybr-text-light">
            <div class="site-meta cybr-text">
                
                <div class="terminal-effect" data-text="&gt; I write notes here during my free time.">
                    &gt; I write notes here during my free time.<span class="terminal-cursor"></span>
                </div>
                
                
                
            </div>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://github.com/Roykesydon'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com/Roykesydon'
                        target="_blank"
                        title="Twitter"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        
        <li >
            <a href='/Blog/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                
                <span 
                
                class="menu-custom"
                data-text='Home'
                style="--color: rgba(204, 245, 255, 0.932)">Home</span>
            </a>
        </li>
        
        
        
        <li >
            <a href='/Blog/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                
                <span 
                
                class="menu-custom"
                data-text='About'
                style="--color: rgba(204, 245, 255, 0.932)">About</span>
            </a>
        </li>
        
        
        
        <li >
            <a href='/Blog/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                
                <span 
                
                class="menu-custom"
                data-text='Archives'
                style="--color: rgba(204, 245, 255, 0.932)">Archives</span>
            </a>
        </li>
        
        
        
        <li >
            <a href='/Blog/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                
                <span 
                
                class="menu-custom"
                data-text='Search'
                style="--color: rgba(204, 245, 255, 0.932)">Search</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
        </div>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#introduction-and-motivating-work">Introduction and Motivating Work</a></li>
    <li><a href="#approach">Approach</a>
      <ol>
        <li><a href="#natural-language-supervision">Natural Language Supervision</a></li>
        <li><a href="#creating-a-sufficiently-large-dataset">Creating a Sufficiently Large Dataset</a></li>
        <li><a href="#selecting-an-efficient-pre-training-method">Selecting an Efficient Pre-Training Method</a></li>
        <li><a href="#choosing-and-scaling-a-model">Choosing and Scaling a Model</a></li>
      </ol>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ol>
        <li><a href="#prompt-engineering-and-ensembling">Prompt Engineering and Ensembling</a></li>
        <li><a href="#analysis-of-zero-shot-clip-performance">Analysis of Zero-Shot CLIP Performance</a></li>
        <li><a href="#representation-learning">Representation Learning</a></li>
      </ol>
    </li>
    <li><a href="#comparison-to-human-performance">Comparison to Human Performance</a></li>
    <li><a href="#data-overlap-analysis">Data Overlap Analysis</a></li>
    <li><a href="#limitations">Limitations</a></li>
    <li><a href="#額外應用">額外應用</a></li>
    <li><a href="#筆記">筆記</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


        
        
        
        <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/Blog/categories/deep-learning/" style="background-color: #936ce1; color: #fff;">
                Deep Learning
            </a>
        
    </header>
    

    
    
    
    
    
    
    <div class="article-title-wrapper article-float-image"  style="--bg-image: url('/Blog/images/categories/deep-learning.jpg');">
        <h2 class="article-title-custom">
            <a href="/Blog/p/clip-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/">CLIP 論文閱讀</a>
        </h2>
        
    </div>
    

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Nov 21, 2023</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    4 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p>paper: <a class="link" href="https://arxiv.org/abs/2103.00020"  target="_blank" rel="noopener"
    >Learning Transferable Visual Models From Natural Language Supervision</a></p>
<h2 id="abstract">Abstract</h2>
<p>現有的 SOTA CV system 可以經過訓練預測一組固定的類別。
但這種監督式的方法也受限了通用性，因為需要額外的 labeled data 來擴展。</p>
<p>直接從 raw text 學習 image 是個有前途的替代方案。</p>
<p>本文證明了「預測哪個是圖片的 caption」這種形式的預訓練是一種高效且可擴展的方法，可以從 internet 上蒐集的 4 億對資料從頭學習到 SOTA image representation。</p>
<p>預訓練後，透過自然語言來引導，就可以在下游任務十線 zero-shot。</p>
<p>本文對 30 個不同的現有電腦視覺資料集進行比較，可以在多數任務和監督式學習的 baseline 競爭，而且無須任資料集來做特別的訓練。</p>
<p>例如在 ImageNet 上做 zero-shot 可以和 ResNet-50 取得相近的準確度。</p>
<h2 id="introduction-and-motivating-work">Introduction and Motivating Work</h2>
<p>直接從原始文本學習的預訓練方法在過去幾年徹底改變了 NLP。</p>
<p>Task-agnostic (與下游任務無關) objectives，比如 autoregressive 和 masked language modeling，讓模型得以隨著 compute, model capacity, 和 data 規模的增長，使能力也逐步提升。</p>
<p>在 &ldquo;text-to-text&rdquo; 這種輸入輸出形式的預訓練，使模型轉移到下游任務的時候，不用特地客製化 output head，或對資料集做特別地處理。</p>
<p>這些結果表明，現代的預訓練方法在 web-scale 的文字集合的表現已經超過了用高品質的人為標記 NLP 資料集。</p>
<p>然而在 CV 等領域，在 ImageNet 這種人為標記的資料集上做預訓練卻依然是標準做法。</p>
<p>直接從網路文本學習的可擴展預訓練方法或許能在 CV 帶來類似的突破。</p>
<p>以往有一些工作嘗試利用幾乎無限量的原始文本而不是有限數量的 &ldquo;gold-labels&rdquo;，
但是這些方法都有一些妥協，比如都利用 softmax 來執行預測，使其沒辦法應付新類別，嚴重限制了 zero-shot 的能力。</p>
<p>作者提了幾個弱監督學習的例子，他們利用額外的資料結合預訓練，來幫忙改善監督式學習的結果。</p>
<p>也提了幾個和 CLIP 類似的工作 VirTex, ICMLM, ConVIRT，想利用 Transformer，從 Natural Language 中學習 image representation。</p>
<p>這些 weakly supervised model 和最近從 NLP 學習 image representation 的方法有一個重大差異，規模。</p>
<p>最近的一些研究，比如一些弱監督學習在數百萬到數十億張照片上訓練了多個 accelerator years。但是和 CLIP 相似的研究只在二十萬張圖片上訓練了幾天。</p>
<p>本文將規模拉高，以縮短規模上的差距。</p>
<p>作者在 internet 上蒐集了 4 億對圖片和文字的資料，做成新的資料集，並提出了 CLIP，ConVIRT 的簡化版本。</p>
<p>作者在 30 幾個資料集上測試，基本上能和監督式的模型競爭。</p>
<p>如果用 linear-probe，比公開可用的 SOTA ImageNet model 還更好。</p>
<h2 id="approach">Approach</h2>
<p><img src="/Blog/images/deep-learning/CLIP/fig1.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<p><img src="/Blog/images/deep-learning/CLIP/ex1.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="natural-language-supervision">Natural Language Supervision</h3>
<p>核心想法是利用 natural language 來學習 perception。</p>
<p>作者稱這不是一個新想法，但以往相似的方法的用語多樣，他介紹了四篇文章，但把從文字和圖片中學習 image representation 的方法個別稱為：無監督、自監督、弱監督、監督式。</p>
<p>擴展 natural language supervision 比起圖像分類簡單的多，不必定好類別，再去標註每張照片的類別。</p>
<p>而且 natural language supervision 還有個優勢，他不只能學習 image representation，還能將其和文字相關聯，使其更好做 zero-shot 的遷移。</p>
<h3 id="creating-a-sufficiently-large-dataset">Creating a Sufficiently Large Dataset</h3>
<p>現有工作主要用三個資料集:</p>
<ol>
<li>MS-COCO</li>
<li>Visual Genome</li>
<li>YFCC100M</li>
</ol>
<p>MS-COCO 和 Visual Genome 都是高品質的人為標記資料集，但是按照現代標準來看，它們很小，每個資料集大約有 100,000 張訓練照片。</p>
<p>相較之下，作者舉了一個最近的研究，用了 3.5 Billion 張 Instagram 照片作為訓練資料。</p>
<p>YFCC100M 是一個可能的替代方案，它有 100 million 張照片，但每張照片的 metadata 資料稀疏，而且良莠不齊。</p>
<p>比如許多檔名是自動產生的，可能是時間，或是相機的參數。</p>
<p>經過過濾，保留帶有自然語言的標題或描述的圖像，資料集縮小了 6 倍，只剩 15000 萬張照片，和 ImageNet 的大小相當。</p>
<p>natural language supervision 的一個主要動機是網路上公開著大量這種形式的 data。
由於現有資料集沒有反映這種可能性，因此只考慮這些資料集會低估這方面研究的潛力。</p>
<p>所以作者建立了一個新的包含 400 million pairs 的資料集，從網路上各種公開的來源蒐集的。</p>
<p>為了盡可能涵蓋所有的 visual concepts，作者在建構資料集的時候準備了 50 萬組特定的 query，每組 query 最多包含 20,000 個 pair，來進行 class balance。</p>
<p>產生的資料集的總字數和 GPT-2 用的 WebText 差不多。</p>
<p>將此資料集稱為 WIT，全名是 WebImageText。</p>
<h3 id="selecting-an-efficient-pre-training-method">Selecting an Efficient Pre-Training Method</h3>
<p><img src="/Blog/images/deep-learning/CLIP/fig2.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<p>最先進的 CV System 需要大量的計算。</p>
<p>作者舉了兩個計算量都非常恐怖的模型，而且他們只能預測 1000 個 ImageNet 的類別。
其中一個花了 19 個 GPU years，另一個花了 33 個 TPUv3 core-years。
乍看之下，從自然語言中學習一組開放的視覺概念似乎令人生畏。</p>
<p>但在作者努力的過程中，他們發現訓練效率是成功擴展自然語言監督的關鍵，也根據該指標選定最終的預訓練方法。</p>
<p>最初的方法和 VirTex 相似，從頭開始訓練一個 CNN，和 text transformer 來預測 caption。</p>
<p>Fig.2 展示的 Transformer 語言模型的計算量是 ResNet-50 Image encoder 的兩倍。
預測 caption 比預測 caption 但採用詞袋的方式還慢三倍。</p>
<p>這樣預測 caption 是一個困難的任務，同一張照片對應的 caption 可能出現的描述甚至有非常多種。
最近在 Contrastive representation learning 方面的研究發現 contrastive objectives 有不錯的表現。</p>
<p>因此作者探索一種方法是，只預測文本和哪一個圖片配對，而不是預測確切的單字。</p>
<p>因為資料集超級大，overfitting 的問題影響不大。</p>
<p>此外，作者發現對於 encoder 的 representation，要轉換到 multi-model embedding space，只需要使用 linear projection 即可，不需要 non-linear，兩者之間差別不大。</p>
<p>Data augmentation 只有使用 random crop，而沒有使用其他的。</p>
<h3 id="choosing-and-scaling-a-model">Choosing and Scaling a Model</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># image_encoder - ResNet or Vision Transformer</span>
</span></span><span class="line"><span class="cl"><span class="c1"># text_encoder - CBOW or Text Transformer</span>
</span></span><span class="line"><span class="cl"><span class="c1"># I[n, h, w, c] - minibatch of aligned images</span>
</span></span><span class="line"><span class="cl"><span class="c1"># T[n, l] - minibatch of aligned texts</span>
</span></span><span class="line"><span class="cl"><span class="c1"># W_i[d_i, d_e] - learned proj of image to embed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># W_t[d_t, d_e] - learned proj of text to embed</span>
</span></span><span class="line"><span class="cl"><span class="c1"># t - learned temperature parameter</span>
</span></span><span class="line"><span class="cl"><span class="c1"># extract feature representations of each modality</span>
</span></span><span class="line"><span class="cl"><span class="n">I_f</span> <span class="o">=</span> <span class="n">image_encoder</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="c1">#[n, d_i]</span>
</span></span><span class="line"><span class="cl"><span class="n">T_f</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">#[n, d_t]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># joint multimodal embedding [n, d_e]</span>
</span></span><span class="line"><span class="cl"><span class="n">I_e</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">I_f</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">T_e</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T_f</span><span class="p">,</span> <span class="n">W_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># scaled pairwise cosine similarities [n, n]</span>
</span></span><span class="line"><span class="cl"><span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">I_e</span><span class="p">,</span> <span class="n">T_e</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># symmetric loss function</span>
</span></span><span class="line"><span class="cl"><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_i</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_t</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i</span> <span class="o">+</span> <span class="n">loss_t</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="experiments">Experiments</h2>
<h3 id="prompt-engineering-and-ensembling">Prompt Engineering and Ensembling</h3>
<p>一種常見的問題是 polysemy，一個單字可能有多種意思，比如 &ldquo;boxer&rdquo; 可能是一種狗，或是拳擊手。
如果一張圖片對應一個單字就會面臨這問題。</p>
<p>另一種是 distribution gap，比如訓練用句子，但測試用單字。
為了緩解這問題，作者發現用 prompt template &ldquo;A photo of a {label}.&rdquo; 比直接用 label 好。</p>
<p>光用這個 prompt template 就提高 1.3 % 在 ImageNet 上的準確度。</p>
<p>如果可以給其他額外訊息會更有幫助，比如對於寵物的資料集，可以用 &ldquo;A photo of a {label}, a type of pet.&quot;。</p>
<p>對於 OCR 資料集，作者發現在要識別的文字或數字前後加上引號可以提高效能。</p>
<p>再來是 prompt ensembling，作者發現用多個 prompt template 來預測，然後綜合結果，可以提高效能。
作者用了 80 個 template。在 ImageNet 上比用單一的 prompt template 提高 3.5 % 的 performance。</p>
<p>綜合考慮 prompt engineering 和 prompt ensembling，作者在 ImageNet 上的準確度提高大概 5%。</p>
<ul>
<li>這裡列幾個作者用的 prompt template:
<ul>
<li>&ldquo;a bad photo of a {}.&rdquo;</li>
<li>&ldquo;a photo of many {}.&rdquo;</li>
<li>&ldquo;a sculpture of a {}.&rdquo;</li>
<li>&ldquo;a photo of the hard to see {}.&rdquo;</li>
</ul>
</li>
</ul>
<h3 id="analysis-of-zero-shot-clip-performance">Analysis of Zero-Shot CLIP Performance</h3>
<p><img src="/Blog/images/deep-learning/CLIP/fig5.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<p>對於一般的物體分類的資料集，CLIP 表現較好。</p>
<p>下面有些複雜、專門、抽象的任務，CLIP 則表現的很差，比如計算場景中有多少物體的 （CLEVRCounts）、衛星圖像分類（EuroSAT）或是 識別最近的汽車距離（KITTI Distance）</p>
<p>對於這種特別難的任務，讓 CLIP 做 zero-shot 不太合理。
可能用 few-shot 的方式會比較好。</p>
<p><img src="/Blog/images/deep-learning/CLIP/fig6.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<p>BiT 是 google 為 Transfer Learning 設計的預訓練模型，在分類問題，Few-shot learning 上有良好的表現。</p>
<h3 id="representation-learning">Representation Learning</h3>
<p>這節探討完全使用下游任務資料集而非 Zero-shot 或 few-shot 的情況。</p>
<p>作者選用 linear-probe 而不是 finetune 來做下游任務的評估。</p>
<p>因為他們的重點是開發與資料集無關的預訓練方法，finetune 有可能讓一個預訓練學習 representation 失敗的模型在微調過程中變好。
而 linear-probe 的限制可以凸顯這些失敗。</p>
<p><img src="/Blog/images/deep-learning/CLIP/fig10.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="comparison-to-human-performance">Comparison to Human Performance</h2>
<p>再來是 CLIP 和人類相比的結果。
挑選了五個人在寵物資料集上比較的結果。</p>
<p><img src="/Blog/images/deep-learning/CLIP/table2.jpg"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="data-overlap-analysis">Data Overlap Analysis</h2>
<p>可能會有人質疑，CLIP 的表現是因為訓練資料集和測試資料集有重疊。
但作者做了一些實驗，有些資料集完全沒有偵測到重疊。
對有重疊的做實驗，發現有重疊的對效果提升影響很小。</p>
<h2 id="limitations">Limitations</h2>
<p>CLIP 雖然可以和作為 Baseline 的 ResNet-50 打平手，但現在的 SOTA 遠高於該 Baseline。</p>
<p>作者發現再繼續加大模型和資料是可以繼續提升性能的，但作者估計要達到現有的 SOTA 需要增加大概 1000 倍的計算量才能達到，使用現有的硬體是不可行的。</p>
<p>CLIP 對細分類、抽象或更難的任務表現不好，作者相信還有許多任務是 CLIP 用 zero-shot 只能達到亂猜等級的。</p>
<p>Zero-Shot 的 CLIP 很難泛化到 out-of-distribution 的資料，比如在 MNIST 上只能達到 88% 的準確度。
作者發現預訓練資料幾乎沒有類似 MNIST 的圖片。</p>
<p>盡管 CLIP 可以靈活應用各種 Zero-Shot 的分類，但基本上還是從你給定的分類選擇。
和真正靈活的方法（生成 image caption）相比，是重大的限制。</p>
<p>一個值得嘗試的簡單想法是把 contrastive objective 和 generative objective，結合。</p>
<p>CLIP 也沒有解決深度學習資料效率低下的問題，CLIP 訓練了 32 個 epoch，如果把預訓練期間的照片以一秒一張來呈現，需要 405 年。
把 CLIP 和 self-supervision 或者和 self-training 做結合是有前途的方向。</p>
<p>雖然作者強調 Zero-Shot Learning，但是作者還是有反覆檢查下游任務測試集的表現，來調整 CLIP。
每次都用 ImageNet 來確認，並不算真正的 zero-shot 的情況。
如果能再創一個新的資料集，專門用來評估 zero-shot 遷移的能力會更恰當。</p>
<p>爬下來的資料有可能帶有社會偏見。</p>
<p>有一些複雜的任務很難用文字來傳達，雖然實際的訓練樣本有用，但 CLIP 並不會針對 few-shot 最佳化。有個違反直覺的結果，可以注意到在某些情況下，few-shot 不見得比 zero-shot 好。</p>
<h2 id="額外應用">額外應用</h2>
<ul>
<li>圖片生成
<ul>
<li>StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery
<ul>
<li>用文字引導生成圖片</li>
</ul>
</li>
<li>CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders</li>
</ul>
</li>
<li>物件偵測
<ul>
<li>Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation
<ul>
<li>將基礎類別再做細分類</li>
</ul>
</li>
</ul>
</li>
<li>OCR
<ul>
<li>Contrastive Language-Image Forensic Search
<ul>
<li>搜索影片中有沒有文本描述的物體</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="筆記">筆記</h2>
<p>prompt engineering
prompt ensemble</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/Blog/tags/deep-learning/">deep-learning</a>
        
            <a href="/Blog/tags/machine-learning/">machine-learning</a>
        
            <a href="/Blog/tags/zero-shot-learning/">zero-shot-learning</a>
        
            <a href="/Blog/tags/multimodal/">multimodal</a>
        
            <a href="/Blog/tags/computer-vision/">computer-vision</a>
        
    </section>


    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css"integrity="sha256-J&#43;iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s="crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js"integrity="sha256-InsNdER1b2xUewP&#43;pKCUJpkhiqwHgqiPXDlIk7GzBu4="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js"integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI="crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/Blog/p/diffusion-%E5%85%A5%E9%96%80/">
        
        

        <div class="article-details">
            <h2 class="article-title">Diffusion 入門</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/Blog/p/detr-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/">
        
        

        <div class="article-details">
            <h2 class="article-title">DETR 論文閱讀</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/Blog/p/i3d-%E8%AB%96%E6%96%87/">
        
        

        <div class="article-details">
            <h2 class="article-title">I3D 論文</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/Blog/p/swin-transformer-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/">
        
        

        <div class="article-details">
            <h2 class="article-title">Swin Transformer 論文閱讀</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/Blog/p/git-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80/">
        
        

        <div class="article-details">
            <h2 class="article-title">GIT 論文閱讀</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2023 Roykesydon
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.16.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
        
    </section>

    <section class="powerby">
        <a href="https://www.vecteezy.com/free-vector/mountain">Mountain Vectors by Vecteezy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >


            
        </main>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/Blog/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </div>

</body>

</html>